Q_ID,session_date,answer,question_category,acceptance_ratio
38411816,2016-07-16 13:41:43.090000+00:00,"You forgot to return self from the __iadd__ method:
def __iadd__(self, other):
    self.a += other
    return self

From the object.__iadd__() documentation:

These methods should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self). 

Without an explicit return statement, a function returns None as a default, so b += 7 produced None which is the result of the assignment.",A,7
30138659,2015-05-09 10:12:12.930000+00:00,"To find out the correct date, you need to know the correct timezone. There could be several timezones in the same country. There are databases that allows you to get the timezone from the name of the city e.g., see Get Timezone from City in Python/Django:
#!/usr/bin/env python
from geopy import geocoders # $ pip  install geopy

g = geocoders.GoogleV3()
place, (lat, lng) = g.geocode('Tokyo')
timezone = g.timezone((lat, lng)) # return pytz timezone object
print(timezone.zone)

Once you know the timezone, you could use a solution from Change Timezone for Date object Python mentioned by @ILostMySpoon:
#!/usr/bin/env python
import calendar
from datetime import datetime
import pytz # $ pip install pytz

now = datetime.now(pytz.timezone('Asia/Tokyo')) # you could pass `timezone` object here
weekday = now.weekday() 
print(calendar.day_name[weekday])
",A,3
35831668,2016-03-06 19:47:06.907000+00:00,You can add .distinct() so that the queryset only returns distinct results.,A,1
14389502,2013-01-17 22:43:55.687000+00:00,"You probably have a local file named wave.py that is being imported instead.
Check with:
import wave

print wave.__file__

and rename or delete that file.",A,5
29571153,2015-04-10 21:44:09.153000+00:00,"There is simply no such element in the page:
>>> import requests
>>> from bs4 import BeautifulSoup
>>> r = requests.get('http://www.google.com/finance?q=NYSE%3Aibm&ei=Hz4oVZq-PISjiQKYu4GoAQ')
>>> soup = BeautifulSoup(r.content)
>>> {c for e in soup.find_all('td') if 'class' in e.attrs for c in e['class']}
set(['name', 'val', 'p', 'i', 'period', 'itxt', 'lft', 't', 'key', 'colHeader', 'linkbtn'])

That's the set of all classes uses on <td> elements in the HTML served. Take into account you cannot rely on the element tree you find in your browser development tools, as they reflect the page after JavaScript code has run.",A,0
29890428,2015-04-27 08:11:07.177000+00:00,"There is no such thing as a OneToManyField. 
It doesn't matter from a practical point of view which side the ManyToManyField lives on. Personally, I'd put it on Ride, both to avoid changing the User model and because conceptually I'd say that rides are the main objects here.
And yes, adding the field will automatically create the linking table.",A,0
26290097,2014-10-10 00:29:44.987000+00:00,"Counter can do most of the hard work for you:
>>> letters = 'imtryingtocreateacaesarcypherdecoder'
>>> c = collections.Counter(letters)

This gives you a mapping, where the letter 'e' is mapped to 6, 'r' to 5, 'c' and 'a' to 4, and so on. To find the most common letter, you just call the method most_common:
>>> c.most_common(1)
[('e', 6)]
>>> c.most_common(1)[0][0]
'e'

So, the only problem you have left is how to take your coded message, throw away all of the non-letters, and turn all of the letters into lowercase. If you look at the methods of the str type, you should be able to figure this out pretty easily.
(You might also want to use filter, or a comprehension; you may find that easier than something like str.translate. You don't actually need a string to pass to Counter, just any iterable of letters; a list or a generator expression is fine.)",A,0
860876,2009-05-13 23:13:53.520000+00:00,"What we do.
Development

c:\someroot\project\thing__init__.py # makes thing a package
c:\someroot\project\thing\foo.py
c:\someroot\project\thing\bar.py

Our ""environment"" (set in a variety of ways
SET PYTHONPATH=""C:\someroot\project""

Some file we're working on
 import thing.foo
 import thing.bar

Deployment

/opt/someroot/project/project-1.1/thing/init.py # makes thing a package
/opt/someroot/project/project-1.1/thing/foo.py
/opt/someroot/project/project-1.1/thing/bar.py

Our ""environment"" (set in a variety of ways
SET PYTHONPATH=""/opt/someroot/project/project-1.1""

This allows multiple versions to be deployed side-by-side.
Each of the various ""things"" are designed to be separate, reusable packages.",A,6
13941774,2012-12-18 21:26:42.310000+00:00,"You need to set the content type header:
data = {""data"" : ""24.3""}
data_json = json.dumps(data)
headers = {'Content-type': 'application/json'}

response = requests.post(url, data=data_json, headers=headers)

If I set url to http://httpbin.org/post, that server echos back to me what was posted:
>>> import json
>>> import requests
>>> import pprint
>>> url = 'http://httpbin.org/post'
>>> data = {""data"" : ""24.3""}
>>> data_json = json.dumps(data)
>>> headers = {'Content-type': 'application/json'}
>>> response = requests.post(url, data=data_json, headers=headers)
>>> pprint.pprint(response.json())
{u'args': {},
 u'data': u'{""data"": ""24.3""}',
 u'files': {},
 u'form': {},
 u'headers': {u'Accept': u'*/*',
              u'Accept-Encoding': u'gzip, deflate, compress',
              u'Connection': u'keep-alive',
              u'Content-Length': u'16',
              u'Content-Type': u'application/json',
              u'Host': u'httpbin.org',
              u'User-Agent': u'python-requests/1.0.3 CPython/2.6.8 Darwin/11.4.2'},
 u'json': {u'data': u'24.3'},
 u'origin': u'109.247.40.35',
 u'url': u'http://httpbin.org/post'}
>>> pprint.pprint(response.json()['json'])
{u'data': u'24.3'}

If you are using requests version 2.4.2 or newer, you can leave the JSON encoding to the library; it'll automatically set the correct Content-Type header for you too. Pass in the data to be sent as JSON into the json keyword argument:
data = {""data"" : ""24.3""}
response = requests.post(url, json=data)
",A,20
39581385,2016-09-19 20:16:59.203000+00:00,"You have two options:

use the tokenize module to look out for token.OP tokens with the value @, followed by token.NAME tokens for label and, after a newline token, class. This is the most light-weight.
use the ast module to parse the source into a tree, then use the ast.walk() function, looking for ast.ClassDef objects. If the object has a ast.Name object with id == 'label' in the decorator_list attribute, you can record the name attribute.

The latter is probably easiest:
import ast

def labelled_classnames(source):
    module = ast.parse(source)
    for node in ast.walk(module):
        if not isinstance(node, ast.ClassDef):
            continue
        if any(isinstance(n, ast.Name) and n.id == 'label' 
               for n in node.decorator_list):
            yield node.name

Demo:
>>> demosource = '''
... class A():
...     pass
...
... @label
... class B(A):
...     pass
...
... @label
... class C(A):
...     pass
...
... class D(A):
...     pass
... '''
>>> list(labelled_classnames(demosource))
['B', 'C']
",A,4
40091938,2016-10-17 17:03:25.933000+00:00,"Python is not JavaScript. You need to refer to u[""groups""][0][""name""].",A,1
16741845,2013-05-24 18:57:01.580000+00:00,"Each row is just a list. You can modify it in-place, or create a new list with the value you want substituted out:
row[0] = y # or row = [y] + row[1:], or ...

If you want to write it back to a file, you need to use a csv.writer for that. For example:
os.rename('FakeAPData.csv', 'FakeAPData.csv.bak')

csv_in = open('FakeAPData.csv.bak', 'rb')
csv_out = open('FakeAPData.csv', 'wb')

writer = csv.writer(csv_out)

for row in csv.reader(csv_in):
    date = datetime.datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S')
    datet = unicode(datetime.datetime.strptime(row[0], '%Y-%m-%d %H:%M:%S'))
    datett = tuple(int(v) for v in re.findall(""[0-9]+"", datet))
    y = calendar.timegm(datett)
    row[0] = y
    writer.writerow(row)


Of course you'll also want to close your files, and clean up all the repeated and unused code. While we're at it, I'd factor out the date-transforming code into a function. And use functions that make it easy, instead of ones that make it difficult and fragile.
So:
def transform_date(date):
    return calendar.gmtime(datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timetuple())

def transform_row(row):
    return [transform_date(row[0])] + row[1:]

name = 'FakeAPData.csv'
bakname = name + '.bak'
os.rename(name, bakname)
with open(bakname, 'rb') as in csv_in, open(name, 'wb') as csv_out:
    writer = csv.writer(csv_out)
    writer.writerows(transform_row(row) for row in csv.reader(csv_in))
",A,2
23384947,2014-04-30 09:52:48.727000+00:00,"You can use Tag.find_next_sibling() here:
for header in soup.find_all('h6'):
    para = header.find_next_sibling('p')

The .find_next_sibling() call returns the first p tag that is a next sibling of the header tag.
Demo:
>>> for header in soup.find_all('h6'):
...     print header.text
...     para = header.find_next_sibling('p')
...     for strong_tag in para.find_all('strong'):
...         print strong_tag.text, strong_tag.next_sibling
...     print
... 
PHYSICAL DESCRIPTION
YOB:  1987
RACE:  WHITE
GENDER:  FEMALE
HEIGHT:  5'05''
WEIGHT:  118
EYE COLOR:  GREEN
HAIR COLOR:  BROWN

SCARS, MARKS, TATTOOS

This could find the wrong <p> tag in case you have no paragraph between the current header and the next:
<h6>Foo</h6>
<div>A div, not a p</div>

<h6>Bar</h6>
<p>This <i>is</i> a paragraph</p>

In this case, search for either a <p> or a <h6> tag:
for header in soup.find_all('h6'):
    next_sibling = header.find_next_sibling(['p', 'h6'])
    if next_sibling.name == 'h6':
        # no <p> tag between this header and the next, skip
        continue

The header.find_next_sibling(['p', 'h6']) call will either find the next paragraph, or the next header, whichever comes first. ",A,1
38042879,2016-06-26 20:28:07.720000+00:00,"This is validation. It should be done in the form; you can define a clean_email method on LeadForm to do it.
To answer your actual question though, you can access request.POST['email'] directly if you want. But don't do that; do it in the form.",A,0
4996342,2011-02-14 19:24:00.610000+00:00,"You missed a step - look at the example in the documentation.
Instead of importing the sitemaps module in your urls.py, import your BlogSitemap class, then create a sitemaps dictionary:
sitemaps = {'blog': BlogSitemap}
",A,8
12253439,2012-09-03 20:05:03.483000+00:00,"Use collections.Counter for each word and use these as sets:
>>> from collections import Counter
>>> str_a, str_b = 'aabbcc', 'aabd'
>>> Counter(str_a) & Counter(str_b)
Counter({'a': 2, 'b': 1})
>>> ''.join((Counter(str_a) & Counter(str_b)).elements())
'aab'

The Counter is a dict subclass, but one that counts all the elements of a sequence you initialize it with. Thus, ""aabbcc"" becomes Counter({'a': 2, 'b': 2, 'c': 2}).
Counters act like multisets, in that when you use 2 in an intersection like above, their counts are set to the mimimum values found in either counter, ignoring anything whose count drops to 0. If you were to compute their union, the maximum counts would be used instead.",A,9
5015549,2011-02-16 10:58:19.203000+00:00,"Class-level variables which get updated are a bad, bad thing.  Our default expectation is that object instances are stateful and classes are stateless.
In this case, we're trying to ""magically"" initialize a collection as a class variable, which is a toweringly bad idea.  A collection is simply an object with simple instance-level attributes.  
The magical Tile Table should not be a concealed, static part of the ImageLoader.  There is no possible reason for that.  It should be an argument to the ImageLoader if you want to avoid loading it more than once.  
Separating these promotes testability.  It's not arbitrary.  It's how unit testing gets done.
What you want is this.
class ImageLoader( object ):
    def __init__( self, theTileTable ):
        self.tile_table= theTileTable

class TileTable( object ):
    def __init__( self, path, some_other_arg ):
         self.tileTable= self._loadTileTable( path, some_other_arg )
    def _loadTileTable(arg1, arg2):
         blah blah

No static anything.  Independent units.  More easily testable.  No weird dependencies.  No magic. ",A,3
16928917,2013-06-04 23:05:10.437000+00:00,"You want to process the links as you find them, and find all forms with one regular expression. Using re.sub() and a replacement function would be the easiest:
def linkify(match):
    link = match.group(1)
    http_link = link if re.match('https?://', link) else 'http://' + link
    return '<a href=""{}"">{}</a>'.format(http_link, link)

links = re.compile(r'(https?://[^\s]+|w{3}\.[^\s]+|[^\s]+\.[^\s]{2,})')
string = links.sub(linkify, string)

By combining the 3 forms into one pattern you prevent matching the same link multiple times, and by using the replacement function you also avoid replacing multiple occurences where a shorter form of the same link is used (a linked http://firstlink.com still contains the firstlink.com part for future replacement calls).
Demo:
>>> string = 'firstlink.com and www.secondlink.com'
>>> links.sub(linkify, string)
'<a href=""http://firstlink.com"">firstlink.com</a> and <a href=""http://www.secondlink.com"">www.secondlink.com</a>'
>>> string = 'firstlink.com and http://firstlink.com'
>>> links.sub(linkify, string)
'<a href=""http://firstlink.com"">firstlink.com</a> and <a href=""http://firstlink.com"">http://firstlink.com</a>'
",A,0
14559458,2013-01-28 09:56:12.877000+00:00,"You can only use unicode escapes (\uabcd) in a unicode string literal. They have no meaning in a byte string. A Python 2 Unicode literal (u'some text') is a different type of Python object from a python byte string ('some text').
It's like using \t versus \T; the former has meaning in python literals (it's interpreted as a tab character), the latter just means a backslash and a capital T (two characters).
To help understand the difference between Unicode and byte strings, please do read the Python Unicode HOWTO; I can also recommend the Joel Spolsky on Unicode article.
Note: in Python 3, the same differences apply, but 'some text' is a Unicode string literal, and b'some text' is the bytestring syntax.",A,6
28638722,2015-02-20 21:42:14.333000+00:00,"You want list.append(), not list.extend():
thelist.append(instance)

list.extend() expects a sequence of elements to add, adding them all to the list. You have just one element, in which case you need to use list.append().
From More on Lists in the Python tutorial:

list.append(x)
Add an item to the end of the list; equivalent to a[len(a):] = [x].
list.extend(L)
Extend the list by appending all the items in the given list; equivalent to a[len(a):] = L.

Emphasis mine.
Demo:
>>> class MyClass:
...     var1=''
...     var2=''
... 
>>> thelist = []
>>> instance = MyClass()
>>> instance.var1 = ""hello""
>>> instance.var2 = ""world""
>>> thelist.append(instance)
>>> thelist
[<__main__.MyClass instance at 0x106d19d40>]
>>> thelist[0].var1
'hello'
",A,3
169318,2008-10-03 23:31:42.347000+00:00,"Agile is a big, vague concept.  Lots of things are Agile.
Scrum is a specific set of techniques for doing sprints and releases.  It's agile because it fits the Agile Manifesto.
There are lots of other specific Agile techniques (all of the xDD's, for example.)
When in doubt, compare the companies actual practices against the Agile Manifesto.  ",A,17
22141901,2014-03-03 08:28:02.850000+00:00,"No need to see the POST data. The issue is clear: you are not passing the CSRF token in the view that generates the form in the first place. If it's not in the generated form, it won't be in the POST data.
The usual pattern in Django is to use the same view to generate the form and to process the POST, but you seem to have split them up. You need to be passing the token in both views. Note the usual way to do this is to use a RequestContext (or the render shortcut), rather than adding it manually to the context.
Edit OK, so for the third time, the problem is in the step that renders the form on GET - where you are not passing the CSRF token. Again, as I said above, you should not be adding it manually at all, but using the render shortcut to ensure that context processors are run: render(request, 'group_detail.html', passed_dict).",A,0
52276940,2018-09-11 13:22:00.093000+00:00,"Python dictionaries do not keep a history, when you remove keys from a dictionary you can't later on 'reset' it. You'd have to just recreate the dictionary or create a copy first. 
To do either, you need to

Put the definition of the dictionary in a function, and each time you need to start, call the function to create the dictionary again.
or create a copy with dict.copy() every time you start a game, so you have a fresh dictionary to edit. Delete keys from the copy, not the original.

Personally, I'd not edit the dictionary. I'd create a list of all the room names (the keys of the dictionary) instead, then shuffle this list once and take the last value from it each time you need a random room, with random.shuffle() and list.pop().
This has two advantages:

No need to create the dictionary any more. You now create a new list instead. (Not that big an advantage). It's basically like the 'create a copy first' option above, but the copy is smaller.
You only have to shuffle once. A shuffled list gives you rooms in random order too, and makes sure you don't get duplicates either. Lists have order and stay shuffled, while dictionaries do not.

That'd look like this:
# new game
roomnames = list(roomdescriptions)  # copy the names of the rooms dictionary
random.shuffle(roomnames)

# picking the next room
if not roomnames:
    # reaching the boss level!
else:
    # take the next random room name, and get the room description for it
    nextroom = roomdescriptions[roomnames.pop()]
",A,0
19585961,2013-10-25 09:24:31.650000+00:00,"In a global function, you can refer directly to the function object by looking up the name.
This does not work in a method; you'd have to look the method up on the class instead:
LKTracker.track_points

This still won't do what you want, however, because you'd get a unbound method object at that moment:
>>> LKTracker.track_points
<unbound method LKTracker.track_points>

Method objects are created on demand (because functions are descriptors), and creating an attribute on a method object is futile; they generally only live for a short while.
You'd need to access the function instead:
>>> LKTracker.track_points.__func__
<function track_points at 0x103e7c500>

but you can do the same thing on self:
self.track_points.__func__

Now you can add a attribute:
track_points = self.track_points.__func__
if not hasattr(track_points, ""gotInitialFeatures""):
    track_points.gotInitialFeatures = None

if not track_points.gotInitialFeatures:
    #do some stuff
track_points.gotInitialFeatures = True

But it would be much easier to just store that attribute on the class instead:
if not hasattr(LKTracker, 'gotInitialFeatures'):
",A,5
3788558,2010-09-24 15:28:57.250000+00:00,"I don't understand why you've defined DemoDataForm in both models.py and forms.py, once as a ModelForm and once as a plain form. Because of that, it's impossible to tell from the code you've posted exactly which class you're instantiating.
I would say, drop the version in forms.py, move the one in models.py into forms.py, and use that. But first you'll need to fix a slight bug - instead of:
fields = ('gender')

you need
fields = ('gender',)

because a one-item tuple always needs a comma, otherwise Python will try to iterate through the string.",A,0
48994492,2018-02-26 18:04:28.873000+00:00,"You opened your file in binary mode:
with open(file_path,""rb"") as csvfile,temp_file:
    #                ^^ b is for binary

Don't do that. The error message tells you you should not do that, it specifically reminds you to open the file in text mode instead:
(did you open the file in text mode?)

Remove the b from the file mode.
The NamedTemporaryFile() class also defaults to binary mode with 'w+b', you need to tell it to open the file in text mode instead; use either 'w+' or 'w'.
Corrected code:
temp_file = NamedTemporaryFile(mode='w+', delete=False)
with open(file_path, ""r"") as csvfile, temp_file:

In Python 2, the csv module could only handle data in binary mode, partly because Python did not have strong Unicode support when the module was first created, and because in Python 2, text mode files on certain platforms translate line endings in a manner incompatible with the CSV standards.
In Python 3, it is strongly recommended you switch off line ending translation by adding newline='' when opening files:
temp_file = NamedTemporaryFile(mode='w+', newline='', delete=False)
with open(file_path, ""r"", newline='') as csvfile, temp_file:

I'd also strongly consider setting the encoding explicitly, rather than rely on the system defaults.
As a side note, there is no need to spell out all the column names when copying dictionaries across, nor do you have to even create a new dictionary for each row as the DictReader() and DictWriter fieldnames lists match. You can just pass the reader directly to the DictWriter.writerows() method:
file_path = ""Data.csv""
fieldnames = [""id"", ""Title"", ""Desc"", ""Comments""]
temp_file = NamedTemporaryFile(mode='w+', newline='', delete=False)
with open(file_path, ""r"", newline='') as csvfile, temp_file:
    reader = csv.DictReader(csvfile, fieldnames=fieldnames)
    writer = csv.DictWriter(temp_file, fieldnames=fieldnames)
    writer.writerows(reader)

at which point I do wonder if you didn't just want to use shutil.copyfileob() and just avoid parsing into dictionaries and then serialising back to CSV.",A,2
2406386,2010-03-09 03:10:51.273000+00:00,"""the garbage collector should free the memory""  
Correct.
Since it doesn't, something else is wrong.  Generally, the problem with infinite memory growth is global variables.
Remove all global variables.
Make all module-level code into smaller functions.",A,2
26545316,2014-10-24 09:35:39.337000+00:00,"Python uses exceptions to communicate incorrect inputs. If time.strptime() doesn't raise an exception, the return value is correct and you don't have to validate it yourself.
If an exception is raised, you can catch it with a try...except statement:
try:
    t = time.strptime(userinput, format)
except ValueError:
    # exception raised, not valid input, give feedback to user

Demo:
>>> import time
>>> time.strptime('01/02/2014 10:42:36', '%d/%m/%Y %H:%M:%S')
time.struct_time(tm_year=2014, tm_mon=2, tm_mday=1, tm_hour=10, tm_min=42, tm_sec=36, tm_wday=5, tm_yday=32, tm_isdst=-1)
>>> time.strptime('31/02/2014 10:42:36', '%d/%m/%Y %H:%M:%S')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mpietre/Development/Library/buildout.python/parts/opt/lib/python3.4/_strptime.py"", line 494, in _strptime_time
    tt = _strptime(data_string, format)[0]
  File ""/Users/mpietre/Development/Library/buildout.python/parts/opt/lib/python3.4/_strptime.py"", line 465, in _strptime
    datetime_date(year, 1, 1).toordinal() + 1
ValueError: day is out of range for month

Note that the exception was thrown here because 31 falls outside the range valid days for February.",A,2
48366260,2018-01-21 11:26:55.497000+00:00,"You can't do this at that point. If you want to pass dynamic initial data based on the request, you need to do it by defining a get_initial method.
Note however the query you are doing is completely pointless. request.user is already an instance of the User model. There is no point getting the username from that object and then passing it into another query based on that username; you just get back the object you started with. Just use request.user.
class ViewCreate(CreateView):
    model = view_C    

    def get_initial(self):
        return {
             'author': self.request.user,
             'publish_date': datetime.date.today()
        }
",A,0
12646467,2012-09-28 19:49:56.400000+00:00,"This is very strange code. When your form is submitted, if it is valid, you put the name value in a variable which is immediately thrown away. You don't save the form, but again throw away the values. Whether or not the form is valid, you redirect immediately to the add/ URL, where you assume that there is a name value in the session, despite never having saved anything to the session previously.
Your base problem is that there isn't a name key in the session, but I'm confused about why you think there would be, given the code you've shown. ",A,1
10978031,2012-06-11 10:04:17.487000+00:00,"Since you've opened and read the file via codecs.open(), it's been decoded to Unicode. So to output it you need to encode it again, presumably back to UTF-8.
for l in a:
    dasFile.write(l.encode('utf-8'))
",A,4
37074917,2016-05-06 14:26:07.950000+00:00,"It's not get_queryset you want to override here. Rather, you need to create a form, and define a custom queryset on the Vessel field:
class MissionForm(forms.ModelForm):
    vessel = forms.ModelChoiceField(queryset=Vessel.objects.all().order_by('name'))
    class Meta:
        model = Mission
        fields = ['title', 'vessel', 'status']

class MissionCreate(CreateView):
    form_class = MissionForm
    model = Mission
",A,2
16594856,2013-05-16 18:31:17.490000+00:00,"Buildout only takes newlines as separators when merging += and -= options. You'll have to insert those newlines.
Bash lets you insert newlines on the command line within quoted strings:
$ bin/buildout ""eggs:eggs+=package3
> package4
> ""

You simply press ENTER after package3 and you can insert newlines, until you enter a closing "" quote.",A,1
7393629,2011-09-12 20:30:59.307000+00:00,"You want MySQLdb.
But do yourself a favour and look into one of the excellent Python wrappers/ORMs: SQLAlchemy, SQLObject, or Django.",A,2
15618442,2013-03-25 15:17:51.880000+00:00,"Read in all rows (mapping all the values to floats), transform to columns using zip(*rows), only keep any that are have non-zero values using any(), transform back to rows using zip(*columns):
with open('data.txt', 'rb') as f:
    rows = list(map(float, row) for row in csv.reader(f, delimiter=','))

rows = zip(*[col for col in zip(*rows) if any(col)])

The latter step as a demonstration:
>>> rows = [[0, 0.1, 0.3, 0.4, 0], [0, 0.2, 0.5, 0.3, 0], [0, 0.7, 0.9, 0.2, 0]]
>>> zip(*[col for col in zip(*rows) if any(col)])
[(0.1, 0.3, 0.4), (0.2, 0.5, 0.3), (0.7, 0.9, 0.2)]
",A,1
18356503,2013-08-21 11:43:33.520000+00:00,"This:

the one I have shown will only check the first instance of ModelB it comes across, and base entirely off that

is not true. The query you have shown will do exactly what you want: exclude all ModelAs if any related model_b has some_bool=True.",A,2
37427933,2016-05-25 04:37:57.380000+00:00,"Just wrap the list in a new dictionary:
return json.dumps({'data': c.fetchall()})
",A,2
20279882,2013-11-29 06:46:44.157000+00:00,"A UsedRange:

… includes any cell that has ever been used. For example, if cell A1 contains a value, and then you delete the value, then cell A1 is considered used. In this case, the UsedRange property will return a range that includes cell A1.

Do you want to work on every cell that has ever been used? If not, why would you use UsedRange? If so, what are you trying to use it for? To find the last row in the UsedRange? You can do that easily. The Range Objects docs show you what you can do with them.
Then, once you know what you want to specify, the same documentation shows how to ask for it. You want B10:B18? Just ws.Range('B10:B18').
Once you have that Range object, you can assign a value or formula to the whole range, iterate over its cells, etc. Again, the same docs show how to do it.",A,2
1349343,2009-08-28 21:03:20.920000+00:00,"
Undo the threading stuff.
Run the profiler on the original unthreaded code.
Replace the AODB business with ordinary ODBC.
Run the new code through the profiler.
Post your results for further discussion.
",A,0
13911772,2012-12-17 09:51:31.900000+00:00,"Unfortunately, you cannot combine -C class filtering with -json output; even in the latest version JSON output is severely broken. Instead, filter the full JSON output yourself. Note that you should avoid using shell=True when using subprocess, pass in a list instead; no need to pipe stdin either, but do capture (silence) stderr.
Then we can recurse over the 'children' structures, picking out anything that has a matching 'class' key:
def get_nic_data():
    lshw_cmd = ['lshw', '-json']
    proc = subprocess.Popen(lshw_cmd, stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
    return proc.communicate()[0]

def find_class(data, class_):
    for entry in data.get('children', []):
        if entry.get('class') == class_:
            yield entry

        for child in find_class(entry, class_):
            yield child

def read_data(proc_output, class_='network'):
    import json

    for entry in find_class(json.loads(proc_output), class_):
        yield entry['vendor'], entry['description'], entry['product']

then loop over read_data(get_nic_data()):
for vendor, description, product in read_data(get_nic_data()):
    print vendor, description, product
",A,7
20582993,2013-12-14 11:59:06.920000+00:00,"Because Python assignment first evaluates the right-hand-side expression, then applies the result to the left-hand-side targets.
So, first, Python creates (<reference to list b>, <reference to list a>) as a tuple, then assigns the first item in that tuple to a, and the second item in that tuple to b. This swaps the references around neatly.
You could expand the assignment to read it like this:
tmp = b, a
a = tmp[0]
b = tmp[1]
",A,3
311986,2008-11-23 01:24:37.763000+00:00,"Download Python.  Learn the Python language.  Learn Object-Oriented programming.
Python includes SQLite, this makes it easier to Learn the SQL language.
Download the Django framework.  This makes it easy to write server-side applications that work simply and reliably.",A,5
34997535,2016-01-25 16:32:47.567000+00:00,"Use the csv module and insert each row into a dictionary:
import csv

with open('input.txt') as tsvfile:
    reader = csv.reader(tsvfile, delimiter='\t')
    datamap = {row[0]: row[1:] for row in reader}

This sidesteps the issue altogether.
You can put a str.split() result into a tuple to create a 'loop variable':
datamap = {row[0]: row[1:] for l in f for row in (l.strip().split(""\t""),)}

Here row is bound to the one str.split() result from the tuple, effectively creating a row = l.strip().split('\t') 'assignment'.",A,6
29490464,2015-04-07 11:26:10.723000+00:00,"You could compare the dates with the day component set to 1:
t.replace(day=1) < f.replace(day=1)

or compare both the year and the month:
if t.year < f.year or (t.year == f.year and t.month < f.month):

The latter is easier tested with a tuple comparison:
if (t.year, t.month) < (f.year, f.month):

Demo:
>>> from datetime import date
>>> t = date(2015, 1, 2)
>>> f = date(2015, 2, 2)
>>> t.replace(day=1) < f.replace(day=1)
True
>>> t.year < f.year or (t.year == f.year and t.month < f.month)
True
>>> (t.year, t.month) < (f.year, f.month)
True
>>> t = date(2014, 12, 30)
>>> f = date(2015, 1, 2)
>>> t.replace(day=1) < f.replace(day=1)
True
>>> t.year < f.year or (t.year == f.year and t.month < f.month)
True
>>> (t.year, t.month) < (f.year, f.month)
True
",A,4
842365,2009-05-09 00:20:14.653000+00:00,"Use decimal to find more precise square roots:
>>> import decimal
>>> decimal.getcontext().prec = 60
>>> decimal.Decimal(8885558).sqrt()
Decimal(""2980.86531061032678789963529280900544861029083861907705317042"")
",A,7
29341244,2015-03-30 08:08:42.253000+00:00,"As mentioned in previous sections of the tutorial, to start a shell in Django you should always do ./manage.py shell, rather than starting Python directly. That sets up the environment for you.",A,4
23327352,2014-04-27 18:36:01.080000+00:00,"You must create your template files in the correct location; in the templates subdirectory next to your python module.
The error indicates that there is no home.html file in the templates/ directory. Make sure you created that directory in the same directory as your python module, and that you did in fact put a home.html file in that subdirectory.  If your app is a package, the templates folder should be created inside the package.
myproject/
    app.py
    templates/
        home.html

myproject/
    mypackage/
        __init__.py
        templates/
            home.html


Alternatively, if you named your templates folder something other than templates and don't want to rename it to the default, you can tell Flask to use that other directory.
app = Flask(__name__, template_folder='template')  # still relative to module
",A,105
24257329,2014-06-17 06:47:46.657000+00:00,"By default, stdout is line buffered, meaning the buffer won't be flushed until you write a newline.
Flush the buffer explicitly each time you print a '.':
print('.', end='', flush=True)

The flush keyword was added in Python 3.3; for older versions, use sys.stdout.flush().
From the print() function documentation:

Whether output is buffered is usually determined by file, but if the flush keyword argument is true, the stream is forcibly flushed.

and from the sys.stdout documentation (the default value for the file argument of the print() function):

When interactive, standard streams are line-buffered. Otherwise, they are block-buffered like regular text files.
",A,9
21854931,2014-02-18 13:13:20.807000+00:00,"Assuming your JSON decodes to a dictionary, for data in x101 iterates through the keys of that dictionary. So data['d1'] will give the TypeError that you see, ""string indices must be integers"".
Since you have given absolutely no details about what the data structure actually looks like, we can only guess, but you perhaps want to iterate through the dict's values with for data in x101.values().
In any case, you should definitely remove that try/except that does nothing except print ""nope"". Errors are there for a reason, and silencing them will only prevent you from debugging properly, as we see here.
Edit
x101 is just a single dict. You say that there will frequently be more than one dict, but it can't possibly work like that: the only way to have multiple dicts is to have them inside a list (ie a JSON array). And if so, they would have to always be in a list, even when there is just one. So your structure should be:
[
  {
    ""angle"": ""10"", 
    ""asset_code"": ""XPS1020"", 
    ""credential"": ""wqw2323ds2"", 
    ""d1"": ""1"", 
    ""d2"": ""2"", 
    ""d3"": ""3"", 
    ""location"": ""Bangalore"", 
    ""operator"": ""pradeep"", 
    ""status"": ""1""
  }
]

and then your code will work as is, whether there is a single dict or many.",A,1
27995163,2015-01-17 01:17:50.507000+00:00,"ping will block in your code as soon as it fills its stdout OS pipe buffer (~65K on my system). You need to read the output:
#!/usr/bin/env python
import signal
from subprocess import Popen, PIPE
from threading import Timer

child = Popen(['ping', '8.8.8.8'], stdin=PIPE, stdout=PIPE, stderr=PIPE)
Timer(5, child.send_signal, [signal.SIGINT]).start() # Ctrl+C in 5 seconds
out, err = child.communicate() # get output
print(out.decode())
print('*'*60)
print(err.decode())
",A,2
20286185,2013-11-29 12:45:44.047000+00:00,"You set start and end just once, at the top. If you wanted to measure particular moments in time in your game, you need to call datetime.datetime.now() at that moment. Referring to start won't do that:
alphabet = ""abcdefghijklmnopqrstuvwxyz""  
print(""Welcome to the alphabet game!"")  
print(""Try to type the alphabet as fast as you can!"")  
start = None

enter = input(""when you are ready type 'go' : "")  
if enter == ""go"":  
    start = datetime.datetime.now()
    print(""GO"")  
    g = input("""")  
if g == alphabet:
    if not start:
        print(""We have yet to start!"")
    else: 
        print(""Well done!"")
        print(""Your time was: "", datetime.datetime.now() - start)  
",A,0
808577,2009-04-30 19:12:48.773000+00:00,"A cdata section might help.
http://reference.sitepoint.com/javascript/CDATASection
http://en.wikipedia.org/wiki/CDATA",A,0
49342121,2018-03-17 21:31:18.190000+00:00,"If the column doesn't need to be actually parsed like a sub-CSV (e.g., there's no quoting or escaping in it), you can just use split to split it:
col2vals = row[2].split('|')


If you're looking to replace each row[2] with the multiple resulting values, use slice assignment:
row[2:3] = row[2].split('|')

Or, if you're looking to break it into separate rows instead, just loop over the results and append/yield/newcsv.writerow/whatever you were doing with each original row.

If the column might have quoting or escaping, you can use csv again, by passing it a StringIO file-like object:
col2 = row[2]
fakefile = io.StringIO(col2)
subreader = csv.reader(fakefile, delimiter='|')
col2vals = next(subreader)

You can of course put the whole thing together into a single line, but I'm not sure the result is all that readable; probably somewhere between the two extremes is best.
row[2:3] = next(csv.reader(io.StringIO(row[2]), delimiter='|'))
",A,0
1961259,2009-12-25 14:43:30.960000+00:00,"To answer the specific questions.
""Do I use the multiprocessing package or the subprocess package to launch the new process?""
Use multiprocessing
""How do I have easy access to the simulation data from the GUI process?""
You don't have access to the simulation processes objects, if that's what you're asking  The simulation is a separate process.  You can start it, stop it, and -- most importantly -- make requests via a queue of commands that go to the simulator.
""The user should be able to browse through the timeline of the simulation easily and smoothly. How can this be done?""
This is just design.  Single process, multiple processes, multiple threads don't have any impact on this question at all.
Each simulation must have some parameters, it must start, it must produce a log (or timeline).  That has to be done no matter what library you use to start and stop the simulation.
The output from the simulation -- which is input to your GUI -- can be done a million ways.

Database.  The simulation timeline could be inserted into a SQLite database and queried by the GUI.  This doesn't work out terribly well because SQLite doesn't have really clever locking.  But it does work.
File.  The simulation timeline is written to a file.  The GUI reads the file.  This works out really, really well.
Request/Reply.  The simulation has multiple threads, one of which is dequeueing commands and responding by -- for example -- sending back the timeline up to the moment, or stopping the simulation or changing parameters and restarting it.
",A,2
11954588,2012-08-14 14:31:12.480000+00:00,"You can use a lambda, or anonymous function:
for excluded in excludeList:
    kolaDataList = filter(lambda l: excluded not in l, kolaDataList)

Alternatively, just use a list comprehension:
for excluded in excludeList:
    kolaDataList = [l for l in kolaDataList if excluded not in l]
",A,3
40244193,2016-10-25 15:39:59.637000+00:00,"You can use a dict comprehension to produce the same output:
with open(SYSCALL_HEADERS) as syscalls:
    SYSCALLS = {
        syscn: sysc 
        for line in syscalls if  ""_NR_"" in line
        for sysc, syscn in (re.split('_NR_| ', line.strip())[2:],)}

but I don't think that that is any more readable.",A,3
20516209,2013-12-11 10:08:54.950000+00:00,"You can just have the first five commands at module level, and call gnb.predict in your view.",A,0
2209609,2010-02-05 19:01:26.297000+00:00,"Start with the data first.  The server-side data is the persistent, essential core of the application.  If this data model isn't right, you have nothing.
You should be able to unit test the data model to prove that you have the right attributes and relationships.  This doesn't require much.  A few test cases to insert, update and query. 
You will support that data model with back-end processing.
This, too, should be unit tested to demonstrate that it works and does all the right things to your data model.  This will be a bit more complex, since this processing is the application.
Then you can think about the data model as exposed by web services to Ajax.
This, also, is testable to prove that the JSON does the right things.  This testing is often fairly complex because this is what the GUI front-end relies on.  This has to be right.
Then, once you have that Ajax data model worked out, you can write the front-end GUI.",A,11
1644274,2009-10-29 14:50:11.297000+00:00,"You should change your Score model to use a OneToOne field, not a ForeignKey - an FK implies there is more than one Score per Comment, which would never work.
However either way, the query can be done like this:
Comment.objects.order_by('score__value')
",A,4
4277389,2010-11-25 13:08:22.603000+00:00,"Welcome to recursion.  Here's a common solution
def thread_plus_replies( someThread ):
    return someThread, [ thread_plus_replies(r) for r in someThread.replies.ordered('votes').all() ]

This kind of thing returns a list of 2-tuples for each thread and all of it's replies.  
If a reply has no subsidiary threads, the follow-on list is empty.  This can get clunky, so some folks like to optimize it.
def thread_plus_replies( someThread ):
    if len(someThread.replies) == 0:
        return someThread
    return someThread, [ thread_plus_replies(r) for r in someThread.replies ]

Sticking with the first one, each thread is a 2-tuple.  We can decorate the 2-tuple with HTML.
def make_html( thread_results ):
    head, tail = thread_results
    return ""<ul><li>{0}</li><li>{1}</li></ul>"".format( head, make_html(tail) )

This will give you nested <ul> tags for your nested threads.",A,2
48771194,2018-02-13 16:20:04.713000+00:00,"You can use a ast.NodeTransformer() subclass to mutate an existing AST tree:
import ast

class RemoveAssignments(ast.NodeTransformer):
    def visit_Assign(self, node):
        return None

    def visit_AugAssign(self, node):
        return None

new_tree = RemoveAssignments().visit(old_tree)

The above class removes None to completely remove the node from the input tree. The Assign and AugAssign nodes contain the whole assignment statement, so the expression producing the result, and the target list (1 or more names to assign the result to).
This means that the above will turn
print('Start!')
foo = 'bar'
foo += 'eggs'
print('Done!')

into
print('Start!')


print('Done!')

If you need to make more fine-grained decisions, look at the child nodes of the assignment, either directly, or by passing the child nodes to self.visit() to have the transformer further call visit_* hooks for them if they exist:
class RemoveFunctionCallAssignments(NodeTransformer):
    """"""Remove assignments of the form ""target = name()"", so a single name being called

    The target list size plays no role.

    """"""
    def visit_Assign(self, node):
        if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):
            return None
        return node

Here, we only return None if the value side of the assignment (the expression on the right-hand side) is a Call node that is applied to a straight-forward Name node.  Returning the original node object passed in means that it'll not be replaced.
To replace top-level function calls (so those without an assignment or further expressions), look at Expr nodes; these are expression statements, not just expressions that are part of some other construct. If you have a Expr node with a Call, you can remove it:
def visit_Expr(self, node):
    # stand-alone call to a single name is to be removed
    if isinstance(node.value, ast.Call) and isinstance(node.value.func, ast.Name):
        return None
    return node

Also see the excellent Green Tree Snakes documentation, which covers working on the AST tree with further examples.",A,1
1549654,2009-10-11 02:09:22.267000+00:00,"As pccardune says, you get the relevant users like this:
friendships = Friendship.objects.filter(from_friend=some_user)

But in fact you can pass this directly into your next query:
second_select = Whatever.objects.filter(friend__in=friendships)
",A,2
14323382,2013-01-14 17:21:27.077000+00:00,"Each Element has an attribute .attrib that is a dictionary; simply use it's mapping methods to ask it for it's keys or values:
for name, value in root.attrib.items():
    print '{0}=""{1}""'.format(name, value)

or
for name in root.attrib:
    print '{0}=""{1}""'.format(name, root.attrib[name])

or use .values() or any of the other methods available on a python dict.
To get an individual attribute, use the standard subscription syntax:
print root.attrib['a']
",A,20
31788782,2015-08-03 13:34:50.307000+00:00,"You can't have more than one value per key, no.
You can create a new list containing your existing lists, and make that the value:
def func():
    for element in data:
        if element[1] in b:
            b[element[1]].append([element[0], element[2], element[3], element[4], element[5]]) 
        else:
            b[element[1]] = [[element[0], element[2], element[3], element[4], element[5]]]

Note that when the key is not present, we add a nested list.
This now produces:
{'Sleep': [['Bed', 'Sheets', 'Warm'], ['TV', 'Computer', 'Phone']]}

You could use the dict.setdefault() method rather than testing for containment here, and perhaps some slicing to build a new list:
def func():
    for element in data:
        b.setdefault(element[1], []).append([element[:1] + element[2:]) 

dict.setdefault() either returns the value for the given key, or if the key is missing, adds the second argument as the value for that key, then returns the value.",A,1
17008815,2013-06-09 10:45:15.507000+00:00,"You are setting the GameStatus initializer to None:
class GameStatus(object):
    __init__ = None

Don't do that. Python expects that to be a method. If you do not want to have a __init__ method, do not specify it at all. At most, make it an empty function:
class GameStatus(object):
    def __init__(self, *args, **kw):
        # Guaranteed to do nothing. Whatsoever. Whatever arguments you pass in.
        pass

If you wanted to create an enum-like object, take a look at How can I represent an 'Enum' in Python?
For Python 2.7, you could use:
def enum(*sequential, **named):
    enums = dict(zip(sequential, range(len(sequential))), **named)
    reverse = dict((value, key) for key, value in enums.iteritems())
    enums['reverse_mapping'] = reverse
    return type('Enum', (), enums)

GameStatus = enum('NotStarted', 'InProgress', 'Win', 'Lose')

print GameStatus.NotStarted          # 0
print GameStatus.reverse_mapping[0]  # NotStarted
",A,1
17901877,2013-07-27 19:52:54.333000+00:00,"You haven't defined a Ball class, you've defined a function called Ball.
The first line of the code itself should be:
class Ball(Widget):

(assuming Ball inherits from Widget like Bubble does).",A,2
18646783,2013-09-05 22:12:11.270000+00:00,"There are no errors in the output you're showing.
If you were trying to install xively-python, this means that you already had an up-to-date version of that package, and everything it depends on, so pip didn't need to do anything. Which is fine.
If you were trying to install something else, which depends on xively-python, it means that pip checked xively-python, saw that it was up-to-date, and proceeded on to install everything else. Which is also fine.
Either way, there's no problem.
In general, you can tell that pip failed if it ends with something like ""Storing complete log in /home/foo/.pip/pip.log"".
If you need to be absolutely sure that pip succeeded, and don't understand the results, you can always follow the pip command with this:
echo $?

0 means success; 2 means pip didn't understand your arguments; anything else (usually 1) means pip got an error.",A,1
14356796,2013-01-16 10:59:34.990000+00:00,"Use the .clear() method:
mydiv.clear()
",A,0
50903373,2018-06-18 05:59:27.447000+00:00,"If bowl_type is ever 5, or if it's 2 and the batter on strike or non strike answer isn't y, you don't append anything to batter_list. So if that ever happens, batter_list won't be as long as your other lists.
If, say, all 6 bowls are type 5, then batter_list will be empty, so of course batter_list[4], for example, is going to be an error.
It's not clear what you want to happen in this case, so it's not obvious how you should fix it.
One possibility is to do a batter_list.append(None) in those cases. Or maybe batter_list.append(""n/a""), or… whatever you want to show up in the print later.
Another possibility is to replace all of those separate lists with one list of dicts:
bowl_info_list = []
for j in range(0, bowls):
    # ...
    if bowl_type == 1:
        bowl_info_list.append({'bowl': 'wd', 'runs': 1, 'batter': bat_strike})
# ...

for bowl_info in bowl_info_list:
    print(f'bowler={name} batter on strike={bowl_info[""batter""]} bowl bowled={bowl_info[""bowl""]} runs scored={bowl_info[""runs""]}')

Or, maybe even better, use a class (maybe a namedtuple or @dataclass) instead of a dict.",A,0
30477584,2015-05-27 08:53:28.297000+00:00,"First, you're creating and truncating the file image.jpg:
f = open(""image.jpg"",""w"")

Then you're sending raspistill's stdout to that same file:
call([""raspistill"",""-o image.jpg""], stdout = f)

When you eventually get around to close-ing the file in Python, now image.jpg is just going to hold whatever raspistill wrote to stdout. Or, if you never close it, it'll be that minus the last buffer, which may be nothing at all.
Meanwhile, you're also trying to get raspistill to create a file with the same name, by passing it as part of the -o argument. You're doing that wrong, as Ionut Hulub's answer explains. Some programs will take ""-o image.jpg"" ""-oimage.jpg"", and ""-o"", ""image.jpg"" as meaning the same thing, some won't. But, even if this one does, at best you've now got two programs fighting over what file gets created and written as image.jpg.
If raspistill has an option to write the still to stdout, then you can use that option, together with passing stdout=f, and making sure to close the file. Or, if it has an option to write to a filename, then you can use that option. But doing both is not going to work.",A,2
18434553,2013-08-25 22:56:01.987000+00:00,"This line won't do what you think it does:
if height.find(""centimeters"" or ""cm"")

Apart from the missing : (presumably that's a typo), the code won't work for two reasons:

str.find() returns -1 if nothing is found, 0 if the searched-for string is found at the start. 0 is considered False in a boolean context and you should instead test for > -1.
You are not testing for either 'centimeters' or 'cm'. You are only testing for 'centimeters'. The or expression is evaluated first, and short-circuits to return the first True-ish value, the first non-empty string, so 'centimeters' in this case.

You should instead test for the presence of strings using in:
if 'centimeters' in height or 'cm' in height:

Demo:
>>> height = '184cm'
>>> height.find(""centimeters"" or ""cm"")
-1
>>> 'centimeters' in height or 'cm' in height
True
>>> height = '184 centimeters'
>>> height.find(""centimeters"" or ""cm"")
4
>>> 'centimeters' in height or 'cm' in height
True
>>> height = 'Only fools and horses'
>>> height.find(""centimeters"" or ""cm"")
-1
>>> 'centimeters' in height or 'cm' in height
False

Your next problem is that int() won't take kindly to extra text in your input text. You already determined that 'centimeter' is present, and that is what throws the exception.
You could use a regular expression, like the Ruby code:
import re

height = int(re.search('(\d+)', height).group(1)) / 2.54

Demo:
>>> import re
>>> height = '184cm'
>>> int(re.search('(\d+)', height).group(1)) / 2.54
72.44094488188976
>>> height = '184 centimeters'
>>> int(re.search('(\d+)', height).group(1)) / 2.54
72.44094488188976
",A,1
48795785,2018-02-14 20:38:39.767000+00:00,"Apache does not run as your user so editing your .bashrc is pointless.
Since you know how to display the value of os.environ['PATH'], you should be able to modify it too:
os.environ['PATH'] += "":/opt/mssql-tools/bin""

(I'm not quite sure what the point of this would be, though; are you calling out to external binaries within your Django code? And if so why don't you use the full path to the binary?)",A,0
27007359,2014-11-19 01:26:06.097000+00:00,"Yes, that's perfectly acceptable if you want to truncate, and if you don't care what happens with negative numbers or overflow.
If you want to round, call roundf first (or, if you want a different rounding rule than ""half rounds away from zero"", write your own code).
If you want to deal with negative numbers or overflow, you need to check before converting.
According to 5.2.9 in the standard, static_cast in this case is defined to give you the same value as unsigned int i(f). And I think most style guides would agree that the static_cast is preferred (as making casts explicit and noticeable is usually a good thing).
In more detail:
According to 4.9.1:

A prvalue of a floating point type can be converted to a prvalue of an integer type. The conversion trun- cates; that is, the fractional part is discarded. The behavior is undefined if the truncated value cannot be represented in the destination type.

I'm not sure exactly how the const-away works in C++14, but I believe it's not a qualification conversion, but part of the lvalue-to-rvalue conversion from 4.1.1:

A glvalue (3.10) of a non-function, non-array type T can be converted to a prvalue … If T is a non-class type, the type of the prvalue is the cv-unqualified version of T.

So, f has one lvalue-to-rvalue conversion from lvalue const float to rvalue float, then one floating-integral conversion from float to unsigned int, so by 4.0.1 it's a standard conversion, so by 5.2.9 it's valid as a static_cast.",A,4
19849758,2013-11-08 00:58:55.577000+00:00,"There's really nothing for pprint to do here. That module is about customizing the way collections are printed out—indenting sub-objects, controlling the order in which dictionary keys or set elements get displayed, etc. You're not trying to print a collection at all, just print some information about it.
The first thing you want to do is keep the collection around instead of rebuilding it for each print statement:
counter = collections.Counter(stringToData)

Next, you have to figure out how to get the data you want out of it. You already know how to find one pair of values:
letter, count = counter.most_common(1)[0]

The other thing you've asked about is the count of vowels and consonants. For that, you'll want to do something like this:
all_vowel = set('aeiouyAEIOUY')
all_consonants = set(string.ascii_letters) - all_vowels
vowels = sum(count for letter, count in counter.iteritems()
             if letter in all_vowels)
cons = sum(count for letter, count in counter.iteritems()
           if letter in all_consonants)

And now you just need to print them out by using some kind of formatting, which you already know how to do:
print ""In your string, there are: %s vowels and %s consonants."" % (vowels, cons)
print (""In your string, the most frequent character is %s, which occurred %s times.""
       % (letter, count))
",A,0
15483797,2013-03-18 18:06:03.983000+00:00,"I'm not sure if this is your problem, but it definitely is at least a related problem:
current = page1
def move(dirn):
    global current

These two current variables do not refer to the same thing. The first one is a local variable in the function wizintro. The second is a global variable.
The reason for this specific error, as opposed to a different one, is this line:
idx = pages.index(current) + dirn

You're referencing a variable named current. You've said that it's global, but you've never assigned any value to it in the global scope. So, it's undefined. So you get an exception.
If you just remove the global current line, then they refer to local variables in two different functions, which still probably isn't what you want. The same line will give effectively the same error—now it's a local variable that you're using without having assigned it any value in the local scope, but that's not any better.
It's pretty clear that you want move to refer to the current from the outer scope. 
If you're using Python 3.x, nonlocal current is probably what you're after. 
If not, there are a few options.
You can use the ""mutable default parameter value"" trick. Replace current with, say, a list of one element (current=[page1]), then pass current=current as an extra parameter to move. As long as nobody overrides the default, move will have a local variable named current that, despite not being the same variable as the one in the outer scope, is a reference to the same value, so current[0] is the same variable. (There are different tricks to bind a local variable into a closure that may feel more friendly to people coming from a Scheme/Haskell/etc. background, but the effect is the same.)
Or you can just make current a global in both scopes.
Or, go the opposite direction: turn wizintro into a class, move and friends into methods, and current into an instance variable. This really seems like what you're going for here.",A,1
49916017,2018-04-19 08:15:35.330000+00:00,"cleaned_data['email_confirm'] doesn't exist at the point clean_email() is called, because the confirm field field has not yet been cleaned.
Fields are cleaned in alphabetical order, so doing the comparison in clean_email_confirm() works. However this is an implementation detail and should not be relied on; as the docs state, any validation that relies on two or more fields should be done in the generic clean() method rather than any of the field-specific methods.",A,2
19527434,2013-10-22 20:13:58.810000+00:00,"The Unicode characters u'\xce0' and u'\xc9' do not have any corresponding ASCII values. So, if you don't want to lose data, you have to encode that data in some way that's valid as ASCII. Options include:
>>> print s.encode('ascii', errors='backslashreplace')
ABRA\xc3O JOS\xc9
>>> print s.encode('ascii', errors='xmlcharrefreplace')
ABRA&#195;O JOS&#201;
>>> print s.encode('unicode-escape')
ABRA\xc3O JOS\xc9
>>> print s.encode('punycode')
ABRAO JOS-jta5e

All of these are ASCII strings, and contain all of the information from your original Unicode string (so they can all be reversed without loss of data), but none of them are all that pretty for an end-user (and none of them can be reversed just by decode('ascii')).
See str.encode, Python Specific Encodings, and Unicode HOWTO for more info.

As a side note, when some people say ""ASCII"", they really don't mean ""ASCII"" but rather ""any 8-bit character set that's a superset of ASCII"" or ""some particular 8-bit character set that I have in mind"". If that's what you meant, the solution is to encode to the right 8-bit character set:
>>> s.encode('utf-8')
'ABRA\xc3\x83O JOS\xc3\x89'
>>> s.encode('cp1252')
'ABRA\xc3O JOS\xc9'
>>> s.encode('iso-8859-15')
'ABRA\xc3O JOS\xc9'

The hard part is knowing which character set you meant. If you're writing both the code that produces the 8-bit strings and the code that consumes it, and you don't know any better, you meant UTF-8. If the code that consumes the 8-bit strings is, say, the open function or a web browser that you're serving a page to or something else, things are more complicated, and there's no easy answer without a lot more information.",A,37
40417922,2016-11-04 08:11:20.390000+00:00,"When you call a function (and print() is no more than a function here), Python has to execute all the expressions that form the arguments first. The results of those expressions are then passed into the function call.
Moreover, referencing an object by name does not pass in a copy of that object, you pass in the object itself. Since a references a list, you are passing in that list, not another list that is a copy of the first.
So in the first example, the results of the expressions a, a.reverse() and a are sent to the print() function. Because a.reverse() alters a list in place, you just sent the very same list to the print() call twice, after that list has been reversed. Both a expressions result in the same list, that has been reversed, being passed to print().
In the second example, you first send in the a list, the print() function writes it out, and then in a second print() call, you reverse that list and pass it in to be printed again.
I've got the impression you are perhaps most confused about how Python treats names and how parameters are passed into a function call. I recommend you read up on these with:

Facts and myths about Python names and values by Ned Batchelder
How do I pass a variable by reference?

If you wanted to pass in a copy of the list that contains the same elements in reverse order, you would have to do so explicitly. You can use either a[::-1] or list(reversed(a)) to produce such a copy.",A,4
20273494,2013-11-28 19:23:24.300000+00:00,"You need to call .pop() on the inner list then:
fruit_list[0].pop()

Your outer fruit_list contains more list objects. If you want to pop an item from one of those lists, call .pop() directly on such a list.
A quick demo:
>>> fruit_list = [['tomato', 'pineapple', 'mango'], ['cherry', 'orange', 'strawberry']]
>>> fruit_list[0]
['tomato', 'pineapple', 'mango']
>>> fruit_list[0].pop()
'mango'
>>> fruit_list
[['tomato', 'pineapple'], ['cherry', 'orange', 'strawberry']]
",A,0
12403710,2012-09-13 09:56:10.453000+00:00,"The full traceback would have been useful.
I expect the problem comes from the related_name attribute in your people field. You've marked this for translation, but that makes no sense: this is an attribute you use in your code, not something for public consumption. Take out the _() call.",A,1
22305139,2014-03-10 16:00:21.733000+00:00,"You need to use just the dateutil.tz.tzoffset() type; pytz.timezone only takes names, not dateutil.tz objects.
The .localize() method is only needed for pytz-supplied timezones as they contain historic offsets as well, and they need to be applied to a datetime object using a little more care than just .replace() can do.
If the timestamp is a UNIX epoch value in UTC, then use fromtimestap with the timezone as a second argument:
>>> print datetime.fromtimestamp(utc_time/1000, tzinfooff)
2014-03-10 08:00:00-04:00

Or you could translate from UTC, using datetime.astimezone():
>>> from dateutil.tz impor tzutc
>>> dt_utc = datetime.utcfromtimestamp(utc_time/1000).replace(tzinfo=tzutc())
>>> print dt_utc.astimezone(tzinfooff)
2014-03-10 08:00:00-04:00
",A,3
25818307,2014-09-12 23:26:26.650000+00:00,"I think you can get much of what you want, distinguishing five kinds, without relying on anything that isn't documented by inspect:

Python instance methods
Python class methods
Python static methods
Builtin instance methods
Builtin class methods or static methods

But you can't distinguish those last two from each other with using CPython-specific implementation details.
(As far as I know, only 3.x has any builtin static methods in the stdlib… but of course even in 2.x, someone could always define one in an extension module.)

The details of what's available in inspect and even what it means are a little different in each version of Python, partly because things have changed between 2.x and 3.x, partly because inspect is basically a bunch of heuristics that have gradually improved over time. 
But at least for CPython 2.6 and 2.7 and 3.3-3.5, the simplest way to distinguish builtin instance methods from the other two types is isbuiltin on the method looked up from the class. For a static method or class method, this will be True; for an instance method, False. For example:
>>> inspect.isbuiltin(str.maketrans)
True
>>> inspect.isbuiltin(datetime.datetime.now)
True
>>> inspect.isbuiltin(datetime.datetime.ctime)
False

Why does this work? Well, isbuiltin will:

Return true if the object is a built-in function or a bound built-in method.

When looked up on an instance, either a regular method or a classmethod-like method is bound. But when looked up on the class, a regular method is unbound, while a classmethod-like method is bound (to the class). And of course a staticmethod-like method ends up as a plain-old function when looked up either way. So, it's a bit indirect, but it will always be correct.*

What about class methods vs. static methods?
In CPython 3.x, builtin static and class method descriptors both return the exact same type when looked up on their class, and none of the documented attributes can be used to distinguish them either. And even if this weren't true, I think the way the reference is written, it's guaranteed that no functions in inspect would be able to distinguish them.
What if we turn to the descriptors themselves? Yes, there are ways we can distinguish them… but I don't think it's something guaranteed by the language:
>>> callable(str.__dict__['maketrans'])
False
>>> callable(datetime.datetime.__dict__['now'])
True

Why does this work? Well, static methods just use a staticmethod descriptor, exactly like in Python (but wrapping a builtin function instead of a function). But class and instance methods use a special descriptor type, instead of using classmethod wrapping a (builtin) function and the (builtin) function itself, as Python class and instance methods do. These special descriptor types, classmethod_descriptor and method_descriptor, are unbound (class and instance) methods, as well as being the descriptors that bind them. There are historical/implementation reasons for this to be true, but I don't think there's anything in the language reference that requires it to be true, or even implies it.
And if you're willing to rely on implementation artifacts, isinstance(m, staticmethod) seems a lot simpler…
All that being said, are there any implementations besides CPython that have both builtin staticmethods and classmethods? If not, remember that practicality beats purity…

* What it's really testing for is whether the thing is callable without an extra argument, but that's basically the same thing as the documented ""function or bound method""; either way, it's what you want.",A,3
21002915,2014-01-08 17:49:35.837000+00:00,"A decorator doesn't produce code; a decorator is really only syntactic sugar:
@property
def something(self):
    return 42

is really interpreted as:
def something(self):
    return 42
something = property(something)

e.g. the expression following the @ sign is evaluated, and the result is called, passing in the function or class following the @ line. Whatever the decorator then returns replaces the original object.
For introspection purposes, the @ line is not retained; you'd have to parse the source code itself to discover any decorators present. A decorator is not obliged to return a new object; you can return the original object unaltered and you cannot, with introspection, know the difference.
Your best bet is to return to the source of the decorator then and just read the code. The property decorator is implemented in C, but the descriptor howto contains a Python implementation that does the same thing:
class Property(object):
    ""Emulate PyProperty_Type() in Objects/descrobject.c""

    def __init__(self, fget=None, fset=None, fdel=None, doc=None):
        self.fget = fget
        self.fset = fset
        self.fdel = fdel
        if doc is None and fget is not None:
            doc = fget.__doc__
        self.__doc__ = doc

    def __get__(self, obj, objtype=None):
        if obj is None:
            return self
        if self.fget is None:
            raise AttributeError(""unreadable attribute"")
        return self.fget(obj)

    def __set__(self, obj, value):
        if self.fset is None:
            raise AttributeError(""can't set attribute"")
        self.fset(obj, value)

    def __delete__(self, obj):
        if self.fdel is None:
            raise AttributeError(""can't delete attribute"")
        self.fdel(obj)

    def getter(self, fget):
        return type(self)(fget, self.fset, self.fdel, self.__doc__)

    def setter(self, fset):
        return type(self)(self.fget, fset, self.fdel, self.__doc__)

    def deleter(self, fdel):
        return type(self)(self.fget, self.fset, fdel, self.__doc__)
",A,5
26307141,2014-10-10 19:56:18.643000+00:00,"Assuming all of your other code is correct, there are five problems in your recursive code.
First, you're trying to use the value returned by the recursive call to count. But you have no return statements anywhere, so what could that value possibly be? So that inc*cnt is just going to try to multiply a string by None and raise a TypeError. If you look at your code, you have no need to return anything upward; count just takes some values, prints something, and calls itself again. So just ignore the None that it returns.
Next, you're trying to increment s[-1]. But s is a string, so that's just going to be the last character. You want to ""increment"" the whole string, right? So just pass s.
Next, you're trying to call yourself with s[:-1]. Again, s is a string; this is going to try to increment the first 7 digits of that string, then the first 6, and so on. Why would you want that? What you want to increment is the value you just incremented. That is, the same thing you just stored in inc. So just pass inc.
Next, you're doing the print after the recursive call. This means you're going to call the function that prints the second and later values, then print the first value. And so on. So they're going to show up in reverse order. If you want the first value first, print before the recursive call.
Finally, you clearly want the original value and the next 4 to get printed, not the next 5 without the original value. So you have to print out the pre-incremented value, not the post-incremented one.
So, the minimal change to your code is:
def count(s, n):
    if n == 0:
        return
    else:
        inc = increment(s)
        print(s)
        count(inc, n-1)
",A,1
15145006,2013-02-28 20:25:18.633000+00:00,"PIL is not correctly packaged; there are some forks around that fix that.
From The problem with installing PIL using virtualenv or buildout, your best bet currently is to install Pillow instead; it's a drop-in replacement for PIL.
For Windows, use a precompiled installer from Christoph Gohlke instead.",A,2
51012917,2018-06-24 18:43:48.550000+00:00,"You shouldn't be doing any of this. Most of your problems are caused by trying to output the form fields yourself, instead of letting Django do it. Remove all of that and either just use {{ form.as_p }}, or iterate through the form fields and output each one:
{{ form.user.label_tag }}
{{ form.user }}
{{ form.user.errors }}
",A,1
40640067,2016-11-16 18:53:15.527000+00:00,"Evaluate the queryset first by converting it to a list before before slicing it again.
posts = list(posts)
right_now = ...
",A,2
294550,2008-11-16 23:23:00.333000+00:00,"See Tooling to Build Test Cases.
It's not clear what your problem is.
If you question is ""how do I handle all the special cases in my generating classes?"" then here's some advice.  If your question is something else, then update your question.

Use a template generator.  Mako, for example, will make your life simpler.
Write an example of your result.  Replace parts with ${thing} placeholders.  Since you started with something that worked, turning it into a template is easy.
When generating code in another language, you need to have all of the class definitions in other other language designed for flexible assembly.  You want to generate as little fresh, new code as possible.  You want to tweak and customize a bit, but you don't want to generate a lot of stuff from scratch.
Special cases are best handled with ordinary polymorphism.  Separate subclasses of a common superclass can implement the various exceptions and special cases.  Really complex situations are handled well by the Strategy design pattern.  
In essence, you have Python classes that represent the real-world objects.  Those classes have attributes that can be fit into a C++ template to generate the C++ version of those objects. 
",A,4
12636809,2012-09-28 09:00:32.850000+00:00,"Use .split():
string = string.split(',', 1)[0]

We split the string on the comma once, to save python the work of splitting on more commas.
Alternatively, you can use .partition():
string = string.partition(',')[0]

Demo:
>>> 'Newyork, NY'.split(',', 1)[0]
'Newyork'
>>> 'Newyork, NY'.partition(',')[0]
'Newyork'

.partition() is the faster method:
>>> import timeit
>>> timeit.timeit(""'one, two'.split(',', 1)[0]"")
0.52929401397705078
>>> timeit.timeit(""'one, two'.partition(',')[0]"")
0.26499605178833008
",A,7
32584774,2015-09-15 11:24:03.917000+00:00,"You have a very peculiar structure here. imageArray() is a view, which returns a full HttpResponse; but you call it from within another view, add_category. What's more, you do nothing at all with the result; it is thrown away and never passed anywhere. So, naturally, it's always going to be blank in the template.
I'm not sure exactly what you're doing, so it's hard to know what you really want here. But I suspect that imageArray() should be a normal utility method, which simply returns a list of images:
def imgArray():
    filepath = os.path.join(STATIC_PATH, ""\\images"")
    images =[f for f in os.listdir(filepath) if f.endswith('.jpg')]
    return images

Then you need to actually do something with that value in your add_category function:
def add_category(request):
    ...
    else:
        imageArray = imgArray()
    return render(request, 'imgpage/add_category.html', {imageArray: imageArray})
",A,1
11853419,2012-08-07 20:15:22.150000+00:00,"If your input is a valid html that contains a json text in it:
>>> from bs4 import BeautifulSoup
>>> html = """"""<p>Any text, bla bla lorem ipsum, bla bla</p>
... <p>test = {""player"":{""id"":""123123"",""name"":""f_teste""}};""""""
>>> soup = BeautifulSoup(html)
>>> import  re
>>> jsonre = re.compile(r'test\s*=\s*(.*);', re.DOTALL)
>>> p = soup('p', text=jsonre)[0]
>>> json_text = jsonre.search(p.get_text()).group(1)
>>> import json
>>> json.loads(json_text)
{u'player': {u'id': u'123123', u'name': u'f_teste'}}

To install bs4, run: pip install beautifulsoup4.
A regex solution would look like:
>>> re.findall(r'""(id)"":""([^""]*)"",""(name)"":""([^""]*)""', html)
[('id', '123123', 'name', 'f_teste')]
",A,2
15054647,2013-02-24 17:52:23.837000+00:00,"Searching for text like that must match the whole text in a tag, and that's - Gnegneri Toure Yaya - in your case.
Make it a regular expression instead:
import re

findtoure = commentary.findAll(text = re.compile('Gnegneri Toure Yaya'))

This will match all elements NavigableString elements that contain that name.",A,3
16927232,2013-06-04 20:50:07.130000+00:00,"Your code is fine, you are doing nothing with the return value of execute(). Add a call to print() so you can see the result of your conversion, and perhaps wait for a keypress:
print(execute())
input('Press any key')

With the input() call the Windows console will remain open until you are done reading what the program has to print. It is not needed when running in IDLE.",A,2
19234574,2013-10-07 20:55:05.933000+00:00,"Unlike urllib2, which returns an object that you can use to get the data, curl needs you to pass it an object that it can use to store the data.
The simple way to do this, used in most of the examples, is to pass a file object as the WRITEDATA option. You might think you could just pass a StringIO here, like this:
# ...
s = StringIO.StringIO()
req.setopt(pycurl.WRITEDATA, s)
req.perform()
data = s.getvalue()

Unfortunately, that won't work, as the file object has to be a real file (or at least something with a C-level file descriptor), and a StringIO doesn't qualify.

You could of course use a NamedTemporaryFile, but if you'd prefer to keep the file in memory—or, better, not store it on memory or on disk, but just process it on the fly—that won't help.

The solution is to use the WRITEFUNCTION option instead:
s = StringIO.StringIO()
req.setopt(pycurl.WRITEFUNCTION, s.write)
req.perform()
data = s.getvalue()

As you can see, you can use a StringIO for this if you want—in fact, that's exactly what the curl object documentation from pycurl does—but it's not really simplifying things too much over any other way of accumulating strings (like putting them in a list and ''.join-ing them, or even just concatenating them onto a string).
Note that I linked to the C-level libcurl docs, not the pycurl docs, because pycurl's documentation basically just says ""FOO does the same thing as CURLOPT_FOO"" (even when there are differences, like the fact that your WRITEFUNCTION doesn't get the size, nmemb, and userdata parameters).

What if you want to stream the data on the fly? Just use a WRITEFUNCTION that accumulates and processes it on the fly. You won't be writing a loop yourself, but curl will be looping internally and driving the process. For example:
z = zlib.decompressobj()
s = []
def handle(chunk):
    s.append(z.decompress(chunk))
    return len(chunk)
req.setopt(pycurl.WRITEFUNCTION, handle)
req.perform()
s.append(z.flush())
data = ''.join(s)

curl will call your function once for each chunk of data it retrieves, so the entire loop happens inside that req.perform() call. (It may also call it again with 0 bytes at the end, so make sure you callback function can handle that. I think z.decompress can, but you might want to verify that.)
There are ways to limit the size of each write, to abort the download in the middle, to get the header as part of the write instead of separately, etc., but usually you won't need to touch those.",A,2
20623355,2013-12-16 23:52:27.257000+00:00,"You want to build the URL from the view instead, using url_for:
<td><a> href=""{{ url_for('on_plot', label=stats[0]) }}"">{{stats[0]}} </a></td>

The url_for() method will build a query string for you and escape the ampersand appropriately.",A,4
31158789,2015-07-01 10:28:10.077000+00:00,"You have quite a few problems with your view.
Firstly, you never call is_valid(), so you don't check that the form actually validates.
The main problem though is that you never call the save method: you only reference it. I'm not sure why you use commit=False in the form save in the first place, but having done so you need to do instance.save() to actually save it to the db. It would be easier just to do form.save() in the first place, though.",A,2
51732032,2018-08-07 17:05:52.890000+00:00,"The short answer is: the timer will still run.
The long answer is that you’re confusing variables and values here. Variables go out of scope, but that doesn’t do anything to the value. (And the same for del; it just deletes the variable, not the value.)
It’s only when the last reference to a value goes away that the value becomes garbage. (In CPython, if the value isn’t involved in a reference cycle, this means it gets destroyed immediately; in most other implementations, it gets destroyed a bit later—as soon as the GC notices it.)
The Timer class is a subclass of Thread. Threads work by calling a run method in a background thread. That method’s stack frame has references to whatever it needs, like the function object itself, and all local variables, just like every function does, so those values won’t become garbage until that method finishes. One of those locals is self, so the Timer object itself is still referenced and therefore not garbage.
(If you look at the source to Timer—which is linked from the docs—you can see that what its run method does is basically just sleep waiting on an Event for 1200 seconds, then call your foo function.)
If you want to see when the Timer object gets destroyed, you can subclass it and add a __del__ method that prints a message (or even have it print the whole stack at that point).",A,1
20557179,2013-12-13 00:44:49.447000+00:00,"It looks like gcc being killed due to insufficient memory (see @Blender's comment) exposed a bug in pip. It mixes bytestrings and Unicode while logging that leads to:
>>> '\n'.join(['bytestring with non-ascii character ☺', u'unicode'])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 36: \
   ordinal not in range(128)

If it is reproducible with the latest pip version; you could report the bug.",A,11
41759678,2017-01-20 09:02:03.743000+00:00,"There is an equivalent for registering error handlers directly, called app.register_error_handler():
class Webserver:
    def __init__(self):
        app = flask.Flask(__name__)
        app.add_url_rule('/', view_func=self.hello)
        app.register_error_handler(404, self.not_found_handler)
        app.run()

Even so, the errorhandler() just needs to be passed the bound method. If you wanted to register a method as the handler for the 404 Not Found HTTP code, for example, just create the decorator object for 404 and call that with the bound method as the argument:
class Webserver:
    def __init__(self):
        app = flask.Flask(__name__)
        app.add_url_rule('/', view_func=self.hello)
        app.errorhandler(404)(self.not_found_handler)
        app.run()

This works because all the @app.errorhandler() decorator does is register the callable, so the return value is the original callable still. You can ignore that here and only use it for the registration action.
The same would work for the app.route() decorator.",A,3
49472952,2018-03-25 05:52:43.850000+00:00,"To not print the date every time, you just need to move the print outside the loop by unindenting it, as Stephen Rauch explained.
If you want print just the date instead of the date and time, you probably actually want to use a date object instead of a datetime object:
date = datetime.date(2000,9,17)
for i in range(50): 
    date += datetime.timedelta(days=1)
    print(date) 


If there's some reason you actually want to use a datetime for your intermediate calculations (which may have implications for weird cases like daylight saving transitions, which maybe you've thought through and want?), you can instead convert to date only at the end, or print any format you want with strftime:
date = datetime.datetime(2000,9,17)
for i in range(50): 
    date += datetime.timedelta(days=1)
    print(date.strftime('%Y-%m-%d')

But if not, it's simpler to just use date.",A,1
15209629,2013-03-04 19:35:08.390000+00:00,"In the normal case, users install your app by running python setup.py install, or something that effectively does the same thing (like pip install foo).
Of course there are cases where they don't need setup.py—e.g., because they're installing a pre-packaged binary egg or Windows installer or whatever—but most packages have to work for the normal case. So, the default packaging commands include it. In the docs, Specifying the files to distribute says:

If you don’t supply an explicit list of files (or instructions on how to generate one), the sdist command puts a minimal default set into the source distribution:
… setup.py (or whatever you called your setup script) …
",A,2
21627460,2014-02-07 12:11:21.070000+00:00,"Flask already handles exceptions in views for you. Using sys.excepthook is not the right way to handle this, the hook will never be called.
Specify a custom exception handler with Flask instead, see the Redirects and Errors section of the Quickstart manual:
from flask import render_template

@app.errorhandler(500)
def page_not_found(error):
    return 'some error message'

Flask also uses the logging module to record exceptions; configure the logging module to write anything of severity logging.ERROR to the console or a log file.
Since you use app.run(debug=True, port=5001), you'll already see exceptions printed to the console.
Note that the 500 error page is only ever invoked when debug is not set to True however; otherwise the Werkzeug debug view is invoked instead.",A,4
15546765,2013-03-21 11:53:05.213000+00:00,"You need to call the .lower() function.
if len(a) != 0 and a.lower() != ""exit"":
    # ...
elif len(a) != 0 and a.lower() == ""exit"":

There is no real need to test for len(a)!=0, simply test for a itself:
if a and a.lower() != ""exit"":
    # ...
elif a and a.lower() == ""exit"":

Empty strings evaluate to False in a boolean context.",A,7
32798316,2015-09-26 14:36:22.490000+00:00,".next() method is called .__next__() in Python 3. You could use next() function to write single-source Python 2/3 compatible code:
from functools import partial

nex = partial(next, iter(iterable))
print(nex())
",A,1
40629373,2016-11-16 10:12:30.273000+00:00,"Your loop is the right approach, but you are rebinding c (not setting a new global name for the class), you can't use a variable to name the class in a class statement (the class will just be called c), and using super(self.__class__, self) will lead to infinite recursion issues.
You can use a factory function instead, and use the globals() dictionary to then add your newly created class to the global namespace. The function provides a closure so we can pass in the actual class object to super():
def _create_login_class(name, base):
    class LoginCls(base):
        @method_decorator(login_required)
        def dispatch(self, *args, **kwargs):
            return super(LoginCls, self).dispatch(*args, **kwargs)

        def get_context_data(self, **kwargs):
            context['phone'] = xxx
            return context

    LoginCls.__name__ = name
    LoginCls.__qualname__ = name   # Python 3
    return LoginCls

_login_class = [
    ('ProtectTemplate', TemplateView),
    ('ProtectList', ListView),
    ('ProtectDetail', DetailView),
]

for _name, _base  in _login_class:
    globals()[_name] = _create_login_class(_name, _base)

The super(LoginCls, self) expression finds LoginCls as a non-local name in the parent function namespace, so always uses the correct context for finding the next class in the MRO.
I used leading-underscore names to indicate that the factory function and the _login_class, _name and _base names are implementation details for the module.
Demo:
>>> class Base:
...     def dispatch(self, *args, **kwargs): print('Base.dispatch({}, {}) called'.format(args, kwargs))
...
>>> class TemplateView(Base): pass
...
>>> class ListView(Base): pass
...
>>> class DetailView(Base): pass
...
>>> method_decorator = lambda *args: lambda f: f
>>> xxx = 'xxx'
>>> login_required = 'login_required'
>>> def _create_login_class(name, base):
...     class LoginCls(base):
...         @method_decorator(login_required)
...         def dispatch(self, *args, **kwargs):
...             return super(LoginCls, self).dispatch(*args, **kwargs)
...         def get_context_data(self, **kwargs):
...             context['phone'] = xxx
...             return context
...     LoginCls.__name__ = name
...     LoginCls.__qualname__ = name   # Python 3
...     return LoginCls
...
>>> _login_class = [
...     ('ProtectTemplate', TemplateView),
...     ('ProtectList', ListView),
...     ('ProtectDetail', DetailView),
... ]
>>> for _name, _base  in _login_class:
...     globals()[_name] = _create_login_class(_name, _base)
...
>>> ProtectTemplate
<class '__main__.ProtectTemplate'>
>>> class SubclassDemo(ProtectTemplate):
...     def dispatch(self, *args, **kwargs):
...         print('SubclassDemo.dispatch({}, {}) called'.format(args, kwargs))
...         super(SubclassDemo, self).dispatch(*args, **kwargs)
...
>>> SubclassDemo().dispatch(42, spam='ham')
SubclassDemo.dispatch((42,), {'spam': 'ham'}) called
Base.dispatch((42,), {'spam': 'ham'}) called
",A,2
37200717,2016-05-13 03:22:54.427000+00:00,"The error ""'float' object has no attribute 'split'"" suggests that type(date) == float in your example that implies that you are trying to run Python 3 code using Python 2 interpreter where input() evaluates its input as a Python expression instead of returning it as a string.
To get the date as a string on Python 2, use raw_input() instead of input():
date_string = raw_input(""Enter date mm/dd/yyyy: "") 

To make it work on both Python 2 and 3, add at the top of your script:
try: # make input() and raw_input() to be synonyms
    input = raw_input
except NameError: # Python 3
    raw_input = input

If you need the old Python 2 input() behavior; you could call eval() explicitly.
To validate the input date, you could use datetime.strptime() and catch ValueError:
from datetime import datetime

try:
    d = datetime.strptime(date_string, '%m/%d/%Y')
except ValueError:
    print('wrong date string: {!r}'.format(date_string))

.strptime() guarantees that the input date is valid otherwise ValueError is raised. On success, d.year, d.month, d.day work as expected.
Putting it all together (not tested):
#!/usr/bin/env python
from datetime import datetime

try: # make input() and raw_input() to be synonyms
    input = raw_input
except NameError: # Python 3
    raw_input = input

while True: # until a valid date is given
    date_string = raw_input(""Enter date mm/dd/yyyy: "") 
    try:
        d = datetime.strptime(date_string, '%m/%d/%Y')
    except ValueError: # invalid date
        print('wrong date string: {!r}'.format(date_string))
    else: # valid date
        break 

# use the datetime object here
print(""Year: {date.year}, Month: {date.month}, Day: {date.day}"".format(date=d))

See Asking the user for input until they give a valid response.
You could use .split('/') instead of .strptime() if you must:
month, day, year = map(int, date_string.split('/'))

It doesn't validate whether the values form a valid date in the Gregorian calendar.",A,0
32920548,2015-10-03 08:36:56.137000+00:00,"I'd use a list comprehension rather than list(map(...)) here:
return [B(A.something, A.something_else).function() for A in a_list]

You could do the same with map() and a lambda but the above is more readable:
return list(map(lambda A: B(A.something, A.something_else).function(), a_list))

If you want to re-use converted_to_b just do so:
return [converted_to_b(A).function() for A in a_list]
",A,1
13302561,2012-11-09 05:28:36.883000+00:00,"First, you can correctly, portably, and efficiently solve the alignment problem using, e.g., std::aligned_storage::value>::type instead of char[sizeof(int)] (or, if you don't have C++11, there may be similar compiler-specific functionality).
Even if you're dealing with a complex POD, aligned_stored and alignment_of will give you a buffer that you can memcpy the POD into and out of, construct it into, etc.
In some more complex cases, you need to write more complex code, potentially using compile-time arithmetic and template-based static switches and so on, but so far as I know, nobody came up with a case during the C++11 deliberations that wasn't possible to handle with the new features.
However, just using reinterpret_cast on a random char-aligned buffer is not enough. Let's look at why:

the reinterpret cast indicates to the compiler that it can treat the memory at buffer as an integer

Yes, but you're also indicating that it can assume that the buffer is aligned properly for an integer. If you're lying about that, it's free to generate broken code.

and subsequently is free to issue integer compatible instructions which require/assume certain alignments for the data in question

Yes, it's free to issue instructions that either require those alignments, or that assume they're already taken care of.

with the only overhead being the extra reads and shifts when the CPU detects the address it is trying to execute alignment oriented instructions is actually not aligned. 

Yes, it may issue instructions with the extra reads and shifts. But it may also issue instructions that don't do them, because you've told it that it doesn't have to. So, it could issue a ""read aligned word"" instruction which raises an interrupt when used on non-aligned addresses.
Some processors don't have a ""read aligned word"" instruction, and just ""read word"" faster with alignment than without. Others can be configured to suppress the trap and instead fall back to a slower ""read word"". But others—like ARM—will just fail.

Assuming that the alignment of the location in buffer from which cast will occur is not conforming, then is it true that the only solution to this problem is to copy the bytes 1 by 1? Is there perhaps a more efficient technique?

You don't need to copy the bytes 1 by 1. You could, for example, memcpy each variable one by one into properly-aligned storage. (That would only be copying bytes 1 by 1 if all of your variables were 1-byte long, in which case you wouldn't be worried about alignment in the first place…)
As for casting a POD to char* and back using compiler-specific pragmas… well, any code that relies on compiler-specific pragmas for correctness (rather than for, say, efficiency) is obviously not correct, portable C++. Sometimes ""correct with g++ 3.4 or later on any 64-bit little-endian platform with IEEE 64-bit doubles"" is good enough for your use cases, but that's not the same thing as actually being valid C++. And you certainly can't expect it to work with, say, Sun cc on a 32-bit big-endian platform with 80-bit doubles and then complain that it doesn't.
For the example you added later:
// Experts seem to think doing the following is bad and
// could crash entirely when run on ARM processors:
buffer += weird_offset;

i = reinterpret_cast<int*>(buffer); 
buffer += sizeof(int);

Experts are right. Here's a simple example of the same thing:
int i[2];
char *c = reinterpret_cast<char *>(i) + 1;
int *j = reinterpret_cast<int *>(c);
int k = *j;

The variable i will be aligned at some address divisible by 4, say, 0x01000000. So, j will be at 0x01000001. So the line int k = *j will issue an instruction to read a 4-byte-aligned 4-byte value from 0x01000001. On, say, PPC64, that will just take about 8x as long as int k = *i, but on, say, ARM, it will crash.
So, if you have this:
int    i = 0;
short  s = 0;
float  f = 0.0f;
double d = 0.0;

And you want to write it to a stream, how do you do it?
writeToStream(&i);
writeToStream(&s);
writeToStream(&f);
writeToStream(&d);

How do you read back from a stream?
readFromStream(&i);
readFromStream(&s);
readFromStream(&f);
readFromStream(&d);

Presumably whatever kind of stream you're using (whether ifstream, FILE*, whatever) has a buffer in it, so readFromStream(&f) is going to check whether there are sizeof(float) bytes available, read the next buffer if not, then copy the first sizeof(float) bytes from the buffer to the address of f. (In fact, it may even be smarter—it's allowed to, e.g., check whether you're just near the end of the buffer, and if so issue an asynchronous read-ahead, if the library implementer thought that would be a good idea.) The standard doesn't say how it has to do the copy. Standard libraries don't have to run anywhere but on the implementation they're part of, so your platform's ifstream could use memcpy, or *(float*), or a compiler intrinsic, or inline assembly—and it will probably use whatever's fastest on your platform.
So, how exactly would unaligned access help you optimize this or simplify it?
In nearly every case, picking the right kind of stream, and using its read and write methods, is the most efficient way of reading and writing. And, if you've picked a stream out of the standard library, it's guaranteed to be correct, too. So, you've got the best of both worlds.
If there's something peculiar about your application that makes something different more efficient—or if you're the guy writing the standard library—then of course you should go ahead and do that. As long as you (and any potential users of your code) are aware of where you're violating the standard and why (and you actually are optimizing things, rather than just doing something because it ""seems like it should be faster""), this is perfectly reasonable.
You seem to think that it would help to be able to put them into some kind of ""packed struct"" and just write that, but the C++ standard does not have any such thing as a ""packed struct"". Some implementations have non-standard features that you can use for that. For example, both MSVC and gcc will let you pack the above into 18 bytes on i386, and you can take that packed struct and memcpy it, reinterpret_cast it to char * to send over the network, whatever. But it won't be compatible with the exact same code compiled by a different compiler that doesn't understand your compiler's special pragmas. It won't even be compatible with a related compiler, like gcc for ARM, which will pack the same thing into 20 bytes. When you use non-portable extensions to the standard, the result is not portable.",A,4
18226051,2013-08-14 07:52:26.833000+00:00,"You could create an Upstart job. Write /etc/init/project.conf file:
start on runlevel 5
exec /var/www/project/service/project.exe

You probably want to specify other options such as description, stop on, respawn, setuid.",A,1
39430876,2016-09-10 21:45:33.727000+00:00,"Your URL is called ""work_detail"", not ""apps.web.views.work_detail"". You should use that name in the get_absolute_url method.",A,1
26474608,2014-10-20 20:45:09.930000+00:00,"
but unfortunately, threads/processes only run simultaneously if there is no argument.

Nope, that's not the problem.
The problem is that you're calling standard_download(link), then passing the result of that call as the target argument. That's not what you want; you want to pass the function itself, and the arguments, to the Process object so that it can call them. That's exactly what the args parameter is for, as shown in most of the examples for multiprocessing. In your case:
Process(target=standard_download, args=(link,)).start()

Or, you can always build a zero-argument function with lambda or partial, of course:
Process(target=functools.partial(standard_download, link)).start()

As a side note, just creating, starting, and leaking a bunch of child processes is a bad idea. You probably want to store them all in a list, and then join them all (whether immediately, or later). For example:
def download_from_file(filename):
    if os.path.exists(filename):
        links = [line.strip() for line in open(filename)]

        for link in links:
            p = Process(target=standard_download, args=(link,))
            p.start()
            yield p

processes = list(download_from_file(filename))
# later, when we need to wait until we're done with all the downloads
for process in processes:
    process.join()
",A,1
51885339,2018-08-16 21:05:36.177000+00:00,"If x is an attribute of some object, rather than a global variable, this is very simple: add a __setattr__ method:
class MyType:
    def __setattr__(self, name, value):
        if name == 'x':
            self.x_callback(value)
        super().__setattr__(name, value)

Obviously there are ways you can make this more flexible:

Add a way to register conditions dynamically instead of just always calling x_callback.
Make it a mixin class that can be attached to any other class letting you register(name, callback) whatever you want.
Combine the above, so you can register(name, condition, callback).
self.x_callback(oldval=self.x, newval=value) so the callback can see old and new values.
if self.x_callback(value): so the callback can accept or reject the change. 
value = self.x_callback(value) so the callback can override the change.
",A,0
43792314,2017-05-04 20:33:47.483000+00:00,"assert_any_call asserts that the mock object has been called with the specified arguments. You're not passing any arguments in your assertion, but in the function itself zipfile.Zipfile() is called with one argument, a file object.",A,3
23822447,2014-05-23 06:32:00.030000+00:00,"Yes, you can use render(request, template, context) rather than render_to_response. The render shortcut uses a RequestContext automatically.",A,0
4334885,2010-12-02 12:18:33.713000+00:00,"Don't muck about with fink. That uses a completely separate install of Python and all its dependencies. It really won't help with your actual problem.
Secondly, you're trying to run things from inside site-packages - this is wrong. Go back to the directory where you downloaded PIL (or download it again, if you don't still have it) and run sudo python setup.py install.
And no, it won't reinstall Python - it uses python to run the setup of the directory you're in.",A,2
23564097,2014-05-09 11:59:34.843000+00:00,"Don't use unichr(); it produces a unicode string with one character. Don't mix Unicode strings and byte strings (binary data), as this'll trigger implicit encoding or decoding. Here an implicit decode is triggered and fails.
Your codepoints are limited to values 0-255, so a simple chr() will do:
stuff = re.sub(""#([0-9A-Fa-f]{2})"", lambda m: chr(int(m.group(0), 16)), stuff)
",A,0
32910332,2015-10-02 15:10:36.667000+00:00,"You are placing only the last replacement result in newstr. All your previous str.replace() results are discarded.
For your input text Hey look Words!, the last vowel encountered is o so only o is replaced. The e replacement did take place and was stored in newstr but that value was then discarded when you set newstr to the result of the o replacement. It thus depends on the input string what vowel exactly will remain replaced; for the sentence 'The cat sat on the mat' it'll be a as that is the last vowel you test and replace.
Just loop directly over vowels and replace each of those characters; it is save to call str.replace() where the first argument is not present. Store the result back in text so that any subsequent replacements stick:
def anti_vowel(text):
    vowels = ""AaEeOoIiUu""
    for vowel in vowels:
        text = text.replace(vowel, """")
    return text

Better still, use the str.translate() method to replace all vowels in one go:
# Python 2 version
def anti_vowel(text):
    vowels = ""AaEeOoIiUu""
    return text.translate(None, vowels)

# Python 3 version
def anti_vowel(text):
    vowels = str.maketrans(dict.fromkeys(""AaEeOoIiUu""))
    return text.translate(vowels)

str.translate() makes all replacements at once; the method changed between Python 2 str and Python 3 str, but in both versions all the vowels are ignored as the new string is built, without any further loops.",A,6
39581819,2016-09-19 20:45:17.740000+00:00,"Use the in_() column method to test a column against a sequence:
q.filter(UserTable.firstname.in_(user['firstnames'])

See the Common Filter Operations section of the Object Relational tutorial:

IN:
query.filter(User.name.in_(['ed', 'wendy', 'jack']))

# works with query objects too:
query.filter(User.name.in_(
        session.query(User.name).filter(User.name.like('%ed%'))
))

",A,1
38458080,2016-07-19 12:06:31.293000+00:00,"See the documentation about using commit=False when you have a many-to-many field.
Note that there is no reason for you to be using commit=False here though. Remove it, and the second save, and Django will save your values directly.",A,1
19942534,2013-11-12 23:59:50.203000+00:00,"I'm willing to bet this is the problem, although it's hard to be sure when you've only shown us 3 lines of code instead of a reproducible example.
You're doing something like this:
old_reader = csv.reader(open('old.csv', 'rb'))
writer = csv.writer(open('new.csv', 'wb'))
for row in old_reader:
    writer.writerow(transform(row))
new_reader = csv.reader(open('new.csv', 'rb'))
for row in new_reader:
    print row

At the time you open new.csv for reading, you haven't yet closed new.csv for writing. So the last buffer hasn't been flushed to disk. So you can't see it.
But then, when your script finishes, the writer goes out of scope, the file object no longer has any references, so it gets flushed and closed. So when you inspect it from outside of the program, after the script finishes, now it's complete. (Note that this behavior is explicitly not guaranteed; you're just getting lucky.)
And this is why you should never leak files by just putting an open in the middle of an expression. Use a with statement instead. For example:
with open('old.csv', 'rb') as oldf, open('new.csv', 'wb') as newf:
    old_reader = csv.reader(oldf)
    writer = csv.writer(newt)
    for row in old_reader:
        writer.writerow(transform(row))
with open('new.csv', 'rb') as newf:
    new_reader = csv.reader(newf)
    for row in new_reader:
        print row
",A,6
45033526,2017-07-11 11:39:45.217000+00:00,"TemplateViews don't know anything about querysets, so they never call a get_queryset method. You should subclass a more appropriate view, perhaps ListView.",A,3
8899202,2012-01-17 17:44:51.107000+00:00,"You can't declare a ManyToMany on instantiation. You need to define and save the instance first, then add the relationships.
p = Page(nom=""indexvide"")
p.save()
p.templates.add(t2)

(Note also naming conventions: lower case for fields, plural for many-to-many.)",A,5
15007700,2013-02-21 16:43:58.007000+00:00,"You can use .rstrip() to remove newlines from the right-hand side of a string:
line.rstrip('\n')

or you can tell it to remove all whitespace (including spaces, tabs and carriage returns):
line.rstrip()

It is a more specific version of the .strip() method which removes whitespace or specific characters from both sides of the string.
For your specific case, you could stick with a simple .strip() but for the general case where you want to remove only the newline, I'd stick with `.rstrip('\n').
I'd use a different method to write your strings though:
with open('list.txt') as input_file:
    print ','.join(['""{}""'.format(line.rstrip('\n')) for line in input_file])

By using ','.join() you avoid the last comma, and using the str.format() method is easier on the eyes than a lot of string concatenation (not to mention faster).",A,8
40150211,2016-10-20 09:14:50.047000+00:00,"You are receiving UTF-16 encoded data. Decode this to Unicode strings first; with the \x00 bytes coming first this is using UTF-16 in big-endian order:
input_str = data.decode('utf-16-be')
if u""restartservice"" in input_str.lower():

Note that I used a Unicode string literal (u'...') to test against the decoded text; that way you avoid implicit decoding and encoding, which can lead to further exceptions if one of the two operands doesn't cleanly decode or encode as ASCII.
Demo:
>>> input_str = '\x00R\x00e\x00s\x00t\x00a\x00r\x00t\x00S\x00e\x00r\x00v\x00i\x00c\x00e\x00#'
>>> input_str.decode('utf-16-be')
u'RestartService#'
>>> 'restartservice' in input_str.decode('utf-16-be').lower()
True

You may want to read up on what Unicode is, and how to best handle this in Python code; I strongly recommend the following articles:

Pragmatic Unicode by Ned Batchelder
The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
",A,1
35453825,2016-02-17 10:19:26.037000+00:00,No you don't. Static files are not templates; don't put them there.,A,2
51449836,2018-07-20 20:21:59.143000+00:00,"What you're trying to do isn't exactly impossible, it's just complicated, and probably wasteful.
If you want to partition an iterable into two iterables, if the source is a list or other re-usable iterable, you're probably better off either doing it in two passes, as in your question. 
Even if the source is an iterator, if the output you want is a pair of lists, not a pair of lazy iterators, either use Martijn's answer, or do two passes over list(iterator).)
But if you really need to lazily partition an arbitrary iterable into two iterables, there's no way to do that without some kind of intermediate storage. 
Let's say you partition [1, 2, -1, 3, 4, -2] into positives and negatives. Now you try to next(negatives). That ought to give you -1, right? But it can't do that without consuming the 1 and the 2. Which means when you try to next(positives), you're going to get 3 instead of 1. So, the 1 and 2 need to get stored somewhere.
Most of the cleverness you need is wrapped up inside itertools.tee. If you just make positives and negatives into two teed copies of the same iterator, then filter them both, you're done.
In fact, this is one of the recipes in the itertools docs:
def partition(pred, iterable):
    'Use a predicate to partition entries into false entries and true entries'
    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
    t1, t2 = tee(iterable)
    return filterfalse(pred, t1), filter(pred, t2)

(If you can't understand that, it's probably worth writing it out explicitly, with either two generator functions sharing an iterator and a tee via a closure, or two methods of a class sharing them via self. It should be a couple dozen lines of code that doesn't require anything tricky.)
And you can even get partition as an import from a third-party library like more_itertools.

Now, you can use this in a one-liner:
lst = [1, 2, -1, 3, 4, -2]
positives, negatives = partition(lst, lambda x: x>=0)

… and you've got an iterator over all the positive values, and an iterator over all of the negative values. They look like they're completely independent, but together they only do a single pass over lst—so it works even if you assign lst to a generator expression or a file or something instead of a list.

So, why isn't there some kind of shortcut syntax for this? Because it would be pretty misleading. 
A comprehension takes no extra storage. That's the reason generator expressions are so great—they can transform a lazy iterator into another lazy iterator without storing anything. 
But this takes O(N) storage. Imagine all of the numbers are positive, but you try to iterate negative first. What happens? All of the numbers get pushed to trueq. In fact, that O(N) could even be infinite (e.g., try it on itertools.count()).
That's fine for something like itertools.tee, a function stuck in a module that most novices don't even know about, and which has nice docs that can explain what it does and make the costs clear. But doing it with syntactic sugar that made it look just like a normal comprehension would be a different story.",A,1
5760006,2011-04-22 21:01:53.703000+00:00,"Use dictionaries for this.  It's much simpler that writing your own sort-merge.
with open('accountpartner.txt') as aFile:
    lookup = {}
    for line in aFile:
        lookup[line[:6]]= line

def make_groups( source ):
    group = []
    key= None
    for line in source:
        if line[:6] != key:
            if group: yield key, group
            group= []
            key= line[:6]
        group.append( line )
    if group: yield key, group

with open('testaccount.txt') as main:
    for key, group in make_groups( main ):
        if key in lookup:
            print key, group, lookup
        else:
            print key, group, None
",A,1
47967927,2017-12-25 10:52:38.363000+00:00,"You need to pass self when you call the superclass init. The best way to do this is via super().
def __init__(self, project_id, name, duration, deadline, done, id = None):
    if not id:
        super(project_task, self).__init__(name, 0)

However, your alternative constructor won't work when it's called from within the init like that; what happens is that you construct a task instance and they throw it away completely. Instead you should have a method to return the relevant values and assign them to self.",A,1
912921,2009-05-26 21:42:32.777000+00:00,"Are you sure that the page depends on 'hash variables'? That would be a very odd way to design a system. In particular, the elements after the hash are not guaranteed to be sent by the browser to the server, and in fact in most cases they are not. That means your PHP script will never receive the variables.",A,-1
12423750,2012-09-14 11:37:39.570000+00:00,"The nested function looks up variables from the parent scope when executed, not when defined.
The function body is compiled, and the 'free' variables (not defined in the function itself by assignment), are verified, then bound as closure cells to the function, with the code using an index to reference each cell. pet_function thus has one free variable (cage) which is then referenced via a closure cell, index 0. The closure itself points to the local variable cage in the get_petters function.
When you actually call the function, that closure is then used to look at the value of cage in the surrounding scope at the time you call the function. Here lies the problem. By the time you call your functions, the get_petters function is already done computing it's results. The cage local variable at some point during that execution was assigned each of the 'cow', 'dog', and 'cat' strings, but at the end of the function, cage contains that last value 'cat'. Thus, when you call each of the dynamically returned functions, you get the value 'cat' printed.
The work-around is to not rely on closures. You can use a partial function instead, create a new function scope, or bind the variable as a default value for a keyword parameter.

Partial function example, using functools.partial():
from functools import partial

def pet_function(cage=None):
    print ""Mary pets the "" + cage.animal + "".""

yield (animal, partial(gotimes, partial(pet_function, cage=cage)))

Creating a new scope example:
def scoped_cage(cage=None):
    def pet_function():
        print ""Mary pets the "" + cage.animal + "".""
    return pet_function

yield (animal, partial(gotimes, scoped_cage(cage)))

Binding the variable as a default value for a keyword parameter:
def pet_function(cage=cage):
    print ""Mary pets the "" + cage.animal + "".""

yield (animal, partial(gotimes, pet_function))


There is no need to define the scoped_cage function in the loop, compilation only takes place once, not on each iteration of the loop.",A,100
26433697,2014-10-17 21:29:47.047000+00:00,"You can simply slice the key:
newKey = key[rand:rand + length]

Demo:
>>> length, rand = 4, 2
>>> key = ""XPMGTDHLYONZBWEARKJUFSCIQV""
>>> key[rand:rand + length]
'MGTD'
",A,4
26090130,2014-09-28 22:04:07.803000+00:00,"There are some other problems in your code, but since you're asking about the syntax error, it's almost certainly this line:
print key, value // This will print the symbols and letters

First, // does not mean ""comment"" in Python, it means ""integer division"". So, you're asking it to divide value by This (which would probably raise a NameError, because it's unlikely you have anything named This in your code), and then including a bunch of other identifiers starting with will. A string of two identifiers in a row isn't valid syntax.
How do you write a comment in Python? Use #, not //:
print key, value # This will print the symbols and letters

Second, if you're using Python 3.x, print is a normal function, like anything else, so its arguments have to go in parentheses, like all of your other function calls. (And given the print call a few lines up, I'm willing to bet you are using Python 3.x.) Most likely you've copied this from some code for Python 2.x. There are some important differences between Python 2 and 3, which means that not all code for Python 2 can be copied and pasted into your Python 3. And this is one of the cases where it doesn't work. So:
print(key, value) # This will print the symbols and letters

But don't make that second change if you're using Python 2.x; otherwise, you'll just end up printing a tuple instead of two strings separated by a space. (For example, print 1, 2 prints 1 2, but print(1, 2) prints (1, 2).)",A,0
30033322,2015-05-04 14:57:52.717000+00:00,"Since you need to validate all matching entries anyway, your best option is to use a straight-up for loop; break out when the None condition isn't met:
found = None
for entry in mylist:
    if entry[2] == 'Project A':
        if entry[1] is None:
            print 'No match'
            break
        if not found or entry > found:
            found = entry
else:
    # only executed when there was no break!
    print found or 'No match'

The else branch of a for loop is executed only when the for loop completed iterating. That will only happen if there were any Project A entries were found where t[1] was set to None.
Since there is still the possibility that there are 0 entries that match the project name, found is set to None first; it is None still by the time you reach the else block the loop if no entries were there to be matched.
Demo, turning the found None case into an exception to show the difference:
>>> def find_max_project(projects, target):
...     found = None
...     for entry in projects:
...         if entry[2] == target:
...             if entry[1] is None:
...                 raise ValueError('no match')
...             if not found or entry > found:
...                 found = entry
...     else:
...         return found
... 

>>> find_max_project([(1, None, ""Project A""), (2, 3, ""Project A""), (3, 6, ""Project B"")], 'Project A')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 7, in find_max_project
ValueError: 'no match'
>>> find_max_project([(1, 4, ""Project A""), (2, 3, ""Project A""), (3, 6, ""Project B"")], 'Project A')
(2, 3, 'Project A')
>>> find_max_project([(1, 2, ""Project A""), (1, 3, ""Project A"")], 'Project A')
(1, 3, 'Project A')
>>> find_max_project([(1, 2, ""Project B""), (1, 3, ""Project B"")], 'Project A')
>>> find_max_project([(1, 2, ""Project B""), (1, 3, ""Project B"")], 'Project A') is None
True

Note that in this version we don't even really need to put the final line in an else block anymore, as using an exception ends the function now.",A,1
25949110,2014-09-20 13:07:14.603000+00:00,"Yes, using threading.local is an excellent method to set a per-request global. Your request will always be handled by one thread, on one instance in the Google cloud. That thread local value will be unique to that thread.
Take into account that the thread can be reused for future requests, and always reset the value at the start of the request.",A,9
34966098,2016-01-23 16:47:35.993000+00:00,"You have CSV data (comma separated). Easiest is to use the csv module:
import csv

all_values = []

with open('scores.txt', newline='') as infile:
    reader = csv.reader(infile)
    for row in reader:
        all_values.extend(row)

Otherwise, split the values. Each line you read is a string with the ',' character between the digits:
all_values = []

with open('scores.txt', newline='') as infile:
    for line in infile:
        all_values.extend(line.strip().split(','))

Either way, all_values ends up with a list of strings. If all your values are only consisting of digits, you could convert these to integers:
all_values.extend(map(int, row))

or
all_values.extend(map(int, line.strip().split(',')))
",A,3
2061553,2010-01-14 01:10:08.430000+00:00,"Edit.  
We refactor it like this.
matches = [ k for k in range(len(target)-len(key)) if target[k:].startswith(key) ]
matchesFound = len(matches)

We don't need C-style condition and assignment conflation.
Rarely do you simply want the count; the actual locations are available for free.",A,0
30277306,2015-05-16 15:13:37.590000+00:00,"Install the lxml library; once installed BeautifulSoup will use it as the default parser.
lxml parser the page using the libxml2 C library, which is significantly faster than the default html.parser backend, implemented in pure Python.
You can then also parse the page as XML instead of as HTML:
soup = BeautifulSoup(output, 'xml')

Parsing your given page with lxml should be faster; I can parse the page almost 50 times per second:
>>> timeit(""BeautifulSoup(output, 'xml')"", 'from __main__ import BeautifulSoup, output', number=50)
1.1700470447540283

Still, I wonder if you are missing some other Python acceleration libraries, as I certainly cannot reproduce your results even with the built-in parser:
>>> timeit(""BeautifulSoup(output, 'html.parser')"", 'from __main__ import BeautifulSoup, output', number=50)
1.7218239307403564

Perhaps you are memory constrained and the large-ish document causes your OS to swap memory a lot? Memory swapping (writing pages to disk and loading other pages from disk) can bring even the fastest programs to a grinding halt.
Note that instead of using str() on tag elements and splitting off the tags, you can get the value from a tag simply by using the .string attribute:
station_6350 = soup.buienradarnl.weergegevens.actueel_weer.weerstations.find(id=6350)
ml = station_6350.windsnelheidMS.string
gr = station_6350.windrichtingGR.string

If you are using the XML parser, take into account that tagnames must match case (HTML is a case-insensitive mark-up language).
Since this is an XML document, another option would be to use the lxml ElementTree model; you can use XPath expressions to extract the data:
from lxml import etree 

response = urlopen(_url)
for event, elem in etree.iterparse(response, tag='weerstation'):
    if elem.get('id') == '6350':
        ml = elem.find('windsnelheidMS').text
        gr = elem.find('windrichtingGR').text
        break
    # clear elements we are not interested in, adapted from
    # http://stackoverflow.com/questions/12160418/why-is-lxml-etree-iterparse-eating-up-all-my-memory
    elem.clear()
    for ancestor in elem.xpath('ancestor-or-self::*'):
        while ancestor.getprevious() is not None:
            del ancestor.getparent()[0]

This should only build the minimal object tree required, clearing out the weather stations you don't need as you go along the document.
Demo:
>>> from lxml import etree
>>> from urllib2 import urlopen
>>> _url = ""http://xml.buienradar.nl/""
>>> response = urlopen(_url)
>>> for event, elem in etree.iterparse(response, tag='weerstation'):
...     if elem.get('id') == '6350':
...         ml = elem.find('windsnelheidMS').text
...         gr = elem.find('windrichtingGR').text
...         break
...     # clear elements we are not interested in
...     elem.clear()
...     for ancestor in elem.xpath('ancestor-or-self::*'):
...         while ancestor.getprevious() is not None:
...             del ancestor.getparent()[0]
... 
>>> ml
'4.64'
>>> gr
'337.8'
",A,4
38324561,2016-07-12 09:11:57.770000+00:00,"To create a copy of the HMAC instance, you need to create an empty instance first.
The _secret_backdoor_key object is used as a sentinel to exit __init__ early and not run through the rest of the __init__ functionality. The copy method then sets the instance attributes directly:
def copy(self):
    """"""Return a separate copy of this hashing object.

    An update to this copy won't affect the original object.
    """"""
    other = self.__class__(_secret_backdoor_key)
    other.digest_cons = self.digest_cons
    other.digest_size = self.digest_size
    other.inner = self.inner.copy()
    other.outer = self.outer.copy()
    return other

You could get the same effect with self.__class__('') (an empty string), but then HMAC.__init__ does a lot of unnecessary work as the attributes on the instance created are going to be replaced anyway. Note that using HMAC('') is a valid way to create an instance, you'd not want an instance devoid of any state in that case. By passing in the sentinel, HMAC.copy() can avoid all that extra work.
You could use a different 'flag' value, like False, but it is way too easy to pass that in because of a bug in your own code. You'd want to be notified of such bugs instead. By using a 'secret' internal sentinel object instead, you avoid such accidental cases.
Using [] as a sentinel unique object is quite an old practice. These days you'd use object() instead. The idea is that the sentinel is a unique, single object that you test against for identity with is. You can't re-create that object elsewhere, the is test only works if you pass in an reference to the exact same single object.",A,8
32651967,2015-09-18 12:23:17.083000+00:00,"Certificate validation did not fail, so the verify argument doesn't apply here. What failed is the cipher negotiation; none of the ciphers requests is willing to use match those the server is willing to use.
If you run your curl command with the -v switch you'll see what cipher suite was negotiated by curl for the successful connection:
$ curl -v -I https://service.isracard.co.il/I_logon.jsp
* Hostname was NOT found in DNS cache
*   Trying 192.118.12.8...
* Connected to service.isracard.co.il (192.118.12.8) port 443 (#0)
* TLS 1.2 connection using TLS_RSA_WITH_RC4_128_SHA
[ .... ]

That's the RC4-SHA cipher, which has some rather troublesome securty issues and should not really be used; it offers no forward secrecy for example. The urllib3 package (bundled with requests) by default excludes that cipher from the default ciphers. You can add it back with:
import requests

requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':RC4-SHA'
try:
    requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST += ':RC4-SHA'
except AttributeError:
    # no pyopenssl support used / needed / available
    pass

and your request works:
>>> import requests
>>> requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS += ':RC4-SHA'
>>> requests.get('https://service.isracard.co.il/I_logon.jsp')
<Response [200]>

I didn't install the pyOpenSSL package so I didn't bother with the try..except guarded part.",A,18
4009018,2010-10-24 16:06:49.187000+00:00,Are you looking for the __import__ function?,A,0
2826240,2010-05-13 11:06:11.370000+00:00,"A neuron-like circuit can be built with a handful of transistors.  Let's say it takes about a dozen transistors on average.  (See http://diwww.epfl.ch/lami/team/vschaik/eap/neurons.html for an example.)
A brain-sized circuit would require 100 billion such neurons (more or less).
That's 1.2 trillion transistors.
A quad-core Itanium has 2 billion transistors.  
You'd need a server rack with 600 quad-core processors to be brain-sized.  Think $15M US to purchase the servers.  You'll need power management and cooling plus real-estate to support this mess.
One significant issue in simulating the brain is scale.  The actual brain only dissipates a few watts.  Power consumption is 3 square meals per day.  A pint of gin.  Maintenance is 8 hours of downtime.  Real estate is a 42-foot sailboat (22 Net Tons of volume as ships are measured) and a place to drop the hook.
A server cage with 600 quad-core processors uses a lot more energy, cooling and maintenance.  It would require two full-time people to keep this ""brain-sized"" server farm running.
It seems simpler to just teach the two people what you know and skip the hardware investment.",A,4
22685515,2014-03-27 10:57:22.990000+00:00,"Based on @Sven Marnach's answer:
#!/usr/bin/env python
import ctypes
import numpy as np
from numpy.ctypeslib import ndpointer

libf = ctypes.cdll.LoadLibrary('/path/to/lib.so')
libf.f.restype = ctypes.c_double
libf.f.argtypes = [ctypes.c_int, ndpointer(ctypes.c_double)]

def f(a):
    return libf.f(a.size, np.ascontiguousarray(a, np.float64))

if __name__==""__main__"":
    # slice to create non-contiguous array
    a = np.arange(1, 7, dtype=np.float64)[::2]
    assert not a.flags['C_CONTIGUOUS']
    print(a)
    print(np.multiply.reduce(a))
    print(f(a))

Output
[ 1.  3.  5.]
15.0
15.0

Removing np.ascontiguousarray() call produces the wrong result (6.0 on my machine).",A,2
22192514,2014-03-05 08:39:52.783000+00:00,"This might be overkill, but one fairly nice option would be to use aggregation to annotate the number of choices, and make that available on a custom Manager model which does the filtering. Something like:
from django.db.models import Count
class PollManager(models.Manager):
    def with_counts(self):
        return self.get_queryset().annotate(choice_count=Count('choice')

    def choices_only(self):
        return self.with_counts().exclude(choice_count=0)


class Poll(models.Model):
    ...
    objects = PollManager()

Now you can just use Poll.objects.choices_only() to return only the polls that have choices.",A,3
21800366,2014-02-15 16:31:17.677000+00:00,"You are looking for the unichr() function:
s = u'{}'.format(unichr(i))

or just plain
s = unichr(i)

unichr(integer) produces a unicode character for the given codepoint.
The \uxxxx syntax only works for string literals; these are processed by the Python compiler before running the code.
Demo:
>>> for i in range(0x2550, 0x257F):
...     print unichr(i)
... 
═
║
╒
╓
╔
╕
╖
# etc.

If you ever do have literal \uxxxx sequences in your strings, you can still have Python turn those into Unicode characters with the unicode_escape codec:
>>> print u'\\u2550'
\u2550
>>> print u'\\u2550'.decode('unicode_escape')
═
>>> print '\\u2550'.decode('unicode_escape')
═
>>> '\\u2550'.decode('unicode_escape')
u'\u2550'

In Python 2, you can use this codec on both byte string and unicode string values; the output is always a unicode string.",A,1
12609318,2012-09-26 19:51:06.210000+00:00,"To preserve the DPI, you need to specify it when saving; the info attribute is not always preserved across image manipulations:
dpi = im.info['dpi']  # Warning, throws KeyError if no DPI was set to begin with

# resize, etc.

out.save(""out.jpg"", dpi=dpi)
",A,7
24720723,2014-07-13 08:09:32.623000+00:00,"You assigned a tuple to the print name:
print = 'Test 1 Avg:',(T1Avg),'Test 2 Avg:', (T2Avg),'Test 3 Avg:' ,(T3Avg)

Now print is bound to a tuple, so when you later use it to try and print, you get an error. You can no longer use the built-in print function from there on out.
You probably just wanted to print there, not produce a tuple:
print('Test 1 Avg:',(T1Avg),'Test 2 Avg:', (T2Avg),'Test 3 Avg:' ,(T3Avg))
",A,2
12367275,2012-09-11 10:03:25.370000+00:00,"Not 100% sure, but I think SQLAlchemy is trying to intepret the % character as a SQL parameter. I'd try doubling the % character to work around this:
sql = ""SELECT COUNT(*) FROM entities WHERE (unit = '%%')""

This depends on what database you are using; different database adapters use different parameter styles. The psycopg2 module, used for PostgreSQL database connections, for example uses the %s style and documents that %% is the correct way to insert a % literal value into your SQL statements.",A,4
15999526,2013-04-14 13:21:42.107000+00:00,"Your triple-quoted string has one "" quote too many:
 """""" (str) --> bool
   Return True if and only if s is a palindrome

   >>> is_palindrome_v1('noon')
   True
   >>> is_palindrome_v1('racecar')
   True
   >>> is_palindrome_v1('dented')
   False
   """"""""
------^
",A,0
13830881,2012-12-12 00:33:54.330000+00:00,"Putting the thumbnail generation in a background thread with threading.Thread will solve your first problem, making the program usable.
If you want a way to interrupt it, the usual way is to add a ""stop"" variable which the background thread checks every so often (e.g., once per thumbnail), and the GUI thread sets when it wants to stop it. Ideally you should protect this with a threading.Condition. (The condition isn't actually necessary in most cases—the same GIL that prevents your code from parallelizing well also protects you from certain kinds of race conditions. But you shouldn't rely on that.)
For the third problem, the first question is: Is thumbnail generation actually CPU-bound? If you're spending more time reading and writing images from disk, it probably isn't, so there's no point trying to parallelize it. But, let's assume that it is.
First, if you have N cores, you want a pool of N threads, or N-1 if the main thread has a lot of work to do too, or maybe something like 2N or 2N-1 to trade off a bit of best-case performance for a bit of worst-case performance.
However, if that CPU work is done in Python, or in a C extension that nevertheless holds the Python GIL, this won't help, because most of the time, only one of those threads will actually be running.
One solution to this is to switch from threads to processes, ideally using the standard multiprocessing module. It has built-in APIs to create a pool of processes, and to submit jobs to the pool with simple load-balancing.
The problem with using processes is that you no longer get automatic sharing of data, so that ""stop flag"" won't work. You need to explicitly create a flag in shared memory, or use a pipe or some other mechanism for communication instead. The multiprocessing docs explain the various ways to do this.
You can actually just kill the subprocesses. However, you may not want to do this. First, unless you've written your code carefully, it may leave your thumbnail cache in an inconsistent state that will confuse the rest of your code. Also, if you want this to be efficient on Windows, creating the subprocesses takes some time (not as in ""30 minutes"" or anything, but enough to affect the perceived responsiveness of your code if you recreate the pool every time a user clicks a new folder), so you probably want to create the pool before you need it, and keep it for the entire life of the program.
Other than that, all you have to get right is the job size. Hopefully creating one thumbnail isn't too big of a job—but if it's too small of a job, you can batch multiple thumbnails up into a single job—or, more simply, look at the multiprocessing API and change the way it batches jobs when load-balancing.
Meanwhile, if you go with a pool solution (whether threads or processes), if your jobs are small enough, you may not really need to cancel. Just drain the job queue—each worker will finish whichever job it's working on now, but then sleep until you feed in more jobs. Remember to also drain the queue (and then maybe join the pool) when it's time to quit.
One last thing to keep in mind is that if you successfully generate thumbnails as fast as your computer is capable of generating them, you may actually cause the whole computer—and therefore your GUI—to become sluggish and unresponsive. This usually comes up when your code is actually I/O bound and you're using most of the disk bandwidth, or when you use lots of memory and trigger swap thrash, but if your code really is CPU-bound, and you're having problems because you're using all the CPU, you may want to either use 1 fewer core, or look into setting thread/process priorities.",A,4
46159416,2017-09-11 15:37:57.393000+00:00,"super() normally relies on the compiler to provide a __class__ closure cell, that is bound to the class object a method is derived on. The closure is created the moment you use the name super() in a method (or if you used __class__):
>>> class Foo(object):
...     def bar(self):
...         super()  # just using super or __class__ is enough
...
>>> Foo.bar.__closure__[0].cell_contents
<class '__main__.Foo'>
>>> Foo.bar.__closure__[0].cell_contents is Foo
True

That closure lets super() work without arguments (the self argument is taken from the local namespace).
However, attr generates a new class object when you specify you wanted to use __slots__; you can't add slots to a class after the fact, so a new class object is created that replaces the one you decorated.
The closure attached to meth is the original pre-decoration class and not the same class object as the new generated class:
>>> Derived.meth.__closure__[0].cell_contents
<class '__main__.Derived'>
>>> Derived.meth.__closure__[0].cell_contents is Derived
False

This breaks the expectations super() has, making it impossible to use the 0 argument variant. The super(Derived, self) variant explicitly looks up the name Derived as a global when called, finding the new generated class, so works.
I go into some more detail on how super() without arguments works, and why, in Why is Python 3.x's super() magic?
This was reported as issue #102 in the tracker, and fixed by altering the closure with some ctypes hackery. This fix will be part of the upcoming 17.3 release.",A,1
1784268,2009-11-23 16:27:58.507000+00:00,Your template has {{download.mini.thumbnail}} instead of {{download.mini_thumbnail}},A,2
21709058,2014-02-11 17:38:45.970000+00:00,"Generally speaking, putting more than one JSON object into a file makes that file invalid, broken JSON. That said, you can still parse data in chunks using the JSONDecoder.raw_decode() method.
The following will yield complete objects as the parser finds them:
from json import JSONDecoder
from functools import partial


def json_parse(fileobj, decoder=JSONDecoder(), buffersize=2048):
    buffer = ''
    for chunk in iter(partial(fileobj.read, buffersize), ''):
         buffer += chunk
         while buffer:
             try:
                 result, index = decoder.raw_decode(buffer)
                 yield result
                 buffer = buffer[index:]
             except ValueError:
                 # Not enough data to decode, read more
                 break

This function will read chunks from the given file object in buffersize chunks, and have the decoder object parse whole JSON objects from the buffer. Each parsed object is yielded to the caller.
Use it like this:
with open('yourfilename', 'r') as infh:
    for data in json_parse(infh):
        # process object

Use this only if your JSON objects are written to a file back-to-back, with no newlines in between. If you do have newlines, and each JSON object is limited to a single line, you have a JSON Lines document, in which case you can use Loading and parsing a JSON file with multiple JSON objects in Python instead.",A,25
1408693,2009-09-11 02:20:28.043000+00:00,Just use decimal.,A,11
20765920,2013-12-24 19:29:55.597000+00:00,"Yes, opening a file in text mode potentially can result in different data being read as newlines are translated for you from the platform native format to \n. Thus, files containing \r\n will give you a different checksum when read on Windows vs. a POSIX platform.
Open files in binary mode instead:
with open(file_loc, 'rb') as file_to_read:

Note that the same applies when writing a file. If you receive data from a POSIX system using \n line endings, and you write this to a file opened for writing in text mode on Windows, you'll end up with \r\n line endings in the written file.
If you are using Python 3, you are complicating matters some more. When you are opening files in text mode, you are translating the data from encoded bytes to decoded Unicode values. What codec is used for that can also differ from OS to OS, and even from machine to machine. The default is locale-defined (using locale.getpreferredencoding(False)), and as long as the data is decodable by the default locale, you can get very different results from reading a file using a different codec. You really want to ensure you use the same codec by setting it explicitly, or better still, open files in binary mode.
Since hashlib requires you to feed it byte strings, this is less of a problem when trying to calculate the digest (you'd have run into that problem and at least have to think about codecs there), but this applies to file transfers too; writing to text file will encode the data to the default codec.",A,6
9745213,2012-03-16 22:17:53.927000+00:00,"Do you mean something like this:
def quotedStringOrWord(pattern):
    return quotedString(pattern) | Word(pattern)
",A,2
28345836,2015-02-05 13:58:03.903000+00:00,"You can create a simple class:
class Namespace:
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

and it'll work the exact same way as the argparse Namespace class when it comes to attributes:
>>> args = Namespace(a=1, b='c')
>>> args.a
1
>>> args.b
'c'

Alternatively, just import the class; it is available from the argparse module:
from argparse import Namespace

args = Namespace(a=1, b='c')

As of Python 3.3, there is also types.SimpleNamespace, which essentially does the same thing:
>>> from types import SimpleNamespace
>>> args = SimpleNamespace(a=1, b='c')
>>> args.a
1
>>> args.b
'c'

The two types are distinct; SimpleNamespace is primarily used for the sys.implementation attribute and the return value of time.get_clock_info().
Further comparisons:

Both classes support equality testing; for two instances of the same class, instance_a == instance_b is true if they have the same attributes with the same values.
Both classes have a helpful __repr__ to show what attributes they have.
Namespace() objects support containment testing; 'attrname' in instance is true if the namespace instance has an attribute namend attrname. SimpleNamespace does not.
Namespace() objects have an undocumented ._get_kwargs() method that returns a sorted list of (name, value) attributes for that instance. You can get the same for either class using sorted(vars(instance).items()).
While SimpleNamespace() is implemented in C and Namespace() is implemented in Python, attribute access is no faster because both use the same __dict__ storage for the attributes. Equality testing and producing the representation are a little faster for SimpleNamespace() instances.
",A,62
18198044,2013-08-12 22:59:15.563000+00:00,"shhdup's answer shows how to do this correctly, but doesn't explain what you're doing wrong.

The actual reason you're getting the error is that you're adding u""'"", which is a unicode object, and row[2], which is a str object. Concatenating a unicode and a str (in either order) automatically decodes the str using sys.getdefaultencoding(), which is almost always 'ascii'.
There are two reasons for this problem. First, this:
u""'"" + row[2]+""'"".encode('utf-8')

Is equivalent to this:
u""'"" + row[2] + (""'"".encode('utf-8'))

… rather than:
(u""'"" + row[2] + ""'"").encode('utf-8')

However, even if you fix that, you're still adding Unicode and bytes strings together inside the parens. So you either want this:
u""'"" + row[2].encode('utf-8') + u""'""

… or
(""'"" + row[2] + ""'"").encode('utf-8')


You're also using encode instead of decode. 
And you're 'utf-8' instead of 'Latin-1' even though your data is Latin-1. 
But your code isn't getting far enough for either of those to matter. And neither one of those problems will give you the error you're getting.",A,6
20578504,2013-12-14 01:01:26.903000+00:00,"There is really no way to put timeouts on tasks in multiprocessing pools, or to abort them once they've started, in the first place. You can only terminate the entire pool.
Which obviously means there's also no way to put different timeouts on each task.
The only way to really do that is to run each one in a process that you can kill if it oversteps the timeout.
The simplest way to do that is to have a thread in the main process for each child process, so the thread can block on proc.join(timeout), then call proc.terminate() if proc.is_alive() is still true.
The fact that you have to use a Process rather than a Pool or a ProcessPoolExecutor means you have to pass return values back manually, which is a pain. To avoid that, you could use a single-process pool/executor, submit the single job, wait for the AsyncResult/future with a timeout, and terminate the pool/executor if it times out, but that seems a little clumsy for different reasons.
Either way, once you've got threads that can wait on single-process tasks with a timeout, you just toss the threads into a pool/executor of ncpu workers and let it do the work for you.",A,2
20647456,2013-12-18 00:26:45.857000+00:00,"No backtracking takes place at all. Your pattern matches two different types of strings; | means or. Each pattern is tried out at each position.
So when the expression finds abcd at the start of your input, that text matches your pattern just fine, it fits the abcd part of the (bc or abcd) pattern you gave it.
Ordering of the alternative parts doesn't play here, as far as the regular expression engine is concerned, abcd|bc is the same thing as bc|abcd. abcd is not disregarded just because bc might match later on in the string.",A,3
45386569,2017-07-29 07:21:16.603000+00:00,"The whole point of migrations is that you run them on both your local database and in production, to keep them in sync.",A,1
14885298,2013-02-14 22:31:41.730000+00:00,"Assuming you have an intervals_overlap(interval1, interval2) function…
The first idea is a naive iteration over every pair of intervals in the list:
for interval1 in intervals:
    for interval2 in intervals:
        if interval1 is not interval2:
            if intervals_overlap(interval1, interval2):
                return True
return False

But you should be able to figure out smarter ways of dong this.",A,0
4085406,2010-11-03 08:56:53.097000+00:00,"Sorry, I don't believe you could get an accurate count on ASP/IIS. It's simply not possible for a server to tell the difference between the user leaving the browser open on a site without doing anything, navigating away to a different page, or closing the browser completely.
Even if the session cookie expires at browser close, that still doesn't tell the server anything - the browser has now closed, so what is going to let the server know? It's simply the client-side cookie that has expired.
The best thing you can do is to estimate based on session expires, as Elf has suggested.",A,1
42482171,2017-02-27 09:34:43.377000+00:00,"You'll have to mock the whole mapper class; accessing the query attribute on the mapper causes a session load:
@patch('app.model.some_model.SomeModel')
def test_some_case(self, some_model_mock):
    filter_by_mock = some_model_mock.query.filter_by
    # more test logic goes here

That's because the .query attribute is a descriptor object; accessing it triggers the binding to a session.
The alternative would be to mock out the _QueryProperty.__get__ method (which backs the .query attribute); only use this if you must test with actual SomeModel instances:
@patch('flask_sqlalchemy._QueryProperty.__get__')
def test_some_case(self, query_property_getter_mock):
    filter_by_mock = query_property_getter_mock.return_value.filter_by
    # more test logic goes here

Demo:
>>> from flask_sqlalchemy import SQLAlchemy
>>> db = SQLAlchemy()
>>> class SomeModel(db.Model):
...     id = db.Column(db.Integer, primary_key=True)
...
>>> from unittest import mock
>>> with mock.patch('__main__.SomeModel') as model_mock:
...     filter_by = model_mock.query.filter_by
...     SomeModel.query.filter_by(SomeModel.id == 'foo')
...
<MagicMock name='SomeModel.query.filter_by()' id='4438980312'>
>>> with mock.patch('flask_sqlalchemy._QueryProperty.__get__') as query_property_getter_mock:
...     filter_by_mock = query_property_getter_mock.return_value.filter_by
...     SomeModel.query.filter_by(SomeModel.id == 'foo')
...
<MagicMock name='__get__().filter_by()' id='4439035184'>
",A,7
30070108,2015-05-06 07:23:34.580000+00:00,You can use the prefix parameter when instantiating your forms to give them non-conflicting ids.,A,2
17436756,2013-07-02 22:18:43.953000+00:00,"First, remember that the most important rule of PEP 8 is:

But most importantly: know when to be inconsistent -- sometimes the style guide just doesn't apply. When in doubt, use your best judgment. Look at other examples and decide what looks best.

And it specifically says to avoid a rule:

When applying the rule would make the code less readable, even for someone who is used to reading code that follows the rules.

That being said, you're already violating the letter and the spirit of PEP 8 just by having these lines of code, unless you can't avoid it without making things worse. As Maximum Line Length says, using backslash continuations is the least preferred way to deal with long lines. On top of that, it specifically says to ""Make sure to indent the continued line appropriately"", which you aren't doing.

The obvious way to break this up is to use some intermediates variables. This isn't C++; there's no ""copy constructor"" cost to worry about. In a real-life example (unlike this toy example), there are probably good names that you can come up with that will be much more meaningful than the long expression they replace.
intermediate = some.long.statement
multiline = intermediate.with.multiline(obj.runtime_attr)
more = multiline.more()


If that isn't appropriate, as PEP 8 explicitly says, it's better to rely on parenthetical continuations than backslash continuations. Is that doable here? Sure:
some.long.statement.with.multiline(
    obj.runtime_attr).more()

Or, if worst comes to worst:
(some.long.statement.
 with.multiline(obj.runtime_attr).more())

This sometimes makes things less readable rather than more, in which case you shouldn't do it. But it's always an option. And if you have to go to extraordinary lengths to make backslash continuation work for you, it's probably going to be worse than even the worst excesses of over-parenthetizing.

At any rate, doing things either of these ways means you can put a comment on the end of each line, so your problem never comes up in the first place.",A,0
18278133,2013-08-16 16:34:01.057000+00:00,Your media_serve_protected function is returning a Forbidden response if the url does not start with media/<id>. But your url is in the form media/root/<id>.,A,2
26985666,2014-11-18 02:38:04.073000+00:00,"There's no cross-platform way to do this in Python 2.x, because file objects, including stdin, are basically a thin wrapper around a lowest-common-denominator version of C stdio.
However, there are solutions for most *nix systems, and maybe for Windows, and between them, that covers everything you're likely to run a Python terminal program on.

On most *nix systems (pretty much everything you're likely to care about but Windows), if you only care about interactive input (that is, you want to skip anything the user types at the prompt, but don't want to skip anything if piped input from a file or another program), you can do this with the termios library:
downloading(HUGE_FILE)
if sys.stdin.isatty():
    sys.stdin.flush()
    termios.flush(sys.stdin, termios.TCIFLUSH)
    sys.stdin.flush()
input_var = raw_input(""Enter:"")

There are some edge cases where this may not work. Also, it can break readline support on some platforms. But it will mostly work.

On Windows, it's not quite as simple—but if stdin is the real console input, you can manually drain characters until there are no more by using the Console I/O APIs.
downloading(HUGE_FILE)
if sys.stdin.isatty():
    sys.stdin.flush()
    while msvcrt.kbhit():
        msvcrt.getch()
    sys.stdin.flush()
input_var = raw_input(""Enter:"")


On both platforms, the stdin.flush calls probably aren't necessary, but in theory it's illegal to switch between terminal/console I/O and stdio I/O without flushing, and in some edge cases you may end up with stuff that's already been read into the buffer that doesn't get thrown away if you don't do it, so, better safe than sorry.

So, how do you know which platform you're on? You could check platform.system() or similar, but it's probably best to just check for the libraries you want to use (especially because there's a whole slew of Unix-like systems you'd have to check for, and not all of them have termios, and some even have it optionally or only as of a certain version or …).
try:
    import termios
    # You can use the Unix code
except ImportError:
    try:
        import msvcrt
        # You can use the Windows code
    except ImportError:
        # You can't use either, just accept the extra characters
",A,2
16947791,2013-06-05 19:02:43.407000+00:00,"When you start Python, it inherits its current working directory from the shell. So, for example:
$ cd ~
$ ~/Documents/.../OrbitalMechanics/OrbitalNotes/stumpff.py

In this case, the current working directory is ~, not ~/Documents/.../OrbitalMechanics/OrbitalNotes/.
This means that any relative paths are relative to ~, so if you just create a file named foo.data, it ends up as ~/foo.data.
So, what if you want to get files into paths like ~/Documents/.../OrbitalMechanics/OrbitalNotes/foo.data? There are two basic ways to do it—either set the cwd to the script path, or use the script path explicitly. And there are two ways to do the first part, from outside or inside of the Python script. And… well, let's just go over all the variations one by one.

If you want to launch the script from its own working directory, you can of course manually cd to that directory first, or you can write a bash alias or function or script that does something like:
$(cd $(dirname $1) ./$(basename $1))

Obviously you'll want to get a bit fancier if you want to pass multiple arguments, or if you want to be able to use it on python  ~/Documents/.../OrbitalMechanics/OrbitalNotes/stumpff.py instead of just ~/Documents/.../OrbitalMechanics/OrbitalNotes/stumpff.py so the script is $2, etc. But it's all basic sh stuff from here.

If you want the script to change its own working directory, you can do that at the start of the script:
import os
import sys

scriptpath = os.path.dirname(__file__)
os.chdir(scriptpath)

(In some cases, you might want to use sys.argv[1] instead of __file__, but the details are hard to explain. If you want to learn, you can read the docs for each. If you don't want to learn, and you run into a problem, print out __file__ and see if it looks right, and, if not, try the other one.)

Or, you can just explicitly use scriptpath as a base instead of using cwd-relative paths. In this case, you'll probably want to use the absolute path to your script as a base (so later calls to chdir won't affect you):
import os
import sys

scriptpath = os.path.abspath(os.dirname(__file__))

# ...

datapath = os.path.join(scriptpath, 'foo.data')
with open(datapath, 'w') as datafile:
    # ...
",A,1
11975896,2012-08-15 19:28:37.873000+00:00,"
your command might consume too much memory. To check, run it with the large file from a console without python to see if you get any errors
your command might generate too much output. To check, run:
subprocess.check_call([""cmd"", ""arg1"", ""arg2""])

if it succeeds then you should read output incrementally and discard the processed output e.g. line by line:
p = subprocess.Popen([""cmd"", ""arg1"", ""arg2""], stdout=subprocess.PIPE)
for line in iter(p.stdout.readline, ''):
    # do something with line
    print line,
p.stdout.close()
exit_code = p.wait() # wait for the process to exit

",A,2
13901372,2012-12-16 12:40:03.513000+00:00,"Using zip() you can pair up elements from two lists:
for a, b in zip(A, B):
    # a is an element from A, b is a sublist from B.

Your sublists contain floating point values, and your list A contains strings. You'll need to figure out a tolerance for comparisons. Perhaps turning the floats to strings with the matching precision would suffice?
for a, b in zip(A, B):
    # a is an element from A, b is a sublist from B.
    b[:] = [i for i in b if format(i, '.4f') != a]

Using a slice assignment (b[:]) we replace the contents of the sublist with all elements that do not match a at 4 digits after the decimal.
Running that on your example input gives me:
[
[0.074, 0.073, 0.03, 0.029, 0.024, 0.021, 0.02], 
[0.015], 
[0.02, 0.015], 
[0.02, 0.017], 
[0.077, 0.076, 0.055, 0.045, 0.021], 
[0.053, 0.052, 0.023, 0.022], 
[]
]

If you only want the first match to be removed, use:
try:
    del b[next(i for i, e in enumerate(b) if format(e, '.4f') == a)]
except StopIteration:
    pass

This'll find the first index that matches, and remove that from b. Result (in this case exactly the same as before):
[[0.074, 0.073, 0.03, 0.029, 0.024, 0.021, 0.02],
 [0.015],
 [0.02, 0.015],
 [0.02, 0.017],
 [0.077, 0.076, 0.055, 0.045, 0.021],
 [0.053, 0.052, 0.023, 0.022],
 []]
",A,1
8613338,2011-12-23 07:23:57.870000+00:00,"Firstly, I don't know why you keep talking about 'running' your HTML file. It's not a script, even if it has Javascript in it - that's just for the client. There's nothing to run, Apache just needs to serve the file.
Secondly, you will obviously need some way of distinguishing requests that go to Django versus those that get served directly. The easiest way would be to put one or other in a subdirectory - either change the WSGIScriptAlias path from \ to my_django_site, or use a normal Alias to set the location of your static files. If you do the latter, you'll need to put the directive above the WSGI parts so that it's matched first.",A,1
141426,2008-09-26 19:28:32.573000+00:00,"Closure on closures

Objects are data with methods
  attached, closures are functions with
  data attached.

def make_counter():
    i = 0
    def counter(): # counter() is a closure
        nonlocal i
        i += 1
        return i
    return counter

c1 = make_counter()
c2 = make_counter()

print (c1(), c1(), c2(), c2())
# -> 1 2 1 2
",A,82
38916966,2016-08-12 11:32:15.900000+00:00,"You are converting a bytes value to a string the wrong way here:
data = str(row[1])

You forced it to be a str() object, but for bytes objects that'll include the b prefix and quotes, because bytes objects don't have a __str__ method, only __repr__ so you get a debug representation.
Decode the row without converting to a string:
data = row[1].decode('utf8')

You really shouldn't hand-craft JSON / Javascript output in your code either. Just use json.dumps(); if you must use per-row streaming, you can still use json.dump() to create each list entry:
import sqlite3
import json

conn = sqlite3.connect('geodata.sqlite')
cur = conn.cursor()
cur.execute('SELECT * FROM Locations')

with open('where.js', 'w', encoding=""utf-8"") as fhand:
    fhand.write(""myData = [\n"")
    for count, row in enumerate(row):
        try:
            js = json.loads(row[1].decode('utf8'))
        except json.JSONDecodeError:
            print('Could not decode a row: ', row[1])
            continue

        if js.get('status') != 'OK':
            continue

        lat = js[""results""][0][""geometry""][""location""][""lat""]
        lng = js[""results""][0][""geometry""][""location""][""lng""]
        if not (lat and lng):
            continue

        where = js['results'][0]['formatted_address']
        where = where.replace(""'"", """")
        print (where, lat, lng)

        if count: fhand.write("",\n"")
        json.dump([lat, lng, where], fhand)

fhand.write(""\n];\n"")

This uses plain open() (in Python 3, there is never a need to use codecs.open()), uses the file as a context manager, and adds in enumerate() to track if you have the first row processed yet.",A,0
35461165,2016-02-17 15:38:06.873000+00:00,"
Is something possible ? like subprocess.popen(myscript,[""answer1"",""answer2""])

Yes. If you know all the answers then you could use Popen.communicate() to pass the answers:
#!/usr/bin/env python
from subprocess import Popen, PIPE

process = Popen(myscript, stdin=PIPE, universal_newlines=True)
process.communicate(""\n"".join([""answer1"", ""answer2""]))

It may fail in some cases and you could use pexpect module (as @Robin Davis suggested), to workaround possible block-buffering issues or issues when input/output is outside of standard streams. See Q: Why not just use a pipe (popen())?",A,1
12428176,2012-09-14 16:06:10.007000+00:00,"Look at the lines before it, there will be a parenthesis or bracket missing.
That, or you have a python version that doesn't support with at all, the syntax wasn't introduced until python 2.6.",A,7
6128473,2011-05-25 17:33:33.303000+00:00,"Zope3 (or Zope Toolkit, ZTK) is a collection of Python libraries, used by various projects, with BlueBream being one of them, classic Zope2 another. As such, there is no need to provide specialized ""Products"" for these anymore.
You use the libraries like you would use other Python libraries; e.g. you write integration code that uses these libraries to build your application.
There may be server-specific packages, such as for Plone, BlueBream or Grok, that depend on features provided by those servers. The fact that those servers rely heavily on the Zope Toolkit is a happistance that has nothing to do with providing add-ons for those servers.",A,0
29220055,2015-03-23 20:47:45.810000+00:00,"You are missing a few rules:

Only the right-most 6 digits count.
Numbers with < 6 digits need padding; add zeros to the left until you have an even number of digits, with an odd last digit before the 5th digit.
The numbers are paired and then are reversed from highest to lowest.

This'll do all that:
def return_path(item_id):
    path = []
    # pad with zeros, grab only the last 6 digits
    str_id = format(item_id, '06d')[-6:]
    # any extra zero pairs at the start are in the path first
    while str_id.startswith('00'):
        path.append('00')
        str_id = str_id[2:]
    # for an odd number of padding zeros, swap out the 0 to the last digit
    if str_id[0] == '0':
        str_id = str_id[1:-1] + '0' + str_id[-1:]
    # grab digits in pairs starting at the end
    path += [str_id[i:i + 2] for i in range(len(str_id) - 2, -1, -2)]
    path.append(str(item_id))
    return os.path.join(*path)

Demo:
>>> return_path(1)
'00/00/01/1'
>>> return_path(12)
'00/00/12/12'
>>> return_path(123)
'00/03/12/123'
>>> return_path(1234)
'00/34/12/1234'
>>> return_path(12345)
'05/34/12/12345'
>>> return_path(123456)
'56/34/12/123456'
>>> return_path(1234567)
'67/45/23/1234567'
>>> return_path(12345678)
'78/56/34/12345678'
",A,3
9348358,2012-02-19 10:17:00.810000+00:00,"You could use subprocess.check_output() and str.splitlines():
from subprocess import check_output as qx

pids = map(int, qx([""pgrep"", procname]).splitlines())

To do it without an external process you could try psutil:
import psutil # pip install psutil

pids = [p.pid for p in psutil.process_iter() if p.name == procname]

Experiment with p.name, p.cmdline and various comparisons with procname to get what you need in your particular case.
And there is also os.getpid() to return the current process id.",A,0
41678753,2017-01-16 14:34:10.370000+00:00,"You should use enumerate.
for i, row in enumerate(tank_data): 
   print row[c], tank_data[i-1][c]
",A,0
32744568,2015-09-23 16:13:22.393000+00:00,"Just chain key look-ups:
outer_dictionary['a']['f']

Here dictionary['a'] returns the value for the key 'a', which is itself a dictionary.
You could also store the intermediary result:
nested_dictionary = outer_dictionary['a']
nested_dictionary['f']

This does the exact same thing, but also leaves another reference to the nested dictionary available as nested_dictionary.
Quick demo:
>>> nested_dictionary = {'a': {'b': 'c', 'd': 'e', 'f': 'g'}, 'h': {'i': 'j', 'k': 'l', 'm': 'n'}}
>>> nested_dictionary['a']
{'b': 'c', 'd': 'e', 'f': 'g'}
>>> nested_dictionary['a']['f']
'g'
",A,4
30510874,2015-05-28 15:26:52.050000+00:00,"Your traceback reveals that the wrong module is being imported:
  File ""/usr/local/lib/python3.4/dist-packages/werkzeug/serving.py"", line 61, in <module>
    from socketserver import ThreadingMixIn, ForkingMixIn
  File ""/home/samtheman/code/lasreader/rclick/socketserver.py"", line 25, in <module>
    MyServer(s.accept()).start()

See that second File line there? That's not the standard library socketserver, that's a different module altogether. As part of that module, it starts a socket server on import, so the Werkzeug import never completes and never gets to run properly.
Remove /home/samtheman/code/lasreader/rclick from your python path or remove that module altogether. ",A,2
31006487,2015-06-23 15:04:16.143000+00:00,core is defined in the __init__.py file: see the code.,A,3
4013953,2010-10-25 11:07:29.213000+00:00,"This code isn't complete - you've got two ForeignKeys from Friends to Person, so at least one of them will need a related_name attribute to prevent clashes. The code simply won't run as it is.
If a Person can only be friends with one other person, you should use a OneToOneField rather than a ForeignKey. Then, you can simply refer to my_person.friend1.person1.surname, where my_person is the Person you starting with, and friend1 is the related_name value I mentioned above.",A,0
21876487,2014-02-19 09:40:54.467000+00:00,"Only if you 'rewind' infile to the start again in between reads:
... infile.read()

infile.seek(0)

... infile.read()

A file is a lot like an audiotape; when you read a reading 'head' moves along the tape and returns the data. file.seek() moves the reading 'head' to a different position, infile.seek(0) moves it to the start again.
You'd be better off reading the file just once though:
content_of_infile = infile.read()

for result in re.findall('somestring(.*?)\}', content_of_infile, re.S):
    # ...

for result in re.findall('sime_other_string(.*?)\}', contents_of_infile, re.S):
    # ...
",A,0
45255064,2017-07-22 13:23:23.943000+00:00,"You need to add another pair of parentheses around yield ...:
def well_known_generator():
    print('start')
    print((yield 100))
    print((yield 200))
    print((yield 300))
    print('end')

The parentheses are part of the yield expression syntax:
yield_atom       ::=  ""("" yield_expression "")""

but the parentheses are optional when it's the only expression in an assignment statement or a statement expression:

The parentheses may be omitted when the yield expression is the sole expression on the right hand side of an assignment statement.

Inside a call expression (such as print(...)), the parentheses can't be omitted.",A,8
17569543,2013-07-10 11:31:01.857000+00:00,"Create a set from the last column; a set comprehension is easiest. If the length of the set is 1, all values in that column are the same:
if len({c[-1] for c in myList}) == 1:
    # all the same.

or as a function:
def same_last_column(multidim):
    return len({c[-1] for c in multidim}) == 1

Demo:
>>> myList = [['SomeVal', 'Val',True],
...          ['SomeVal', 'blah', True]]
>>> len({c[-1] for c in myList}) == 1
True
>>> myList = [['SomeVal', 'Val',False],
...          ['SomeVal', 'blah', True]]
>>> len({c[-1] for c in myList}) == 1
False
",A,3
1099353,2009-07-08 17:05:24.773000+00:00,"Next payment date is an example of a ""fact-free fact table"".  There's no measure, just FK's between at least two dimensions: the security and time.
You can denormalize the security to have a type-1 SCD (overwritten with each load) that has a few important ""next payment dates"".
I think it's probably better, however, to carry a few important payment dates along with the facts.  If you have a ""current balance"" fact table for loans, then you have an applicable date for this balance, and you can carry previous and next payment dates along with the balance, also.  
For the whole payment schedule, you have a special fact-free fact table that just has applicable date and the sequence of payment dates on into the future.  That way, when the schedule changes, you can pick the payment sequence as of some particular date.",A,1
24086389,2014-06-06 16:15:33.703000+00:00,"You are creating a new object without any other references and Python re-uses the memory location when the object is destroyed again after id() is done with it. In CPython, the result of id() happens to be the memory location of the object. From the id() function documentation:

CPython implementation detail: This is the address of the object in memory.

The line id(12345) creates a new int() instance; because it is bound as an argument to id() it has 1 reference to it. id() determines the memory location, and returns that result. In returning, the arguments are cleaned up and the reference count to the int() instance drops to 0, and Python cleans it up. The memory is freed.
The next line creates a new int() instance. There is memory available at the same location, so it is reused.
When, instead, you first bind the new object no a name, you created an extra reference to the object and it is not cleared up. The memory is not freed, and a new object will have to use a new memory address.
This part too is documented, again from the id() documentation:

This is an integer (or long integer) which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value.

Emphasis mine.
When rebinding (x = [] when x is already set) Python first creates a new object, then rebinds x to point to that new object. This unbinds the old list after the new list is created. This means the old memory location is still occupied when the new list object is created.
To map that out to specific steps:

You create an object with id() == 4301901696.
4301901696 is bound to x -> Reference count for 4301901696 is 1.
You create an object with id() == 4301729448.
4301901696 is unbound from x. Reference count for 4301901696 drops to 0 and is cleared from memory.
4301729448 is bound to x. Reference count for 4301729448 is 1.
You create a new object, 4301901696 is empty so the new object gets id() == 4301901696.
4301729448 is unbound from x. Reference count for 4301729448 drops to 0 and is cleared from memory.
4301901696 is bound to x. Reference count for 4301901696 is 1.
etc.

This too is part of the documentation, the assignment statement documenation tells you what order assignment takes place in:

An assignment statement evaluates the expression list [...] and assigns the single resulting object to each of the target lists, from left to right.

where expression list is everything on the right-hand-side of the = symbol.",A,9
35140588,2016-02-01 21:23:18.250000+00:00,"Since you made it an instance method, you would need to call it on the instance you created in the __new__ method (which is static; e.g. not bound to an instance or a class):
self.browser = webdriver.Ie(self._instance.get_driver_path(driver))

You could make that method static or a class method instead, at which point you can use it via the first argument to __new__ (which is the class, not an instance; usually you don't use the name self but cls instead):
@staticmethod
def get_driver_path(driver):

Note that a staticmethod then doesn't take a self argument, but your get_driver_path() implementation doesn't use that anyway.",A,2
29673951,2015-04-16 11:55:15.873000+00:00,"Use the messages framework to add a message in the first view and display it in the second one.
Or simply put an item in the session, and pop it out again:
request.session['signup'] = True
return redirect('wherever')

...
signup = request.session.pop('signup', False)
if signup:
    ... do something ...
",A,18
14465076,2013-01-22 18:01:20.760000+00:00,"The date function is used as the default function for the dumps function; the function is called for any type that JSON doesn't know how to handle.
The JSON standard only knows about numbers, strings, booleans, nulls (None in python), objects (dicts in python) and lists. If you want to handle anything outside of that limited set of types, you need to express that 'something' in terms of those types.
The default() is used for that purpose; it is called for any object that cannot be serialized to JSON as-is. A datetime object is one such type. When you a datetime object to json.dumps(), the default function will be asked to create a serializable version of it.
Note that returning obj is not the correct way to handle types you don't know how to convert. Raise TypeError instead:
def date(obj):
    try:
        return obj.isoformat()
    except AttributeError:
        raise TypeError()
",A,1
18389676,2013-08-22 19:53:56.930000+00:00,"The problem is this line:
for file in sub_dir:

If you print(file) each time through the loop, it should be obvious what's wrong: subdir is a string, so you're just iterating over the characters of that string one by one.
What you probably wanted here is:
for file in os.listdir(sub_dir):

As the docs explain, listdir:

Return a list containing the names of the entries in the directory given by path. The list is in arbitrary order. It does not include the special entries '.' and '..' even if they are present in the directory.
",A,8
52953004,2018-10-23 15:41:41.857000+00:00,"You'll have to parse your file as text, and extract the information using text manipulation techniques.
You could use a regular expression here:
import re

hello_pattern = re.compile(r'Hello\(i=(\d+), time=(\d+), x=(\d+), y=(\d+)\)')

This pattern matches the representation of your namedtuple values, and gives you access to the numbers for each values, albeit as strings:
>>> line = '[Hello(i=0, time=0, x=0, y=0), Hello(i=0, time=0, x=0, y=0),...]'
>>> hello_pattern.findall(sample)
[('0', '0', '0', '0'), ('0', '0', '0', '0')]

You can then convert those strings to integers again and recreate the instances; for a single line the list comprehension would be:
[Hello(*map(int, match)) for match in hello_pattern.findall(line)]

and a demo of this on the same line:
>>> from collections import namedtuple
>>> Hello = namedtuple('Hello', 'i time x y')
>>> [Hello(*map(int, match)) for match in hello_pattern.findall(sample)]
[Hello(i=0, time=0, x=0, y=0), Hello(i=0, time=0, x=0, y=0)]

So the complete solution would be:
import re
from collections import namedtuple

Hello = namedtuple('Hello', 'i time x y')  # or import your own ready definition
hello_pattern = re.compile(r'Hello\(i=(\d+), time=(\d+), x=(\d+), y=(\d+)\)')

data = []
with open('file.txt') as inputfile:
    for line in inputfile:
        recovered = [
            Hello(*map(int, match))
            for match in hello_pattern.findall(line)
        ]
        data.append(recovered)
",A,4
19871410,2013-11-09 01:39:53.130000+00:00,"For the question as originally asked, replacing the backslash-hex sequences in strings like ""\\03www\\06google\\03com\\0"" with dots…
If you want to do this with a regular expression:

\\ matches a backslash.
[0-9A-Fa-f] matches any hex digit.
[0-9A-Fa-f]+ matches one or more hex digits.
\\[0-9A-Fa-f]+ matches a backslash followed by one or more hex digits.

You want to find each such sequence, and replace it with a dot, right? If you look through the re docs, you'll find a function called sub which is used for replacing a pattern with a replacement string:
re.sub(r'\\[0-9A-Fa-f]+', '.', DNS)

I suspect these may actually be octal, not hex, in which case you want [0-7] rather than [0-9A-Fa-f], but nothing else would change.

A different way to do this is to recognize that these are valid Python escape sequences. And, if we unescape them back to where they came from (e.g., with DNS.decode('string_escape')), this turns into a sequence of length-prefixed (aka ""Pascal"") strings, a standard format that you can parse in any number of ways, including the stdlib struct module. This has the advantage of validating the data as you read it, and not being thrown off by any false positives that could show up if one of the string components, say, had a backslash in the middle of it.
Of course that's presuming more about the data. It seems likely that the real meaning of this is ""a sequence of length-prefixed strings, concatenated, then backslash-escaped"", in which case you should parse it as such. But it could be just a coincidence that it looks like that, in which case it would be a very bad idea to parse it as such.",A,1
574910,2009-02-22 13:17:28.767000+00:00,"The ESB is best applied to building ""Composite"" applications.
First, you have to expose a lot of fine-grained services from a lot of discrete applications.  
This sets the stage for building composite applications.
The point is to create composite services that don't exist in any single application.  These services only exist in the ESB.  They are built from fine-grained services.  
Note that the composites rely on fine-grained services, both of which live in the ESB, reducing the overheads involved in locating and executing the fine-grained services.  However, the real work is done by external applications, which introduces some overhead.
Note that the performance ESB-based applications so totally defeats other methods of interaction that wringing your hands over ""latency"" misses the huge victory from immediate, direct integration.",A,4
16144392,2013-04-22 10:05:27.633000+00:00,"You are parsing XML that doesn't have child nodes for that specific slidename[a] element. You probably want to skip that one.
You are not using Python loops to their full potential. There is no need to loop over indices when you can loop directly over the list itself:
for a in tagname:
    try:
        print a.childNodes[0].nodeValue
    except IndexError:
        print 'No childNodes for this element'
",A,3
14892749,2013-02-15 10:31:32.153000+00:00,"According to this announcement the purpose of chunked upload is to make it possible to deal with spotty connections by letting you upload a large file in chunks instead. It's not about deduplication.
If you take a look through the Core API documentation (not that much to read, really), there is no mention anywhere of de-duplication being offered through the API. Wether you use Python or any other language or library, without the published API supporting de-duplication, there is no way you can access this functionality.",A,1
12273383,2012-09-05 01:28:30.790000+00:00,"If you want to count occurrences of the pattern:
count_regex = sum(1 for _ in re.finditer(r'ACG[AG][CT]G', s))

If you want to count occurrences of a fixed string that matches first the pattern:
m = re.search(r'ACG[AG][CT]G', s)
count_fixed = s.count(m.group(0), m.start(0)) if m else 0
",A,1
49399032,2018-03-21 05:46:30.217000+00:00,"Separate processes do not share memory. Changes you make in one child won't be visible in another.
The docs explain this, and also why you often don't want shared state (you need to add synchronization, which is tricky to get right), and also what you can do instead.
You can use explicit shared memory, but only with basic ""C types"" like 32-bit integers or arrays of floats. And then you have to use explicit synchronization like Lock in the right places to make your code safe.
The ideal solution, when possible, is to redesign your code to pass data over queues instead of sharing it.
If that's not appropriate for your problem, you can use a Manager, which basically fakes shared high-level Python objects on top of a queue. This can be excruciatingly slow for some uses, but when it isn't, it can be the simplest answer.",A,1
22570114,2014-03-21 21:47:14.790000+00:00,"You appear to have Python literals in a string. Use ast.literal_eval() to parse these as Python would.
Demo:
>>> from ast import literal_eval
>>> literal_eval('[1,2,3]')
[1, 2, 3]
>>> literal_eval(""['1','2','3']"")
['1', '2', '3']

ast.literal_eval() can handle None, booleans, numbers, strings, unicode strings, tuple, list, dictionary and set literals, arbitrarily nested.",A,2
39642072,2016-09-22 14:37:57.390000+00:00,"This validation isn't being done by Django, but by your browser itself. You can disable that by putting novalidate in the surrounding <form> element.",A,4
36642457,2016-04-15 08:49:56.840000+00:00,"No, string comparison would not work here; the values do not sort according to the full date, but according to the month and day of the month, with the first 10 days of the month eratically dispersed throughout that sort as the day number is not zero-padded. If the year was listed first, followed by a zero-padded numeric month, day, hours (using a 24 hour clock) and minutes, then you could do it as strings only, because then the lexicographical sort happens to match the way the date and time would be sorted.
Instead, parse your string to a datetime object using the datetime.datetime.strptime() class method:
dt = datetime.datetime.strptime(string, '%b %d, %Y %I:%M %p')
if dt < datetime.datetime.now():
    # in the past.

Demo:
>>> import datetime
>>> string = 'Apr 17, 2016 02:00 AM'
>>> dt = datetime.datetime.strptime(string, '%b %d, %Y %I:%M %p')
>>> dt
datetime.datetime(2016, 4, 17, 2, 0)
>>> dt < datetime.datetime.now()
False

If you were to format your dates using the ISO8601 combined date-time format, then you'd have a format that can be sorted as strings only:
>>> datetime.datetime.now().isoformat()
'2016-04-15T09:55:09.907130'
",A,5
51254039,2018-07-09 21:07:01.163000+00:00,"Daniel has the correct cause, but the solution is much simpler; just remove the inner template tag completely, along with the quotes.
{% url 'tender_list' date=tenderEnquiries.d_assigned|date:""d-m-Y"" %}
",A,0
23281871,2014-04-24 23:52:18.520000+00:00,"There is no ""-u userfoo"" user. There is probably just ""userfoo"". Note: no -u prefix. Try:
from plumbum.cmd import sudo

as_userfoo = sudo[""-u"", sudo_user]
print(as_userfoo(""whoami""))
",A,3
21655159,2014-02-09 04:09:58.863000+00:00,"The page uses broken HTML, and different parsers will try to repair it differently. Install the lxml parser, it parses that page better:
>>> BeautifulSoup(html, 'html.parser').find(""div"",{""id"":""cntPos""}).find(""table"",{""class"":""cntTb""}).tbody.find_all(""tr"")[1].find(""td"",{""class"":""cntBoxGreyLnk""}) is None
True
>>> BeautifulSoup(html, 'lxml').find(""div"",{""id"":""cntPos""}).find(""table"",{""class"":""cntTb""}).tbody.find_all(""tr"")[1].find(""td"",{""class"":""cntBoxGreyLnk""}) is None
False

This doesn't mean that lxml will handle all broken HTML better than the other parser options. Also look at html5lib, a pure-Python implementation of the WHATWG HTML spec and thus more closely follows how current browser implementations handle broken HTML.",A,5
25069673,2014-07-31 22:01:42.967000+00:00,"What you're doing here is almost just zip.
You want a flat iterable, rather than an iterable of sub-iterables, but chain fixes that.
And you want to take only the first N values, but islice fixes that.
So, if the lengths are all equal:
>>> list(chain.from_iterable(zip(a, b)))
[1, 6, 2, 7, 3, 8, 4, 9, 5, 10]
>>> list(islice(chain.from_iterable(zip(a, b)), 7))
[1, 6, 2, 7, 3, 8, 4]

But if the lengths aren't equal, that will stop as soon as the first iterable finishes, which you don't want. And the only alternative in the stdlib is zip_longest, which fills in missing values with None.
You can pretty easily write a zip_longest_skipping (which is effectively the round_robin in Peter's answer), but you can also just zip_longest and filter out the results:
>>> list(filter(None, chain.from_iterable(zip_longest(a, b, c, d))))
[1, 6, 1, 2, 2, 7, 3, 4, 3, 8, 5, 6, 4, 9, 7, 8, 5, 10]

(Obviously this doesn't work as well if your values are all either strings or None, but when they're all positive integers it works fine… to handle the ""or None"" case, do sentinel=object(), pass that to zip_longest, then filter on x is not sentinel.)",A,3
4318967,2010-11-30 21:40:34.387000+00:00,"It doesn't make sense to check ""XmlHttpRequest"" in Python. That's a Javascript wrapper around a piece of browser functionality, not anything that Python knows or cares about.
If you really need to check that the POST is coming from Ajax, you can check the request headers - jQuery, like other JS frameworks, always sets a HTTP_X_REQUESTED_WITH header. And, in fact, there's an example that uses that exact header on the Bottle documentation for Accessing Request data.",A,0
21941670,2014-02-21 17:52:44.720000+00:00,"You cannot create weak references to method objects. Method objects are short lived; they are created on the fly as you access the name on the instance. See the descriptor howto how that works.
When you access a method name, a new method object is created for you, and when you then add that method to the WeakSet, no other references exist to it anymore, so garbage collection happily cleans it up again.
You'll have to store something less transient. Storing instance objects themselves would work, then call a predefined method on the registered callbacks:
def __del__(self):
    for f in self.destroyCallback:
        f.destroyedObjectListener(self)

and to register:
a1.destroyCallback.add(b)

You can also make b itself a callable by giving it a __call__ method:
class ClassB:
    def __call__(self,obj):
        print('ClassB object %d is called because obj %d '
              'is being destroyed' % (id(self), id(obj)))

Another approach would be to store a reference to the underlying function object plus a reference to the instance:
import weakref


class ClassA:
    def __init__(self):
        self._callbacks = []

    def registerCallback(self, callback):
        try:
            # methods
            callback_ref = weakref.ref(callback.__func__), weakref.ref(callback.__self__)
        except AttributeError:
            callback_ref = weakref.ref(callback), None
        self._callbacks.append(callback_ref)

    def __del__(self):
        for callback_ref in self._callbacks:
            callback, arg = callback_ref[0](), callback_ref[1]
            if arg is not None:
                # method
                arg = arg()
                if arg is None:
                    # instance is gone
                    continue
                callback(arg, self)
                continue
            else:
                if callback is None:
                    # callback has been deleted already
                    continue
                callback(self)

Demo:
>>> class ClassB:
...     def listener(self, deleted):
...         print('ClassA {} was deleted, notified ClassB {}'.format(id(deleted), id(self)))
... 
>>> def listener1(deleted):
...     print('ClassA {} was deleted, notified listener1'.format(id(deleted)))
... 
>>> def listener2(deleted):
...     print('ClassA {} was deleted, notified listener2'.format(id(deleted)))
... 
>>> # setup, one ClassA and 4 listeners (2 methods, 2 functions)
... 
>>> a = ClassA()
>>> b1 = ClassB()
>>> b2 = ClassB()
>>> a.registerCallback(b1.listener)
>>> a.registerCallback(b2.listener)
>>> a.registerCallback(listener1)
>>> a.registerCallback(listener2)
>>> 
>>> # deletion, we delete one instance of ClassB, and one function
... 
>>> del b1
>>> del listener1
>>> 
>>> # Deleting the ClassA instance will only notify the listeners still remaining
... 
>>> del a
ClassA 4435440336 was deleted, notified ClassB 4435541648
ClassA 4435440336 was deleted, notified listener2
",A,16
50598390,2018-05-30 07:08:21.477000+00:00,"You are supposed to use manage.py, not django-admin.py.",A,-1
4832098,2011-01-28 19:11:42.500000+00:00,"When things are ill-structured, you don't need much of a tool.
http://www.w3.org/2001/tag/doc/leastPower.html
You need a lot of patience and clarity to get from what you have to more formal requirements.
Plain-old word-processing is often best.
Since you want to do estimating, a spreadsheet is about all the structure that the problem can stand right now.
A big-old-matrix with requirements on one axis and assumptions on the other will allow you to teak, adjust and assess impact.
If you spend time loading all your questions and answers into some tool, you spend a lot of time playing with the tool -- not the issues.  Also, as ideas come and go, you hate to delete the really EPIC FAIL ideas from the tool. 
Often, you should feel free to start again ""from scratch"", discarding the bad ideas.
Write, write and rewrite until the questions, answers and requirements get to a level of manageability.
Then migrate the easy stuff until a more rigid and formal tool.  Leave the complex, ill-defined and unfocused stuff in a word processor.",A,1
25785081,2014-09-11 10:28:09.833000+00:00,"You don't appear to be calling rev.save() anywhere, so changes will not be persisted to the database.",A,2
21699389,2014-02-11 10:35:23.373000+00:00,"You could use itertools.islice and map():
from itertools import islice

strengthA, skillA, strengthB, skillB = map(float, islice(f, 4))

map() loops over the second argument, calling float() on each value. islice(f, 4) will produce 4 elements from f when looped over, then stops producing more. Together with the tuple-assignment, that means 4 float values are produced to be assigned to the 4 names on the left.
This does require that you do not use .readline() on the file object elsewhere, and only iterate over the object. Iteration uses an internal buffer that .readline() does not use, so mixing iteration and .readline() will lead to strange results (e.g. skipped file contents). Use next(f) instead of f.readline() to read a single line, or use looping.",A,2
17226978,2013-06-21 02:51:46.010000+00:00,"On Unix shell=True implies that 2nd and following arguments are for the shell itself, use a string to pass a command to the shell:
import subprocess

cmd = 'echo $HOME'
proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
print proc.communicate()[0],

You could also write it as:
import subprocess

cmd = 'echo $HOME'
print subprocess.check_output(cmd, shell=True),

From the subprocess' docs:

On Unix with shell=True, the shell defaults to /bin/sh. If args is a
  string, the string specifies the command to execute through the shell.
  This means that the string must be formatted exactly as it would be
  when typed at the shell prompt. This includes, for example, quoting or
  backslash escaping filenames with spaces in them. If args is a
  sequence, the first item specifies the command string, and any
  additional items will be treated as additional arguments to the shell
  itself. That is to say, Popen does the equivalent of:

Popen(['/bin/sh', '-c', args[0], args[1], ...])
",A,10
28111947,2015-01-23 14:22:13.227000+00:00,"You could use .communicate() method to pass data to the child process:
cmd = '/usr/sbin/sendmail -t -i'.split()
if len(sys.argv) > 2:
  cmd += [""-f"", sys.argv[2]]
p = Popen(cmd, stdin=PIPE, close_fds=True)
p.communicate(data)

Notice that you don't need shell=True here.
I've removed stdout=PIPE because I don't see the output used in your code. If you want to suppress the output; see How to hide output of subprocess in Python 2.7.",A,0
21167760,2014-01-16 16:45:58.497000+00:00,"The setup.py test runner is rather limited; it only supports letting you specify a specific module. The documentation for the command-line switches is given when you use the --help switch:
python setup.py test --help
Common commands: (see '--help-commands' for more)
  [ ... cut ... ]

Options for 'test' command:
  --test-module (-m)  Run 'test_suite' in specified module
  --test-suite (-s)   Test suite to run (e.g. 'some_module.test_suite')

  [ ... more cut ... ]

so python setup.py test -m your.package.tests.test_module would limit running the tests from the test_module.py file only.
All the test command does, really, is make sure your egg has been built already, extract the test_suite value from setup() metadata, configure a test loader that understands about zipped eggs, then run the unittest.main() function.
If you need to run a single test only, have already built your egg, are not running this with a zipped egg, then you can also just use the unittest command line interface, which does pretty much everything else:
python -m unittest yourpackage.tests.TestClass.test_method

would instruct unittest to only run a very specific test method.",A,15
26069657,2014-09-26 23:22:52.323000+00:00,"bear_moved is not compared with choice. It is tested for boolean truth only if choice == ""taunt bear"" is true.
The test if bear_moved: on its own is true or false based entirely on the value of bear_moved. So if bear_moved = False, then the test if bear_moved: is going to fail. IF bear_moved = True, then if bear_moved: is going to succeed:
>>> bear_moved = False
>>> if bear_moved:
...     print('The bear moved')
... 
>>> bear_moved = True
>>> if bear_moved:
...     print('The bear moved')
... 
The bear moved

not inverts the test; now when you set it as bear_moved = False, if not bear_moved: is going to succeed, and vice versa:
>>> bear_moved = False
>>> if not bear_moved:
...     print('The bear has not moved')
... 
The bear has not moved
>>> bear_moved = True
>>> if not bear_moved:
...     print('The bear has not moved')
... 
>>> 

Coming back to choice, the test first looks at choice == ""taunt bear"". If that is true, only then does and also test bear_moved. So for if choice == ""taunt bear"" and bear_moved: to succeed, two things must be true: choice must be set to ""taunt bear"" and the value of bear_moved must be true.
In the while loop, the first time around, bear_moved is set to False. If you pick taunt bear, the test elif choice == ""taunt bear"" and not bear_moved: is going to be true: choice == ""taunt bear"" is true, and not bear_moved is true because bear_moved is False. A message is printed and bear_moved is now changed to True.
The while loop continues, and you can pick ""taunt bear"" again, but this time bear_moved is set to True. The test elif choice == ""taunt bear"" and not bear_moved: is not going to succeed, because even though choice == ""taunt bear"" is true, the not bear_moved part is not true! However, the other test, elif choice == ""taunt bear"" and bear_moved: is true now, and thus you die.
Incidentally, the moment bear_moved = True has been executed, the elif choice = ""open door"" and bear_moved: test becomes available, provided you used ""open door"" as a choice of course.",A,2
48244188,2018-01-13 20:51:57.857000+00:00,"Just as Records is a list, so you use ['Records'][0]['Sns']..., so is Dimensions, so again you need to access the first element.",A,1
4159852,2010-11-11 22:21:47.580000+00:00,"Python
import csv
with open( ""some_file.csv"", ""rb"" ) as source:
    rdr= csv.reader( source )
    col_5= 0
    for row in rdr:
        col_5 += row[5]
print col_5

Not a one-liner, but pretty terse.",A,1
3562514,2010-08-25 02:45:51.507000+00:00,"RESTful web services are what many folks do for this.
Start here: http://en.wikipedia.org/wiki/Representational_State_Transfer
Then revise this to be a more specific question.
You might need to look at direct VPN/SSL connection from their server to yours.",A,1
27278515,2014-12-03 17:59:47.683000+00:00,"Use a stack:
def build_multilevel(entries):
    result = []
    stack = [result]
    for i, entry in enumerate(entries):
        if entry == 'OPEN':
            # convert last element of the top-most list on the stack
            # to a new, nested list, and push that new list on top
            stack[-1][-1] = [stack[-1][-1]]
            stack.append(stack[-1][-1])
        elif entry == 'CLOSE':
            stack.pop()
        else:
            stack[-1].append(entry)
    return result

Demo:
>>> L1=[
... 'YYYYY', 'OPEN',  ' 111',   ' 222', 'CLOSE', 
... 'XXXX','OPEN', '  333', '  444', 'OPEN', '    555', '    666', 'CLOSE','CLOSE'
... ]
>>> def build_multilevel(entries):
...     result = []
...     stack = [result]
...     for i, entry in enumerate(entries):
...         if entry == 'OPEN':
...             # convert last element of the top-most list on the stack
...             # to a new, nested list, and push that new list on top
...             stack[-1][-1] = [stack[-1][-1]]
...             stack.append(stack[-1][-1])
...         elif entry == 'CLOSE':
...             stack.pop()
...         else:
...             stack[-1].append(entry)
...     return result
... 
>>> build_multilevel(L1)
[['YYYYY', ' 111', ' 222'], ['XXXX', '  333', ['  444', '    555', '    666']]]
",A,1
23193242,2014-04-21 07:53:05.327000+00:00,"That's what the minidom always does; if you set the newl argument to writexml() to '\n', then the doctype line is wrapped.
There is no way around this feature other than not set the newl to '\n'.",A,0
30788186,2015-06-11 18:02:05.757000+00:00,"You return on your first test:
for n in phonebook:
    if name == n[""fname""]:
        return n[""id""]
    else:
        print ""No match found.""
        return 0

So if the first entry in your phonebook, bound to n, is a match (so name = ""Mario""), you return n['id'], otherwise you return 0. Your for loop never gets to iterate over the rest of the phonebook here as you exited the function already.
Remove the else block altogether; you won't know if there is no match until you have tried all entries, so put your print after the for loop:
for n in phonebook:
    if name == n[""fname""]:
        return n[""id""]

print ""No match found.""
return 0
",A,5
15832988,2013-04-05 11:38:58.327000+00:00,"Split the text first, then convert to int:
[map(int, elem.split()) for elem in originallist]

For Python 3, where map() returns an generator, not a list, you can nest the list comprehension:
[[int(n) for n in elem.split()] for elem in originallist]

which would work equally well under Python 2.
Quick demo:
>>> originallist = ['136 145', '136 149', '137 145', '138 145', '139 145', '142 149', '142 153', '145 153']
>>> [[int(n) for n in elem.split()] for elem in originallist]
[[136, 145], [136, 149], [137, 145], [138, 145], [139, 145], [142, 149], [142, 153], [145, 153]]

You can remove the nesting by moving the elem.split() loop to the outer list comprehension, to the end:
[int(n) for elem in originallist for n in elem.split()]

which gives:
[136, 145, 136, 149, 137, 145, 138, 145, 139, 145, 142, 149, 142, 153, 145, 153]
",A,2
46163696,2017-09-11 20:13:47.153000+00:00,"There is no name function defined in Python, no. Annotations are still Python expressions and must reference valid names.
You can instead use type hinting to say bar is a callable; use typing.Callable:
from typing import Callable

def foo(bar: Callable[[], Any]):
    bar()

This defines a callable type that takes no arguments and whose return value can be anything (we don't care).",A,7
15537072,2013-03-20 23:59:05.290000+00:00,"I don't think there's a good way to eliminate the three if order statements without reorganizing the algorithm. The position at which ret.append happens is dependent on the value, so you pretty much have to check it all three times, one way or another.
But there's one obvious way to reorganize things to remove a couple ifs:
def traverse(node, order):
    if node is None: return
    if order == Order.PREORDER: ret.append(node.data)
    traverse(node.right, order)
    if order == Order.INORDER: ret.append(node.data)
    traverse(node.down, order)
    if order == Order.POSTORDER: ret.append(node.data)

Of course it's one line longer, but it's only 4 ifs instead of 6.
Another possibility is to change things to keep track of all three positions, and insert into the appropriate position after the fact:
def traverse(node, order):
    if node is None: return
    prepos = len(ret)
    traverse(node.right, order)
    inpos = len(ret)
    traverse(node.down, order)
    postpos = len(ret)
    pos = (prepos, inpos, postpos)[order]
    ret[pos:pos+1] = node.data

This removes all of the ifs, but I don't think the result is exactly easier to read or comprehend…
Really, the way to make this easier to read and comprehend is probably to switch to a functional algorithm (recursive mutable algorithms are rarely fun to think through)… but that's just going to make the three if bodies larger, not get rid of them.",A,2
20117693,2013-11-21 09:59:43.847000+00:00,"From the documentation to ast.literal_eval:

Safely evaluate an expression node or a string containing a Python
  expression.  The string or node provided may only consist of the following
  Python literal structures: strings, numbers, tuples, lists, dicts, booleans,
  and None.

The whole point of that function is that you can only use built-in datatypes. datetime.date is not one.",A,0
19111876,2013-10-01 08:53:23.560000+00:00,"You are looking for the heapq module; a heap lets you keep the top N items, efficiently, as you go along. There is even a dedicated function for your exact usecase:
from heapq import nlargest
from operator import itemgetter

largest_15 = nlargest(15, (line.split()[:2] for line in f), itemgetter(1))

This passes in a generator expression, taking care of the loop efficiently.
Under the hood, this method:

takes the first 15 elements and creates a heap from that.
calls heappushpop() on the heap for all the following items; this adds the item to the heap, then removes the smallest item. As a result the heap only ever contains the 15 largest items.
when the iterable is done, the heap list is sorted.
",A,3
15862023,2013-04-07 11:50:32.220000+00:00,"You could create yourself a parser, instead of using document_fromstring:
from cStringIO import StringIO
from lxml import etree

for num, entry in enumerate(d.entries):
    text = entry.content[0]['value'].encode('utf8')
    parser = etree.HTMLParser()
    tree   = etree.parse(StringIO(text), parser)
    print  ''.join(tree.xpath('.//text()'))

For Blogger.com Atom feed exports, this works to print the text content of the .content[0].value entry.",A,4
51459398,2018-07-21 19:25:33.530000+00:00,"If you know how to extract the numbers with comma groupings, the easiest thing to do is just transform that into something int can handle:
for match in matches:
    i = int(match.replace(',', ''))

For example, if match is '1,289,868', then match.replace(',', '') is '1289868', and obviously int(<that>) is 1289868.",A,1
21255459,2014-01-21 10:28:48.830000+00:00,"The problem is that grid returns None, not self.
So, when you do this:
decimalView = ttk.Entry(mainframe, state = DISABLED, background = ""gray99"", width = 30, textvariable = decimal).grid(column = 2, row = 2, sticky = W)

… you're setting decimalView to None. Which is why the error message tells you that it can't find an xview attribute on None.
And this isn't some weird quirk of Tkinter; almost every method in Python that mutates an object in any way returns None (or, sometimes, some other useful value—but never self), from list.sort to file.write.
Just write it on two lines: construct the Entry and assign it to decimalView, and then grid it.
Besides the minor benefit of having working code, you'll also have more readable code, that doesn't scroll off the right side of the screen on StackOverflow or most text editors.",A,7
38931940,2016-08-13 10:53:22.583000+00:00,"You didn't return the response from the super call in post.
Note it's rarely a good idea to override that method, and you certainly didn't need to here. You can access those kwargs directly in form_valid.",A,0
3077079,2010-06-19 20:17:35.640000+00:00,"The problem is simply that you're getting the content types defined twice - once when you do syncdb, and once from the exported data you're trying to import. Since you may well have other items in your database that depend on the original content type definitions, I would recommend keeping those.
So, after running syncdb, do manage.py dbshell and in your database do TRUNCATE django_content_type; to remove all the newly-defined content types. Then you shouldn't get any conflicts - on that part of the process, in any case.",A,32
54723239,2019-02-16 12:49:35.427000+00:00,"The click function only applies to elements that exist when it is defined. Instead you should use on so it applies to elements that are added dynamically:
$('.infinite-container').on(""click"", "".like-btn"", function(e){
",A,0
22339878,2014-03-12 00:57:49.190000+00:00,"Your twos_comp(val, bits) function expects arguments to be integers. You could get integers directly without converting input into '0xff' strings first using bytearray:
>>> data = '\xff\xff\xff\xff'
>>> b = bytearray(data)
>>> b[0] == 0xff
True
",A,0
17286074,2013-06-24 22:16:48.347000+00:00,"Same as @perreal's and @Elazar's answers, but with better names:
from collections import defaultdict

size = defaultdict(int)
for _, group_id in item_labels:
   size[group_id] += 1

item_labels.sort(key=lambda (_, group_id): size[group_id])
print item_labels
# -> [('c', 1), ('b', 2), ('e', 2), ('a', 3), ('d', 3), ('f', 3)]
",A,2
52539313,2018-09-27 14:25:08.663000+00:00,"You are passing the result from encrypt_data() directly to decrypt_data():
print(decrypt_data(encrypt_data(...))

encrypt_data() returns the encrypted data, not a filename:
       encrypted_file = encryptor.encrypt(chunk) 
return encrypted_file

(You are only producing the last chunk of encrypted data there, not all of it, but that’s not the cause of this error).
decrypt_data() doesn’t accept encrypted data however. It accepts a filename:
def decrypt_data(in_fd, chunk_size, priv_key):
    # ...
    with open(in_fd, 'rb') as in_file:

What threw me at first was that on anything but Windows that’ll give you a “file not found” error, but on Windows, a binary value for a file path will first be decoded as UTF-8, and that fails for the encrypted data. 
To fix this, you have three options:

Have the encryption function open a new file, write the encrypted data to that file and return the filename instead of the encrypted data. Then you at least pass on the correct information to the decryption function. 
Create the file for the encrypted data the encrypted data the encryption function returns at the point where you call the encryption function. Don’t pass the result directly to the decryption function, pass the filename. 
Change the decryption function to accept the data directly and not read it from a file. 

As a side note, in the decryption function you use:
decryptor.decrypt(eval(str(chunk)))

That's... a rather odd way of passing chunk directly to the decryption function. This is enough:
decryptor.decrypt(chunk)

chunk is a bytes object, str(bytesvalue) gives you ""b'...'"" (where b' at the start and ' at the end are now part of the string) and eval() gives you the original bytesvalue again. Just pass the original in, no need to waste cycles there.",A,0
26245655,2014-10-07 21:29:56.973000+00:00,"When you write for line in f:, that loops over the file, reading one line at a time.
Then, inside that loop, you ignore line and instead do f.readline(), which reads the next line.
So, you read line 0, ignore it, read line 1, and process it. Then read line 2, ignore it, read line 3, and process it. And so on.
Just use line instead of f.readline() and everything will be fine.",A,3
21584663,2014-02-05 17:50:12.993000+00:00,"You can use a dictionary view with set operations to select only keys from your input sequence:
min(f.viewkeys() & o, key=f.get)

In Python 3, all you need to use is:
min(f.keys() & o, key=f.get)

as the dict.keys() method now returns dictionary views.
Alternatively, you could use a key function that excludes keys you don't want to include:
set_o = set(o)
min(f, key=lambda k: f[k] if k in set_o else float('inf'))

I'm using a set here to make membership tests efficient. float('inf') (positive infinity) is guaranteed to always test larger than any other value.
Demo:
>>> o = [1, 2, 4, 6]
>>> f = {1: 10, 2: 5, 3: 1, 4: 3, 5: 7, 6: 9}
>>> min(f.viewkeys() & o, key=f.get)
4
>>> set_o = set(o)
>>> min(f, key=lambda k: f[k] if k in set_o else float('inf'))
4
",A,4
661300,2009-03-19 07:19:50.483000+00:00,"Update: On Python 2.6+, use ctypes.get_errno().
Python 2.5
Belowed code is not reliable (or comprehensive, there are a plefora of ways errno could be defined) but it should get you started (or reconsider your position on a tiny extension module (after all on Debian python setup.py install or easy_install should have no problem to build it)).  From  http://codespeak.net/pypy/dist/pypy/rpython/lltypesystem/ll2ctypes.py
if not hasattr(ctypes, 'get_errno'):
    # Python 2.5 or older
    if sys.platform == 'win32':
        standard_c_lib._errno.restype = ctypes.POINTER(ctypes.c_int)
        def _where_is_errno():
            return standard_c_lib._errno()

    elif sys.platform in ('linux2', 'freebsd6'):
        standard_c_lib.__errno_location.restype = ctypes.POINTER(ctypes.c_int)
        def _where_is_errno():
            return standard_c_lib.__errno_location()

    elif sys.platform in ('darwin', 'freebsd7'):
        standard_c_lib.__error.restype = ctypes.POINTER(ctypes.c_int)
        def _where_is_errno():
            return standard_c_lib.__error()
    ctypes.get_errno = lambda: _where_is_errno().contents.value 

Where standard_c_lib:
def get_libc_name():
    if sys.platform == 'win32':
        # Parses sys.version and deduces the version of the compiler
        import distutils.msvccompiler
        version = distutils.msvccompiler.get_build_version()
        if version is None:
            # This logic works with official builds of Python.
            if sys.version_info < (2, 4):
                clibname = 'msvcrt'
            else:
                clibname = 'msvcr71'
        else:
            if version <= 6:
                clibname = 'msvcrt'
            else:
                clibname = 'msvcr%d' % (version * 10)

        # If python was built with in debug mode
        import imp
        if imp.get_suffixes()[0][0] == '_d.pyd':
            clibname += 'd'

        return clibname+'.dll'
    else:
        return ctypes.util.find_library('c')

# Make sure the name is determined during import, not at runtime
libc_name = get_libc_name() 
standard_c_lib = ctypes.cdll.LoadLibrary(get_libc_name())
",A,14
20604394,2013-12-16 05:58:12.013000+00:00,"Yes, there are 3 child processes and 1 parent. Children return 4, 2, 1.
To collect all statuses, you could use a while loop:
if(pid)
{
  while (waitpid(-1, &status, 0) != -1) /* ignoring signals, errors */
    counter += WEXITSTATUS(status);
}
return counter;

Parent returns 7 in this case.
If you use only two waitpid() calls then they may return any pair from {4,2,1} set e.g., {4,1} or {2,1} so parent prints ;5 and ;3 correspondingly.
Due to stdio buffering and fork() interaction, the output may multiply. See printf anomaly after “fork()”
Either fflush() before fork() or use write/_exit in children instead.",A,1
12151503,2012-08-28 01:01:14.973000+00:00,"You are comparing strings with python datetime.date objects; you need to convert the date object to a string for the comparison, using the .strftime() method:
today = date.today().strftime('%Y-%m-%d')
print today in dates # Will print ""True"" or ""False""

To illustrate this further:
>>> from datetime import date
>>> date.today()
datetime.date(2012, 8, 28)
>>> date.today() == '2012-08-28'
False
>>> date.today().strftime('%Y-%m-%d') == '2012-08-28'
True

Alternatively, you can use the .isoformat() method, which uses the exact same output format:
>>> date.today().isoformat()
'2012-08-28'
",A,9
17371216,2013-06-28 18:20:35.090000+00:00,"The forms documentation is quite explicit about this:

These [initial] values are only displayed for unbound forms, and they’re not used as fallback values if a particular value isn’t provided.

I'm not sure what the point is of including the fields as ""hidden"" but then filling them in on instantiation. If you don't want the fields in the form, exclude them and set them on the model instance itself:
class WorkflowForm(ModelForm):
    class Meta:
        exclude = ('purchase1', 'state', 'date_created')


...

if form.is_valid():
    new_flow = form.save(commit=False)
    new_flow.state = 'request'
    new_flow.purchase1 = po
    new_flow.save()
",A,0
48630139,2018-02-05 19:33:40.183000+00:00,"Your PerfvRtrOnlyFilter class is trying to perform a query at class level, which means it runs even the class is first imported. This is before any migration can run.
You just always do any database access inside a method or function.",A,0
25732259,2014-09-08 20:03:17.517000+00:00,"If you pass in a list object to bytes.join(), you could potentially add elements to the the list in another thread while the bytes.join() call is iterating.
The bytes.join() method has to make two passes over the sequence; once to calculate the total length of the bytes objects contained, a second time to then build the actual output bytes object. Changing the number of items while iterating over it would put a spanner in that calculation.
You normally couldn't do this to the list as the GIL is not released, but if any of the objects in the list are not bytes objects the buffer protocol is used instead. As the a comment on the original patch states:

The problem with your approach is that the sequence could be mutated while another thread is running (_getbuffer() may release the GIL). Then the pre-computed size gets wrong.
",A,7
19594726,2013-10-25 16:12:08.880000+00:00,"Use the any() function with a list comprehension to keep entries with numbers instead:
transposedlist = [entry for entry in transposedlist if any(e.isdigit() for e in entry)]

The any() function here returns True if there is any value in the iterable (a generator expression here) that is True, False otherwise. If the expression (e.isdigit() for e in entry) only yields False then that entry will not be included in the new list.
Demo:
>>> transposedlist = [('-', '*', '*', '1'), ('7', '6', '6', '1'), ('-', '*', '1', '*'), ('-', '*', '*', '*'), ('1', '3', '3', '*'), ('-', ' ', ' ', '*')]
>>> any(e.isdigit() for e in transposedlist[0])
True
>>> any(e.isdigit() for e in transposedlist[3])
False
>>> [entry for entry in transposedlist if any(e.isdigit() for e in entry)]
[('-', '*', '*', '1'), ('7', '6', '6', '1'), ('-', '*', '1', '*'), ('1', '3', '3', '*')]
",A,0
48368255,2018-01-21 15:11:48.517000+00:00,"Your _execute_query method returns a list when your query starts with select or with:
if sql_strip.startswith(""select "") or \
        (sql_strip.startswith(""with "")
         # and ""update "" not in sql_strip and ""insert "" not in sql_strip
         ):

    # ...

    rows = self._psql_cur.fetchall()
    print ""FETCHED RESULT: "", rows
    print sql
    return rows, True

rows is a list, not a cursor, so won't have the attribute. Either return the cursor there, or use len() to get a count of rows.",A,0
14814847,2013-02-11 15:06:57.413000+00:00,"No, variables in Python are not pointers.
They refer to objects on a heap instead, and assigning to a variable doesn't change the referenced object, but the variable. Variables and objects are like labels tied to balloons; assignment reties the label to a different balloon instead.
See this previous answer of mine to explore that idea of balloons and labels a bit more.
That said, some object types implement specific in-place addition behaviour. If the object is mutable (the balloon itself can change), then an in-place add could be interpreted as a mutation instead of an assignment.
So, for integers, item += 1 is really the same as item = item + 1 because integers are immutable. You have to create a new integer object and tie the item label to that new object.
Lists on the other hand, are mutable and lst += [other, items] is implemented as a lst.__iadd__([other, items]) and that changes the lst balloon itself. An assignment still takes place, but it is a reassigment of the same object, as the .__iadd__() method simply returns self instead of a new object. We end up re-tying the label to the same balloon.
The loop simply gives you a reference to the next item in the list on each iteration. It does not let you change the original list itself (that's just another set of balloon labels); instead it gives you a new label to each of the items contained.",A,14
48406410,2018-01-23 16:22:11.120000+00:00,"You need to move the last line back one indent, so it is run both in the case that the request is not a POST and also when it is a POST but the form is not valid.",A,1
11629071,2012-07-24 10:38:28.103000+00:00,"What you've done doesn't seem to make any sense at all. You have two text strings, encodinghex and decodinghex, which you're using eval to turn into code to execute. But in your mapping dict alongside those you've also got various actual methods, which you're also trying to pass to eval - which is bound to fail in itself, but even before that your code is trying to add the existing text string to the actual function value, which is impossible.",A,1
51141360,2018-07-02 17:50:43.383000+00:00,"There are a number of problems with this code:

You need to store the results of those get calls.
As suggested by Joel, you need to convert them to float or int.
You should create the results Label once at startup, and config the text in this callback, instead of creating a new Label every time the user hits Submit.
Instead of repeating all of the same code 6 times, just calculate a result in the elif chain, and then use it at the end.

I already explained most of this in my answer to your previous question.
The result should look something like this:
result_label = Label(window, text = str(n_textentry ** m_textentry), bg = ""white"")
result_label.grid(row = 13, column = 0, sticky = N)
def click():
    n = int(n_textentry.get())
    m = int(m_textentry.get())
    operation = int(operation_textentry.get())

    if operation == 1:
        result = n+m
    elif operation == 2:
        result = n-m
    elif operation == 3:
        result = n*m
    elif operation == 4:
        result = n/m
    elif operation == 5:
        result = n**m
    else:
        result = ""Invalid Operation""
    result_label.config(text=str(result))

As I mentioned before, you probably want some error handling for the case where the user leaves one of the entries blank, or inputs text instead of a number, or divides by zero, etc. The simplest way to do this with a try: around the whole click function:
def click():
    try:
        n = int(n_textentry.get())
        # etc.
    except Exception as e:
        result_label.config(text=repr(e))
",A,1
12651298,2012-09-29 08:47:53.007000+00:00,"class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):
    pass

is enough. Your client probably don't make concurrent requests. If you make the requests in parallel the threaded server works as expected. Here's the client:
#!/usr/bin/env python
import sys
import urllib2

from threading import Thread

def make_request(url):
    print urllib2.urlopen(url).read()

def main():
    port = int(sys.argv[1]) if len(sys.argv) > 1 else 8000
    for _ in range(10):
        Thread(target=make_request, args=(""http://localhost:%d"" % port,)).start()

main()

And the corresponding server:
import time
from BaseHTTPServer   import BaseHTTPRequestHandler, HTTPServer, test as _test
from SocketServer     import ThreadingMixIn


class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):
    pass

class SlowHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header(""Content-type"", ""text/plain"")
        self.end_headers()

        self.wfile.write(""Entered GET request handler"")
        time.sleep(1)
        self.wfile.write(""Sending response!"")

def test(HandlerClass = SlowHandler,
         ServerClass = ThreadedHTTPServer):
    _test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

All 10 requests finish in 1 second. If you remove ThreadingMixIn from the server definition then all 10 requests take 10 seconds to complete.",A,14
27298361,2014-12-04 15:57:30.247000+00:00,"You have multiple numbers per string, so you'll need to split those on whitespace first:
[float(i.replace(',', '.')) + 1 for s in invoer for i in s.split()] 

In a list comprehension sequential for loops should be read as nested loops; the outer loop is for s in invoer, then for each s we loop over for i in s.split(). Each i in that loop is converted to a float, then incremented by 1.
Demo:
>>> invoer = ['5,4 4,5 8,7', '6,3 3,2 9,6 4,3', '7,6', '9,8', '5,5 7,8 6,5 6,4']
>>> [float(i.replace(',', '.')) + 1 for s in invoer for i in s.split()] 
[6.4, 5.5, 9.7, 7.3, 4.2, 10.6, 5.3, 8.6, 10.8, 6.5, 8.8, 7.5, 7.4]
",A,1
37736505,2016-06-09 21:28:15.723000+00:00,"Use the zip() function and a defaultdict dictionary to collect values per unique value:
from collections import defaultdict
try:
    # Python 2 compatibility
    from future_builtins import zip
except ImportError:
    # Python 3, already there
    pass

values = defaultdict(int)
for key, value in zip(a, b):
    values[key] += value

a1, b1 = zip(*sorted(values.items()))

zip() pairs up the values from your two input lists, now all you have to do is sum up each value from b per unique value of a.
The last line pulls out the keys and values from the resulting dictionary, sorts these, and puts just the keys and just the values into a1 and b1, respectively.
Demo:
>>> from collections import defaultdict
>>> a = [ 1, 2, 3, 1, 2, 3, 1, 2, 3]
>>> b = [ 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> values = defaultdict(int)
>>> for key, value in zip(a, b):
...     values[key] += value
...
>>> zip(*sorted(values.items()))
[(1, 2, 3), (12, 15, 18)]

If you don't care about output order, you can drop the sorted() call altogether.",A,1
43610392,2017-04-25 12:15:58.817000+00:00,"If there is no other text, split the strings on whitespace, convert each to an integer and feed the result to a bytearray() object to decode:
as_binary = bytearray(int(b, 2) for b in inputtext.split())
as_unicode = as_binary.decode('utf8')

By putting the integer values into a bytearray() we avoid having to concatenate individual characters and get a convenient .decode() method as a bonus.
Note that this does expect the input to contain valid UTF-8. You could add an error handler to replace bad bytes rather than raise an exception, e.g. as_binary.decode('utf8', 'replace').
Wrapped up as a function that takes a codec and error handler:
def to_text(inputtext, encoding='utf8', errors='strict'):
    as_binary = bytearray(int(b, 2) for b in inputtext.split())
    return as_binary.decode(encoding, errors)

Most of your samples are not actually valid UTF-8, so the demo sets errors to 'replace':
>>> to_text('11001010', errors='replace')
u'\ufffd'
>>> to_text('01001010', errors='replace')
u'J'
>>> to_text('11001010', errors='replace')
u'\ufffd'
>>> to_text('11010010 11001110', errors='replace')
u'\ufffd\ufffd'

Leave errors to the default if you want to detect invalid data; just catch the UnicodeDecodeError exception thrown:
>>> to_text('11010010 11001110')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in to_text
  File ""/Users/mjpieters/Development/venvs/stackoverflow-2.7/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: 'utf8' codec can't decode byte 0xd2 in position 0: invalid continuation byte
",A,5
17559475,2013-07-09 22:38:47.860000+00:00,"writerow expects a sequence or iterable of values. But you just have one value.
The reason it sort of works, but does the wrong thing, is that your one value—a bytes string—is actually itself a sequence. But it's not a sequence of the comma-separated values, it's a sequence of bytes.
So, how do you get a sequence of the separate values?

One option is to use split. Either message.split(b', ') or map(str.strip, message.split(b',')) seems reasonable here. That will give you this sequence:
[b'7407.75961', b'3', b'0.865', b'1.423', b'9.022', b'5']


But really, this is exactly what the csv module is for. You can, e.g., wrap the input in a BytesIO and pass it to a csv.reader, and then you just copy rows from that reader to the writer.

But if you think about it, you're getting data in csv format, and you want to write it out in the exact same csv format, without using it in any other way… so you don't even need the csv module here. Just use a plain old binary file:
with open(csvf, 'wb') as csv_handle:
    while True:
        try:
            message, address = s.recvfrom(8192) 
            print(message)                      #display data on screen
            csv_handle.write(message + b'\n')
        except(KeyboardInterrupt, SystemExit): 
            raise
        except:
            traceback.print_exc()


While we're at it, you almost never need to do this:
except(KeyboardInterrupt, SystemExit): 
    raise
except:
    traceback.print_exc()

The only exceptions in Python 3.x that don't inherit from Exception are KeyboardInterrupt, SystemExit, GeneratorExit (which can't happen here), and any third-party exceptions that go out of their way to act like KeyboardInterrupt and SystemExit. So, just do this:
except Exception:
    traceback.print_exc()
",A,0
20788751,2013-12-26 17:32:23.687000+00:00,"[^\(] matches spaces as well, as does ..
In case A the \s+ matches the one space, but cannot match the (. But in case B \s+ matches all spaces before ( except the last one, where [^\(] matches the last space, satisfying the pattern.",A,0
28795754,2015-03-01 16:08:21.953000+00:00,"The wrapper class does not have a .append attribute, so Python falls back to the wrapper.__getattr__ method. From the object.__getattr__ special method documentation:

Called when an attribute lookup has not found the attribute in the usual places (i.e. it is not an instance attribute nor is it found in the class tree for self).

The wrapped object (the list object with [1, 2, 3]) does have a append attribute (a method), so getattr(self.wrapped, 'append') returns it.
The returned method is called, passing in 4, appending that to the self.wrapped list object.
You can easily reproduce the steps yourself:
>>> wrapped = [1, 2, 3]
>>> getattr(wrapped, 'append')
<built-in method append of list object at 0x107256560>
>>> getattr(wrapped, 'append')(4)
>>> wrapped
[1, 2, 3, 4]
",A,3
25968631,2014-09-22 07:23:38.740000+00:00,"The two URLs that are printed show exactly what is happening. You are posting to a URL without a final slash, but you have the default APPEND_SLASH setting, so Django is redirecting to the URL with a final slash appended. Redirects are always GETs.
Make sure you post to the URL with the slash.",A,0
51232576,2018-07-08 13:41:06.423000+00:00,"Your parameter is called slug, not string.
path('<slug:slug>',...
",A,0
51796072,2018-08-11 02:54:43.790000+00:00,"That's not how you use hasattr.
First, you use hasattr when you have the attribute name as a string value:
attrname = 'age'
hasattr(a, attrname)

You can't pass it a.age—that's already trying to look up a.age, and then pass the result of that as the attribute name. That will be false if there is an age (because you don't have an attribute named 23), and it won't even get to hasattr if there isn't one.

But there are two other problems with your code.
First, hasattr returns True if the attribute is there, False if it isn't. It only raises an exception if there's a typo in your code or a serious bug in your class (and you don't want an except: to catch those and just make the problem impossible to debug).
Second, EmpDict is the class. The class doesn't have any of these attributes, so that's always going to return false. You want to know if the instance, self, has them.
So:
def is_attr_available(self, attr):
    return hasattr(self, attr)

And then, you could change your code to call 
print(a.is_attr_available('age'))

But really, why do you even want this function? Why not just:

try: the a.age?
… or provide a default value like None so the attribute always exists?
",A,2
28252995,2015-01-31 15:00:56.397000+00:00,"You have a UTF-8 BOM in your data file; this is what my Python 2 interactive session states is being converted to a float:
>>> '0'
'\xef\xbb\xbf0'

The \xef\xbb\xbf bytes is a UTF-8 encoded U+FEFF ZERO WIDTH NO-BREAK SPACE, commonly used as a byte-order mark, especially by Microsoft products. UTF-8 has no byte order issues, the mark isn't required to record the byte ordering like you have to for UTF-16 or UTF-32; instead Microsoft uses it as an aid to detect encodings.
On Python 3, you could open the file using the utf-8-sig codec; this codec expects the BOM at the start and will remove it:
infile = open('text', 'r', encoding='utf-8-sig')

On Python 2, you could use the codecs.BOM_UTF8 constant to detect and strip;
for line in infile:
    if line.startswith(codecs.BOM_UTF8):
        line = line[len(codecs.BOM_UTF8):]
    x, y = line.split()

As the codecs documentation explains it:

As UTF-8 is an 8-bit encoding no BOM is required and any U+FEFF character in the decoded string (even if it’s the first character) is treated as a ZERO WIDTH NO-BREAK SPACE.
Without external information it’s impossible to reliably determine which encoding was used for encoding a string. Each charmap encoding can decode any random byte sequence. However that’s not possible with UTF-8, as UTF-8 byte sequences have a structure that doesn’t allow arbitrary byte sequences. To increase the reliability with which a UTF-8 encoding can be detected, Microsoft invented a variant of UTF-8 (that Python 2.5 calls ""utf-8-sig"") for its Notepad program: Before any of the Unicode characters is written to the file, a UTF-8 encoded BOM (which looks like this as a byte sequence: 0xef, 0xbb, 0xbf) is written. As it’s rather improbable that any charmap encoded file starts with these byte values (which would e.g. map to
LATIN SMALL LETTER I WITH DIAERESIS
RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
INVERTED QUESTION MARK

in iso-8859-1), this increases the probability that a utf-8-sig encoding can be correctly guessed from the byte sequence. So here the BOM is not used to be able to determine the byte order used for generating the byte sequence, but as a signature that helps in guessing the encoding. On encoding the utf-8-sig codec will write 0xef, 0xbb, 0xbf as the first three bytes to the file. On decoding utf-8-sig will skip those three bytes if they appear as the first three bytes in the file. In UTF-8, the use of the BOM is discouraged and should generally be avoided.
",A,5
36942612,2016-04-29 15:47:18.257000+00:00,"If you want to get Unicode escapes similar to Java in Python; you could use JSON format:
>>> import json
>>> import sys
>>> s = u'Örnsköldsvik;SE;Ornskoldsvik;Ångermanlandsgatan 28 A'
>>> json.dump(s, sys.stdout)
""\u00d6rnsk\u00f6ldsvik;SE;Ornskoldsvik;\u00c5ngermanlandsgatan 28 A""

There is also, unicode-escape codec but you shouldn't use it: it produces Python-specific escaping (how Python Unicode string literals look like):
>>> print s.encode('unicode-escape')
\xd6rnsk\xf6ldsvik;SE;Ornskoldsvik;\xc5ngermanlandsgatan 28 A
",A,1
41769085,2017-01-20 17:18:11.233000+00:00,"Yes, that is exactly what an ancestor query is. As long as you use an ancestor in your query, it is strongly consistent.",A,2
31915538,2015-08-10 09:03:41.023000+00:00,"If you are using mock you can create mocks as attributes of a parent mock, when patching out those functions:
try:
    # Python 3
    from unittest.mock import MagicMock, patch, call
except ImportError:
    # Python 2, install from PyPI first
    from mock import MagicMock, patch, call
import unittest

from module_under_test import function_under_test

class TestCallOrder(unittest.TestCase):
    def test_call_order(self):
        source_mock = MagicMock()
        with patch('module_under_test.function1', source_mock.function1), \
                patch('module_under_test.function2', source_mock.function2), \
                patch('module_under_test.function3', source_mock.function3)

            # the test is successful if the 3 functions are called in this
            # specific order with these specific arguments:
            expected = [
                call.function1('foo'),
                call.function2('bar'),
                call.function3('baz')
            ]

            # run your code-under-test
            function_under_test()

            self.assertEqual(source_mock.mock_calls, expected)

Because the 3 functions are attached to source_mock, all calls to them are recorded on the parent mock object in the Mock.mock_calls attribute and you can make assertions about their call order.
I attached the 3 function mocks simply by looking them up as attributes on the source_mock object, but you could also use the Mock.attach_mock() method to attach mocks you created in a different way to a parent.",A,2
20983780,2014-01-07 22:59:40.963000+00:00,"
I just want subprocess.check_output to return an empty string if grep doesn't find anything.

Well, too bad. grep considers no matches to be failure, and the whole point of the check in check_output is to check for failure, so you're explicitly asking to do things this way. Here are the relevant docs:

If the return code was non-zero it raises a CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute and any output in the output attribute.

And for grep:
The following exit values shall be returned:
  0 One or more lines were selected.
  1 No lines were selected.
  >1 An error occurred.

So, if you want to treat ""no lines"" as success, but actual errors as errors, you have to handle that 1 value differently than other non-zero values. And check_output has no idea that you want to do that.
So, either you have to handle the CalledProcessError, or you have to do your own checking. In other words, either this:
try:
    tyrs = subprocess.check_output('grep ""^A"" %s | grep TYR' % pocket_location, shell = True).split('\n')
except subprocess.CalledProcessError as e:
    if e.returncode > 1:
        raise
    tyrs = []

… or this:
p = subprocess.Popen('grep ""^A"" %s | grep TYR' % pocket_location, shell=True,
                     stdout=subprocess.PIPE)
output, _ = p.communicate()
if p.returncode == 1: # no matches found 
    tyrs = []
elif p.returncode == 0: # matches found
    tyrs = output.split('\n')
else:
    # error, do something with it
",A,9
26789296,2014-11-06 21:00:36.710000+00:00,"The from <module> import * syntax ignores names starting with an underscore, if the target module has no __all__ attribute (if it does only names listed in that attribute are imported). From the import statement documentation:

If the list of identifiers is replaced by a star ('*'), all public names defined in the module are bound in the local namespace of the import statement.
The public names defined by a module are determined by checking the module’s namespace for a variable named __all__; if defined, it must be a sequence of strings which are names defined or imported by that module. The names given in __all__ are all considered public and are required to exist. If __all__ is not defined, the set of public names includes all names found in the module’s namespace which do not begin with an underscore character ('_').

(bold emphasis mine).
Moreover, you did not import the settings_local module itself; the NameError is throws because you didn't import the module, only attributes contained in the module. This works:
import settings_local

print settings_local.__status__

Your other options would be:

Two import statements:
from settings_local import *
try:
    from settings_local import __status__
except ImportError:
    # no __status__ defined in the local settings
    pass

Add an __all__ list to your settings_local.py module:
__all__ = ['CAT', '__status__']

__status__ = ""Development""
CAT = 1


Note that Python advises against creating new __*__ names:

Any use of __*__ names, in any context, that does not follow explicitly documented use, is subject to breakage without warning.
",A,4
20470547,2013-12-09 12:20:32.390000+00:00,"Test it with timeit:
$ bin/python -mtimeit -n10000000 -s 'n = 1.345' 'int(n)'
10000000 loops, best of 3: 0.234 usec per loop
$ bin/python -mtimeit -n10000000 -s 'n = 1.345' 'n // 1'
10000000 loops, best of 3: 0.218 usec per loop

So floor division is only a faster by a small margin. Note that these values are very close, and I had to crank up the loop repeat count to iron out random influences on my machine. Even with such a high count, you need to repeat the experiments a few times to see how much the numbers still vary and what comes out faster most of the time.
This is logical, as int() requires a global lookup and a function call (so state is pushed and popped):
>>> import dis
>>> def use_int(n):
...     return int(n)
... 
>>> def use_floordiv(n):
...     return n // 1
... 
>>> dis.dis(use_int)
  2           0 LOAD_GLOBAL              0 (int)
              3 LOAD_FAST                0 (n)
              6 CALL_FUNCTION            1
              9 RETURN_VALUE        
>>> dis.dis(use_floordiv)
  2           0 LOAD_FAST                0 (n)
              3 LOAD_CONST               1 (1)
              6 BINARY_FLOOR_DIVIDE 
              7 RETURN_VALUE        

It is the LOAD_GLOBAL and CALL_FUNCTION opcodes that are slower than the LOAD_CONST and BINARY_FLOOR_DIVIDE opcodes; LOAD_CONST is a simple array lookup, LOAD_GLOBAL needs to do a dictionary lookup instead.
Binding int() to a local name can make a small difference, giving it the edge again (as it has to do less work than // 1 floor division):
$ bin/python -mtimeit -n10000000 -s 'n = 1.345' 'int(n)'
10000000 loops, best of 3: 0.233 usec per loop
$ bin/python -mtimeit -n10000000 -s 'n = 1.345; int_=int' 'int_(n)'
10000000 loops, best of 3: 0.195 usec per loop
$ bin/python -mtimeit -n10000000 -s 'n = 1.345' 'n // 1'
10000000 loops, best of 3: 0.225 usec per loop

Again, you need to run this with 10 million loops to see the differences consistently.
That said, int(n) is a lot more explicit and unless you are doing this in a time-critical loop, int(n) wins it in readability over n // 1. The timing differences are too small to make the cognitive cost of having to work out what // 1 does here worthwhile.",A,17
21257516,2014-01-21 11:58:57.480000+00:00,"You cannot; continue is a statement and the conditional expression is an expression, and you cannot use a statement inside an expression. After all, the continue statement doesn't produce a value for the conditional expression to return.
Use an if statement instead:
if i == 5:
    x = i
else:
    continue

or better:
if i != 5:
    continue
x = i
",A,9
7429157,2011-09-15 10:08:22.103000+00:00,"Most installation requires:
sudo python setup.py install
Otherwise, you won't be able to write to the installation directories.
I'm pretty sure that (unless you were root), you got an error when you did
python2.7 setup.py install",A,21
21959084,2014-02-22 19:07:53.237000+00:00,"The urllib and urllib2 libraries have been cleaned up for Python 3, moving various disparate parts into a clear package hierarchy. The APIs themselves haven't changed all that much. It's mostly a tightening of the naming conventions.
You only need to compare the Python 2 urllib2 documentation with the urllib.request page for Python 3 (with only exceptions moved to request.error) to see that the basic functionality has not been updated.
requests itself remains the better choice for handling your HTTP client needs in Python 3.",A,2
37264260,2016-05-16 22:28:30.837000+00:00,"To get the last line from subprocess' stdout:
#!/usr/bin/env python3
from collections import deque
from subprocess import Popen, PIPE

with Popen('adb -d logcat <filter-spec>'.split(), stdout=PIPE) as adb:
    last_line = deque(adb.stdout, maxlen=1).pop() # get last line

See adb logcat options.
If you want to emulate the 'adb logcat | grep keyword' shell command literally, see How do I use subprocess.Popen to connect multiple processes by pipes?",A,0
29040327,2015-03-13 19:21:01.770000+00:00,"You can use next(iter(setobj)) to get the only element:
>>> s = set([u'http://imdb.com/title/tt0118583/'])
>>> next(iter(s))
u'http://imdb.com/title/tt0118583/'

You can even specify a default for when the set is empty:
next(iter(setobj), None)

returns None if there is no element to return otherwise.",A,3
14193025,2013-01-07 09:24:21.360000+00:00,"from django.db.models import Count
call_log.objects.values('call_from').annotate(Count('total'))
",A,0
36424589,2016-04-05 11:02:46.617000+00:00,"Yes, list.sort() sorts in place, and returns None to indicate it is the list itself that has been sorted. The sorted() function returns a new sorted list, leaving the original list unchanged.
Use int as a key:
sorted(key_s, key=int)

This returns a new list, sorted by the numeric value of each string, but leaving the type of the values themselves unchanged.
Without a key, strings are sorted lexicographically instead, comparing character by character. Thus '9' is sorted after '10', because the character '1' comes before '9' in character sets, just like 'a' comes before 'z'.
The key argument lets you apply a Schwartzian_transform, informing the sorting algorithm what to sort by. Each value in the list is sorted according to key(value) (so int(value) here) instead of the original value itself.
Demo:
>>> key_s = ['115', '114', '117', '116', '111', '110', '113', '112', '68', '119', '118', '44', '45', '42', '43', '41', '76', '108', '109', '71', '107', '79', '13', '15', '14', '17', '16', '37']
>>> sorted(key_s, key=int)
['13', '14', '15', '16', '17', '37', '41', '42', '43', '44', '45', '68', '71', '76', '79', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119']
",A,4
11852648,2012-08-07 19:21:22.153000+00:00,"This is exactly how .takewhile() should behave. While the condition is true, it'll return elements from the underlying iterable, and as soon as it's false, it permamently switches to the iteration-done stage.
Note that this is how iterators must behave; raising StopIteration means just that, stop iterating over me, I am done.
From the python glossary on ""iterator"":

An object representing a stream of data. Repeated calls to the iterator’s next() method return successive items in the stream. When no more data are available a StopIteration exception is raised instead. At this point, the iterator object is exhausted and any further calls to its next() method just raise StopIteration again.

You could combine takewhile with tee to see if there are any more results in the next batch:
import itertools

def readParag(filename):
    with open(filename) as f:
        while True:
            paras = itertools.takewhile(lambda l: l.strip(), f)
            test, paras = itertools.tee(paras)
            test.next()  # raises StopIteration when the file is done
            yield (l.strip() for l in paras)

This yields generators, so each item yielded is itself a generator. You do need to consume all elements in these generators for this to continue to work; the same is true for the groupby method listed in another answer.",A,6
686314,2009-03-26 15:38:37.363000+00:00,Use the struct module to build a buffer and write that.,A,11
30505961,2015-05-28 11:57:47.937000+00:00,"You are missing the closing parentheses on these lines:
num1 = int(input(""Enter no1: "")
num2 = int(input(""Enter no2: "")

as such you'll get a syntax error on the num2 line, because Python never found the end of the logical line started on the preceding line.
Balance the parentheses:
num1 = int(input(""Enter no1: ""))
num2 = int(input(""Enter no2: ""))

and your code otherwise will work.",A,3
26713129,2014-11-03 11:19:02.677000+00:00,"You got the indentation wrong in your decorator; unindent the return wrapper line.
As it stands, your decorator returns None, so cc.job is set to None.
The corrected version would be:
def catch_exceptions(job_func):
    @functools.wraps(job_func)
    def wrapper(*args, **kwargs):
        try:
            job_func(*args, **kwargs)
        except:
            import traceback
            print(traceback.format_exc())
    return wrapper

You probably want to avoid using an entirely bare except there; you are now catching keyboard interrupts and system exit exceptions too. In a thread that doesn't matter all that much (the exceptions, if uncaught, do not propagate to the main thread anyway), but usually you want to use except Exception: at the very least.",A,4
717744,2009-04-04 20:19:53.583000+00:00,"This is more of a complaint than a question.  Do you have a more specific question on recursion?  Like multiplication, it's not a thing people write a lot about.
Speaking of multiplication, think of this.
Question:
What's a*b?
Answer:
If b is 1, it's a.
Otherwise, it's a+a*(b-1).
What's a*(b-1)?  See the above question for a way to work it out.",A,8
23315264,2014-04-26 19:00:40.603000+00:00,"No, the output is not the same; one is a Unicode value, the other an undecoded bytes value.
For ASCII, that looks the same, but when you load any web page that uses characters outside the ASCII characterset, the difference will be much clearer.
Take UTF-8 encoded data, for example:
>>> '–'
'–'
>>> '–'.encode('utf8')
b'\xe2\x80\x93'

That's a simple U+2013 EN DASH character. The bytes representation shows the 3 bytes UTF-8 uses to encode the codepoint.
You really want to read up on Unicode vs. encoded data here, I recommend:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
",A,3
33013942,2015-10-08 11:03:06.250000+00:00,"There are several ways to resolve this e.g.:

Enter all 7 digits at once:
number = int(input('Enter 7 digit number: '))

Or put digits in a list:
digits = [int(input('Enter the {ordinal} digit: '.format(ordinal=ordinal))
          for ordinal in ""first second third fourth fifth sixth final"".split()]

and convert the list into a number using the decimal position arithmetic
(123 = 1*100 + 2*10 + 3 = (1*10 + 2)*10 + 3):
from functools import reduce

number = reduce(lambda x,y: x*10 + y, digits)

or to get the same result using str:
number = int(''.join(map(str, digits)))

Note: if you have to preserve the way the digits are entered now then you could define the digits list as:
digits = [Digits1, Digits2, Digits3, Digits4, Digits5, Digits6, Digits7]

",A,0
41526656,2017-01-07 21:17:30.200000+00:00,"data = {""devices"": [item[0] for item in devices]}
json_data = json.dumps(data)
",A,1
25063364,2014-07-31 15:44:48.803000+00:00,"Your assumption is unfortunately completely wrong. Keyword arguments are passed in the URL, they are simply sent to the view function in a different way - in kwargs instead of args.
The simplest way to solve your problem is just to check the user in the download function itself.
def download(request, pk):
    obj = Download.objects.get(pk)
    if obj.customer_id != request.user.id:
        return HttpResponseForbidden()
",A,1
36471116,2016-04-07 08:49:20.193000+00:00,"You named your method __init_varwith two initial underscores, making it a class private method. The name is mangled to make sure that subclasses don't clash with it. From the Reserved classes of identifiers section:

__*
  Class-private names. Names in this category, when used within the context of a class definition, are re-written to use a mangled form to help avoid name clashes between “private” attributes of base and derived classes.

and in the Identifiers (Names) section:

Private name mangling: When an identifier that textually occurs in a class definition begins with two or more underscore characters and does not end in two or more underscores, it is considered a private name of that class. Private names are transformed to a longer form before code is generated for them. The transformation inserts the class name, with leading underscores removed and a single underscore inserted, in front of the name. For example, the identifier __spam occurring in a class named Ham will be transformed to _Ham__spam.

So A.__init_var is renamed to A._A__init_var, and A.__init__() is altered to call A._A__init_var(). The renamed B._B__init_var() method is never called because it has a different name:
>>> dir(A)
['_A__init_var', '__doc__', '__init__', '__module__']
>>> import dis
>>> dis.dis(A.__init__)  # disassemble the bytecode for A.__init__
  3           0 LOAD_FAST                0 (self)
              3 LOAD_ATTR                0 (_A__init_var)
              6 CALL_FUNCTION            0
              9 POP_TOP
             10 LOAD_CONST               0 (None)
             13 RETURN_VALUE
>>> dir(B)
['_A__init_var', '_B__init_var', '__doc__', '__init__', '__module__']

Remove one of those initial underscores, so name it _init_var:
class A:
    def __init__(self):
        self._init_var()

    def _init_var(self):
        self.var = 11

class B(A):
    def __init__(self):
        A.__init__(self)

    def _init_var(self):
        self.var = 10
",A,4
5674454,2011-04-15 08:56:06.323000+00:00,"Are you sure your second extra requirement is correct? Surely with just two names you want ""and"" and no comma?
Anyway, there's nothing particularly difficult about this. The trick is to get the queryset's length first - note that it's actually more efficient to use len rather than .count() here, because count() on an unevaluated queryset does a separate call to the database.
def get_author_display(self):
    authors = self.authors.all()
    length = len(authors)
    output = []
    for i, author in enumerate(authors):
        val = ""%s %s"" % (author.first_name, author.last_name)
        if i == (length-1) and length > 2:
             val = ""and "" + val
        output.append(val)
    return ', '.join(output)

And no, there's nothing wrong with calling the method that: authors will never have CHOICES set, so won't get an automatic get_foo_display method.",A,1
2280323,2010-02-17 11:54:58.150000+00:00,"Consider using some kind of template tool (even string.Template) instead of lots of string manipulation.
For example, a generic dictionary-to-tag is pretty simple with the silly little built-in Template class.
import string
tag= string.Template( ""<$tag>$value</$tag>"" )
for k,v in someProjectDictionary:
    print tag.substitute( key=k, value=v )

However, graduating to Jinja or Mako makes this much, much simpler, since your entire XML document becomes simple iteration within the template language.
{% for p in project_data %}
<project>
    {% for k in p %}
    <{{k}}>{{p[k]}}</{{k}}>
    {% endfor %}
</project>
{% endfor %}        

You can create a separate file for this and use Jinja to handle the substitution into your XML template from your data.
from jinja2 import Environment, PackageLoader
env = Environment(loader=PackageLoader('yourapplication', 'templates'))
template = env.get_template('mytemplate.html')
print template.render(project_data=the_project_data_list)
",A,0
1920390,2009-12-17 09:09:13.480000+00:00,"No, there isn't a built-in way. You could do this in the view, or write a shortcut method on foo. Alternatively a custom template tag could do it. ",A,3
53069880,2018-10-30 17:34:33.777000+00:00,"You'd want to create datetime.time() objects, then make the sequence cyclic with itertools.cycle() and combine these with datetime.date objects:
import datetime
from itertools import cycle, product, repeat
from bisect import bisect

def datetime_series(*timestrings, reference=None):
    times = sorted(datetime.time(*map(int, t.split(':'))) for t in timestrings)
    if reference is None:
        reference = datetime.datetime.now()
    date, tnow = reference.date(), reference.time()

    next_pos = bisect(times, tnow)  # index of next time object to use
    times = cycle(times + [None])
    for _ in range(next_pos):  # skip forward across the time objects
        next(times)

    for time in times:
        if time is None:
            # next day
            date += datetime.timedelta(days=1)
            time = next(times)
        yield datetime.datetime.combine(date, time)

The above uses None as a sentinel to detect that a the date value needs to be incremented a step, and the time strings are accepted as separate arguments, and you can pass in a different reference date (the default is to use now).
It's also an endless a generator, so you can iterate over it step by step, or use itertools.slice() to limit the number of results.
Personally, I'd alter make the function accept datetime.time() objects rather than pass in strings, so you just sort the arguments without having to parse and go from there.
Demo:
>>> from itertools import islice
>>> from pprint import pprint
>>> import datetime
>>> datetime.datetime.now()
datetime.datetime(2018, 10, 30, 17, 39, 46, 91967)
>>> m = datetime_series('8:15', '14:28', '19:43', '1:21')
>>> next(m)
datetime.datetime(2018, 10, 30, 19, 43)
>>> next(m)
datetime.datetime(2018, 10, 31, 1, 21)
>>> m = datetime_series('8:15', '14:28', '19:43', '1:21')
>>> ten_results = list(islice(m, 10))
>>> pprint(ten_results)
[datetime.datetime(2018, 10, 30, 19, 43),
 datetime.datetime(2018, 10, 31, 1, 21),
 datetime.datetime(2018, 10, 31, 8, 15),
 datetime.datetime(2018, 10, 31, 14, 28),
 datetime.datetime(2018, 10, 31, 19, 43),
 datetime.datetime(2018, 11, 1, 1, 21),
 datetime.datetime(2018, 11, 1, 8, 15),
 datetime.datetime(2018, 11, 1, 14, 28),
 datetime.datetime(2018, 11, 1, 19, 43),
 datetime.datetime(2018, 11, 2, 1, 21)]
>>> question_1 = datetime.datetime(2008, 7, 31, 21, 26, 37)  # https://meta.stackexchange.com/a/30138
>>> question_1_series = datetime_series('8:15', '14:28', '19:43', '1:21', reference=question_1)
>>> pprint(list(islice(question_1_series, 10)))
[datetime.datetime(2008, 8, 1, 1, 21),
 datetime.datetime(2008, 8, 1, 8, 15),
 datetime.datetime(2008, 8, 1, 14, 28),
 datetime.datetime(2008, 8, 1, 19, 43),
 datetime.datetime(2008, 8, 2, 1, 21),
 datetime.datetime(2008, 8, 2, 8, 15),
 datetime.datetime(2008, 8, 2, 14, 28),
 datetime.datetime(2008, 8, 2, 19, 43),
 datetime.datetime(2008, 8, 3, 1, 21),
 datetime.datetime(2008, 8, 3, 8, 15)]
",A,0
4809471,2011-01-26 20:08:21.850000+00:00,"
The last big issue is that the code under test is designed to run forever, until a Ctrl-C, and I don't see any way to force it to raise an exception or otherwise kill the thread 

The point of Test-Driven Development is to rethink your design so that it is testable.
Loop forever -- while seemingly fine for production use -- is untestable.
So make the loop terminate.  It won't hurt production.  It will improve testability.
The ""designed to run forever"" is not designed for testability.  So fix the design to be testable.",A,4
821346,2009-05-04 18:41:39.767000+00:00,"""Is that a problem [with a Singleton configuration]?
Not really.
""all the other classes dependent on this singleton""
Not really.  You can easily design things so that some classes get configuration, and provide it to other classes.
This is a question of resposnsibility -- what class/package/method has responsibility for this configuration issue.  And the answer should be very, very few classes need to know the configuration.
Your model, for example, should probably be focused on the problem domain independent of implementation nuance.  It may not need any configuration.
""Is there another, ideal solution that I haven't thought of?""
Not really. The configuration problem is hard.  You may have a default configuration, overrides to the default configuration, command-line options.  It's quite complex. 
""How do you people usually represent this type of data?""
A Singleton for the most part.  Generally, a few key ""top-level"" classes in a given application must know the configuration and use that information to configure all of the other classes.
For a command-line program, the class that is invoked by the ""main"" program to parse arguments will also know about the configuration.  And that's it.
For a web app, a top-level piece of the web app uses the configuration to build, configure, customize or instantiate other things.  In many cases, a key Factory will use the configuration to decide what to create.  A key Builder may require configuration to customize composite objects its building.
Configuration is not ""global"".  It is not seen by all classes.  It is -- like the database -- a precious resource and it is carefully wrapped by classes that have responsibility for handling configuration.
""Is there anything else I should be aware of concerning this problem?""
Yes.  The configuration Singleton is not global.  It is a scarce, precious resource that is used in a few key places.  The fewer the better.",A,1
38984317,2016-08-16 20:54:44.283000+00:00,"You don't have a field called Acudiente in your view; you have one called Nomacud instead.
Really though you should be using Django forms for all this.",A,0
28430762,2015-02-10 11:40:19.780000+00:00,"You have strings in your dictionary, but sum() will start with the number 0 to sum numeric values only:
>>> sum(['1', '2'])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'int' and 'str'

Convert your scores to numbers, either int() or float() depending on your format:
scores = int(columns[1].strip())
",A,1
47676532,2017-12-06 14:21:58.810000+00:00,"You've defined a nested function. That simply isn't visible outside the containing function; in fact, making it invisible from outside is pretty much the only good reason for defining nested functions in Python. Don't do that; move it outside the study_time function.
(Also, don't use double-underscore prefixes like that. They don't make any sense outside a class; and even there you should rarely if ever use them.)",A,4
36426838,2016-04-05 12:49:17.357000+00:00,"Not all of your item objects are bound methods. pytest can run plain functions as tests too, and there is no TestCase associated with those. 
You could skip these:
def pytest_collection_modifyitems(session, config, items):
    new=[]
    for item in items:
        if not hasattr(item._obj, 'im_class'):
            # not a method, can't check if safe
            continue
        if not _safe_to_run(item._obj.im_class):
            continue
        new.append(item)
    items[:] = new
",A,3
41630864,2017-01-13 09:04:49.060000+00:00,"!s, and its brethren !a and !r apply str(), ascii() and repr() respectively before interpolation and formatting. These are called conversion flags, and are part of the Format String Syntax spec, not the per-field formatting spec applied to values when interpolating:

The conversion field causes a type coercion before formatting. Normally, the job of formatting a value is done by the __format__() method of the value itself. However, in some cases it is desirable to force a type to be formatted as a string, overriding its own definition of formatting. By converting the value to a string before calling __format__(), the normal formatting logic is bypassed.

Bold emphasis mine.
:s only applies afterwards to the conversion result (or the original object if no conversion had been applied), and only if the __format__ method for the type of object supports that formatting option. Usually, only objects of type str support this formatter; it's there as the default, mostly because the Format Specification Mini-Language allows for the existence of a type character and because the older  % printf-style formatting had a %s format. If you tried to apply the s type to an object that doesn't support it, you'd get an exception.
Use !s (or !a or !r) when you have an object that is not itself a string and either doesn't support formatting otherwise (not all types do) or would format differently from their str(), ascii() or repr() conversions:
>>> class Foo:
...     def __str__(self):
...         return ""Foo as a string""
...     def __repr__(self):
...         return ""<Foo as repr, with åéæ some non-ASCII>""
...     def __format__(self, spec):
...         return ""Foo formatted to {!r} spec"".format(spec)
...
>>> print(""""""\
... Different conversions applied:
... !s: {0!s:>60s}
... !r: {0!r:>60s}
... !a: {0!a:>60s}
... No conversions: {0:>50s}
... """""".format(Foo()))
Different conversions applied:
!s:                                    Foo as a string
!r:             <Foo as repr, with åéæ some non-ASCII>
!a:    <Foo as repr, with \xe5\xe9\xe6 some non-ASCII>
No conversions: Foo formatted to '>50s' spec

Note: all formatting specified by the format spec are the responsibility of the __format__ method; the last line does not apply the alignment operation in the >50s formatting spec, the Foo.__format__ method only used it as literal text in a formatting operation (using a !r conversion here).
For the converted values, on the other hand, the str.__format__ method is used and the output is aligned to the right in a 50 character wide field, padded with spaces on the left.",A,33
48222979,2018-01-12 09:17:54.700000+00:00,"<type>-like is any object that acts just like that type. Python widely relies on duck typing; if it walks and quacks like a duck, it is a duck. If it behaves like a string, it probably is one.
So the documentation is telling you that if you are going to create a custom type that wants to be treated like a string everywhere, it may be an idea to subclass basestring.  That way any code that explicitly wants to test for string types (bytes or unicode text) by using isinstance(obj, basestring) will accept your custom type too.
You still have to implement the minimum set of expected methods; inheriting from basestring won't magically make your type work. That minimum set depends on how the code you are trying to dupe is manipulating the strings. Your type has to be string-like enough to dupe whatever you are using your type for.
Yes, that's deliberately vague, and not all code can be duped (some built-in functions and types expect to be able to use the C API on string objects, or even reach into the str internals). Often it is easier to just subclass str or unicode to ensure nothing is missed.
The language has since stepped away from such base classes, and has defined abstract base classes instead, which, together with hooks for customising subclass and instance checks now are the recommended way to do duck typing in one check.",A,4
22730576,2014-03-29 11:36:50.267000+00:00,">>> import re
>>> re.sub(r'(\s|^)(\$x)(\s|$)', r'\1$Q\3', '$x start $x bar$xy $$x end $x')
'$Q start $Q bar$xy $$x end $Q'
",A,0
53520003,2018-11-28 12:57:01.330000+00:00,"The data you get from that API doesn't have title or name as attributes at the top level. Rather, they are inside the articles element, which itself is a list.
{% for article in news.articles %}
    <p><strong>{{ article.title }}</strong><strong>{{ article.source.name }}</strong> public repositories.</p>
{% endif %}
",A,1
1658087,2009-11-01 19:32:23.700000+00:00,"There's a trick to block diagrams.
They're just marketing.
If you want precise semantics, you have to use UML, which doesn't use block diagrams with simple casual ""on top of"".  The ""on top of"" is clearly too vague.
If you have a ""pyramid"", and you'd rather not show that, then your lower-level blocks are too fine-grained.
Lump the detailed lower-level blocks into a bigger framework block.  Then you can have fewer big blocks on the bottom.",A,0
12015233,2012-08-18 02:04:03.340000+00:00,"Update: If you need to run it exactly once at the first opportunity after 7:15am:
now = datetime.now()
dt = now.replace(hour=7, minute=15) #note: ignore seconds, microseconds
if dt < now:
   dt += timedelta(days=1) #note: might miss by an hour on DST transition day
assert dt >= now
delay = (dt - datetime.now()).total_seconds()
Timer(delay, myscript.main).start()

It runs main at 7:15am or shortly after the computer has awoken if it is asleep at 7:15am.

As other have said an existing solution such as cron, Windows task scheduler might be preferable for running your script at specified times.
To avoid the explicit busy loop you could use threading.Timer().
#!/usr/bin/env python
from datetime  import datetime, timedelta
from threading import Timer

import myscript

def run_at(dt, func, step=timedelta(days=1), tolerance=timedelta(minutes=1)):
    if abs(dt - datetime.now()) < tolerance:
       func() # don't schedule if it raises an exception

    now = datetime.now()
    while dt < now:
        dt += step

    delay = (dt - datetime.now()).total_seconds()
    Timer(delay, run_at, [dt, func]).start()

dt = datetime.now().replace(hour=7, minute=15)
run_at(dt, myscript.main)

It calls the main function everyday at 7:15 in local time.
It skips the call if the previous one is not complete yet or your computer is suspended at 7:15. It might skip DST transition day. 
It should work If the system clock is set back/forward between the calls.
To handle more complex scheduling specifications you could use the crontab syntax in Python,  example.",A,1
564878,2009-02-19 11:31:25.517000+00:00,"First, I'm not sure that simply settings argv[0] in a C program portably changes the name shown in ps.  Maybe it does in some unixen, but my understanding is that it's not expected to.
Second, since Windows is specifically non-POSIX compliant, only a few things are ""portable"" between POSIX and non-POSIX.  Since you specifically say 'ps', I'll assume that POSIX is your priority and Windows may not work.
More importantly, my understanding of changing argv[0] is that it requires a call to exec to make these changes.  Specifically, the exec call has both a path to an executable and a separate argv list.  Making your own call allows you to break the shell convention of putting the executable name in argv[0].
You have OS library process management which gives you direct access to the OS library for doing this.  You should consider breaking your script into two parts -- a starter and the ""real work"".  The starter establishes the run-time environment and exec's the real work with the desired parameters.  
In C, you're replacing your own process with another.  In Python, you're replacing the old Python interpreter with a new one that has a different argv[0].  Hopefully, it won't balk at this.  Some programs check argv[0] to decide what they're doing.
You also have subprocess.popen that you can use to set your desired args and executable.   In this case, however, the parent process should lingers around to collect the child when the child finishes.  The parent may not be doing anything more than a Popen.wait",A,4
18483327,2013-08-28 08:50:47.987000+00:00,"Use the zip() function to combine the two lists:
dict(zip(i, j))

Demo:
>>> i=[""abc"",""def"",""ghi"",""jkl""]
>>> j=[[""a"",""b"",""c"",""d""],[""q"",""w"",""e"",""r""],[""t"",""y"",""u"",""i""]]
>>> dict(zip(i, j))
{'abc': ['a', 'b', 'c', 'd'], 'ghi': ['t', 'y', 'u', 'i'], 'def': ['q', 'w', 'e', 'r']}

zip() pairs up elements from lists into a sequence of tuples; the dict() constructor takes a sequence of tuples and interprets them as key-value pairs.",A,6
31485450,2015-07-17 22:08:09.380000+00:00,"You could just set the width option to a really large value, such as sys.maxsize:
>>> import sys
>>> import pprint as PP
>>> PP.pprint(""ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZZZ"",width=sys.maxsize,compact=True)
'ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZZZ'

Setting width to -1 has no special meaning.
The only other option is to subclass the PrettyPrinter() class, and wrapping the _format() method:
import sys
from pprint import PrettyPrinter

class NoStringWrappingPrettyPrinter(PrettyPrinter):
    def _format(self, object, *args):
        if isinstance(object, str):
            width = self._width
            self._width = sys.maxsize
            try:
                super()._format(object, *args)
            finally:
                self._width = width
        else:
            super()._format(object, *args)

NoStringWrappingPrettyPrinter().pprint(yourobject)

This sets the width to sys.maxsize only for strings:
>>> NoStringWrappingPrettyPrinter().pprint([""ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZZZ"", ""CATS""])
['ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZZZ',
 'CATS']
",A,9
20593474,2013-12-15 10:25:12.853000+00:00,"You imported the module into your namespace; x is an attribute of the module:
import modulename

class y(modulename.x):

Alternatively, use the from modulename import syntax to bind objects from a module into your local namespace:
from modulename import x

class y(x):
",A,4
32841223,2015-09-29 10:06:04.867000+00:00,"You have two configuration files, but no way of distinguishing between them. Apache will simply match everything against the original one, since it's first alphabetically, and never get to the new one - hence why it is looking in /var/www, since that's evidently the DocumentRoot set in 001-default.
Do you actually need this to be in a separate file? There's no reason you couldn't add this configuration inside 001-default. Otherwise, you will need to have them on separate ports, or use a ServerName directive to distinguish them.",A,2
37228140,2016-05-14 14:55:31.020000+00:00,"You are reading your file line by line, but a single line from your file is not a valid JSON document:
>>> import json
>>> json.loads('{\n')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mjpieters/Development/Library/buildout.python/parts/opt/lib/python3.5/json/__init__.py"", line 319, in loads
    return _default_decoder.decode(s)
  File ""/Users/mjpieters/Development/Library/buildout.python/parts/opt/lib/python3.5/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/Users/mjpieters/Development/Library/buildout.python/parts/opt/lib/python3.5/json/decoder.py"", line 355, in raw_decode
    obj, end = self.scan_once(s, idx)
json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 2 column 1 (char 2)

Decode the whole file in one go; this is easiest done with the json.load() function:
with open('jason.json') as f:
    data.append(json.load(f))
",A,3
12044344,2012-08-20 20:04:57.987000+00:00,"<a href=""/index"">Start over </a>

or even better
<a href=""{% url ""index"" %}"">Start over </a>
",A,9
26841280,2014-11-10 10:11:29.020000+00:00,"You are using 'fancy' curly quotes in that line:
>>> u'‘database(PDB)’'
u'\u2018database(PDB)\u2019'

That's a U+2018 LEFT SINGLE QUOTATION MARK at the start and U+2019 RIGHT SINGLE QUOTATION MARK at the end.
Use ASCII quotes (U+0027 APOSTROPHE or U+0022 QUOTATION MARK) or declare an encoding other than ASCII for your source.
You are also using an U+2013 EN DASH:
>>> u'Columns(str) –'
u'Columns(str) \u2013'

Replace that with a U+002D HYPHEN-MINUS.
All three characters encode to UTF-8 with a leading E2 byte:
>>> u'\u2013 \u2018 \u2019'.encode('utf8')
'\xe2\x80\x93 \xe2\x80\x98 \xe2\x80\x99'

which you then see reflected in the SyntaxError exception message.
You may want to avoid using these characters in the first place. It could be that your OS is replacing these as you type, or you are using a word processor instead of a plain text editor to write your code and it is replacing these for you. You probably want to switch that feature off.",A,2
2498737,2010-03-23 09:32:13.207000+00:00,Don't forget that the context to the subtemplate - and hence to the second template tag - is whatever is returned from the first template tag function. So you'll need to ensure that the request object is included in the dictionary you return there.,A,1
21186662,2014-01-17 13:00:18.747000+00:00,"No, that wouldn't even run (kek.html is not defined in the view, so it's a NameError).
You can use the {% include %} tag in the template for this:
<section id='main_page'>
    <div id=""main_div"">
        {% include 'kek.html' %}
    </div>
</section>

You should really do the Django tutorial, which explains how templates work (among other things).
Edit after comment
In that case you should treat the name of the include as a variable and pass it from the view:
{% include template_to_include %}

...
return render_to_response(
        'index.html', {'template_to_include': 'kek.html'},
)

Note the quotes.",A,2
6441228,2011-06-22 14:06:32.217000+00:00,"As the release notes for 1.2.4 state, arbitrary cross-model lookups via querystring are no longer allowed, as they are a security risk. That patch is not meant to re-enable them.
You need to specify the allowed relations explicitly in the admin's list_filter property. Unfortunately, this was only possible from version 1.3, so you'll need to upgrade.",A,2
13794050,2012-12-10 02:06:40.270000+00:00,"You want to use the built-in codec unicode_escape.
If t is already a bytes (an 8-bit string), it's as simple as this:
>>> print(t.decode('unicode_escape'))
Róisín

If t has already been decoded to Unicode, you can to encode it back to a bytes and then decode it this way. If you're sure that all of your Unicode characters have been escaped, it actually doesn't matter what codec you use to do the encode. Otherwise, you could try to get your original byte string back, but it's simpler, and probably safer, to just force any non-encoded characters to get encoded, and then they'll get decoded along with the already-encoded ones:
>>> print(t.encode('unicode_escape').decode('unicode_escape')
Róisín

In case you want to know how to do this kind of thing with regular expressions in the future, note that sub lets you pass a function instead of a pattern for the repl. And you can convert any hex string into an integer by calling int(hexstring, 16), and any integer into the corresponding Unicode character with chr (note that this is the one bit that's different in Python 2—you need unichr instead). So:
>>> re.sub(r'(\\u[0-9A-Fa-f]+)', lambda matchobj: chr(int(matchobj.group(0)[2:], 16)), t)
Róisín

Or, making it a bit more clear:
>>> def unescapematch(matchobj):
...     escapesequence = matchobj.group(0)
...     digits = escapesequence[2:]
...     ordinal = int(digits, 16)
...     char = chr(ordinal)
...     return char
>>> re.sub(r'(\\u[0-9A-Fa-f]+)', unescapematch, t)
Róisín

The unicode_escape codec actually handles \U, \x, \X, octal (\066), and special-character (\n) sequences as well as just \u, and it implements the proper rules for reading only the appropriate max number of digits (4 for \u, 8 for \U, etc., so r'\\u22222' decodes to '∢2' rather than ''), and probably more things I haven't thought of. But this should give you the idea.",A,13
24652184,2014-07-09 11:13:08.730000+00:00,"The SpeechRecognition library requires Python 3.3 or up:

Requirements
[...]
The first software requirement is Python 3.3 or better. This is required to use the library.

and from the Trove classifiers:

Programming Language :: Python
  Programming Language :: Python :: 3
  Programming Language :: Python :: 3.3
  Programming Language :: Python :: 3.4  

The urllib.request module is part of the Python 3 standard library; in Python 2 you'd use urllib2 here.",A,27
29331536,2015-03-29 16:14:10.110000+00:00,"You need to load the templatetag library in every template.
{% extends ""mysite/base.html"" %}
{% load thumbnail %}
",A,1
16908390,2013-06-04 01:24:32.707000+00:00,"The problem is that your getInfo method makes calls into the UI, like this:
[self.navigationController popToRootViewControllerAnimated:YES];

… but you're running it on a background thread.
The whole point of -showWhileExecuting: onTarget: withObject: animated: is that it automatically runs your code in a background thread.
So, you need to protect things the same way as you do when manually running in a background thread.
So, any UI code in your method needs to use performSelectorOnMainThread: and friends, or dispatch_async or other means of doing the same thing.
In this particular case, you want to dispatch a method that takes a primitive (BOOL) argument, which means you can't just use -performSelectorOnMainThread: withObject:. But you also presumably want to wait until it's done, which means you can't just use dispatch_async.
You can just write a wrapper method that takes no arguments:
- (void)popNavigationControllerToRoot {
    [self.navigationController popToRootViewControllerAnimated:YES];
}

… and then:
[self performSelectorOnMainThread:@selector(popNavigationControllerToRoot)
                    waitUntilDone:YES];

Or you can use wrappers around NSInvocation, like the ones here:
[[self.navigationController dd_invokeOnMainThreadAndWaitUntilDone:YES] 
 popToRootViewControllerAnimated:YES];
",A,0
33844913,2015-11-21 15:08:27.083000+00:00,"Pass in the object as a local or global:
exec(dummy_exec_data, {}, {'newsItems': newsItems})

This passes in the name explicitly as part of the locals namespace to avoid accidentally passing in too much information.
Demo:
>>> newsItems = []
>>> exec('newsItems.append(""foo"")', {}, {'newsItems': newsItems})
>>> newsItems
['foo']
",A,2
36631456,2016-04-14 18:44:16.123000+00:00,"You haven't provided the traceback which would enable us to easily diagnose the problem, but this line looks suspicious:
 terrible_sentiment = SentimentPercentage.objects.get('**pt_terrible')

The argument should not be in quotes.",A,2
25987989,2014-09-23 06:08:20.507000+00:00,"I don't know exactly what library you're using, and when you say ""this is building a queue…"" I have no idea what ""this"" you're referring to… but an obvious answer is to stick your own queue in front of whatever it's using, so you can manipulate that queue directly. For example:
import queue
import threading

def skip_get(q):
    value = q.get(block=True)
    try:
        while True:
            value = q.get(block=False)
    except queue.Empty:
        return value

q = queue.Queue()

def file_event_callback(event):
    # code 256 for adding file to folder
    if event.mask == 256:
        fileChanged = event.name
        q.put(fileChanged)

def consumer():
    while True:
        fileChanged = skip_get(q)
        if fileChanged is None:
            return
        # do stuff with fileChanged

Now, before you start up the observer, do this:
t = threading.Thread(target=consumer)
t.start()

And at the end:
observer.join()
q.put(None)
t.join()


So, how does this work?
First, let's look at the consumer side. When you call q.get(), this pops the first thing off the queue. But what if nothing is there? That's what the block argument is for. If it's false, the get will raise a queue.Empty exception. If it's true, the get will wait forever (in a thread-safe way) until something appears to be popped. So, by blocking once, we handle the case where there's nothing to read yet. By then looping without blocking, we consume anything else on the queue, to handle the case where there are too many things to read. Because we keep reassigning value to whatever we popped, what we end up with is the last thing put on the queue.
Now, let's look at the producer side. When you call q.put(value), that just puts value on the queue. Unless you've put a size limit on the queue (which I haven't), there's no way this could block, so you don't have to worry about any of that. But now, how do you signal the consumer thread that you're finished? It's going to be waiting in q.get(block=True) forever; the only way to wake it up is to give it some value to pop. By pushing a sentinel value (in this case, None is fine, because it's not valid as a filename), and making the consumer handle that None by quitting, we give ourselves a nice, clean way to shutdown. (And because we never push anything after the None, there's no chance of accidentally skipping it.) So, we can just push None, then be sure that (barring any other bugs) the consumer thread will eventually quit, which means we can do t.join() to wait until it does without fear of deadlock.

I mentioned above that you could do this more simply with a Condition. If you think about how a queue actually works, it's just a list (or deque, or whatever) protected by a condition: the consumer waits on the condition until there's something available, and the producer makes something available by adding it to the list and signaling the condition. If you only ever want the last value, there's really no reason for the list. So, you can do this:
class OneQueue(object):
    def __init__(self):
        self.value = None
        self.condition = threading.Condition()
        self.sentinel = object()
    def get(self):
        with self.condition:
            while self.value is None:
                self.condition.wait()
            value, self.value = self.value, None
            return value
    def put(self, value):
        with self.condition:
            self.value = value
            self.condition.notify()
    def close(self):
        self.put(self.sentinel)

(Because I'm now using None to signal that nothing is available, I had to create a separate sentinel to signal that we're done.)
The problem with this design is that if the producers puts multiple values while the consumer is too busy to handle them, it can miss some of them—but in this case, that ""problem"" is exactly what you were looking for.
Still, using lower-level tools always means there's a lot more to get wrong, and this is especially dangerous with threading synchronization, because it involves problems that are hard to wrap your head around, and hard to debug even when you understand them, so you might be better off using a Queue anyway.",A,0
5594930,2011-04-08 12:18:11.830000+00:00,"
to workaround python bug #1124861 on Python2.4 you could attach stdin to a NUL device


import os
from subprocess import PIPE, STDOUT, Popen

lines = []
p = Popen(cmd, bufsize=1, stdin=open(os.devnull), stdout=PIPE, stderr=STDOUT)
for line in iter(p.stdout.readline, ''):
      print line,          # print to stdout immediately
      lines.append(line)   # capture for later
p.stdout.close()
p.wait()
",A,12
2477610,2010-03-19 13:27:47.167000+00:00,"requirements.txt:
-i http://dist.repoze.org/zope2/2.10/simple
zopelib

Example:
$ pip install -r requirements.txt
...
Successfully installed zopelib
",A,35
33753782,2015-11-17 09:51:20.137000+00:00,"You have made either str or map a string:
>>> map = 'some string'
>>> x = [1, 2]
>>> "" "".join(map(str, x))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'str' object is not callable
>>> del map
>>> str = 'some string'
>>> "" "".join(map(str, x))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'str' object is not callable

The error message tells you what type of object you tried to apply the (...) call syntax to; it is not the name used in the call expression.
In the interactive session, simply delete the name and the built-in type will re-emerge:
>>> str = 'some string'
>>> str
'some string'
>>> del str
>>> str
<type 'str'>
",A,3
13607810,2012-11-28 14:48:11.167000+00:00,"Just put celldict inside another dict:
json.dumps({'success': True, 'data': celldict.values()})

You'll have to add the Week key to the celldict dictionaries first:
for d in celldict.itervalues():
    celldict['Week'] = '1238'

or use create a copy of each dict on-the-fly:
json.dumps({'success': True, 'data': [dict(d, Week='1238') for d in celldict.values()]})

The latter method, with some indentation, produces:
>>> print json.dumps({'success': True, 'data': [dict(d, Week='1238') for d in celldict.values()]}, indent=4)
{
    ""data"": [
        {
            ""OUT3FA_5"": 24, 
            ""Week"": ""1238"", 
            ""Total_IN1"": 22, 
            ""IN1"": 59
        }, 
        {
            ""OUT3FA_5"": 12, 
            ""Week"": ""1238"", 
            ""Total_IN1"": 37, 
            ""IN1"": 37
        }
    ], 
    ""success"": true
}

Reading between the lines, it seems as if the 1224 and 1225 keys in your input example are actually the week numbers you are referring to. If so, they are easily incorporated:
json.dumps({'success': True, 'data': [dict(d, Week=k) for k, d in celldict.iteritems()]})

would produce:
{
    ""data"": [
        {
            ""OUT3FA_5"": 24, 
            ""Week"": ""1225"", 
            ""Total_IN1"": 22, 
            ""IN1"": 59
        }, 
        {
            ""OUT3FA_5"": 12, 
            ""Week"": ""1224"", 
            ""Total_IN1"": 37, 
            ""IN1"": 37
        }
    ], 
    ""success"": true
}
",A,9
34203814,2015-12-10 14:00:09.193000+00:00,"You need to pass in a sequence for the second argument. Using just parentheses does not create a sequence. To top this off, if photoID then that is a sequence too, one that consists of individual characters.
To create a tuple, you need to use a comma. Parentheses are optional here:
parameters = photoID,

or
parameters = (photoID,)

If you find it easier to avoid mistakes here, you could also make it a list:
parameters = [photoID]

You only have to do this once.
As a side note, you can use the MySQLdb connection object, as well as the cursor, as context managers:
with connection, cursor:
    ratings_delete = """"""
        DELETE FROM ratings
        WHERE photoID= %s
    """"""

    cursor.execute(ratings_delete, (photoID,))

    photo_delete = """"""
        DELETE FROM photo
        WHERE photoID = %s
    """"""

    cursor.execute(photo_delete, (photoID,))

The with statement will then take care of closing the cursor and connection for you, and if nothing has gone wrong in the block (no exceptions were raised), will also commit the transaction for you.",A,0
21087460,2014-01-13 09:15:59.163000+00:00,"The root problem is that one of response, result[0], or result[1] is actually a unicode string, not an encoded str string.
So, when you call (picking one arbitrarily) response.decode('cp1250', 'replace'), you're asking to decode something that's already decoded to Unicode. What Python 2.x does with this is to first encode it to your default encoding (ASCII) so that it can decode it as you requested. And that's why you're getting a UnicodeEncodeError from trying to call decode.*
To fix this, you're going to have to figure out which one of the three is wrong, and why. That's not possible with a giant mess of a statement with 4 decode calls in it, but it's easy if you break it up into separate statements, or just add some print debugging to see what's in those variables right before they get used.
However, it would make your life a whole lot easier to reorganize your code completely. Instead of converting everything back and forth all over the place, giving yourself dozens of places to make a simple mistake that ends up causing an un-debuggable error halfway across your program, just decode all of your input at input time, process everything as Unicode, then encode everything at output time.
By the way, if you haven't read Python's Unicode HOWTO, and the blog post The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!), go read them before going any further.

* If you think this is a silly design for a language… well, that's the main reason Python 3 exists. In Python 3, you can't decode a unicode or encode a bytes, so the error shows up as early as possible, and tells you exactly what's wrong, instead of making you try to hunt down where you called the wrong method on the wrong type and got an error that makes no sense. So if you want to use Python 2 instead of 3, you don't get to complain that Python 2's design is sillier than 3's.",A,1
1893312,2009-12-12 12:45:47.217000+00:00,"Like this
results = foo()
x= results[0]
y= results[1:]

Or change your function to return a two-tuple
def foo():
    ....
    return someList[0], someList[1:]
",A,3
7522103,2011-09-22 22:12:44.390000+00:00,"
The other way is just to have a couple of base users in the DB for the java software to connect and then ...

Use LDAP to store users, schema associations, privileges, roles, authorizations, etc., etc.
It is always up to the software -- via authorization rules -- to make sure only the right DBs are modified, only the right functions are called, etc 
The base users would have to appropriate permissions for specific user types, 

so a software glitch or compromise in the java software could lead to bad things for the DB.

That's always true.  So use testing to prevent this mysterious ""glitch"".
The database is not magically ""more secure"" or magically ""less subject to software glitches"".  DB security is just as trustworthy as application security.  Both are just as trustworthy as the administrative procedures and human factors.
Use application-level authorization.  Use LDAP.  
Don't impute magical powers to RDBMS authorization schemes.  They're not ""better"" or ""more trustworthy"".  
Application authorization checks -- in most languages and frameworks -- are single line-of-code decorators (or if statements) that explicitly define what group of users has access to the functionality.  It's not difficult.  It's not subject to ""glitches"".",A,1
581356,2009-02-24 11:05:43.043000+00:00,"What is the best setup for a development environment?
Doesn't much matter.  We use Django, which runs in Windows and Unix nicely.  For production, we use Apache in Red Hat.
Is having to reload webserver to see the changes considered normal?
Yes.  Not clear why you'd want anything different.  Web application software shouldn't be dynamic.  Content yes.  Software no.
In Django, we develop without using a web server of any kind on our desktop.  The Django ""runserver"" command reloads the application under most circumstances.  For development, this works great.  The times when it won't reload are when we've damaged things so badly that the app doesn't properly.
What is the best setup to deploy a working Python app to production and why?
""Best"" is undefined in this context.  Therefore, please provide some qualification for ""nest"" (e.g., ""fastest"", ""cheapest"", ""bluest"")
Is it worth diving straight with a framework or to roll something simple of my own?
Don't waste time rolling your own.  We use Django because of the built-in admin page that we don't have to write or maintain.  Saves mountains of work.
How exactly are Python apps served if I have to reload httpd to see the changes?
Two methods:

Daemon - mod_wsgi or mod_fastcgi have a Python daemon process to which they connect.  Change your software.  Restart the daemon.
Embedded - mod_wsgi or mod_python have an embedded mode in which the Python interpreter is inside the mod, inside Apache.  You have to restart httpd to restart that embedded interpreter.

Do I need to look into using multi-threaded?
Yes and no.  Yes you do need to be aware of this.  No, you don't need to do very much.  Apache and mod_wsgi and Django should handle this for you.",A,8
12584802,2012-09-25 14:11:33.307000+00:00,"If all you want to do is print, then you still have the keys; just print those too:
from operator import itemgetter 

print '{' + ', '.join([': '.join(map(repr, item)) for item in sorted(items, key=itemgetter(1), reverse=True)]) + '}'

Output:
>>> print '{' + ', '.join([': '.join(map(repr, item)) for item in sorted(mydict.items(), key=itemgetter(1), reverse=True)]) + '}'
{'platinum': 1000, 'gold': 500, 'silver': 300, 'bronze': 100}
",A,1
50687081,2018-06-04 18:56:54.490000+00:00,"Your code as originally posted was legal, because the long string concatenation expression was inside parentheses, and parentheses automatically allow you continue an expression onto the next line:
output = runcommand(""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile +
"" --FRT --FRACT --tensor --nii "" + dtinii + "" -g "" + bvalfile +
"" -b "" + bvecfile)

However, when you take the exact same expression outside of parentheses, it's no longer valid:
command = ""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile +
"" --FRT --FRACT --tensor --nii "" + dtinii + "" -g "" + bvalfile +
"" -b "" + bvecfile

… because now it's three separate expressions, one on each line. And the first one raises a SyntaxError, because it ends with a + with no right arguments.
And if you move the hanging + to the next line, as in your final posted code:
command = ""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile
+ "" --FRT --FRACT --tensor --nii "" + dtinii + "" -g "" + bvalfile
+ "" -b "" + bvecfile

… now it's syntactically valid, but nonsensical—the first line is fine, but the second line is trying to apply the unary + operator to a string, so you get a TypeError.

You can fix this by putting the expression back inside parentheses:
command = (""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile
+ "" --FRT --FRACT --tensor --nii "" + dtinii + "" -g "" + bvalfile
+ "" -b "" + bvecfile)

… or by using backslash continuation:
command = ""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile \
+ "" --FRT --FRACT --tensor --nii "" + dtinii + "" -g "" + bvalfile \
+ "" -b "" + bvecfile

Although really, even though indentation isn't meaningful to Python inside expressions the way it is in statements, it is meaningful to human readers, so you should try to make it more readable, maybe something like this:
command = (
    ""'C:\Program Files\BrainSuite18a\bdp\bdp.exe' "" + bfcfile
    + "" --FRT --FRACT --tensor --nii "" + dtinii
    + "" -g "" + bvalfile
    + "" -b "" + bvecfile
)


However, the real fix here should be not using string concatenation in the first place. You're not using the shell here, and in fact you're fighting against it, and not entirely successfully. So just use a list of arguments and don't ask for the shell, and now we don't have to worry about quoting or spacing or getting string concatenation right:
command = [
    r""C:\Program Files\BrainSuite18a\bdp\bdp.exe"", bfcfile,
    ""--FRT"", ""--FRACT"", ""--tensor"", ""-nii"", dtinii,
    ""-g"", bvalfile, ""-b"", bvecfile
]

And now, call it without shell=True:
returned_value = subprocess.call(command)


As a side note, notice that I added an r before the path to the program. A \b in a Python string literal doesn't mean a backslash and a letter b, it means a ^H backspace character. When using Windows pathnames with backslashes as literal values in your source code, always either escape them, or use a raw r prefix.
Also, I'm not sure why you added the str(…) calls all over your code to things that are already strings, but… that's never going to help anything, it just makes things harder to read, so I left them out, as in your original version.",A,0
30081894,2015-05-06 16:01:17.017000+00:00,"The fundamental misunderstanding here is in thinking that range is a generator. It's not. In fact, it's not any kind of iterator.
You can tell this pretty easily:
>>> a = range(5)
>>> print(list(a))
[0, 1, 2, 3, 4]
>>> print(list(a))
[0, 1, 2, 3, 4]

If it were a generator, iterating it once would exhaust it:
>>> b = my_crappy_range(5)
>>> print(list(b))
[0, 1, 2, 3, 4]
>>> print(list(b))
[]

What range actually is, is a sequence, just like a list. You can even test this:
>>> import collections.abc
>>> isinstance(a, collections.abc.Sequence)
True

This means it has to follow all the rules of being a sequence:
>>> a[3]         # indexable
3
>>> len(a)       # sized
5
>>> 3 in a       # membership
True
>>> reversed(a)  # reversible
<range_iterator at 0x101cd2360>
>>> a.index(3)   # implements 'index'
3
>>> a.count(3)   # implements 'count'
1


The difference between a range and a list is that a range is a lazy or dynamic sequence; it doesn't remember all of its values, it just remembers its start, stop, and step, and creates the values on demand on __getitem__.
(As a side note, if you print(iter(a)), you'll notice that range uses the same listiterator type as list. How does that work? A listiterator doesn't use anything special about list except for the fact that it provides a C implementation of __getitem__, so it works fine for range too.)

Now, there's nothing that says that Sequence.__contains__ has to be constant time—in fact, for obvious examples of sequences like list, it isn't. But there's nothing that says it can't be. And it's easier to implement range.__contains__ to just check it mathematically ((val - start) % step, but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why shouldn't it do it the better way?
But there doesn't seem to be anything in the language that guarantees this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact CPython 3.0-3.1 didn't include it.)

If range actually were a generator, like my_crappy_range, then it wouldn't make sense to test __contains__ this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is 1 still in the generator? Should testing for 1 cause it to iterate and consume all the values up to 1 (or up to the first value >= 1)?",A,648
51374546,2018-07-17 06:26:02.680000+00:00,"First, let's create some constants for KiB, MiB, GiB, etc. Don't worry about hex here, we just want the numbers in whatever form makes most sense to you—whether that's 1024 or 0x400 or 2**10 doesn't matter, since those are all the same number. So:
KiB = 1024
MiB = 1024 * KiB
GiB = 1024 * MiB
TiB = 1024 * GiB

Now, how much is 12MiB? Easy:
12 * MiB

Now, how much is 12MiB in hex?
hex(12 * MiB)

This gives you '0xc00000'. 

As far as ""adding in hex"", that's not a thing you should do. Just add the numbers as numbers, then format the result in hex. For example, if you want to add 32KiB + 16KiB + 1MiB:
hex(32*KiB + 16*KiB + 1*MiB)


You wanted a bit more control over the output. If that's important, you'll need to use format strings (whether via the format function, the str.format method, or f-string literals) instead of hex.
You can specify a width to pad to, and a fill character to pad with. And you can use # to add the 0x automatically (in which case we have to pad to 8+2=10 characters instead of 8), or add the 0x manually (in which case we only have to pad to 8 characters, which makes a bit more sense):
format(12 * MiB, '#010x')
f""{12*MiB:#010x}""
f""0x{12*MiB:08x}""

You also wanted to separate the digits into blocks of 4. We can use a grouping character to get pretty close, but then we have to add an extra 1 to the width:
f""0x{12*MiB:09_x}""

… at which point we might as well use # again:
f""{12*MiB:#011_x}""

Now you've got '0x00c0_0000'. If you really need spaces between groups instead of underscores, then you'll need to get a bit ugly:
f""{12*MiB:#11_x}"".replace('_', ' ')

(I believe there was a proposal to allow arbitrary grouping characters, but it was rejected, so the only options are _, ,, or n for ""do whatever my locale says"".)

So, if you want to print out your table:
print( ""size        hex '0x' notation"")
print(f""1 MiB       {12*MiB:#011_x}"")
print(f""12 KiB      {12*KiB:#011_x}"")
print(f""128 KiB     {128*KiB:#011_x}"")

Or you can write a function. While we're at it, let's use an Enum instead of a bunch of separate constants:
import enum
class Prefixes(enum.Enum):
    Ki = 1024
    Mi = 1024 * Ki
    Gi = 1024 * Mi
    Ti = 1024 * Gi
def printrow(n, prefix):
    label = f""{n} {prefix.name}B""
    print(f""{label:<12}{n * prefix.value:#011_x}"")
print(""size        hex '0x' notation"")
printrow(1, Prefixes.Mi)
printrow(12, Prefixes.Ki)
printrow(128, Prefixes.Ki)

This is obviously a lot more extensible, if you want to add more rows to the table.",A,3
16507825,2013-05-12 13:23:30.737000+00:00,"You are not quoting the message value here, you are generating text like if(Warturtle==""Warturtle""){.
You'll need to apply quoting, but to do so sanely I recommend you use JSON; that format is a subset of JavaScript and produces valid JavaScript literals.
You'll need to do so in your view (using json.dumps()), or you could use this Django snippet and do it straight in the template (no need to call serializers.serialize in the annotate method):
if({{message.get()|jsonify}} ==""Warturtle""){
",A,1
19024857,2013-09-26 09:47:31.947000+00:00,"You need an and between the conditions in the parentheses.
Boolean logic is tricky, and if often helps to go through with examples. Consider this line:
FAILED blah blah Date: blah

So, going through your conditions one by one:

Failed is in the line, so let's check the next condition.
""Result:"" is not in the line, so that part is true.
We've got an OR, and we already have a True result, so there's no need to even check if ""Date:"" is in the line.
So both sides of the AND are true, so the whole thing is true.

Obviously, exactly the same would happen if the line contained ""Result:"" but not ""Date:"".
If you had AND inside the parens, it would need to check that both ""Date"" was not in the line and ""Result"" is not in the line, rather than being happy if either were not there.
A different way to express the condition which might be clearer is this:
if ""[FAILED]"" in line and not ((""Result:"" in line) or (""Date:"" in line)):
",A,1
22878943,2014-04-05 09:18:16.143000+00:00,"It might be a buffering issue. Dialog can't show anything until the child process (youtube-dl) flushes its stdout/stderr buffers. The following works:
#!/usr/bin/env python3
import sys
from subprocess import Popen, PIPE, STDOUT
from dialog import Dialog # $ pip install pythondialog

d = Dialog(dialog=""dialog"")
d.set_background_title(""Random title"")
with Popen([sys.executable or ""python"", ""-u"", ""-c"", r""""""import sys, time

for i in range(100):
    print(i)
    if i % 10 == 0:
        sys.stderr.write('.\n')
    time.sleep(.1)
""""""], stdout=PIPE, stderr=STDOUT, bufsize=0) as p:
    d.progressbox(fd=p.stdout.fileno(), text=""youtube-dl output:"")

bufsize=0 disables buffering on Python (parent) side. -u makes child's streams unbuffered (line-buffered on Python 3). ",A,1
20298944,2013-11-30 10:06:40.480000+00:00,"Well, first of all, Python is not Java. Some things are just different.
According to the semantics of Python, name resolution in classes has to work that way. Otherwise there'd be no way of calling methods from an instance, since methods are also class attributes.
You're right that an instance variable elsewhere could shadow the class variable. But in my opinion, that's a feature, not a bug. If you really wanted to be sure you were always accessing the class variable, you could do self.__class__.foo, but I don't think there's any need.",A,3
28642688,2015-02-21 06:02:10.623000+00:00,"The first thing that you can try is to run the command as is:
import subprocess

subprocess.check_call(r'""C:\Program Files (x86)\ULINK DM2012 PRO NET\v970\DriveMaster.exe"" /s:c:\Program Files (x86)\ULINK DM2012 PRO NET\v970\Scripts\ATA\SATA_TestBatch.srt')
",A,1
23475976,2014-05-05 15:14:50.160000+00:00,"You stored the str.lower method, not the result. Add () to actually call that method:
raw_input(""Would you like to play again? Y|N"").lower()

Python methods are objects just like everything else, you can store those just like you can store a string:
>>> 'NO'.lower
<built-in method lower of str object at 0x1058d8c88>
>>> 'NO'.lower()
'no'

Instead of using a flag variable, use break to exit the loop, and True to make the loop endless:
while True:
    game()
    play_again = raw_input(""Would you like to play again? Y|N"").lower
    if play_again == ""n"":
        break

Here the break keyword will end the loop right there without first having to loop back to the top and testing a variable.",A,3
54479731,2019-02-01 12:39:51.563000+00:00,"All you need to do is replace urllib2 with urllib.request. You are not using anything that has moved to other urllib.* modules:
import urllib.request

opener = urllib.request.build_opener(urllib.request.HTTPHandler)
request = urllib.request.Request(url=event['ResponseURL'], data=data)
request.add_header('Content-Type', '')
request.get_method = lambda: 'PUT'
url = opener.open(request)

You can always run the 2to3 command-line tool on your Python 2 code and see what changes it makes; the default action is to output changes on stdout in unified diff format.
The urllib fixer will then also add imports for urllib.error and urllib.parse at the top, because it knows that code that imported urllib2 could need any of the 3 urllib.* modules; it isn't smart enough to limit the import only to those that are actually needed after transforming the rest of the urllib2 references in the module.",A,1
23403870,2014-05-01 07:37:08.353000+00:00,"Yes, using or for it's short-circuiting properties was the norm, before the conditional expression construct was added to the language.
It was, and still is, perfectly pythonic to use:
foo = bar or baz

to fall back to a default if bar is false-y (evaluates to false in a boolean context). The fact that it short-circuits also lets you do things like:
foo = bar or expensive_calculation(baz)  # only if bar is false-y

and not have expensive_calculation() execute if bar is truth-y (evaluates to true in a boolean context). Similarly, you can use and to ensure that preconditions are met:
foo = bar and bar(baz)  # call `bar()` only if it is truth-y

You should use a conditional expression for:
foo = bar and spam or eggs

however. That's what the conditional expression aims to replace. The idea was that if bar is truth-y, then spam is picked, else eggs is picked. But that breaks down if spam is false-y! This was a common source of errors, while
foo = spam if bar else eggs

always picks spam if bar is truth-y.",A,4
15305248,2013-03-08 23:44:05.357000+00:00,"lxml has several issues related to handling Unicode. It might be best to use bytes (for now) while specifying the character encoding explicitly:
#!/usr/bin/env python
import glob
from lxml import html
from bs4 import UnicodeDammit

for filename in glob.glob('*.html'):
    with open(filename, 'rb') as file:
        content = file.read()
        doc = UnicodeDammit(content, is_html=True)

    parser = html.HTMLParser(encoding=doc.original_encoding)
    root = html.document_fromstring(content, parser=parser)
    title = root.find('.//title').text_content()
    print(title)

Output
Unicode Chars: 은 —’
Unicode Chars: 은 —’
Unicode Chars: 은 —’
",A,16
26093118,2014-09-29 05:35:45.583000+00:00,"If you really need this to be fast, the fastest option is to monkeypatch yourself at initialization:
def __init__(self, wrappee):
    for name, value in inspect.getmembers(wrappee, callable):
        if not hasattr(self, name):
            setattr(self, name, value)

This will give your Wrapper instances normal data attributes whose values are bound methods of the Wrappee. That should be blazingly fast. Is it?
class WrapperA(object):
    def __init__(self, wrappee):
        self.wrappee = wrappee
        for name, value in inspect.getmembers(wrappee, callable):
            if not hasattr(self, name):
                setattr(self, name, value)

class WrapperB(object):
    def __init__(self, wrappee):
        self.wrappee = wrappee
    def __getattr__(self, name):
        return getattr(self.wrappee, name)

In [1]: %run wrapper
In [2]: o2 = Wrappee()
In [3]: o1a = WrapperA(o2)
In [4]: o1b = WrapperB(o2)
In [5]: %timeit o2.bar()
10000000 loops, best of 3: 154 ns per loop
In [6]: %timeit o1a.bar()
10000000 loops, best of 3: 159 ns per loop
In [7]: %timeit o1b.bar()
1000000 loops, best of 3: 879 ns per loop
In [8]: %timeit o1b.wrapper.bar()
1000000 loops, best of 3: 220 ns per loop

So, copying bound methods has a 3% cost (not sure why it even has that much…). Anything more dynamic than this would have to pull attributes from self.wrapper, which has a minimum 66% overhead. The usual __getattr__ solution has 471% overhead (and adding unnecessary extra stuff to it can only make it slower).
So, that sounds like an open and shut win for the bound-methods hack, right?
Not necessarily. That 471% overhead is still only 700 nanoseconds. Is that really going to make a difference in your code? Probably not unless it's being used inside a tight loop—in which case you're almost certainly going to want to copy the method to a local variable anyway.
And there are a lot of downsides of this hack. It's not the ""one obvious way to do it"". It won't work for special methods that aren't looked up on the instance dict. It's statically pulling the attributes off o2, so if you create any new ones later, o1 won't be proxying to them (try building a dynamic chain of proxies this way…). It wastes a lot of memory if you have a lot of proxies. It's slightly different between Python 2.x and 3.x (and even within the 2.x and 3.x series, if you rely on inspect), while __getattr__ has very carefully been kept the same from 2.3 up to the present (and in alternate Python implementations, too). And so on.
If you really need the speed, you may want to consider a hybrid: a __getattr__ method that caches proxied methods. You can even do it in two stages: something that's called once, you cache the unbound method in a class attribute and bind it on the fly; if it's then called repeatedly, you cache the bound method in an instance attribute.",A,5
452403,2009-01-16 23:26:08.133000+00:00,"Quotes about Python:

""Python is fast enough for our site
  and allows us to produce maintainable
  features in record times, with a
  minimum of developers,"" said Cuong Do,
  Software Architect, YouTube.com.
",A,5
17726210,2013-07-18 14:23:52.537000+00:00,"Methods are just attributes, so use getattr() to retrieve one dynamically:
MethodWanted = 'children'

getattr(ObjectToApply, MethodWanted)()

Note that the method name is children, not .children(). Don't confuse syntax with the name here. getattr() returns just the method object, you still need to call it (jusing ()).",A,9
1994563,2010-01-03 10:35:09.107000+00:00,"Why would you want to do that? runserver is for development only, it should never be used in production. And if you're running via Apache, it should manage threads/processes for you anyway.",A,7
54079301,2019-01-07 17:46:20.243000+00:00,"If you followed the typical imports listed in the documentation:
>>> from gremlin_python import statics
>>> from gremlin_python.structure.graph import Graph
>>> from gremlin_python.process.graph_traversal import __
>>> from gremlin_python.process.strategies import *
>>> from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection

then bothE is available as __.bothE.
The methods in the __ namespace can be added to your notebook globals with:
>>> statics.load_statics(globals())

so you can access bothE directly without a prefix.
Quoting from the documentation:

Moreover, by importing the statics of Gremlin-Python, the class prefixes can be omitted.
>>> statics.load_statics(globals())


and

Finally, statics includes all the -methods and thus, anonymous traversals like .out() can be expressed as below. That is, without the __.-prefix.
>>> g.V().repeat(out()).times(2).name.fold().toList()
[[ripple, lop]]



Caveat: I am not a Gremlin-Python user nor is it practical for me to install Gremlin to verify the above completely. I based this on reading the documentation and a scan of the project source code.",A,0
29394599,2015-04-01 15:16:25.717000+00:00,"You should leave __dict__ be, only use it for attributes that live directly on the instance.
If you need to produce a dictionary with attribute names and values that includes the property, you could just add another property or method that produces a new dicitonary:
@property
def all_attributes(self):
    return dict(vars(self), prop1=self.prop1)

This could be automated with introspection to detect all property objects, but take into account that you use properties to avoid having to do the calculation up-front in the first place; triggering calculations for all properties may not be all that desirable.",A,3
33801925,2015-11-19 10:56:39.940000+00:00,"You used a , comma where you should have used a : colon:
pages_dict = {""url"": ""html_file"", ""/"", ""index.html""}
                                     ^

This is easily corrected to:
pages_dict = {""url"": ""html_file"", ""/"": ""index.html""}

The @app.route() decorator registers endpoints, each of which have to have a unique name. By default the endpoint name is taken from the function, so if you are reusing a function you need to provide a name explicitly:
for k, v in pages_dict.items():
    @app.route(k, endpoint=k)
    def render():
        return render_template(v)

You'll still run into issues with closures here; the v used in render() is going to be bound to the last value from the loops. You probably want to pass it in as an argument to render() instead:
for k, v in pages_dict.items():
    @app.route(k, endpoint=k)
    def render(v=v):
        return render_template(v)

This binds v as a local in render() rather than leave it a closure. See Local variables in Python nested functions for more details.",A,1
34433283,2015-12-23 10:13:07.753000+00:00,"You can't deceive the type() function or the is operator, no.
Don't use type() to check for types. Use isinstance() and you can just subclass int:
>>> class MyType(int):
...     pass
...
>>> isinstance(MyType(), int)
True

or test for the numbers.Integral ABC and you don't even have to subclass, only register your type:
>>> from numbers import Integral
>>> isinstance(42, Integral)
True
>>> isinstance(MyType(), Integral)
True
>>> class Foo: pass
...
>>> isinstance(Foo(), Integral)
False
>>> Integral.register(Foo)
>>> isinstance(Foo(), Integral)
True

If you can't change the assertion you are simply out of luck. At most you can monkey-patch f() to not return your type in that module.",A,0
37318577,2016-05-19 09:03:22.857000+00:00,"Use isinstance(b, set).
The types references to built-in types are just there for convenience* and are otherwise deprecated (they have been removed from the Python 3.x version of the types module). As such set was never added.
For example, types.ListType is just an alias for list:
>>> import types
>>> types.ListType is list
True
>>> isinstance([1, 2, 3], list)
True

If you must have a SetType reference in the types module, simply add the alias yourself:
types.SetType = set

If you are looking for an abstract base class for the basic container type, use the collections module ABCs; these signal what methods a type support and checking against these types can let you detect a wider set of types that support specific operations (rather than be an instance of just the list and set types):
>>> from collections import Set, MutableSequence
>>> isinstance([1, 2, 3], MutableSequence)
True
>>> isinstance({1, 2, 3}, Set)
True


* History lesson: Once upon a time, the built-in types like int and list were not really classes, and the built-in names int() and list() were simply functions to convert to the built-in types. That means that the names were simply not the same object as types.IntType and types.ListType and you had to use the types module to be able to use isinstance() on a list object. That time is now long, long behind us, and the role of the types module has changed.",A,7
8680666,2011-12-30 14:51:37.680000+00:00,"Why have you set the primary key field manually? Unless you need to call the field something different from id, or give it different attributes, you should just leave it out of the model and let Django add it automatically.
Your immediate problem is that you've used IntegerField instead of AutoField, but if you removed your definition automatically that would be done for you.",A,1
19200375,2013-10-05 17:02:50.597000+00:00,"Break out of the for loop when you find a match, then give the loop an else: suite. That suite will only be executed when the loop was not broken out of:
def cipher_attack(partial_alphabetic, ciphertext):
    var = """"
    for letter in ciphertext:
        for i, pa in enumerate(partial_alphabetic):
            if pa == letter:
                var += alphabet[i % 26]
                break
        else:
            var += ""!""                    
    return var

Note that you can loop over ciphertext and partial_alphabetic directly, no need to create an index first. Having the index, however, helps match against alphabet, but creating the index in addition is easier with enumerate().
Demo:
>>> import string
>>> alphabet = string.ascii_letters
>>> def cipher_attack(partial_alphabetic, ciphertext):
...     var = """"
...     for letter in ciphertext:
...         for i, pa in enumerate(partial_alphabetic):
...             if pa == letter:
...                 var += alphabet[i % 26]
...                 break
...         else:
...             var += ""!""                    
...     return var
... 
>>> cipher_attack(""!wertyuiopasdfghjklzxcvbnm"", ""rqr"")
'd!d'
",A,1
18452363,2013-08-26 20:20:20.220000+00:00,"bdist_egg is a command supplied by setuptools. Make sure you import from that project in setup.py, not from distutils:
from setuptools import setup

The tutorial does tell you to do this, but it appears you missed that part.",A,13
39811695,2016-10-01 21:38:11.847000+00:00,"You can already use += on existing keys:
>>> from collections import OrderedDict
>>> thedict = OrderedDict()
>>> thedict['a'] = []  # set initial list
>>> thedict['a'] += ['foo']
>>> thedict['a'] += ['bar']
>>> thedict
OrderedDict([('a', ['foo', 'bar'])])

Note that += on a list is basically the same thing as list.extend(), so you need to append lists.
If you want this to work for keys that don't exist yet, implement a __missing__ method, rather than __setitem__:
class ListDict(OrderedDict):
    def __missing__(self, key):
        self[key] = []
        return self[key]

When a key is missing during dict[key] lookup, __missing__ is called and it's return value is returned instead of raising a KeyError.
Now both + and += work on missing keys too:
>>> thedict = ListDict()
>>> thedict['a'] += ['foo', 'bar']
>>> thedict['b'] = thedict['b'] + ['spam', 'ham']
>>> thedict
ListDict([('a', ['foo', 'bar']), ('b', ['spam', 'ham'])])

If you concatenation must work without adding lists, you could produce a custom list subclass too:
class ConcatList(list):
    def __add__(self, value):
        return type(self)(super(ContactList, self).__add__([value]))
    def __iadd__(self, value):
        self.append(value)
        return self

then use that type in __missing__ (and convert any new lists set directly in __setitem__):
class ListDict(OrderedDict):
    def __missing__(self, key):
        self[key] = ConcatList()
        return self[key]
    def __setitem__(self, key, value):
        if not isinstance(key, ConcatList):
           value = ConcatList([value])
        super(ListDict, self).__setitem__(key, value)

after which you can forgo brackets:
>>> thedict = ListDict()
>>> thedict['a'] += 'foo'
>>> thedict['b'] = thedict['b'] + 'bar'
>>> thedict
ListDict([('a', ['foo']), ('b', ['bar'])])
",A,2
41012261,2016-12-07 08:07:12.870000+00:00,"& is a metacharacter in your shell, and tells your shell to put the command in the background. This is why you see the PID of the backgrounded process echoed on your terminal:
[1] 8682

Escape it by using a backslash or quoting if you don't want the shell to interpret it:
$ python test.py sds\&ss
$ python test.py ""sds&ss""
",A,6
31744279,2015-07-31 10:45:47.377000+00:00,"Because exec doesn't carry the context over to the statement being executed.
pass can be used anywhere, so the context doesn't matter. continue can only be used in the context of a loop, but that context is not available to exec.
You can only use continue in an exec statement if the loop itself is also part of the executed code:
f = 'for i in range(10): continue'
exec f

In other words, you can only use exec for complete statements, where a single continue (or break) is not complete.",A,4
14563105,2013-01-28 13:28:40.770000+00:00,"You define a list literal by putting the elements in square brackets:
string = ['this is a longish sentence about things',
          'this is a longish sentence about things number 2',
          'this is a longish sentence about things number 3']

Alternatively, you build the list by defining an empty list, then appending the elements:
string = []
string.append('this is a longish sentence about things')
string.append('this is a longish sentence about things number 2')
string.append('this is a longish sentence about things number 3')

I strongly recommend you read the Python tutorial before continuing, it explains building python types and how to manipulate them for you.",A,7
7793096,2011-10-17 11:29:36.130000+00:00,"You can send a flag from the urlconf to the view by using the optional third parameter:
url(r'^alumni/(?P<blog_author>[^/]+)/$', 'entries', {'alumni': True}, name='blog'),
",A,7
27611349,2014-12-22 22:23:13.733000+00:00,"Your decorator will be passed the method function when Python applies the decorator. You do not need give your decorator factory the method as an argument:
@cachedAttributes(doctyperegex = re.compile(r'<!DOCTYPE.*?>\s*', re.S|re.U),
              htmlregex = re.compile(r'<html.*?>\s*', re.S|re.U),
              metaregex = re.compile(r'<meta(.*?)>', re.S|re.U))

Your cachedAttributes() callable then returns the actual decorator:
def cachedAttributes(**kwargs):
    def decorator(func):
        func.__dict__.update(kwargs)
        return func
    return decorator

So Python calles cachedAttributes(), which returns a decorator. That is then called passing in the function object and the return values is used as the replacement of the decorated function. That can be the original function object, like you are doing here.",A,1
37877633,2016-06-17 09:14:15.313000+00:00,"Your problem is not in where to place the return statement. The issue is that you redefine ip_list within each iteration of the for loop, so there is no point at which you ever have all the information to return it.
I'm not sure exactly what format you want to return the values in, but perhaps you could define a combined_ip_list variable as an empty list before the for loop, append the value of ip_list instead of printing it at the end of that loop, then return that value outside the loop.",A,0
40327593,2016-10-30 09:17:46.463000+00:00,"The trick to recursion is to solve the most basic case and call itself to solve a (now) smaller problem:
def repeat_int(seq_or_int, nested=False):
    try:
        first, *rest = seq_or_int
    except TypeError:  # got int
        return [seq_or_int] * 2  # repeat int
    except ValueError:  # empty
        result = []
    else:
        result = repeat_int(first, nested=True) + repeat_int(rest)
    return [result] if nested else result  # nest in a list if necessary

Example:
>>> repeat_int(1)
[1, 1]
>>> repeat_int([1, 2])
[1, 1, 2, 2]
>>> repeat_int([1, [2, 3]])
[1, 1, [2, 2, 3, 3]]

There are two basic cases here:

the input is not an iterable (a scalar such as int)—return the repeated input in a list
the input is empty—return an empty list.

The Python 3 iterable unpacking syntax chops off the first item in the input collection:
first, *rest = seq_or_int

To get the result for the rest of the collection, the recursive call is made with (now) smaller list: repeat_int(rest).
All that is left is to deal with the first item that is the same as the input may be a collection too. To preserve the structure, the nested indicator is passed and if it is set then the result list is wrapped in another list.",A,0
13939605,2012-12-18 18:57:05.733000+00:00,"You are missing a colon in the {3} field definition:
working_line =('{0} {1:<20} {2:<15} {3:<10}'.format(source,group,symbol,position))
",A,3
12428087,2012-09-14 16:00:16.793000+00:00,"Use sys.executable.
>>> import sys
>>> sys.executable
'/usr/bin/python'

os.path.split() removes the last component for you if all you need is the path:
>>> import os.path
>>> os.path.split(sys.executable)
('/usr/bin', 'python')
",A,12
4584108,2011-01-03 11:38:08.123000+00:00,"This has nothing to do with Django, but is the responsibility of your database engine. Databases have a setting called 'collation', which determines how characters are sorted. For instance, for MySQL you probably want the greek_general_ci collation.",A,5
13852865,2012-12-13 04:17:25.427000+00:00,"@hyperboreus pointed out the cause of the error (the else branch executing before all items are seen).
To find an item in a sorted (""ordered"") list you could use bisect module that performs binary search (O(log(n)) instead of linear search item in alist (O(n)) e.g., for a million items binary search would require around a couple dozen operations against a million operations for the linear search.
from bisect import bisect

findsort = lambda x, L: ""found"" if L and L[bisect(L,x) - 1] == x else ""not found""
",A,2
17855145,2013-07-25 10:15:06.787000+00:00,"Yes, do not use tabs when indenting. It is too easy to mix tabs and spaces.
Quoting from the PEP 8 Python Style Guide:

Never mix tabs and spaces.
The most popular way of indenting Python is with spaces only. The second-most popular way is with tabs only. Code indented with a mixture of tabs and spaces should be converted to using spaces exclusively. When invoking the Python command line interpreter with the -t option, it issues warnings about code that illegally mixes tabs and spaces. When using -tt these warnings become errors. These options are highly recommended!
For new projects, spaces-only are strongly recommended over tabs. Most editors have features that make this easy to do.

Emphasis mine.
First, detect inconsistent indentation by running your code with python -tt scriptname.py. Fix those errors.
Then convert tabs to spaces, and configure your editor to only use spaces for indentation. The Python Style Guide recommends 4 spaces per indentation level, and so do I.",A,2
12585977,2012-09-25 15:15:35.183000+00:00,"From the list displays documentation:

When a list comprehension is supplied, it consists of a single expression followed by at least one for clause and zero or more for or if clauses. In this case, the elements of the new list are those that would be produced by considering each of the for or if clauses a block, nesting from left to right, and evaluating the expression to produce a list element each time the innermost block is reached.

Thus, your expression can be rewritten as:
thingys = []
for y in l:
    for x in y:
        thingys.append(x)
",A,5
15140202,2013-02-28 16:08:55.273000+00:00,"input() returns a string, not a number. You'll have to convert that:
add1 = int(input())

or
add1 = float(input())

depending on what you want your calculator to support, otherwise you are indeed performing maths operations with strings.",A,3
8598159,2011-12-22 00:40:06.020000+00:00,"There is no direct way to do it. As a workaround you could patch easygui.rootWindowPosition:
from Tkinter import Tk
import easygui

# calculate window position
root = Tk()
pos = int(root.winfo_screenwidth() * 0.5), int(root.winfo_screenheight() * 0.2)
root.withdraw()
rootWindowPosition = ""+%d+%d"" % pos

# patch rootWindowPosition
easygui.rootWindowPosition = rootWindowPosition
print(easygui.enterbox())
",A,2
21183429,2014-01-17 10:23:29.907000+00:00,"You can't get a form field to automatically put its value into the URL the form is submitting to (except by mucking about with the form in Javascript, which would be horrible).
But as you can see, you do get that data in the request parameters: since you're doing a GET, it's in request.GET['searchname']. So why don't you drop the username parameter to the URL/view in the first place, and just use that?",A,1
33988740,2015-11-29 22:09:01.100000+00:00,"All you need to do is to remove the second for loop and replace it with a ','.join(matching_words) call where you use j now in the string concatenation now:
newlist = ['{}:{}'.format(l, ','.join([w for w in list2 if w[0] == l])) for l in list1]

This isn't very efficient; you loop over all the words in list2 for each letter. To do this efficiently, you would be better of to preprocess the lists into a dictionary:
list2_map = {}
for word in list2:
    list2_map.setdefault(word[0], []).append(word)

newlist = ['{}:{}'.format(l, ','.join(list2_map.get(l, []))) for l in list1]

The first loop builds a dictionary mapping initial letter to a list of words, so that you can directly use those lists instead of using a nested list comprehension.
Demo:
>>> list1 = ['A', 'B']
>>> list2 = ['Apple', 'Banana', 'Balloon', 'Boxer', 'Crayons', 'Elephant']
>>> list2_map = {}
>>> for word in list2:
...     list2_map.setdefault(word[0], []).append(word)
...
>>> ['{}:{}'.format(l, ','.join(list2_map.get(l, []))) for l in list1]
['A:Apple', 'B:Banana,Balloon,Boxer']

The above algorithm loops twice through all of list2, and once through list1, making this a O(N) linear algorithm (adding a single word to list2 or a single letter to list1 increases the amount of time with a constant amount). Your version loops over list2 once for every letter in list1, making it a O(NM) algorithm, creating increasing the amount of time it takes exponentially whenever you add a letter or word.
To put that into numbers, if you expanded list1 to cover all 26 ASCII uppercase letters and expanded list2 to contain 1000 words, your approach (scanning all of list2 for words with a given letter) would make 26000 steps. My version, including pre-building the map, takes only 2026 steps. With list2 containing 1 million words, your version has to make 26 million steps, mine 2 million and 26.",A,2
29957722,2015-04-30 01:55:54.947000+00:00,"Looking at the BitBucket repo, I'm pretty sure the problem is that this code is Python 2.x-only. Someone's done some work to clean it up for an eventual port, but there's still more to be done.
This particular error is near the top of snapshot.py:
from util import partTypeNum

In Python 2.6, this is a relative import (it's ""deprecated"" by PEP 328, but I'm pretty sure you don't actually get the warning by default…), so it first looks in the same package as snapshot.py, where it finds a util.py, before looking through your sys.path.
In Python 3.4, it's an absolute import, so it just looks in your sys.path (well, it calls your top-level module finders, but usually that means looking in your sys.path), and there is no util.py there.
If you're trying to finish porting this sample code to 3.x yourself, just change it to an explicit relative import:
from .util import partTypeNum
",A,2
21342994,2014-01-24 21:39:52.193000+00:00,"Inside the Union, the device member is stored using 8 bits.
However, ctypes will automatically convert back and forth to native int types for you, using (a good approximation of) the usual C rules for converting between types. For example, try this:
>>> x.device = 257
>>> x.device
1

So, you don't have anything to fix here.

However, you do have a problem:

I'm trying to make an 11-bit ID field for a CAN communication channel.

A Union of an 8-bit int and a 3-bit int is 8 bits, just as in C. You want a Struct. See Structures and unions and the type references linked from there for details.
On top of that, while a Struct of a 3-bit int and an 8-bit int only has 11 bits of useful information, it will almost certainly take up at least 16 bits of memory, with 5 bits of padding.* You can only address memory a byte (8 bits) at a time. So… I'm not sure exactly what you were trying to do with this ID, but I suspect it's not going to work. If you explain your actual problem, ideally with some complete sample code, we can probably help.

* In fact, at least in current CPython versions, it will actually take up 32 bits of memory. See eryksun's comments for more details.",A,2
50866343,2018-06-14 21:43:15.677000+00:00,"No, they are not the same.
callable(output.write) just checks whether output.write is callable. Things that are callable include:

Bound method objects (whose type is types.MethodType).
Plain-old functions (whose type is types.FunctionType)
partial instances wrapping bound method objects (whose type is functools.partial)
Instances of you own custom callable class with a __call__ method that are designed to be indistinguishable from bound method objects (whose type is your class).
Instances of a subclass of the bound method type (whose type is that subclass).
…

type(output.write) == types.MethodType accepts only the first of these. Nothing else, not even subclasses of MethodType, will pass. (If you want to allow subclasses, use isinstance(output.write, types.MethodType).)
The former is almost certainly what you want. If I've monkeypatched an object to replace the write method with something that acts just like a write method when called, but isn't implemented as a bound method, why would your code want to reject my object?

As for your side question in the comments:

I do want to know if the exceptions.ValueError is necessary

No, it's not.
In Python 2.7, the builtin exceptions are also available in the exceptions module:
>>> ValueError is exceptions.ValueError
True

In Python 3, they were moved to builtins along with all the other builtins:
>>> ValueError is builtins.ValueError
True

But either way, the only reason you'd ever need to refer to its module is if you hid ValueError with a global of the same name in your own module.

One last thing:
As user2357112 points out in a comment, your solution doesn't really ensures anything useful.
The most common problem is almost certainly going to be output.write not existing at all. In which case you're going to get an AttributeError rather than the ValueError you wanted. (If this is acceptable, you don't need to check anything—just call the method and you'll get an AttributeError if it doesn't exist, and a TypeError if it does but isn't callable.) You could solve that by using getattr(output, 'write', None) instead of output.write, because None is not callable.
The next most common problem is probably going to be output.write existing, and being callable, but with the wrong signature. Which means you'll still get the same TypeError you were trying to avoid when you try to call it. You could solve that by, e.g., using the inspect module.
But if you really want to do all of this, you should probably be factoring it all out into an ABC. ABCs only have built-in support for checking that abstract methods exist as attributes; it doesn't check whether they're callable, or callable with the right signature. But it's not that hard to extend that support. (Or, maybe better, just grabbing one of the interface/protocol modules off PyPI.) And I think something like isinstance(output, StringWriteable) would declare your intention a lot better than a bunch of lines involving getattr or hasattr, type checking, and inspect grubbing.",A,5
18195698,2013-08-12 20:02:25.453000+00:00,"You need to close the csv file before you open it with Excel:
with open (_csvFilename, 'wb') as _csvFile
    _csvFile = csv.writer(_csvFile, quoting=csv.QUOTE_ALL)

    _Header = ['Name']+self.makeIntoList (self.root.tss.series () [0].getAllTimes (), self.originalTimesteps + _futurePeriods)
    _csvFile.writerow (_Header)

xl = Dispatch('Excel.Application')
wb = xl.Workbooks.Open(_csvFilename)  
xl.Visible = True 

By using a with statement the open file object is automatically closed when the block indented under the statement completes.
Windows doesn't like it when more than one application has a file open.",A,2
21223226,2014-01-19 21:57:34.587000+00:00,"You can generate them with:
sql = 'INSERT INTO home_title VALUES ({})'.format(', '.join(['%s'] * 57))

This interpolates 57 '%s' strings joined together with a comma and space.
Demo with a lower argument count:
>>> 'INSERT INTO home_title VALUES ({})'.format(', '.join(['%s'] * 5))
'INSERT INTO home_title VALUES (%s, %s, %s, %s, %s)'
",A,4
35256767,2016-02-07 17:38:01.600000+00:00,"This is not a client-side caching issue. In effect, Github servers are caching the version, serving you content until they are updated in time.
Github serves your data from a series of webservers, distributed geographically to ease loading times. These servers don't all update at the same time; until a change has propagated to all servers you'll see old and new content returned on that URL, depending on what machine served you the content for a specific request.
You can't really use GitHub to detect when a new version has been released, not reliably. Instead, generate a unique filename (generate a GUID perhaps) that at a future time will contain the new version information. Give that filename out with the current version, and try and poll that. Releasing a new version then consists of generating the filename for the version after, and putting the information to the current 'new version' URL. Each version links to the next file, and when it appears you only need to load it once.",A,1
43561820,2017-04-22 16:42:27.203000+00:00,"Yes, there are better ways.
If your method names do not follow a pattern but there is a fixed number of them, use a list, then use case as an index:
cases = [None, self.case1, self.case2, ..., self.case10]
cases[case]()

You can build this list up-front with a decorator, but then you'd have to make sure you bind the function objects to self first:
class Class:
    def __init__(self, case):
        self._cases[case].__get__(self)()

    def case1(self):
        ...
    def case2(self):
        ...
    def case10(self):
        ...

    _cases = [case1, case2, ..., case10]

If your methods really are sequentially numbered, you can generate the method name and use getattr() to look up the method:
getattr(self, 'case{}'.format(case))()
",A,2
25239329,2014-08-11 08:55:36.487000+00:00,"I'm assuming you wanted to test for sublists in the last element of each list, not the first 3 elements.
You'll need to test against each element in each nested list; using any() would make that a little more efficient:
[x for x in regs if any(sub[1] == -1 for sub in x[-1])]

The any() test loops over the generator expression until a True value is found, at which point it stops iterating and returns True. This means that x is only included if there is a sublist in x[-1] whose second element is equal to -1.",A,2
14609698,2013-01-30 17:09:12.437000+00:00,"SublimeText clears auto-indent whitespace automatically if the trim_automatic_white_space setting is enabled (default). This only affects blank lines.
Python does not care about whitespace on blank lines; blank lines do not need to match the indentation of the rest of the code. However, if you copy lines to the python interpreter, empty lines signal the end of a block and that block is then compiled; this is different from running a saved file directly.
If you see indentation errors when running your python file, you are mixing tabs and spaces elsewhere in your code. Run your code with python -tt modulename.py to test. For python code, you really want to use spaces only (convert tabs to spaces, set sublime to use spaces for indentation).",A,6
17294002,2013-06-25 09:43:22.360000+00:00,"Don't use a regular expression; just use .endswith('/'):
for path in lst:
    if path.endswith('/'):
        dir_list.append(path)
    else:
        file_list.append(path)

.endswith() performs better than a regular expression and is simpler to boot:
>>> sample = ['fileA.jpg', 'fileB.jpg', 'images/'] * 30
>>> import random
>>> random.shuffle(sample)
>>> from timeit import timeit
>>> import re
>>> def re_partition(pattern=re.compile(r'/$')):
...     for e in sample:
...         if pattern.search(e): pass
...         else: pass
... 
>>> def endswith_partition():
...     for e in sample:
...         if e.endswith('/'): pass
...         else: pass
... 
>>> timeit('f()', 'from __main__ import re_partition as f, sample', number=10000)
0.2553541660308838
>>> timeit('f()', 'from __main__ import endswith_partition as f, sample', number=10000)
0.20675897598266602
",A,7
32896980,2015-10-01 21:15:13.030000+00:00,"The form is not valid. It can never be valid, because you are sending a JSON dictionary with the form data inside the key ""the_post"", but you haven't told the view to look there.
And the view can't report back the status of the form, because you construct a response - ""response_data"" - and then ignore it.
Finally, your view does not send back JSON anyway - it sends back the rendered template, as if it were a normal non-Ajax view - so the receiving JS has no idea what to do with it.",A,1
13903767,2012-12-16 17:34:12.733000+00:00,"Simple:
value = [v[0] - v[1] for v in value]
",A,4
25973438,2014-09-22 11:50:35.350000+00:00,"functools.update_wrapper() sets the attribute on the instance, but help() looks at the information on the type.
So printer.__doc__ gives you the instance attribute, help() prints information about type(printer), e.g. the memoized class, which does not have a __doc__ attribute.
This is not a bug, this is all by design; help() will always look at the class when you pass in an instance. Don't use a class as decorator if you want help() to work for the decorated function.",A,11
11168715,2012-06-23 10:18:09.233000+00:00,"You need to pass in unicode to the findAll method:
# -*- coding: utf-8
... 
img_urls = form_soup.findAll('a', title=u'查看大图')

Note the u unicode literal marker in front of the title value. You do need to specify an encoding on your source file for this to work (the coding comment at the top of the file), or switch to unicode escape codes instead:
img_urls = form_soup.findAll('a', title=u'\u67e5\u770b\u5927\u56fe')

Internally, BeautifulSoup uses unicode, but you are passing it a byte-string with non-ascii characters in them. BeautifulSoup tries to decode that to unicode for you and fails as it doesn't know what encoding you used. By providing it with ready-made unicode instead you side-step the issue.
Working example:
>>> from BeautifulSoup import BeautifulSoup
>>> example = u'<P class=img_s>\n<A href=""/pic/93/b67793.jpg"" target=""_blank"" title=""<A href=""/pic/93/b67793.jpg"" target=""_blank"" title=""\u67e5\u770b\u5927\u56fe""><IMG src=""/pic/93/s67793.jpg""></A></P>'
>>> soup = BeautifulSoup(example)
>>> soup.findAll('a', title=u'\u67e5\u770b\u5927\u56fe')
[<a href=""/pic/93/b67793.jpg"" target=""_blank"" title=""查看大图""><img src=""/pic/93/s67793.jpg"" /></a>]
",A,6
27402918,2014-12-10 14:00:25.430000+00:00,"I'm not quite sure why you have used a regex there. You need to use an actual URL:
return HttpResponseRedirect('/bars/%s/' % bar_name)

or even better use the reverse function which accepts a view name and the relevant parameters
return HttpResponseRedirect(reverse('songs.views.songs', bar_name=bar_name))

or even better again, use the redirect function which does both the reversing and the redirecting:
return redirect('songs.views.songs', bar_name=bar_name)
",A,2
8444389,2011-12-09 10:49:33.357000+00:00,"You should think about what you're asking here. Jinja2 is a server-side technology, and the template is rendered before it gets sent to the browser. Your Ajax function runs on the client side, after the list variable has been turned into HTML. There is no way to do what you're asking.
Either get your Ajax function to return rendered HTML rather than JSON (ie render the Jinja template in the Ajax server-side handler) or use a client-side template technology - the author of Jinja2, Armin Ronacher, has also written JsonJinja which might do what you want.",A,8
17661465,2013-07-15 18:39:20.577000+00:00,"Instance variables are always accessed through the class instance itself. Inside the method, this is (by convention) known as self. So you need to use self.headers etc.
Note though that by defining headers at the top of the class, you have defined a class variable which is shared by all members. You don't want this, and there is no need to define headers there. Just assign it within __init__.
Plus, as StoryTeller points out, you'll need to call the superclass __init__ method manually inside the derived class methods, since that defines the attribute in the first place:
super(post_httprequest, self).__init__()

And in order for that to work, as abarnert points out, you'll need to inherit your base class from object.
Finally, please do use PEP8-compliant names: PostHttpRequest, etc.",A,5
47832780,2017-12-15 12:50:30.267000+00:00,"You missed the canonical method, dict.get():
color_1 = data.get('color')

It'll return None if the key is missing. You can set a different default as a second argument:
color_2 = dict.get('color', 'red')
",A,9
17570045,2013-07-10 11:57:12.530000+00:00,"You are using str methods on an open file object.
You can read the file as a list of lines by simply calling list() on the file object:
with open('goodlines.txt') as f:
    mylist = list(f)

This does include the newline characters. You can strip those in a list comprehension:
with open('goodlines.txt') as f:
    mylist = [line.rstrip('\n') for line in f]
",A,23
32829980,2015-09-28 19:00:22.483000+00:00,"You need to reload your local version of card back from the database after you call the view. Otherwise they have nothing to do with each other, and card has no way of knowing that the db has been updated.
resp = self.client.post(reverse('check_answer'), data=data)
card = Card.objects.get(id=card.id)
self.assertEqual(card.score, 1)
",A,1
33686315,2015-11-13 05:08:56.713000+00:00,"
what the other bits in the function codetype co_flags do?

From dis.COMPILER_FLAG_NAMES:
  1 OPTIMIZED
  2 NEWLOCALS
  4 VARARGS
  8 VARKEYWORDS
 16 NESTED
 32 GENERATOR
 64 NOFREE
128 COROUTINE
256 ITERABLE_COROUTINE

It is from Python 3 but flags that are available in Python 2.7 have kept the same values from Include/code.h. They are documented only by Jython.

the function (the inner codetype) is 71

71 = 1 + 2 + 4 + 64:
  1 OPTIMIZED
  2 NEWLOCALS
  4 VARARGS
 64 NOFREE

Example (in Python 3):
>>> import dis
>>> def f(a, b, c, *args):
...     print(a, b, c, args)
... 
>>> dis.show_code(f)
Name:              f
Filename:          <stdin>
Argument count:    3
Kw-only arguments: 0
Number of locals:  4
Stack size:        5
Flags:             OPTIMIZED, NEWLOCALS, VARARGS, NOFREE
Constants:
   0: None
Names:
   0: print
Variable names:
   0: a
   1: b
   2: c
   3: args


why the container is set to 64 as all it does is set functions up eg there is no variable arguments used?

As I understand ""the container"" is a module (check co_name). NOFREE just means that there are no freevars, cellvars that is (always?) true for a module. On pypy co_flags is 0 (the flags are the same there).",A,1
36057861,2016-03-17 10:30:34.710000+00:00,"You've got out of sync. Delete your database file, and the Python files under polls/migrations, and run makemigrations again.
This was caused by you not having the pub_date field when you initially migrated. There are ways of fixing this, but for the purposes of the tutorial, you should just start again.",A,0
31211107,2015-07-03 16:33:25.460000+00:00,"You almost certainly have a string with a space variant in it, not the bog-standard U+0020 SPACE but more likely U+00A0 NO-BREAK SPACE. The two characters look exactly the same when printed:
>>> 'MavNandi\xa07'
'MavNandi\xa07'
>>> print('MavNandi\xa07')
MavNandi 7

There may be others; the Unicode stardard defines 17 space separators for example, many of these look like a regular space (with varying width).
As you can see the interpreter, when echoing the value uses the \xa0 escape sequence to differentiate the two. You can produce the same output by using the repr() function when printing:
>>> print(repr('MavNandi\xa07'))
'MavNandi\xa07'

There is also a ascii() function, this works just like repr() but replaces any character outside the ASCII range with an escape sequence, making it perhaps easier to see combining characters and distinguish between Unicode glyphs that would otherwise look alike.
You could use a regular expression to replace all whitespace characters, including no-break spaces, tabs, newlines and anything else the Unicode standard designates as whitespace:
import re

username = re.sub('\s+', '', username)

The \s character class matches any such whitespace character, the + matches 1 or more such character when replacing; the second argument replaces all such matches with the empty string.
Demo:
>>> import re
>>> re.sub('\s+', '', 'MavNandi\xa07')
'MavNandi7'
",A,5
15576803,2013-03-22 17:49:38.607000+00:00,"Your nested LevelLayout XML document uses a namespace. I'd use:
tree.xpath('.//LevelLayout[@levelGuid=""4a54f032-325e-4988-8621-2cb7b49d8432""]//*[local-name()=""Name""]')

to match the Name element with a shorter XPath expression (ignoring the namespace altogether).
The alternative is to use a prefix-to-namespace mapping and use those on your tags:
nsmap = {'acd': 'http://schemas.datacontract.org/2004/07/ArcherTech.Common.Domain'}

tree.xpath('/PackageLevelLayout/LevelLayouts/LevelLayout[@levelGuid=""4a54f032-325e-4988-8621-2cb7b49d8432""]/acd:LevelLayout/acd:LevelLayoutSectionBase/acd:LevelLayoutItemBase/acd:Name',
    namespaces=nsmap)
",A,3
32075472,2015-08-18 14:38:48.010000+00:00,"You shouldn't be using a ValuesQueryset at all here. The queryset parameter for a ModelChoiceField expects, not surprisingly, a standard queryset.
email_accounts = EmailAccount.objects.filter(user__user=self.request.user)
form.fields['account'].queryset = email_accounts
",A,0
13635266,2012-11-29 21:41:47.990000+00:00,"It seems like there are a few reasonable ways to design this.
Let me refer to your jobs as the main job, the 9 intermediate jobs, and the many inner jobs the intermediate jobs can spin off. I'm assuming the intermediate jobs have a ""merge"" step after the inner jobs all finish, and the same for the outer job.
The simplest design is that the main job fires off the intermediate jobs and then waits for them all to finish before doings its merge step. Then intermediate jobs then fire off the inner jobs and wait for them all to finish before doing their merge steps.
This can work with a single shared queue, but you need a queue that doesn't block the worker pool while waiting, and I don't think multiprocessing's Pool and Queue can do that out of the box. As soon as you've got all of your processes waiting to join their children, nothing gets done.
One way around that is to change to a continuation-passing style. If you know which one of the intermediate jobs will finish last, you can pass it the handles to the other intermediate jobs and have it join on them and do the merge, instead of the outer job. And the intermediate similarly pass off the merge to their last inner job.
The problem is that you usually have no way of knowing what's going to finish last, even without scheduling issues. So that means you need some form of either sharing (e.g., a semaphore) or message passing between the jobs to negotiate that among themselves. You can do that on top of multiprocessing. The only problem is that it destroys the independence of your jobs, and you're suddenly dealing with all the annoying problems of shared concurrency.
A different alternative is to have separate pools and queues for each intermediate job, and some kind of load balancing between the pools that can ensure that each core is running one active process.
Or, of course, a single pool with a more complicated implementation than multiprocessing's, which does either load balancing or cooperative scheduling, so a joiner doesn't block a core.
Or a super-simple solution: Overschedule, and pay a little cost in context switching for simplicity. For example, you can run 32 workers even though you've only got 8 cores, so you've got 22 active workers and 10 waiting. Each core has 2 or 3 active workers, which will slow things down a bit, but maybe not too badly—and at least nobody's idle, and you didn't have to write any code beyond passing a different parameter to the multiprocessing.Pool constructor.
At any rate, multiprocessing is very simple, and it has almost no extra concepts that won't apply to other solutions. So it may take less time to play with it until you run into a brick wall or don't, than to try to figure out in advance whether it'll work for you.",A,1
1519440,2009-10-05 11:07:39.277000+00:00,"Just found this: http://ejr44.blogspot.com/2008/05/python-for-windows-mobile-cab.html
Looks like a complete set of .CAB files to provide Python on Windows Mobile.",A,1
278056,2008-11-10 14:57:19.397000+00:00,"Functors can be defined this way in Python.  They're callable objects.  The ""binding"" merely sets argument values.
class SomeFunctor( object ):
    def __init__( self, arg1, arg2=None ):
        self.arg1= arg1
        self.arg2= arg2
    def __call___( self, arg1=None, arg2=None ):
        a1= arg1 or self.arg1
        a2= arg2 or self.arg2
        # do something
        return

You can do things like
x= SomeFunctor( 3.456 )
x( arg2=123 )

y= SomeFunctor( 3.456, 123 )
y()
",A,0
48265938,2018-01-15 15:25:57.497000+00:00,"Dictionaries (like all built-in containers) are represented with their contents shown as representations, essentially using the repr() function on each. For strings, those are meant to be as helpful as possible in that they are shown as string literals that can simply be copied and pasted to recreate their value. That means they also show unprintable characters or characters with special meaning as escape sequences. Your newline characters are just such characters.
In other words, you can't do what you want just by inserting \n characters in the string values.
Instead, you need to do your own formatting if you really want to show your dictionary that way. Just print out the keys and values yourself:
def represent_dict(d):
    print('{', end='')  # no newline
    first = True
    for key, value in d.items():
        # newline at the start, not end, with commas
        print('{}\n{!r}: {!r}'.format('' if first else ',', key, value), end='')
        first = False
    print('}')  # closing with a brace and a newline

Remove the \n addition in your reading code; it can be simplified to just produce the dictionary directly with a dictionary comprehension:
def read_dictionary():
    with open(""text.tsv"") as tsvfile:
        tsvreader = csv.reader(tsvfile, delimiter = ""\t"")
        return {row[0]: row[1:] for row in tsvreader}

represent_dict(read_dictionary())

You generally should keep presentation and data structure separate. Those newlines in the keys can easily cause issues elsewhere, and they are only there for the presentation output.
Demo of the output:
>>> dictionary = {""string1"":[""a"",""b"",""c""], ""string2"":[""d"",""e"",""f""],
...               ""string3"":[""g"",""h"",""i""], ""string4"":[""j"",""k"",""l""]}
>>> represent_dict(dictionary)
{
'string1': ['a', 'b', 'c'],
'string2': ['d', 'e', 'f'],
'string3': ['g', 'h', 'i'],
'string4': ['j', 'k', 'l']}
",A,4
30045066,2015-05-05 05:40:06.753000+00:00,"I'm willing to bet the problem is that you're mixing up NumPy arrays and normal Python lists.

You're using NumPy arrays all over your code. These know how to do all kinds of cool things, like element-wise operations. For example:
>>> a = np.array([1, 2, 3, 4])
>>> a * 1.5
array([ 1.5,  3. ,  4.5,  6. ])

But Python lists don't know how to do that. (On the other hand, they know how to do other things that NumPy arrays don't, like append new values to the end, or make copies automatically when you slice instead of only when you explicitly tell them to.) Multiplying a Python list by a number just means to repeat the list that many times. It makes sense for an integer, but not for a float—and it's not what you want here even for an integer:
>>> a = [1, 2, 3, 4]
>>> a * 2
[1, 2, 3, 4, 1, 2, 3, 4]
>>> a * 1.5
TypeError: can't multiply sequence by non-int of type 'float'


Your Harmonic function is returning a list, not an array:
def Harmonic(x,time):
    y0 = x[1]
    y1 = (-1)*x[0]
    return ([y0,y1])

But you're trying to multiply the result of that function by a number. Which would make sense for an array, but not for a list.
So you probably just want to change that:
def Harmonic(x,time):
    y0 = x[1]
    y1 = (-1)*x[0]
    return np.array([y0,y1])
",A,3
21805981,2014-02-16 01:11:03.810000+00:00,"re.findall returns a list of found strings. So, imgUrl is a list.
You can't write a list of strings to a file, only a string. Hence the error message.
If you want to write out the string representation of the list (which is easy, but unlikely to be useful), you can do this:
outfile.write(str(imgUrl))

If you want to write just the first URL, which is a string, you can:
outfile.write(imgUrl[0])

If you want to write all of the URLs, one on each line:
for url in imgUrl:
    outfile.write(url + '\n')

Or, since it's HTML and the whitespace doesn't matter, you can write them all run together:
outfile.write(''.join(imgUrl))


You then have a second problem. For some reason, you've opened the file in binary mode. I don't know why you're doing this, but if you do, you can only write bytes to the file, not strings. But you don't have a list of bytes, you have have a list of strings. So, you need to encode those strings into bytes. For example:
for url in imgUrl:
    outfile.write(url.encode('utf-8') + b'\n')

Or—much better—just don't open the file in binary mode:
outfile = open('abc.htm', 'w')

If you want to specify an explicit encoding, you can still do that without using binary mode:
outfile = open('abc.htm', 'w', encoding='utf-8')


You may also have a third problem. From your comments, it appears that imgUrl[0] gives you an IndexError. That means that it's empty. Which means your regex is not actually finding any URLs to write in the first place. In that case, you obviously can't successfully write them out (unless you're expecting an empty file).
And the reason (or at least a reason) the regex is not finding anything is that you're not actually searching the downloaded HTML (which you've stored in image) but the URL to that HTML (which you've stored in url):
imgUrl = re.findall('<img src=""(.*)"" />', url)

… and obviously there are no matches for your regexp in the string 'http://www.techradar.com/news/internet/web/12-best-places-to-get-free-images-for-your-site-624818'.",A,1
25021649,2014-07-29 17:40:34.127000+00:00,"You'll have to read the whole CSV file into memory; you can do so by calling list() on the reader object:
with open(file_to_parse, 'rb') as inf:
    reader = csv.DictReader(inf, skipinitialspace=True)
    rows = list(reader)

for row in reversed(rows):

Note that I used the file as a context manager here to ensure that the file is closed. You also want to open the file in binary mode (leave newline handling to the csv module). The rest of the configuration you passed to the DictReader() are the defaults, so I omitted them.
I set skipinitialspace to True, as judging from your sample input and output you do have spaces after your delimiters; the option removes these.
The csv.DictReader() object takes care of reading that first line as the keys.
Demo:
>>> import csv
>>> sample = '''\
... fruit, vegetables, cars
... orange, carrot, ford
... apple, celery, chevy
... grape, corn, chrysler
... '''.splitlines()
>>> reader = csv.DictReader(sample, skipinitialspace=True)
>>> rows = list(reader)
>>> for row in reversed(rows):
...     print row
... 
{'cars': 'chrysler', 'vegetables': 'corn', 'fruit': 'grape'}
{'cars': 'chevy', 'vegetables': 'celery', 'fruit': 'apple'}
{'cars': 'ford', 'vegetables': 'carrot', 'fruit': 'orange'}
",A,2
28580795,2015-02-18 10:06:15.660000+00:00,"You are putting your PUT payload in the headers. Put it in the body instead. Your payload is not JSON, so there is no need to try and treat it as such.
The headers need to be specified as a dictionary with the header name and token as key and value. The parameters for the payload can be treated in the same way.
You are also using the wrong requests method; to send a PUT request, use the put function instead:
headers = {'public-api-token': '(my token)'}
payload = {'urlToShorten': 'google.com'}
response = requests.put(
    'https://api.shorte.st/v1/data/url',
    data=payload, headers=headers)

print(response.json())

The response object has a json() method to decode the JSON data returned by the API; it'll give you the Python data structure corresponding to the JSON text.
I don't have a token for the service you are using; I created a demo using the httpbin.org service; it reflects what was sent as a JSON response:
>>> import requests
>>> headers = {'public-api-token': '(my token)'}
>>> payload = {'urlToShorten': 'google.com'}
>>> response = requests.put(
...     'http://httpbin.org/put',
...         data=payload, headers=headers)
>>> from pprint import pprint
>>> pprint(response.json())
{u'args': {},
 u'data': u'',
 u'files': {},
 u'form': {u'urlToShorten': u'google.com'},
 u'headers': {u'Accept': u'*/*',
              u'Accept-Encoding': u'gzip, deflate',
              u'Content-Length': u'23',
              u'Content-Type': u'application/x-www-form-urlencoded',
              u'Host': u'httpbin.org',
              u'Public-Api-Token': u'(my token)',
              u'User-Agent': u'python-requests/2.5.0 CPython/2.7.9 Darwin/14.1.0'},
 u'json': None,
 u'origin': u'94.118.96.0',
 u'url': u'http://httpbin.org/put'}

If you compare that to the output produced for curl sending a PUT request to the same URL you'll see the same results are produced:
$ curl -H ""public-api-token: (my token)"" -X PUT \
     -d ""urlToShorten=google.com"" http://httpbin.org/put
{
  ""args"": {}, 
  ""data"": """", 
  ""files"": {}, 
  ""form"": {
    ""urlToShorten"": ""google.com""
  }, 
  ""headers"": {
    ""Accept"": ""*/*"", 
    ""Content-Length"": ""23"", 
    ""Content-Type"": ""application/x-www-form-urlencoded"", 
    ""Host"": ""httpbin.org"", 
    ""Public-Api-Token"": ""(my token)"", 
    ""User-Agent"": ""curl/7.37.1""
  }, 
  ""json"": null, 
  ""origin"": ""94.118.96.0"", 
  ""url"": ""http://httpbin.org/put""
}
",A,5
28252201,2015-01-31 13:37:32.497000+00:00,"There are multiple ways to compute fibonacci series asynchroniously. First, check that the explosive variant fails in your case:
@asyncio.coroutine
def coro_sum(summands):
    return sum(summands)

@asyncio.coroutine
def coro_fib(n):
    if   n == 0: s = 0
    elif n == 1: s = 1
    else:
        summands, _ = yield from asyncio.wait([coro_fib(n-2), coro_fib(n-1)])
        s = yield from coro_sum(f.result() for f in summands)
    return s

You could replace summands with:
a = yield from coro_fib(n-2) # don't return until its ready
b = yield from coro_fib(n-1)
s = yield from coro_sum([a, b])

In general, to prevent the exponential growth, you could use asyncio.Queue (synchronization via communication), asyncio.Semaphore (synchonization using mutex) primitives.",A,1
353686,2008-12-09 18:06:38.637000+00:00,"""Is what I wrote above, importing a model from another app and setting it as a foreign key, how Django apps interact?""
Yep.  Works for me.
We have about 10 applications that borrow back and forth among themselves.
This leads to a kind of dependency in our unit test script.
It looks like this.

""ownership"". We have a simple data ownership application that defines some core ownership concepts that other applications depend on.  There are a few simple tables here.
""thing"". [Not the real name].  Our thing application has data elements owned by different user groups.  There are actually several complex tables the model for this app.  It depends on ""ownership"".
""tables"". [Not the real name].  Some of our users create fairly complex off-line models (probably with spreadsheets) and upload the results of that modeling in ""tables"".  This has a cluster of fairly complex tables.  It depends on ""ownership"".
""result"".  [Not the real name].  Our results are based on things which have owners.  The results are based on things and tables, and are responses to customer requests.  This isn't too complex, perhaps only two or three core tables.  It depends on ""things"" and ""table"".  No, it doesn't completely stand-alone.  However, it is subject to more change than the other things on which it depends.  That's why it's separate.
""processing"".  We schedule and monitor big batch jobs.  This is in this application.  It's really generic, and can be used in a variety of ways.  It completely stands alone.
""welcome"".  We have a ""welcome"" app that presents a bunch of mostly static pages.  This doesn't have too many tables.  But it's on it's second incarnation because the first was too complex.  It completely stands alone.

The only relationship among the dependent apps is some table names.  As long as we preserve those tables (and their keys) we can rearrange other apps as we see fit.",A,3
20388234,2013-12-04 23:22:51.483000+00:00,"You have a file that already had a newline at the end:
aaaaaaaaaaaaaaaaaaa\n
bbbbbbbbbbbbbbbbbbb\n
ccccccccccccccccccc\n
ddddddddddddddddddd\n

and you wrote a newline plus the additional line:
aaaaaaaaaaaaaaaaaaa\n
bbbbbbbbbbbbbbbbbbb\n
ccccccccccccccccccc\n
ddddddddddddddddddd\n
\n
eeeeeeeeeeeeeeeeeee

Write your additional line with the newline at the end:
with open(filename, ""a"") as updatedFile:
    nextLine=""%s\n"" % (lineToAdd)
    updatedFile.write(nextLine)

so that you end up with:
aaaaaaaaaaaaaaaaaaa\n
bbbbbbbbbbbbbbbbbbb\n
ccccccccccccccccccc\n
ddddddddddddddddddd\n
eeeeeeeeeeeeeeeeeee\n
",A,5
52238885,2018-09-08 19:50:27.553000+00:00,"There is no equivalent to opcode tracing in versions before 3.7. If there were, the feature wouldn't have been added to 3.7 in the first place.
If you can upgrade to 3.7, then what you want is easy:
def tracefunc(frame, event, arg):
    if event == 'call':
        frame.f_trace_opcodes = True
    elif event == 'opcode':
        if frame.f_code.co_code[frame.f_lasti] == dis.opmap['MAKE_FUNCTION']:
            makefunctiontracefunc(frame)
    return tracefunc
sys.settrace(tracefunc)

But if you can't… there are a number of more complicated things you could do, depending on what your reasons are for wanting this, but none of them are remotely easy:

Use line tracing, and inspect the code until the next line. This is trivial for def, but for lambda (and comprehensions1) it's going to be a big pain, because a lambda (or even five of them) can appear in the middle of a statement. You could ast.parse the source, or examine the bytecode, to figure out that there are functions being defined within, but there's still no way to call your hook right at the time of definition.
Instead of using tracing, write an import hook that modifies the code as it's being imported. The easy way to do this is probably at the AST level: after you parse the source, use a NodeTransformer to inject calls to some function2 before or after each def and lambda node, then compile the transformed tree. But you could also do it at the bytecode level with bytecode or byteplay, before or after each MAKE_FUNCTION.3
Script pdb instead of writing your own debugger. I'm not sure if this will even help, because pdb has no way to step through part of an expression in the first place.
Debug CPython itself, and add a breakpoint in the MAKE_FUNCTION handler in the ceval loop that calls your code. Of course your code is in the debugger's interpreter—which can be Python for gdb and lldb, but it's still not the same Python interpreter you're debugging. And, while it's possible to recursively evaluate code into the debugged interpreter (or trigger its pdb), it's not easy, and you segfault all over the place while working it out.


1. Comprehensions (except list comprehensions, in 2.x) are implemented by defining and then calling a function. So, any of the methods that rely on the MAKE_FUNCTION opcode or similar are going to also fire on comprehensions, while those that rely on source or AST parsing will not (unless you do so explicitly, of course).
2. Obviously you also need to inject an import at the top of every module to make that function available, or inject the function into the builtins module.
3. And MAKE_CLOSURE, for earlier versions of Python.",A,3
25057015,2014-07-31 10:49:59.337000+00:00,"You handle exceptions to have your code continue; the alternative is to have the default handler (just before interpreter exit) print a traceback.
You handle exceptions that are not errors in your code; sometimes it is more efficient to handle exceptions than it is to test for the possibility that an exception will occur. This is called asking for forgiveness as opposed to asking for permission.
Take collecting information in lists in a dictionary for example. You could use:
if key not in dictionary:
    dictionary[key] = ['default', 'initial', 'value']
listval = dictionary[key]
listval.extend(['other', 'values'])

That'd be asking for permission, while:
try:
    listval = dictionary[key]
except KeyError:
    listval = dictionary[key] = ['default', 'initial', 'value']
listval.extend(['other', 'values'])

would be asking for forgiveness. This uses exception handling explicitly; if the case where the key is missing is the exception (not common), asking for forgiveness is faster than asking for permission.
If the error is on the part of the end user and can be anticipated, you also need to handle exceptions as your program stopping with an ugly traceback is not user friendly.
But exceptions thrown through incorrect use of an API should not be caught; such incorrect use is indicative of a bug in the code somewhere, and you as a developer then really want to know what went wrong, and where.",A,2
54732451,2019-02-17 10:47:50.383000+00:00,"Unfortunately, the multiprocessing library is not suited to conversion to asyncio, what you have is the best you can do if you must use BaseProxy to handle your IPC (Inter-Process communication).
While it is true that the library uses blocking I/O here you can't easily reach in and re-work the blocking parts to use non-blocking primitives instead. If you were to insist on going this route you'd have to patch or rewrite the internal implementation details of that library, but being internal implementation details these can differ from Python point release to point release making any patching fragile and prone to break with minor Python upgrades. The _callmethod method is part of a deep hierarchy of abstractions involving threads, socket or pipe connections, and serializers. See multiprocessing/connection.py and multiprocessing/managers.py.
So your options here are to stick with your current approach (using a threadpool executor to shove BaseProxy._callmethod() to another thread) or to implement your own IPC solution using asyncio primitives. Your central database-access process would act as a server for your other processes to connect to as a client, either using sockets or named pipes, using an agreed-upon serialisation scheme for client requests and server responses. This is what multiprocessing implements for you, but you'd implement your own (simpler) version, using asyncio streams and whatever serialisation scheme best suits your application patterns (e.g. pickle, JSON, protobuffers, or something else entirely).",A,4
18677530,2013-09-07 20:17:32.303000+00:00,"This should be elementary logic. grade1 and grade2 are independent variables, and either, both or neither can be over 50.",A,0
44284085,2017-05-31 11:52:36.633000+00:00,"This is definitely a bug introduced in Python 3.6.0b1.  The iter() implementation recently switched to using _PyObject_FastCall() (an optimisation, see issue 27128), and it must be this call that is breaking this.
The same issue arrises with other C classmethod methods backed by Argument Clinic parsing:
>>> from asyncio import Task
>>> Task.all_tasks()
set()
>>> next(iter(Task.all_tasks, None))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration

If you need a work-around, wrap the callable in a functools.partial() object:
from functools import partial

j = iter(partial(datetime.datetime.now), None)

I filed issue 30524 -- iter(classmethod, sentinel) broken for Argument Clinic class methods? with the Python project. The fix for this has landed and is part of 3.6.2rc1.",A,43
15456323,2013-03-17 00:25:14.133000+00:00,"You can only add supported attributes to nodes and edges. These attributes have specific meaning to GrpahViz. 
To show extra information on edges or nodes, use the label attribute.",A,2
17390210,2013-06-30 12:09:00.267000+00:00,"You are only storing references to the original list. You want to create copies instead:
bak.append(x[:])

or
bak.append(list(x))

The [:] syntax creates a new list from a slice from the first to the last element of the original list.",A,4
42333471,2017-02-19 22:06:18.573000+00:00,"What makes you think the file is an encoded string? It's not a string at all; a pdf is not readable, it's a binary format. You can't just iterate through and print it out.",A,0
30793582,2015-06-12 00:30:48.433000+00:00,"
Why am I getting this error?

There are a few possible reasons, but they all come down to the same thing: you have something which is not UTF-8, but you're trying to read it as if it were.
First, if you actually have raw binary data, not text, you can't decode it as text. (Before version 2.0 of the protocol, there was no such distinction, so if you're learning from a tutorial or sample code written for an older version, it will be misleading.) The packing side needs to explicitly pack it as binary data. If it's using the same msgpack library, it does so by using the use_bin_type=True argument. If it's using some other library, you'll have to read the docs for that other library.
Second, if you have text, but the packer is packing it as, say, Latin-1, you can't unpack it as UTF-8. For example, that byte 0xcc means Ì in Latin-1 and related character sets, which is a perfectly valid character, but it's an error in UTF-8 unless it follows a prefix byte. (And really, you're lucky you got an error, rather than silently doing the wrong thing and spewing mojibake all over.) Again, if the packing side is using the same msgpack library, it's just a matter of passing encoding='utf8' on both sides. If it's using a different library, you'll have to look at the docs for that other library. (And if you can't change the other side, then you need to figure out what encoding it's using—which may be difficult, because it may depend on the library, or the platform, or even the user's specified locale or system default character set…)


I want to catch this error and irrespective of this error continue for rest of data.

Well, that's probably not what you want to do… but if you do, your existing code should work fine. It's raising a UnicodeDecodeError inside an except UnicodeDecodeError as e:. If that isn't working, most likely your actual code doesn't look like what you've shown us here.
It's worth noting that you seem to be mixing tabs and spaces in your source code, which is going to cause a lot of problems—things that look like they're at the same indentation level to you as a human may look like they're at different indentation levels to the Python compiler, and vice-versa. I can't quite see how that could cause the exact code you posted to fail (except by raising an IndentationError at compile time, which doesn't sound like what you're describing), but it can easily cause problems like this in even slightly more complex code.",A,3
472681,2009-01-23 12:14:02.910000+00:00,"Wrong?  Partially.
""Your codebase grows incrementally, and never has a big planning/hierarchical design stage.""
Correct.
""That, to me, suggests that software design (worthy though it is) has been subsumed into refactoring...""
Not quite correct.
There's a huge gulf between Big Design Up Front (BDUF) and a more Agile design approach.
BDUF dictates that all design is completed before any coding.  This is still popular (just read an RFP yesterday which absolutely required all design be reviewed by the customer before any coding could begin.  Sigh.)
Agile suggests that perhaps all design isn't a helpful goal.  You need to do enough design that TDD will work.  You can't, for example, start TDD until you have a working infrastructure that allows someone to write tests and incrementally evolve a solution knowing that there won't be a weird production deployment problem to solve.
Design is still king.  Agile Design is better than monolithic design.
A consequence of Agile Design is YAGNI, DRY and Less-is-More.  These don't replace design, they're a consequence of how you prioritize and do design.
BDD and TDD are ways to structure your time so you have focus on what people need, what they do and what really matters.  TDD, in particular, focuses on testable behaviors of the software.  Not zero-value nuance, but actual behavior.
Premature optimization is interesting, but unrelated.  Even Agile teams can run down a low-value rat-hole pursuing a nuance or optimization that doesn't add any value.  Premature optimization is a habit of overthinking (== ""hand wringing"") a technology choice without facts about actual performance.
Agile is supposed to help you focus on the big picture: What actual people will actually do with the actual software and avoid technology rat-holes.
It doesn't replace engineering.  It refocuses it.",A,4
39191759,2016-08-28 13:27:46.550000+00:00,"Your URL pattern doesn't end with a slash, but your Ajax request is for a URL that does end with a slash. Fix one or the other; probably better for consistency to ensure that the pattern has a slash.
r'^price/(?P<pk>\d+)/$'
",A,2
2157049,2010-01-28 18:49:38.167000+00:00,"getattr(this_prize,choice)

http://docs.python.org/library/functions.html#getattr",A,64
45699897,2017-08-15 19:12:59.900000+00:00,"Yes; they are both WSGI apps. Just configure a router container to hold them, and map URL prefixes to one or the other. How you do this depends on your WSGI server (some have native support).
Alternatively, have Flask host the Pyramid WSGI app; any Flask route that returns an object that is not a string or a Response object, will be treated as a WSGI app and have the correct data passed in.",A,0
19778338,2013-11-04 22:18:29.457000+00:00,"You are being confused by the behaviour of the or operator; it returns the first expression that only if it is a true value; neither 0 nor False is true so the second value is returned:
>>> 0 or 'bar'
'bar'
>>> False or 'foo'
'foo'

Any value that is not numerical 0, an empty container, None or False is considered true (custom classes can alter that by implementing a __bool__ method (python 3), __nonzero__ (python 2) or __len__ (length 0 is empty).
The second expression is not even evaluated if the first is True:
>>> True or 1 / 0
True

The 1 / 0 expression would raise a ZeroDivision exception, but is not even evaluated by Python.
This is documented in the boolean operators documentation:

The expression x or y first evaluates x; if x is true, its value is returned; otherwise, y is evaluated and the resulting value is returned.

Similarly, and returns the first expression if it is False, otherwise the second expression is returned.",A,6
22864442,2014-04-04 13:38:06.210000+00:00,"Because instance attributes shadow class attributes, but they are independent.
When you try to access t2.count, there is no instance attribute (t2.__dict__ does not have a count key), so Python next looks at type(t2) to see if it can find the attribute there. The class does have such an attribute, so test.count is returned.
t1.count on the other hand finds the instance attribute and returns that. test.count is never considered.",A,2
8467248,2011-12-11 20:34:18.337000+00:00,"You just need to exclude it from the form, then set it in the view.
class AnimalForm(ModelForm):
    class Meta:
        model = Animal
        exclude = ('publisher',)

... and in the view:
    form = AnimalForm(request.POST)
    if form.is_valid():
        animal = form.save(commit=False)
        animal.publisher = request.user
        animal.save()

(Note also that the first else clause - the lines immediately following the redirect - is unnecessary. If you leave it out, execution will fall through to the two lines at the end of the view, which are identical.)",A,35
44355438,2017-06-04 14:58:58.447000+00:00,"If you want to remove something from the randomly chosen list, you don't need to have the variable name. The list object itself will do.
In other words, just remove the selected number from the result that random.choice() gives you:
picked_list = random.choice(d)
picked = random.choice(picked_list)
picked_list.remove(picked)

If you must have some kind of name, don't use separate variables for the input lists. Use a dictionary:
lists = {
    'a': [1, 2, 3],
    'b': [1, 2, 3],
    'c': [1, 2, 3],
}
list_picked = random.choice(lists)  # returns 'a', 'b', or 'c'
picked = random.choice(lists[list_picked])
lists[list_picked].remove(picked)

Note that it'll be more efficient to just shuffle your lists up-front (putting them in random order), then pop elements as you need them:
d = [a, b, c]
for sublist in d:
    random.shuffle(sublist)

# later on
picked = random.choice(d).pop()  # remove the last element

You'll have to figure out what it means to have an empty list there though.",A,3
28031350,2015-01-19 18:34:28.373000+00:00,"Well, like the docs say, you can't.
One way of approaching this might be to keep another property on the model that records how many values it has in bar. You would need to update this when the entity is saved: a good way would be to override put() to do self.bar_count = len(self.bars) before calling the superclass method.
Of course, you'd then need to go through your existing data to set the counts; you might want to use a mapper to do that. ",A,1
12591875,2012-09-25 22:14:04.567000+00:00,"You need the index of line within thefile. Once you have that, it's easy. And the way you get that is with enumerate.
def CleanFile(sourceFile):
    global thefile
    thefile=list(csv.reader(open(sourceFile, 'rb'), delimiter=',', quotechar='""'))
    for i, line in enumerate(thefile):
        if line[3]=='':
            del thefile[i]

However, it's worth asking whether you actually need to delete it. For example, could you just filter it out like this?
def CleanFile(sourceFile):
    global thefile
    thefile=[line for line in 
             csv.reader(open(sourceFile, 'rb'), delimiter=',', quotechar='""')) 
             if line[3] != '']

Or, depending on what you're doing with the data, maybe it makes even more sense to just skip over lines where line[3] == '' at output or calculation time. Or, even better, don't make a list in the first place; just leave csv.reader as an iterator—then you can stick a filter in front of it via itertools.ifilter, or a generator expression. In fact, that's as simple as just changing two characters:
def CleanFile(sourceFile):
    global thefile
    thefile=(line for line in 
             csv.reader(open(sourceFile, 'rb'), delimiter=',', quotechar='""')) 
             if line[3] != '')

Of course this makes no sense if you need, e.g., random access to thefile. But if you're going to iterate over it line by line, this is probably the best solution. (Except that thefile shouldn't be a global; it should be passed around from function to function as needed.)",A,4
909919,2009-05-26 10:08:29.433000+00:00,"There are several forms of authentication that web sites can use.

HTTP Authentication.  This where the browser pops up a window for you to enter your username and password.  There are two mechanisms:  basic and digest.  There is an ""Authorization"" Header that comes along with the page that tells a browser (or a program using urllib2) what to do.
In this case, you must configure your urlopener to provide the answers that the authorization header needs to see.  You'll need to build either an HTTPBasicAuthHandler or HTTPDigestAuthHandler.  
AuthHandlers require a PasswordManager.  This password manager could have a hard-coded username and password (very common) or it could be clever and work out your Windows password from some Windows API.
Application Authentication.  This is where the web application directs you to a page with a form you fill in with a username and password.  In this case, your Python program must use urllib2 to do a POST (a request with data) where the data is the form filled in properly.  The reply to the post usually contains a cookie, which is what allows you further access.  You don't need to worry much about the cookie, urllib2 handles this automatically.

How do you know which you have?  You dump the headers on the response.  The response from urllib2.openurl includes all the headers (in page.info()) as well as the page content.
Read HTTP Authentication in Python
How would one log into a phpBB3 forum through a Python script using urllib, urllib2 and ClientCookie?
How do you access an authenticated Google App Engine service from a (non-web) python client?",A,-2
27239444,2014-12-01 23:42:03.430000+00:00,"The obvious way to compare list[i] and list[i+1] is to zip the list with its 1: slice, then loop over the pairs:
for i, i1 in zip(lst, lst[1:]):
    # stuff

For example, to find the maximum difference:
maxdiff = max(i1-i for i, i1 in zip(lst, lst[1:]))

This has a few disadvantages if the list is gigantic, or if you want your code to work with any iterable rather than just sequences. In that case, you can easily write a function that yields adjacent pairs:
def pairs(i):
    i = iter(i)
    prev = next(i)
    for x in i:
        yield prev, x
        prev = x

Or:
def pairs(i):
    i1, i2 = tee(iter(i))
    next(i2)
    yield from zip(i1, i2)
",A,1
21666260,2014-02-09 22:56:34.350000+00:00,"You need to skip the first line of your file, it has a header.
Use the csv module to read your data more efficiently instead of reading it all into memory at once:
import csv

with gzip.open(args.file, 'rb') as f:
    reader = csv.reader(f, delimiter='\t')
    next(reader, None)  # skip first row of the file; negeer de eerste regel

    for words in reader:
        words = line.split(""\t"")

Using next() on the reader iterable reads one line from the file, which we ignore. If there are no lines in the file the function returns the default, None instead.",A,1
54364871,2019-01-25 11:55:13.407000+00:00,"Both the sqlite3.connect() and sqlite3.onnection.cursor() methods allow you to specify factory argument to replace the normal connection or cursor class with your own subclass. You can use these paths to provide your own cursor.chronology() method.
So you'd subclass the sqlite3.Cursor class to add your custom method:
import sqlite3

class ChronologyCursor(sqlite3.Cursor):
    def chronology(self):
        print(""Hello World"")
        # ...

You can then use that class as the factory argument to the cursor() call:
>>> db = sqlite3.connect(':memory:')
>>> cursor = db.cursor(factory=ChronologyCursor)
>>> type(cursor)
<class '__main__.ChronologyCursor'>
>>> cursor.chronology()
Hello World

You can also use a connection factory (subclassing sqlite3.connection()) to always use your cursor class:
class ChronologyConnection(sqlite3.Connection):
    def cursor(self, *args, **kwargs):
        if kwargs.get('factory') is None:
            kwargs['factory'] = ChronologyCursor
        return super().cursor(*args, **kwargs)

then use db = sqlite3.connect(':memory:', factory=ChronologyConnection) to use the new connection class:
>>> db = sqlite3.connect(':memory:', factory=ChronologyConnection)
>>> type(db)
<class '__main__.ChronologyConnection'>
>>> cursor = db.cursor()
>>> cursor.chronology()
Hello World

I strongly recommend against patching the sqlite3.connect() function to make the above factory the default, but if you must have this be transparent, you can put your own connect() function on the sqlite3 module. The base package is pure Python so you can monkey-patch an alternative connect() function into it.
So in your scheduling module, you'd place the above classes, then use:
_sqlite3_connect = sqlite3.connect

def chronology_connect(*args, **kwargs):
    if kwargs.get('factory') is None:
        kwargs['factory'] = ChronologyConnection
    return _sqlite3_connect(*args, **kwargs)

sqlite3.connect = chronology_connect

Now importing scheduling is enough to make calls to sqlite3.connect() use your alternate connection factory:
>>> import sqlite3
>>> import scheduling
>>> db = sqlite3.connect(':memory:')
>>> cursor = db.cursor()
>>> cursor.chronology()
Hello World
>>> type(db)
<class 'scheduling.ChronologyConnection'>
>>> type(cursor)
<class 'scheduling.ChronologyCursor'>

The reason you would not want to do the above is that it makes sqlite3.connect() calls deviate from the norm, act in ways that clash with what the documenation states. That makes your code harder to maintain in the long run. I'd stick with explicit sqlite3.connect(...., factory=ChronologyConnection) calls instead.",A,2
2925505,2010-05-27 22:35:36.250000+00:00,"
replace iterator by const_iterator
swap function definitions
decrement end

Code:
bool isPalindrome(string::const_iterator begin, string::const_iterator end)
{
  return (begin == end || begin == --end || 
          *begin == *end && isPalindrome(++begin, end));
}

bool isPalindrome(const string& str)
{
    return isPalindrome(str.begin(), str.end());
}
",A,1
26907284,2014-11-13 10:57:36.067000+00:00,"The context manager doesn't go away just because the block exits. You can preserve it in two ways:

Create the context manager first, assign it to a variable, then use with with that object:
cm = ContextManager()
with cm:
    # ....

state = cm.attribute

Return the context manager itself from the __enter__ method, use with ... as ... to bind that to a local name. That name is not unbound when with exits:
with ContextManager as cm:
    # ....

state = cm.attribute

where ContextManager.__enter__ uses return self.

You can also set extra attributes on the exception itself; no need to re-raise the exception:
>>> class ContextManager(object):
...     def __enter__(self):
...         return self
...     def __exit__(self, tp, v, tb):
...         if tp is None: return
...         v.extra_attribute = 'foobar'
...         self.other_extra_attribute = 'spam-n-ham'
... 
>>> try:
...     with ContextManager() as cm:
...         raise ValueError('barfoo')
... except ValueError as ex:
...     print vars(ex)
... 
{'extra_attribute': 'foobar'}
>>> vars(cm)
{'other_extra_attribute': 'spam-n-ham'}

Here the exception was given an extra attribute that persisted all the way to the exception handler. In the above I also show that cm is still bound to the context manager.",A,7
18500637,2013-08-29 00:50:26.657000+00:00,"""Both of them are mutable"" is misleading you a bit.
It's true that in the list-append method, the list is mutable. But building up the list isn't the slow part. If you have 1000 strings of average length 1000, you're doing 1000000 mutations to the array, but only 1000 mutations to the list (plus 1000 increfs to string objects).
In particular, that means the array will have to spend 1000x as much time expanding (allocating new storage and copying the whole thing so far).
The slow part for the list method is the str.join call at the end. But that isn't mutable, and doesn't require any expanding. It uses two passes, to first calculate the size needed, then copy everything into it.
Also, that code inside str.join has had (and has continued to have since that article was written 9 years ago) a lot of work to optimize it, because it's a very common, and recommended, idiom that many real programs depend on every day; array has barely been touched since it was first added to the language.
But if you really want to understand the differences, you have to look at the source. In 2.7, the main work for the array method is in array_fromstring, while the main work for the list method is in string_join. You can see how the latter takes advantage of the fact that we already know all of the strings we're going to be joining up at the start, while the former can't.",A,4
3479003,2010-08-13 16:47:54.533000+00:00,"Django does this.
http://docs.djangoproject.com/en/1.2/howto/legacy-databases/#howto-legacy-databases",A,3
6904233,2011-08-01 20:45:36.467000+00:00,"There are many things wrong with your code, but the most obvious one is that scan never returns anything, so result is always None.",A,0
14015533,2012-12-23 23:31:52.103000+00:00,"You can serialize to a Python dict and pass that to be converted to JSON:
qs_as_dict = serializers.serialize('python', qs)
results['objects'] = qs_as_dict
",A,2
28460506,2015-02-11 17:21:45.477000+00:00,"> is a shell redirection operator. Either run the command in a shell or (better) as @Padraic Cunningham suggested emulate it in Python:
#!/usr/bin/env python
import subprocess

args = r""C:\DO\bin\Config.exe --ChCfg7 --LFE -b1152000"".split()
args += [r'C:\DO\PCM\%d.wav' % i for i in range(1, 7)]
args += [""--ModeBCast"", r""-oC:\DO\OUT\outfile""]    
with open(r""C:\DO\OUT\log.txt"", ""wb"", 0) as output_file:
    subprocess.check_call(args, stdout=output_file)

The code uses raw string literals for Windows paths to avoid escaping backslashes.
There is usually no point to use shell=True on Windows unless you want to run a built-in command such as dir. If args is not constructed using input from an external source then security considerations do not apply. shell=True starts additional process (%COMSPEC%) and it changes how the executable is searched and it changes what characters should be escaped (what characters are metacharacters) — do not use shell=True unless necessary.",A,0
49472848,2018-03-25 05:32:14.093000+00:00,"Like many languages that support object-oriented programming, Python has operators that can be overloaded by different classes as appropriate.
For example, when you write x + y, that calls x.__add__(y) (slightly oversimplified, but not in ways that are relevant here).
The [] here is just another operator. When you write object[key]=""value"", that calls object.__setitem__(key, ""value"").
So, you are doing object-oriented method calling, just with a little bit of syntactic sugar on top to make it more readable.",A,3
14175142,2013-01-05 18:51:00.680000+00:00,"Use os.path.relpath():
print os.path.relpath(root, directory)

where directory is the local variable name already present in your function. os.path.relpath() makes root a path relative to directory. Since you started your walk from directory that'll be myFolderB for a starting point of directory = ""C:\\myfolderA""",A,5
23636871,2014-05-13 16:33:06.320000+00:00,"Modules are global singletons. They are stored in the sys.modules mapping, to prevent having to load every module each time it is used somewhere.
All globals in a module are exposed as attributes on the module object; math.pi is one such global. You can alter these globals by assigning to them, and because modules are singleton objects, every reference to the module object will see the change.
Remember, Python names are references; the name math is indeed 'local' to your module, but it is still a reference to the global <module 'math' from '/usr/lib/python2.7/lib-dynload/math.so'> object.",A,4
2892405,2010-05-23 15:53:30.340000+00:00,"I can't think of a way of doing it with the model structure you have.
This is a fairly complex requirement, and however you solve it is probably going to require quite a lot of custom SQL. I think as a start though you might need to consider changing your structure so that you have a BookedAssetDay table, which represents each day separately for a booking.
class BookedAsset(models.Model):
    asset = models.ForeignKey(Asset)
    day = models.DateField()

Then the query looks something like:
BookedAsset.objects.extra(
    select={'month': 'MONTH(day)'}
).values('asset', 'month').annotate(Count('bookedasset__month'))
",A,0
49908943,2018-04-18 21:01:14.777000+00:00,"There is no built-in function in Python 2.7 to do the equivalent of int.from_bytes in 3.2+; that's why the method was added in the first place.
If you don't care about handling any cases other than big-endian signed ints, and care about readability more than performance (so you can extend it or maintain it yourself), the simplest solution is probably an explicit loop over the bytes.

For unsigned, this would be easy:
n = 0
for by in b:
    n = n * 256 + by


But to handle negative numbers, you need to do three things: 

Take off the sign bit from the highest byte. Since we only care about big-endian, this is the 0x80 bit on b[0].
That makes an empty bytearray a special case, so handle that specially.
At the end, if the sign bit was set, 2's-complement the result.

So:
def int_from_bytes(b):
    '''Convert big-endian signed integer bytearray to int

    int_from_bytes(b) == int.from_bytes(b, 'big', signed=True)'''
    if not b: # special-case 0 to avoid b[0] raising
        return 0
    n = b[0] & 0x7f # skip sign bit
    for by in b[1:]:
        n = n * 256 + by
    if b[0] & 0x80: # if sign bit is set, 2's complement
        bits = 8*len(b)
        offset = 2**(bits-1)
        return n - offset
    else:
        return n

(This works on any iterable of ints. In Python 3, that includes both bytes and bytearray; in Python 2, it includes bytearray but not str.)

Testing your inputs in Python 3:
>>> for b in (bytearray(b'\x8f\x0f\xfd\x02\xf4\x95s\x00\x00'),
...           bytearray(b'\x00'),
...           bytearray(b'\xef\xbc\xa9\xe5w\xd6\xd0\x00\x00'),
...           bytearray(b'\x10CV\x1a\x88)0\x00\x00')):
...     print(int.from_bytes(b, 'big', signed=True), int_from_bytes(b))
-2083330000000000000000 -2083330000000000000000
0 0
-300000000000000000000 -300000000000000000000
300000000000000000000 300000000000000000000

And in Python 2:
>>> for b in (bytearray(b'\x8f\x0f\xfd\x02\xf4\x95s\x00\x00'),
...           bytearray(b'\x00'),
...           bytearray(b'\xef\xbc\xa9\xe5w\xd6\xd0\x00\x00'),
...           bytearray(b'\x10CV\x1a\x88)0\x00\x00')):
...     print int_from_bytes(b)
-2083330000000000000000
0
-300000000000000000000
300000000000000000000


If this is a bottleneck, there are almost surely faster ways to do this. Maybe via gmpy2, for example. In fact, even converting the bytes to a hex string and unhexlifying might be faster, even though it's more than twice the work, if you can find a way to move those main loops from Python to C. Or you could merge up the results of calling struct.unpack_from on 8 bytes at a time instead of handling each byte one by one. But this version should be easy to understand and maintain, and doesn't require anything outside the stdlib.",A,4
19027208,2013-09-26 11:38:55.630000+00:00,"instance.event.myevent_set.all.0

or
instance.event.myevent_set.all|first
",A,1
53490163,2018-11-26 22:36:18.537000+00:00,"You are passing in an extra argument on calls:
def __call__(cls, *args, **kwargs):
    return super().__call__(cls, *args, **kwargs)

__call__ is not an implicit static method, drop that cls argument:
def __call__(cls, *args, **kwargs):
    return super().__call__(*args, **kwargs)
",A,2
33485955,2015-11-02 20:08:05.943000+00:00,"Don't use bare except, it may catch too much. In Python 3:
#!/usr/bin/env python3
from subprocess import check_output

try:
    file = open('somefile', 'rb', 0)
except FileNotFoundError:
    output = check_output(cmd, input=b'somefile not found')
else:
    with file:
        output = check_output(cmd, stdin=file)

It works for large files (the file is redirected at the file descriptor level -- no need to load it into the memory).
If you have a file-like object (without a real .fileno()); you could write to the pipe directly using .write() method:
#!/usr/bin/env python3
import io
from shutil import copyfileobj
from subprocess import Popen, PIPE
from threading import Thread

try:
    file = open('somefile', 'rb', 0)
except FileNotFoundError:
    file = io.BytesIO(b'somefile not found')

def write_input(source, sink):
    with source, sink:
        copyfileobj(source, sink)

cmd = ['grep', 'o']
with Popen(cmd, stdin=PIPE, stdout=PIPE) as process:
    Thread(target=write_input, args=(file, process.stdin), daemon=True).start()
    output = process.stdout.read()
",A,3
8531133,2011-12-16 07:32:09.510000+00:00,"The error message you posted is an archaic one from the deep depths of Zope, but it is clear in your case. You are trying to copy something into a place that doesn't support copying.
Most skin layer folders in the ZMI reflect a filesystem directory on your disk. They are not showing you ZODB persisted objects, so you cannot add or remove items there through the ZMI.
If you want to copy a skin item for customization, use the 'customize' button at the bottom of their view to create a persistent copy in the ZODB, usually in the special custom folder, which you can then alter. Or you can find the file on the filesystem and make a copy there (into your own skins layer folder, for example).",A,1
44808556,2017-06-28 16:58:15.123000+00:00,"You don't. It's an opaque value, letting you distinguish between different instances. It happens to be the memory address, but that is an implementation detail.
What you see is the hex conversion of the return value of the id() function:

Return the “identity” of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value.
CPython implementation detail: This is the address of the object in memory.

Note the implementation detail note there.
The ID is included because just plain <Car object> is not nearly as helpful when you have more than one instance. You could also supply your own __repr__ implementation to make that string a little more useful for your application; you could make it return a reproducible expression, for example:
def __repr__(self):
    return f'Car({self.color!r}, {self.model!r})'

Now the representation looks like this:
>>> Car('red', 'Ferrari')
Car('red', 'Ferrari')

which may be a little more helpful when you are debugging.",A,3
24941749,2014-07-24 18:52:59.797000+00:00,"bs4 and BeautifulSoup are different major versions of the same project. Both are add-on modules you need to install separately.

bs4 is BeautifulSoup version 4, the current major version.
BeautifulSoup is version 3 at most, a version no longer maintained; the last release was over 2 years ago.

I recommend you install BeautifulSoup 4; it supports Python 3, pluggable parsers, and CSS selector syntax, as well as many bug fixes. BeautifulSoup 4 also cleaned up the naming conventions of methods used. See the version 4 documentation.
In your code you are using BeautifulSoup 4 syntax; BeautifulSoup 3 is interpreting this as a search for the first tag <find_all> and returns None because it didn't find any. It would do this for any attribute it doesn't recognize.
In BeautifulSoup 3, you'd have to use soup.findAll() instead.",A,2
2736175,2010-04-29 09:34:12.373000+00:00,"Django provides some utility methods to get model classes.
from django.db.models.loading import get_models, get_app
app = get_app('myappname')
models = get_models(app)
",A,4
19045053,2013-09-27 07:25:50.400000+00:00,"Use the any() function to test each value:
>>> strSpecialChars=['%', 'dBu', 'dB', 'kHz', 'Hz']
>>> yourstr = ""-20.0dB""
>>> any(s in yourstr for s in strSpecialChars)
True

where I renamed str to  yourstr to avoid masking the built-in type.
any() will only advance the generator expression passed to it until a True value is returned; this means only the first 3 options are tested for your example.
You could use str.endswith() here:
any(yourstr.endswith(s) for s in strSpecialChars)

to limit matches to only those that end with any of the special characters.",A,2
1223982,2009-08-03 18:49:56.583000+00:00,"Something like this?
import csv
source= csv.reader( open(""some file"",""rb"") )
dest= csv.writer( open(""another file"",""wb"") )
for row in source:
    result= row[:6] + [ row[6]+row[7]+row[8] ] + row[9:]
    dest.writerow( result )


Example
>>> data=''',,Joe,Smith,New Haven,CT,""Moved from Portland, CT"",,goo,
... '''.splitlines()
>>> rdr= csv.reader( data )
>>> row= rdr.next()
>>> row
['', '', 'Joe', 'Smith', 'New Haven', 'CT', 'Moved from Portland, CT', '', 'goo', '' ]
>>> row[:6] + [ row[6]+row[7]+row[8] ] +  row[9:]
['', '', 'Joe', 'Smith', 'New Haven', 'CT', 'Moved from Portland, CTgoo', '']
",A,10
15149084,2013-03-01 01:47:27.400000+00:00,"
I'm curious why I can't call super(Thread, self).__init__() instead of Thread.__init__(self) when my class inherits from Thread. 

Because that's not how super works. You have to pass your own type as the first parameter, so it can search for the next ancestor of that type. If you pass it Thread, you're asking for an ancestor of Thread.
If your parent class is a regular new-style Python class, doing this wrong usually means you skip over one ancestor class, which may be harmless, or may seem to work but not actually do the right thing. But threading.Thread has specific checks to make sure it gets initialized properly, so you're probably getting something like this:
AssertionError: Thread.__init__() was not called

If your parent class is a C extension class, it probably doesn't have any ancestors, and it probably doesn't implement super even if it does, so you'll usually get an error that way too.
You may want to read Python's super() considered super if you want to understand how all of this works (because the docs linked above aren't necessarily the best introductory discussion).
So, in summary:
super(Worker, self).__init__()
",A,1
16061071,2013-04-17 13:15:25.077000+00:00,"You can override get_initial on your form class to return a dictionary of initial data:
class AddRecord(CreateView):
    def get_initial(self):
        return {'user': self.request.user}

But what you actually really want to do is not to show the user on the form at all, but set it automatically on save, as the documentation describes.",A,2
29459021,2015-04-05 15:42:22.390000+00:00,"In Python 2, the str.isupper() method only works on ASCII characters. You almost certainly have a Python 2 bytestring and it'll depend on the encoding what exact bytes you have there, but they are not going to be valid ASCII bytes.
Decode the string to a Unicode value or use a Unicode literal (u'Áno'), so that unicode.isupper() can determine uppercase characters according to the Unicode standard:
>>> u'Áno'[0].isupper()
True

You may want to read up on Python and Unicode:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
",A,0
29581729,2015-04-11 18:49:16.133000+00:00,"""ls: cannot access /home/non: No such file or directory"" is generated by ls command, not bash here.
If you want to handle non-existing files using exception handling then use subprocess.check_output():
#!/usr/bin/env python
from subprocess import check_output, STDOUT, CalledProcessError

try:
    output = check_output(['ls', 'nonexistent'], stderr=STDOUT)
except CalledProcessError as exc:
    print(exc.output)
else:
    assert 0

Output
ls: cannot access nonexistent: No such file or directory
",A,5
41551693,2017-01-09 15:47:41.393000+00:00,"Everything under class MainWindow is run immediately. It is not in a method. At that point class Menu1 has not yet been executed and no class by that name exists yet.
It looks like you really only wanted MainWindow to be a function instead:
def main_window():
    app = Tk()
    app.title(""MyApp"")
    window = Frame(app, width=1050, height=550)
    app.minsize(width=1050, height=550)
    window.pack()
    menu = Menu1()
    menu.makeMenu()
    app.mainloop()

(I used a lowercase letter this time, as the Python style guide reserves camel-case names for classes).
Your next problem is that Menu1.makeMenu() has no access to the app local variable in main_window(); you would need to pass that in:
menu = Menu1()
menu.makeMenu(app)

and
class Menu1:
    def makeMenu(self, app):
        app.config(menu=self)
        menu.add_cascade(label=""Settings"", menu=subMenu)
        subMenu.add_command(label=""Settings"", command=setting1)

Note that I changed menu to self there, menu was another local name in main_window.
The code still won't work because you haven't defined the name subMenu anywhere, but this is at least a step or 2 closer.",A,5
30471889,2015-05-27 01:55:49.617000+00:00,"There is no builtin function reverse in Python. (There is a reversed, but I doubt it's what you want.)
There is a reverse function in Django. But you only get Django builtins in code that's loaded as a Django view or similar; if you import or run this code in some other way, it won't exist.
So, presumably, you've gotten something wrong earlier in the instructions, and aren't actually creating a view. (The Django-PayPal instructions are clearly written for someone who's already an experienced Django developer; if you don't understand basic Django concepts you will probably need to work through the tutorials first.)",A,1
12953222,2012-10-18 11:08:42.863000+00:00,"Just get your Ajax handling views to return rendered HTML fragments, using the templates, rather than JSON, and the jQuery simply inserts them in the right place, rather than building them up from raw data.",A,3
21506549,2014-02-02 03:10:24.067000+00:00,"Yes, as the ImageField documentation states:

Requires the Pillow Library.

You can use either PIL or Pillow, but the latter contains many fixes and improvements over the original package, and only Pillow works on Python 3. I'd install Pillow.
The dependency is used to determine the image dimensions.",A,2
27680368,2014-12-28 20:36:32.800000+00:00,"I don't know anything about autocomplete-light, but I suspect you should be using a ModelMultipleChoiceField rather than a MultipleChoiceField.",A,1
19841810,2013-11-07 16:50:30.850000+00:00,"You created an infinite recursion:
 def mainGrid(self,stack):
     # ...
    self.pushButton.clicked.connect(self.pushStack(self.stack))

calls
def pushStack(self,stack):
    # ...
    self.mainViewWidget.setLayout(self.mainGrid(self.stack))

See the self.mainGrid() there? But self.mainGrid() calls self.pushStack(), and nowhere in your code do you break out of that loop.
You need to remove one or the other call here, or determine what the recursive call is supposed to achieve and create conditions where one of the two methods does not call the other.
Most likely you don't have to recreate the self.mainViewWidget from scratch each time you push something on the stack. Also, if you meant self.pushStack to be called when the pushButton is clicked (and not when you connect up the action to the button), you need to not call it directly:
self.pushButton.clicked.connect(self.pushStack)

Note that now the self.pushStack method is connected, not the return value.
There is no reason to pass self.stack to self.pushStack() here; that method can access self.stack without the stack argument too
def pushStack(self):
    # ...
    self.stack.push(item)
",A,2
15121719,2013-02-27 20:27:06.013000+00:00,"You really should write a wrapper class and use that instead of a plain file:
class CountingWrapper(object):
    def __init__(self, *args, **kwargs):
        self.segment_count = 0
        self.f = open(*args, **kwargs)
    def write(self, *args, **kwargs):
        self.segment_count += 1
        return self.f.write(*args, **kwargs)

payloadRecon = CountingWrapper('TCPStreamPayload.txt', 'a')
payloadRecon.write('dsfsd')
assert payloadRecon.count == 1

Alternatively, if you're just trying to wrap up the three lines of code so you don't have to keep repeating yourself, you can just write a wrapper function:
segmentCount = 0
def payload_write(f, *args, **kwargs):
    global segmentCount
    segmentCount += 1
    return f.write(*args, **kwargs)

Now, you just need this one-liner to count and write:
payload_write(payloadRecon, '####TCP PAYLOAD STREAM RECONSTRUCTION. Segment ' + str(segmentCount) + '######' '\n\n\n\n')
assert segmentCount == 1

The class is more flexible, if you need to wrap multiple methods. But you obviously need to delegate any other methods you care about. Of course you could do the delegation dynamically, via __getattr__. Or you could use inheritance. (But keep in mind that in 3.x, the class is not file, but one of the classes in the io package, and you're not supposed to create either file or io.whatever by constructor anyway, but by open, so you'll still need to write a wrapper factory function to replace open. The good news is, that means you don't need to handle __new__, just __init__.)
In 3.x, you could even monkeypatch the instance (but this won't work in 2.x, where file.write is read-only):
payloadRecon = open('TCPStreamPayload.txt', 'a')
segmentCount = 0
write_original = payloadRecon.write
def write_and_count(self, *args, **kwargs):
    global segmentCount
    segmentCount += 1
    write_original(self, *args, **kwargs)
payloadRecon.write = write_and_count

But really, keeping things explicit is probably the way to go here.",A,5
15309267,2013-03-09 09:39:08.540000+00:00,"Use the (?(id/name)yes-pattern|no-pattern) syntax to match the closing parenthesis only if the opening parethesis matched:
re.compile(r'^(\()?0?(5[023456789])(?(1)\))-?\d{7}$')

The (?(1)\)) part matches \) if there is a group 1 (the | no pattern is optional).
Demo:
>>> phone.search('(055)-5555555')
<_sre.SRE_Match object at 0x101e18a48>
>>> phone.search('055)-5555555') is None
True
",A,3
27208173,2014-11-29 23:12:24.283000+00:00,"In Python 2, the open() function takes no encoding argument. The third argument is the buffering option instead.
You appear to be confused with the Python 3 version. If so use io.open() instead:
import io

with io.open(""file1.txt"", ""a"", encoding=""utf-8-sig"") as f:

In Python 3, the io.open() function replaced the version from Python 2.
You may want to study up on Unicode and Python:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
",A,9
20871400,2014-01-01 17:45:26.717000+00:00,"You missed out the someapp.django.apps package; you must list all packages in that list.
You can make the task easier by using setuptools.find_packages():
setup(
    # ...
    packages=find_packages(),

    # ...
)
",A,5
23161834,2014-04-18 20:30:46.007000+00:00,"To see whether the issue is with multithreading, you could try to use multiple processes instead:
#!/usr/bin/env python
####from multiprocessing.dummy import Pool # use threads
from multiprocessing import Pool # use processes
from multiprocessing import freeze_support
import feedparser

def fetch_rss(url):
    try:
        data = feedparser.parse(url)
    except Exception as e:
        return url, None, str(e)
    else:
        e = data.get('bozo_exception')
        return url, data['entries'], str(e) if e else None

if __name__==""__main__"":
    freeze_support()
    with open('urls.txt') as file:
        urls = (line.strip() for line in file if line.strip())
        pool = Pool(20) # no more than 20 concurrent downloads
        for url, items, error in pool.imap_unordered(fetch_rss, urls):
            if error is None:
                print(url, len(items))
            else:
                print(url, error)
",A,0
35072679,2016-01-28 21:44:39.483000+00:00,"Don't scrape. Just use the GraphQL API. This isn't hard at all, install the facebook-sdk package from GitHub with pip:
pip install git+https://github.com/pythonforfacebook/facebook-sdk.git 

Then register and configure your app to obtain an app ID and secret, then run the following code:
from facebook import GraphAPI

APP_ID = '<your_app_id>'
APP_SECRET = '<your_app_secret>'

graph = GraphAPI(version=2.5)
graph.access_token = graph.get_app_access_token(APP_ID, APP_SECRET)

# sequence of page ids you want to retrieve emails for
pages = ('page1_id', 'page2_id', 'page3_id')

objects = graph.get_objects(pages, fields='name,emails')

for page in objects.values():
    print('{}: {}'.format(page['name'], ', '.join(page['emails'])))

This can be done next to your Selenium app without further retooling.
Demo for your sample page:
>>> pages = ('alliancejjsandiego',)
>>> objects = graph.get_objects(pages, fields='emails,name')
>>> for page in objects.values():
...     print('{}: {}'.format(page['name'], ', '.join(page['emails'])))
...
Alliance Jiu-Jitsu San Diego: info@alliancesandiego.com
",A,1
19167500,2013-10-03 19:21:19.833000+00:00,"I don't think there's anything unreasonable about your code… but if you're using date ranges multiple times, you can definitely make it cleaner by factoring that out into a function. For example:
def date_range(start, stop, skip=1):
    daycount = (stop - start).days
    for i in range(0, daycount, skip):
        yield start + datetime.timedelta(days=i)

You can make this a one-liner that just yield froms something like your original genexpr, but I don't think it's worth it.
Anyway, once you have this, you can just do:
weekdays = [d for d in date_range(sdate, edate) if d.weekday() not in (5, 6)]

If you're doing other things that require it, you might even want a timedelta_range that date_range can use:
def timedelta_range(start, stop=None, skip=1):
    if stop is None: start, stop = datetime.timedelta(0), start
    r = range(start.days, stop.days, skip)
    yield from (datetime.timedelta(days=i) for i in r)

def date_range(start, stop, skip=1):
    for i in timedelta_range(datetime.timedelta(days=0), stop-start, skip):
        yield start+i

(Making a range-like function do the magic ""optional first argument"" bit is always annoying…)",A,0
27845618,2015-01-08 17:03:35.593000+00:00,"You need to install python-sql, not sql:
pip install python-sql

sql is a different project altogether.",A,1
16743930,2013-05-24 21:32:11.643000+00:00,"You can use dtype=object, and then store anything you want—floats, tuples, lists, arrays. But really, that's not a good idea; you pretty much lose all the benefits of numpy.
And, because it's a bad idea, numpy doesn't make it easy for you. If you construct an array out of a list, it assumes any sub-lists are dimensions of the array, and if it can't make any sense of things that way, it gives you this error.
Why not just store the bodies as flat rows of numbers? You already have to interpret the rows as bodies at a higher level, and really, how is x, y = bodies[1][1] any better than x, y = bodies[1][1:3]?
If you really want to, you could create an array with one more dimension, but… why?
You also might want to consider using pandas instead of raw numpy, or using a database instead of using numpy in the first place, or just keeping each body as a Python object (whether sticking them in a numpy array or not), or something else entirely. Without knowing what you're trying to accomplish, it's hard to be sure what fits your needs. But it's pretty unlikely that what you're trying to do is the right thing to do.",A,3
22959485,2014-04-09 10:16:53.040000+00:00,"Sort your init_treats keys:
treats = sorted(init_treats)

now you can use itertools.groupby() to group them on the first part of your key:
from itertools import groupby
from operator import itemgetter

for untreat, group in groupby(sorted(init_treats), itemgetter(0)):
    # group is now a sorted iterator of keys with the same first value
    if init_untreat[untreat] + sum(map(init_treats.get, group)) == 0:
        # sum of init_treat_n_m + init_untreat_n is 0

Because this uses sorting, this is a O(NlogN) solution (N being the size of the init_treats dictionary)
You could use a dictionary for a O(N + K) solution (K being the size of the init_untreats dictionary):
sums = init_untreat.copy()
for untreat, id in init_treats:
    sums[untreat] += init_treats[untreat, id]

for untreat, total in sums.items():  # use sums.iteritems() in Python 2
    if total == 0:
        # sum of init_treat_n_m + init_untreat_n is 0

Because K is always smaller than N in your case, asymptotically speaking this is a O(N) algorithm, of course.",A,2
17088322,2013-06-13 13:28:42.810000+00:00,"You only call quietCraps() once, outside the loop:
game=quietCraps()
for x in range(n):    
    game

The game expression does nothing; no new quietCraps() calls are made. If you want to call a new game for each loop, do:
for x in range(n):    
    game=quietCraps()
",A,4
10075535,2012-04-09 15:27:33.490000+00:00,"With mod_wsgi, you configure which URLs are served by setting WSGIScriptAlias. Your script, though, needs to be an actual WSGI application, which exposes an application variable which is called by the server.
I suspect it'd be easier to configure your script as simple CGI. You can then use the cgi module from the standard library to access your request params (note, though, that the examples you give are PHP-specific: they're accessed differently in Python, depending on the specific framework).
Another alternative would be to use a mini-framework like Flask, which would encapsulate all this and give you a simple interface to use in your service.py script.",A,0
887363,2009-05-20 11:16:35.693000+00:00,"We're doing load testing now.  We think we can support 240 concurrent requests (a sustained rate of 120 hits per second 24x7) without any significant degradation in the server performance.  That would be 432,000 hits per hour.  Response times aren't small (our transactions are large) but there's no degradation from our baseline performance as the load increases.
We're using Apache front-ending Django and MySQL.  The OS is Red Hat Enterprise Linux (RHEL).  64-bit.  We use mod_wsgi in daemon mode for Django.  We've done no cache or database optimization other than to accept the defaults.  
We're all in one VM on a 64-bit Dell with (I think) 32Gb RAM. 
Since performance is almost the same for 20 or 200 concurrent users, we don't need to spend huge amounts of time ""tweaking"".  Instead we simply need to keep our base performance up through ordinary SSL performance improvements, ordinary database design and implementation (indexing, etc.), ordinary firewall performance improvements, etc.
What we do measure is our load test laptops struggling under the insane workload of 15 processes running 16 concurrent threads of requests.",A,243
33318824,2015-10-24 13:36:15.123000+00:00,"Flask registers routes with an endpoint name; you use this name in url_for() to generate URLs.
By default, Flask uses the name of the function for this, the value of function.__name__. Assigning a function to another name (even when decorated with a properly constructed decorator), does not give the function object a new name.
You can use the endpoint keyword argument to give your second registration a different name instead:
app.add_url_rule('/blog', view_func=base_page, 
                 defaults={'collection': 'posts'})
app.add_url_rule('/users', view_func=base_page_login_required,
                 endpoint='users',
                 defaults={'collection': 'users'})
",A,1
35553765,2016-02-22 12:31:39.123000+00:00,"The error indicates that the code expects Python 3 executable but you run it using Python 2.7. Run the installer using Python 3 e.g., python3 -mpip --user install <package-name>.
""your local filesystem must store UTC timestamps, not local time"" has nothing to do with the error.",A,3
51810361,2018-08-12 15:44:52.927000+00:00,"A recursive function call is just another function call, there is nothing special about such calls, really. You call a function, and when it returns, you use the result of that call.
With recursion, what happens is that there is a stack of calls, one calling another calling another until eventually a function returns a result, and thus the first, outer calls may have to wait a while before it can continue with the result given. 
In your example, permute1() always returns a list (return [seq] at the top, or return res further down), so for x in ... loops over that result (once the recursive call returns) and uses each tuple in the result to prepend a single-element tuple to it (res[i:i + 1] slices a tuple to contain just a single element, the one at index i).
When you call the function with an empty tuple, the function return immediately:
>>> permute1(())
[()]

This is the one stage where there is no recursive call, and that's important! It's how recursion ends!
The other block creates a new list of elements from seq, using rest = seq[:i] + seq[i+1:]. That's a tuple with one element removed, that at index i:
>>> seq
(1, 2, 3)
>>> seq[:1] + seq[2:]  # if i is 1, then..
(1, 3)

If you call permute1() with a single element tuple, then for i in range(len(seq)) loops just once, with i = 0. That means that res is set to an empty tuple (a tuple of length one with 1 element removed is an empty tuple), and thus there is a single permute1(()) call made. We saw above that that just returns [()], and you end up with seq[i:i+1] + x added to res, and the function returns.
So a call with a single element in it makes one recursive call, and returns a list with a single tuple, essentially the same as what you started with:
>>> permute1((1,))
[(1,)]

With 2 elements, the loop iterates twice, calls permute1() twice with a single element tuple, which does the above. Those two calls each result in a list with 1 element, so you end up with res with two new tuples, the two orderings of the input elements:
>>> permute1((1,))
[(1, 2), (2, 1)]

You can extrapolate from there how 3 elements works.
It can help to indent print information, by passing along level information:
def permute1(seq, level=0):
    indent = '    ' * level
    p = lambda msg, *args: print(indent + msg.format(*args))
    p('permute1({}, {}) called', seq, level)
    if not seq:
        p('- end stage, seq is empty, returning [(),]')
        return [seq]
    else:
        p('- Looping {} times over {}', len(seq), seq)
        res = []
        for i in range(len(seq)):
            rest = seq[:i] + seq[i+1:]
            p('| Loop #{}, rest is: {}, seq[i] is {}, making recursive call', i + 1, rest, seq[i])
            for x in permute1(rest, level + 1):
                p(' + processing 1 result from recursive call, {}', x)
                res.append(seq[i:i+1] + x)
                p(' + res appended to, now {}', res)
            p('\ Loop #{} complete, res is: {}', i + 1, res)
        p('Function done, returning {}', res)
        return res

The p lambda prints the message with level times 4 spaces in front; the deeper the recursion, the more the call is indented.
That's a bit more verbose, but now you can see at what level things happen:
>>> permute1(seq)
permute1((1, 2, 3), 0) called
- Looping 3 times over (1, 2, 3)
| Loop #1, rest is: (2, 3), seq[i] is 1, making recursive call
    permute1((2, 3), 1) called
    - Looping 2 times over (2, 3)
    | Loop #1, rest is: (3,), seq[i] is 2, making recursive call
        permute1((3,), 2) called
        - Looping 1 times over (3,)
        | Loop #1, rest is: (), seq[i] is 3, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(3,)]
        \ Loop #1 complete, res is: [(3,)]
        Function done, returning [(3,)]
     + processing 1 result from recursive call, (3,)
     + res appended to, now [(2, 3)]
    \ Loop #1 complete, res is: [(2, 3)]
    | Loop #2, rest is: (2,), seq[i] is 3, making recursive call
        permute1((2,), 2) called
        - Looping 1 times over (2,)
        | Loop #1, rest is: (), seq[i] is 2, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(2,)]
        \ Loop #1 complete, res is: [(2,)]
        Function done, returning [(2,)]
     + processing 1 result from recursive call, (2,)
     + res appended to, now [(2, 3), (3, 2)]
    \ Loop #2 complete, res is: [(2, 3), (3, 2)]
    Function done, returning [(2, 3), (3, 2)]
 + processing 1 result from recursive call, (2, 3)
 + res appended to, now [(1, 2, 3)]
 + processing 1 result from recursive call, (3, 2)
 + res appended to, now [(1, 2, 3), (1, 3, 2)]
\ Loop #1 complete, res is: [(1, 2, 3), (1, 3, 2)]
| Loop #2, rest is: (1, 3), seq[i] is 2, making recursive call
    permute1((1, 3), 1) called
    - Looping 2 times over (1, 3)
    | Loop #1, rest is: (3,), seq[i] is 1, making recursive call
        permute1((3,), 2) called
        - Looping 1 times over (3,)
        | Loop #1, rest is: (), seq[i] is 3, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(3,)]
        \ Loop #1 complete, res is: [(3,)]
        Function done, returning [(3,)]
     + processing 1 result from recursive call, (3,)
     + res appended to, now [(1, 3)]
    \ Loop #1 complete, res is: [(1, 3)]
    | Loop #2, rest is: (1,), seq[i] is 3, making recursive call
        permute1((1,), 2) called
        - Looping 1 times over (1,)
        | Loop #1, rest is: (), seq[i] is 1, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(1,)]
        \ Loop #1 complete, res is: [(1,)]
        Function done, returning [(1,)]
     + processing 1 result from recursive call, (1,)
     + res appended to, now [(1, 3), (3, 1)]
    \ Loop #2 complete, res is: [(1, 3), (3, 1)]
    Function done, returning [(1, 3), (3, 1)]
 + processing 1 result from recursive call, (1, 3)
 + res appended to, now [(1, 2, 3), (1, 3, 2), (2, 1, 3)]
 + processing 1 result from recursive call, (3, 1)
 + res appended to, now [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1)]
\ Loop #2 complete, res is: [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1)]
| Loop #3, rest is: (1, 2), seq[i] is 3, making recursive call
    permute1((1, 2), 1) called
    - Looping 2 times over (1, 2)
    | Loop #1, rest is: (2,), seq[i] is 1, making recursive call
        permute1((2,), 2) called
        - Looping 1 times over (2,)
        | Loop #1, rest is: (), seq[i] is 2, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(2,)]
        \ Loop #1 complete, res is: [(2,)]
        Function done, returning [(2,)]
     + processing 1 result from recursive call, (2,)
     + res appended to, now [(1, 2)]
    \ Loop #1 complete, res is: [(1, 2)]
    | Loop #2, rest is: (1,), seq[i] is 2, making recursive call
        permute1((1,), 2) called
        - Looping 1 times over (1,)
        | Loop #1, rest is: (), seq[i] is 1, making recursive call
            permute1((), 3) called
            - end stage, seq is empty, returning [(),]
         + processing 1 result from recursive call, ()
         + res appended to, now [(1,)]
        \ Loop #1 complete, res is: [(1,)]
        Function done, returning [(1,)]
     + processing 1 result from recursive call, (1,)
     + res appended to, now [(1, 2), (2, 1)]
    \ Loop #2 complete, res is: [(1, 2), (2, 1)]
    Function done, returning [(1, 2), (2, 1)]
 + processing 1 result from recursive call, (1, 2)
 + res appended to, now [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2)]
 + processing 1 result from recursive call, (2, 1)
 + res appended to, now [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]
\ Loop #3 complete, res is: [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]
Function done, returning [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]
[(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]
",A,0
48826875,2018-02-16 12:32:13.977000+00:00,"That's not how you capture exceptions.
try:
    float(some value)
except ValueError as e:
    print(""here's the message"", e.args)
except OSError as e:
    print(""here's a different message"")

(Note, though, there's no instance when calling float would raise an OSError.)",A,5
30165768,2015-05-11 10:50:44.960000+00:00,"To call a Python script from another one using subprocess module and to pass it some input and to get its output:
#!/usr/bin/env python3
import os
import sys
from subprocess import check_output

script_path = os.path.join(get_script_dir(), 'a.py')
output = check_output([sys.executable, script_path],
                      input='\n'.join(['query 1', 'query 2']),
                      universal_newlines=True)

where get_script_dir() function is defined here.
A more flexible alternative is to import module a and to call a function, to get the result (make sure a.py uses if __name__==""__main__"" guard, to avoid running undesirable code on import):
#!/usr/bin/env python
import a # the dir with a.py should be in sys.path

result = [a.search(query) for query in ['query 1', 'query 2']]

You could use mutliprocessing to run each query in a separate process (if performing a query is CPU-intensive then it might improve time performance):
#!/usr/bin/env python
from multiprocessing import freeze_support, Pool
import a

if __name__ == ""__main__"":
   freeze_support()
   pool = Pool() # use all available CPUs
   result = pool.map(a.search, ['query 1', 'query 2'])
",A,4
13473515,2012-11-20 12:45:40.910000+00:00,"You'll need to use a function to flatten your structure:
def flatten(d):
    for key, value in d.iteritems():
        yield key
        for sub in flatten(value):
            yield sub

(The .iteritems() should be replaced with .items() if you are using Python 3).
On python 3.3 and newer, you can also use the new yield from syntax:
def flatten(d):
    for key, value in d.items():
        yield key
        yield from flatten(value)

This will recursively yield all the keys. To turn that into a list use:
list(flatten(elements))

Since Python dictionaries are unordered, the ordering of the keys returned is not going to be sorted. You'll have to explicitly sort the result if you want your keys to have a specific ordering.",A,24
17231306,2013-06-21 08:58:38.423000+00:00,"I'm not sure why you are going to /~bangwsgi/. Your WSGI app is being served at /, as defined by the first parameter to WSGIScriptAlias.",A,0
53277225,2018-11-13 08:59:39.137000+00:00,"As I mentioned, ModelChoiceField is not the right thing to do here. That's for allowing the user to choose from related items from a single model that will be saved into a ForeignKey. You don't have a ForeignKey, and what's more you're setting the choices attribute in your init rather than queryset. You should make it a plain ChoiceField with an empty choices parameter:
jurisdiction = forms.ChoiceField(choices=())

(For the sake of completeness: if you did need to use ModelChoiceField you can put anything you like into the queryset parameter when you're overwriting it in __init__, because it will never be evaluated. But managers have a none method which returns an empty queryset, so you could do queryset=State.objects.none().)",A,0
20145873,2013-11-22 13:16:56.573000+00:00,"List the three criteria in your key:
sorted(inputlist, key=lambda e: (len(e[0]), e[0], e[1]))

Now you are sorting each element first by the length, then by comparing the first element directly (which in only used when the first element is of equal length), then by the value of the last integer.
Python sorts tuples and lists like these lexicographically; compare the first element, and only if that doesn't differ, compare the second element, etc.
The second element here is e[0] which will only be used if both compared entries have nested lists of equal length. These are again compared lexicographically, so pairing up elements until a pair differs.",A,17
6290193,2011-06-09 08:44:16.700000+00:00,What? There's a whole page on authentication which hardly mentions the admin site.,A,0
26539561,2014-10-24 00:11:30.597000+00:00,"The only problem with your result is that you're getting two match groups instead of one. The only reason that's happening is that you're creating two capture groups instead of one. You're putting separate parentheses around the first half and the second half, and that's what parentheses mean. Just don't do that, and you won't have that problem.
So, with this, you're half-way there:
(\d{1,3}(?:\s*\d{3})*,\d+)\s


Debuggex Demo
The only problem is that the ,\d+ part is now mandatory instead of optional. You obviously need somewhere to put the ?, as you were doing. But without a group, how do you do that? Simple: you can use a group, just make it a non-capturing group ((?:…) instead of (…)). And put it inside the main capturing group, not separate from it. Exactly as you're already doing for the repeated \s*\d{3} part.
(\d{1,3}(?:\s*\d{3})*(?:,\d+)?)\s


Debuggex Demo",A,0
369443,2008-12-15 19:28:45.320000+00:00,"None of these listed in TheFreeCountry are acceptable?  None of them are in Python, but I should think that starting from XBLite  might be more helpful than starting from Yacc/Bison/PLY.
Also, Vb2py might be a better starting position than PLY.
If you must go the PLY route, however, consider the MOLE Basic grammar as a starting point rather than trying to roll your own from scratch.",A,3
17762265,2013-07-20 12:29:23.397000+00:00,"Why do you expect it do anything? GAE is, specifically, a non-relational (NoSQL) datastore. There is quite simply no SQL to produce.
You should be aware that GAE, even with django-non-rel, is quite different from standard Django, and following the Django tutorial is only likely to confuse you.",A,1
46135707,2017-09-09 22:34:13.063000+00:00,"HTTP is a stateless protocol. Your only option to track a login session is by somehow tying a request to a specific user. Unless browsers start sending some other unique identifying piece of information, cookies are your best option. Another alternative is to use Basic authentication, where the user is asked to enter a username and password in a standard dialog box (this can't be styled) and the browser will then send this data (unencrypted) along with every request.
All other techniques are far more involved, see User recognition without cookies or local storage for example. Also see How can I uniquely identify an user when cookies are not an option? for more options.
Flask-Login does support Basic Authentication; the documentation covers two different techniques for supporting this option. If you do choose to use this, make sure your site is only accessible over HTTPS encryption to prevent the username / password combination from being stolen, letting someone else log in.",A,2
42329979,2017-02-19 16:48:43.563000+00:00,"Because you've defined the relationship to User as one-to-one. By definition, that means a user can only have one profile.
If you want to allow multiple profiles, use a ForeignKey instead.",A,0
13454717,2012-11-19 13:23:48.363000+00:00,"This appears to be a Mountain Lion only problem, and only if your python was installed fresh on it (or so the reports seem to indicate).
The work-around is to run python with the -S switch:
python2.7 -S bootstrap.py

or to create a virtualenv, then run the bootstrap.py script with the virtualenv python.",A,1
51583747,2018-07-29 19:20:08.460000+00:00,"Your logic here is mostly backward, but not quite entirely backward. 
First, for an empty list, you're returning True. That's wrong—an empty list doesn't contain anything, so it doesn't contain item.
Next, for a size-1 list whose only element is item, you return True. That's correct. Probably not a necessary test, but correct.
Next, if the first element is item, you check the rest of the list. That's wrong—a list that contains item plus some other stuff contains item.
Finally, for anything else (a multi-element list whose first element is not item), you return False. That's wrong—a list that contains a bunch of stuff, one of those things not being item, may still contain item. Here's where you need to check the rest of the list.
If you turn this correct logic into code the same way you turned your incorrect logic into code, your code will work.",A,1
31320110,2015-07-09 14:13:35.153000+00:00,"Use a collections.OrderedDict() instance to do the grouping:
from collections import OrderedDict

def group_ordered(list_in):
    result = OrderedDict()
    for value in list_in:
        result.setdefault(value, []).append(value)
    return [v for group in result.values() for v in group]

Because this specialised dictionary object tracks insertion order of the key, the output is ordered by first occurrence of a group value.
Demo:
>>> group_ordered([1,3,2,3,6,3,1])
[1, 1, 3, 3, 3, 2, 6]
>>> group_ordered([1,2,3,4,5,6,1])
[1, 1, 2, 3, 4, 5, 6]
",A,6
23445627,2014-05-03 14:20:01.273000+00:00,"Python uses and, not AND; you don't need nearly as many parentheses either (as and has very low operator precedence):
if criteria is not None and 2 <= criteria <= 5:

When you spell it AND, it is seen as a variable name instead, making the expression invalid.
You can simplify the test to:
if criteria and 2 <= criteria <= 5:

as None is false anyway and 0 would not fit in the range.
In Python 2, None can be compared to numbers directly (is always smaller) and if 2 <= criteria <= 5: would do, but it doesn't hurt to be explicit and compatible with Python 3.
Demo:
>>> for criteria in (None, 0, 3, 5, 10):
...     if criteria and 2 <= criteria <= 5:
...         print(criteria, 'is valid')
...     else:
...         print(criteria, 'does not match')
... 
None does not match
0 does not match
3 is valid
5 is valid
10 does not match
",A,4
19446681,2013-10-18 09:52:20.557000+00:00,"This is almost certainly a bad idea… but it is doable, in a few different ways.

Most simply, you can just have each function save its name in the object, e.g.:
def stash_name(func):
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        self._stashed_name = func.__name__
        return func(self, *args, **kwargs)
    return wrapper

class Test(object):
    @stash_name
    def foo(self, x):
        print x
    @stash_name
    def bar(self):
        print

Now, after calling d.connect(), d._stashed_name will be ""connect"".

At the opposite extreme, if you want to get really hacky, you can do this without any cooperation from the preceding method. Just use sys._getframe(1) to find your calling context, then you can examine the frame's f_code to see how you were called.
You can use the dis module to see real bytecode. But basically, it will looks like this pseudo-bytecode:
LOAD_NAME d
LOAD_ATTR connect
<possibly other ops to prepare arguments>
CALL_FUNCTION 1 (or any other CALL_FUNCTION_* variant)
LOAD_ATTR setAttributes
<various other ops to prepare arguments>
CALL_FUNCTION 0

In this case, you can either get the attribute name from the LOAD_ATTR, or get the value that was pushed and look at its im_func.__name__, depending which one you want.
Of course there will be other cases that don't look like this. For example, let's say I called it as getattr(d, ''.join('con', 'next'))() instead of d.connect(). Or I looked up the unbound method and built a bound method on the fly. Or… What would you want to do in each such case? If you have the answers to all such cases, then you can work out the rule that generates those answers, then figure out how to get that from the bytecode.",A,0
30666086,2015-06-05 12:02:26.783000+00:00,"The recommended way to return a value from one python ""script"" to another is to import the script as a Python module and call the functions directly:
import another_module

value = another_module.get_value(34)

where another_module.py is:
#!/usr/bin/env python
def get_value(*args):
    return ""Hello World "" + "":"".join(map(str, args))

def main(argv):
    print(get_value(*argv[1:]))

if __name__ == ""__main__"":
    import sys
    main(sys.argv)

You could both import another_module and run it as a script from the command-line. If you don't need to run it as a command-line script then you could remove main() function and if __name__ == ""__main__"" block.
See also, Call python script with input with in a python script using subprocess.",A,4
15798124,2013-04-03 21:06:42.083000+00:00,"You're actually very close; the only problem is that you're returning a tuple instead of a list. (Whatever a and b are, a, b is a tuple of those two things, just like [a, b] is a list of those two things.)
One way to solve this is to call list to make a list out of the tuple:
def make_ends(x):
    return list((x[0], x[-1]))

But the easy way to do it is just to create a list in the first place:
def make_ends(x):
    return [x[0], x[-1]]


You then ask another question:

Here was my earlier build, but it didn't do anything except return the original string:

def go_right(str):
    if str >= 2:
            a = str[-2:0] + str
            return a

Let's go through this step by step. 
First, str >= 2 is comparing a string to a number. In Python 2.x, either all strings are bigger than all numbers, or all strings are smaller than all numbers. (That's left up to the implementation.) So, this isn't a very useful check. 
Maybe you wanted to check if len(str) >= 2 instead? But even then, I'm not sure what that would get you. If the length were 0 or 1, what would you want to do? As it is, you'd return None, which probably isn't right.
Meanwhile, str[-2:0] asks for all elements that come after 2-before-the-end, but before the start. There are no elements before the start, so this is empty. Then you add the original value to this empty collection, so you get the original value.

Also, as a side note, calling a parameter str is a bad idea. Besides the fact that it hides the built-in str (which is a function that turns anything into its string representation), it also strongly implies that what you're dealing with is a string, not a list.",A,4
38688648,2016-07-31 21:44:32.083000+00:00,"You opened the file in binary mode, so you'd have to encode your bytes.
If you drop the 'b' part from the file mode (so open with 'w+' rather than 'w+b'), you get an implementation of the TextIOBase interface instead, which will encode strings for you given an encoding (the default is to use the result of locale.getdefaultencoding(), you probably want to supply an explicit encoding argument to the open() call instead).
The alternative would be for you to encode your strings manually, using the str.encode() method on each chunk. Leaving encoding to the TextIOBase implementation is going to be a little faster however, because the I/O layer can encode without having to look up a method object on each str chunk, nor do the resulting bytes have to be boxed in a Python bytes object again.
Also, for encodings that require a byte order mark, it is best to leave writing that marker to the file implementation.
However, if you could produce bytes objects in the first place, you'd avoid having to encode at all.",A,3
37751620,2016-06-10 15:09:53.330000+00:00,"You don't need to convert it to datetime, you could get the current Unix time using time.time(). It allows to find the time difference in days given a Unix time from the past:
import time

DAY = 86400 # POSIX day - the exact value
days_ago = (time.time() - sub.created_utc) // DAY
",A,0
14678120,2013-02-03 22:32:33.040000+00:00,"Use the getattr() function to access dynamic attributes:
method = 'GET'
getattr(handler.request, method).add()

which would do exactly the same thing as handler.request.GET.add().",A,5
22500914,2014-03-19 09:11:33.393000+00:00,"The command looks ok. The filename is passed as a bytestring that contains text encoded using utf-8. If your filesystem uses utf-8 then it is correct filename:
>>> print ""/mnt/path/Long 73\xc2\xb0 58' W.avi"".decode('utf-8')
/mnt/path/Long 73° 58' W.avi

It might be a bug in pymediainfo. Try passing the environment argument explicitely as a workaround e.g., environment=os.environ.",A,0
1318506,2009-08-23 12:58:02.310000+00:00,"Funny. I blogged about this: The E. W. Dijkstra Archive (Update). I think this may be pretty important, because it parallels the A Discipline of Programming book. 
See also EWD316, A Short Introduction to the Art of Programming.",A,8
32192894,2015-08-24 22:42:19.463000+00:00,"To convert a datetime object that represents time in UTC to POSIX timestamp:
from datetime import timezone

seconds_since_epoch = utc_time.replace(tzinfo=timezone.utc).timestamp()

To convert a datetime object that represents time in the local timezone to POSIX timestamp:
import tzlocal # $ pip install tzlocal

local_timezone = tzlocal.get_localzone()
seconds_since_epoch = local_timezone.localize(local_time, is_dst=None).timestamp()

See How do I convert local time to UTC in Python? If the tz database is available on a given platform; a stdlib-only solution may work.
Follow the links if you need solutions for <3.3 Python versions.",A,1
39869974,2016-10-05 09:16:23.537000+00:00,"Your code wouldn't actually work, because assigning to the local variable petition means Python will treat all references to that name as local, so won't find the global reference which is to the module, causing a NameError.
You can import the module with a different name:
 import petition as petition_module

Or you can import the things you need directly:
 from petition import Petition, check_if_someone_can_sponsor_a_petition

For more clean up you might consider making check_if_someone_can_sponsor_a_petition a classmethod of Petition.",A,0
33616997,2015-11-09 19:46:15.340000+00:00,"You are using recursion where you should use a while loop.
When a user picks something other than yes or no (or variants thereof), you end up in the else branch and call choice(). That function call eventually returns, and at that moment add_charge is not set.
You can 'fix' that issue by assigning the return value:
else:
    ans = f_ans() # if ans is other then y or n, ask user again and assign value
    add_charge = choice(ans)

but you should really use a loop here; that way you don't run into namespace issues and won't ever run out of stack either. All it takes is someone holding the ENTER key for you to run into the maximum recursion limit.
You can use str.lower() and a set membership test to simplify your tests as well:
def choice():
    while True:
        ans = f_ans()
        if ans.lower() in {'y', 'yes'}:
            return 100
        elif ans.lower() in {'n', 'no'}:
            return 0

The return statements will exit the function, but if neither branch is met (the user gave something other than a valid answer), the while loop goes back to the top and f_ans() is called again to ask for a valid answer.",A,2
6253533,2011-06-06 14:29:03.217000+00:00,"You can't do it that way, because self doesn't exist at that point - and even if you could, that would be executed at define time, so the rank would be static for all instantiations of the form.
Instead, do it in __init__:
provider = UserModelChoiceField(User.objects.none())

 def __init__(self, user, *args, **kwargs):
    super(NewSwapForm, self).__init__(*args, **kwargs)
    rank = User.objects.get(id=user.id).firefighter_rank_set.get().rank # ??
    self.fields['provider'].queryset = User.objects.order_by('last_name').filter(
           firefighter__hirestatus='active', firefighter_rank__rank=rank)

I've put a question mark next to the rank line, because rank_set.get() isn't valid... not sure what you meant there.",A,2
46455860,2017-09-27 19:32:18.157000+00:00,"You are asking several for loops to all iterate over the same, single iterator. You did not create copies, they share one object. 
The outer loop advances the charsnum object by one step, then the next for loop advances it once to start its iteration, and so on. The innermost for loop takes the remaining items to iterate over, and by the time you go back up to the for z loop there is nothing left to iterate over.
You can do the same thing, advancing the iterator like for does, with the next() function:
>>> chars = 'abbbababa'
>>> charsnum = enumerate(chars)
>>> i = next(charsnum)
>>> j = next(charsnum)
>>> z = next(charsnum)
>>> k = next(charsnum)
>>> i, j, z, k
((0, 'a'), (1, 'b'), (2, 'b'), (3, 'b'))
>>> next(charsnum)  # next k value in the innermost loop
(4, 'a')
>>> # etc. etc.

You could create separate enumerate() objects for each for loop:
for i in enumerate(chars):
    for j in enumerate(chars):
        for z in enumerate(chars):
            for k in enumerate(chars):
                print(i,j,z,k)

but that's just more work than you need. For nested loops repeating the same iterable, use itertools.product() and tell it to repeat; it'll keep copies as needed:
from itertools import product

for i, j, z, k in product(enumerate(chars), repeat=4):
    print(i, j, z, k)

This won't eliminate repeated characters, however; this produces the cartesian product of 4 (index, char) tuples out of all possible such tuples from your char string.
You probably want to create permutations form the unique letters, and string out the patterns those make into something the same length:
from itertools import permutations

for pattern in permutations(set(chars)):
    repeat = (len(chars) + 1) // len(pattern)
    result = (''.join(pattern) * repeat)[:len(chars)]
    print(result)

This produces
babababab
ababababa
",A,5
678632,2009-03-24 18:32:23.483000+00:00,"In most cases, you can find a natural hierarchy to your objects.  Sometimes there is some kind of ""master"" and all other objects have foreign key (FK) references to this master and to each other.
In this case, you can use an XML-like structure with each master object ""containing"" a lot of subsidiary objects.  In this case, you insert the master first, and all the children have FK references to an existing object.
In some cases, however, there are relationships that can't be simple FK's to an existing object.  In this case you have circular dependencies and you must (1) break this dependency temporarily and (2) recreate the dependency after the objects are loaded.
You do this by (a) defining your model to have an optional FK, (b) and having a temporary ""natural key"" reference.  You'll load data without the proper FK (it's optional).  
Then, after your data is loaded, you go back through a second pass and insert all of the missing FK references.  Once this is done you can then modify your model to make the FK mandatory.
Program 1 - export from old database to simple flat-file.  CSV format or JSON format or something simple.
for m in OldModel.objects.all():
    aDict = { 'col1':m.col1, 'old_at_fk':m.fktoanothertable.id, 'old_id':id }
    csvwriter.writerow( aDict )

Program 2 - read simple flat-file; build new database model objects.
# Pass 1 - raw load

for row in csv.reader:
    new= NewModel.create( **row )

# Pass 2 - resolve FK's

for nm in NewModel.objects.all():
    ref1= OtherModel.objects.get( old_id=nm.old_at_fk )
    nm.properfk = ref1
    nm.save()
",A,1
940131,2009-06-02 15:13:08.340000+00:00,"You've almost got it.
foo = Foo(**request.GET)

should do the trick.",A,1
18635110,2013-09-05 11:27:51.640000+00:00,"Function default values are evaluated when the function is defined, not when it is called. Your default values are set in stone the moment Python loads the module.
Use sentinel values instead:
def __init__( self, von=None, bis=None, somemore=something ):
    if von is None:
        von = _today()
    if bis is None:
        bis = _today()

Now the values are set every time the function is called instead.
Note that your _today() function can be improved as well; create a date object, and use the date.strftime() function to format the date:
def _today():
    """"""Todays date formatted as a string""""""

    todays_date = datetime.date.today()
    return todays_date.strftime('%d.%m.%Y')

This will return dates as with zeropadded digits; you could also use:
    return '{0.day}.{0.month}.{0.year}'.format(todays_date)

to return non-zero-padded values. Demo:
>>> import datetime
>>> adate = datetime.date(2013, 2, 14)
>>> adate.strftime('%d.%m.%Y')
'14.02.2013'
>>> '{0.day}.{0.month}.{0.year}'.format(adate)
'14.2.2013'
",A,3
50650878,2018-06-01 20:53:28.813000+00:00,"The timestamp method was added in Python 3.3. So if you're using Python 2.0, or even 2.7, you don't have it. 
There are backports of current datetime to older Python versions on PyPI, but none of them seems to be official, or up-to-date; you might want to try searching for yourself.
There are also a number of third-party replacement libraries that add functionality that isn't in (2.x) datetime, including the ability to convert to Unix timestamps.

You can just copy the function out of the source code from 3.3 or later:
def timestamp(self):
    ""Return POSIX timestamp as float""
    if self._tzinfo is None:
        s = self._mktime()
        return s + self.microsecond / 1e6
    else:
        return (self - _EPOCH).total_seconds()

… but you will have to modify things a bit to get them to work, because:

_EPOCH is deleted at the end of the module.
The 3.x _EPOCH is a tz-aware object built with a proper UTC timezone, which you don't have in 2.x unless you're using a third-party library like pytz.
The _mktime method and _tzinfo attribute don't exist on 2.x datetime, so you need to simulate what they do as well.

If you don't need the same function to work equally well for naive, GMT, and tz-aware datetimes, it won't be that hard, but it's still not quite trivial—and if you do need the full functionality, it's going to be more painful.

Or it may be easier to port the equivalent code given in the docs.
For aware datetime instances:
(dt - datetime(1970, 1, 1, tzinfo=timezone.utc)).total_seconds()

Of course you still don't have that timezone.utc, but for this purpose, you don't need a full timezone object; you can use an instance of the example UTC class in the 2.x tzinfo docs.
… for naive:
timestamp = dt.replace(tzinfo=timezone.utc).timestamp()

… or:
timestamp = (dt - datetime(1970, 1, 1)) / timedelta(seconds=1)

Since you don't have aware datetimes, that last one is all you need.

If your Python is old enough, timedelta may not have a __div__ method. In that case (if you haven't found a backport), you have to do division manually as well, by calling total_seconds on each one, making sure at least one of them is a float, and dividing the numbers:
timestamp = ((dt - datetime(1970, 1, 1)).total_seconds() / 
    float(timedelta(seconds=1).total_seconds()))

But in this particular case, it should be pretty obvious that the divisor is just going to be 1.0, and dividing by 1.0 is the same as doing nothing, so:
timestamp = (dt - datetime(1970, 1, 1)).total_seconds()
",A,4
6611550,2011-07-07 13:41:59.550000+00:00,"You shouldn't need to do any of this. The manage.py script does it all for you, which is why you should always use it (instead of django-admin.py) once you're inside a project.
Just do this:
./manage.py shell

That's it.",A,3
28558238,2015-02-17 09:09:53.990000+00:00,"The trick is that Django is expecting a tuple of tuples in ADMINS, but you have actually only given a single tuple: the outer parentheses are effectively ignored, because in Python it is not the parens that define a tuple, but the comma.
The code in the mail_admins function tries to extract the emails from the setting by doing [a[1] for a in settings.ADMINS], so in your case it will take the second letter of the name and the second letter of the email address - presumably those both happen to be ""m"".
Do this instead:
ADMINS = (('{name}', '{email@mycompany.com}'),)

Note the extra comma before the final close paren.",A,1
48622192,2018-02-05 12:03:33.880000+00:00,"I think the problem is that you're not calling super in your get_form_kwargs method, so the POSTed data is not actually being passed to the form. You probably need:
def get_form_kwargs(self):
    kwargs = super().get_form_kwargs()
    kwargs['site_id'] = self.site_id
    return kwargs
",A,1
53212275,2018-11-08 16:40:22.533000+00:00,"You need to ensure that only a single task is using the adb connection to execute a command at any given time. This means you need to either use synchronisation primitives to coordinate access, or use a queue to feed a single worker task commands to execute.
Next, because an adb connection is entirely synchronous and, as with all I/O, relatively slow, I'd use a thread pool executor to run operations on a adb connection off the asyncio loop, so that asyncio is free to run some other tasks that are not currently blocked on I/O. Otherwise, there is no point to putting .Shell() commands in a async def coroutine, you are not actually cooperating and making room for other tasks to be run.
Last but not least, if even with serialised access to the connection object you find that it can't take too many commands per time period, you would want to use some kind of rate limiting technique. I've created an asyncio leaky bucket algorithm implementation before that can take care of this, if so required.
Both a queue or a lock would ensure that commands are executed in first-come-first-serve order, but a queue would require some kind of deferred response mechanism to return command results. A queue would let you queue up related commands (you can add multiple entries using queue.put_nowait() without yielding or you can allow grouped commands), without having to wait for a lock first.
Because you want to retry connections, I'd encapsulate the connection object in a asynchronous context manager, that can then also handle locking and executing commands with an executor:
import asyncio
import collections
from concurrent.futures import ThreadPoolExecutor
from functools import partial

try:  # Python 3.7
    base = contextlib.AbstractAsyncContextManager
except AttributeError:
    base = object  # type: ignore

_retry_exceptions = (...,)  # define exceptions on which to retry commands?

class asyncnullcontext(base):
    def __init__(self, enter_result=None):
        self.enter_result = enter_result
    async def __aenter__(self):
        return self.enter_result
    async def __aexit__(self, *excinfo):
        pass

class AsyncADBConnection(base):
    def __init__(
        self,
        host,
        adbkey=None,
        rate_limit=None,
        max_retry=None,
        loop=None
    ):
        self._lock = asyncio.Lock(loop=loop)
        self._max_retry = max_retry
        self._loop = None
        self._connection = None
        self._executor = ThreadPoolExecutor()

        self._connect_kwargs = {
            ""serial"": host,
            ""rsa_keys"": [Signer(adbkey)] if adbkey else []
        }

        if rate_limit is not None:
            # max commands per second
            self._limiter = AsyncLeakyBucket(rate_limit, 1, loop=loop)
        else:
            self._limiter = asyncnullcontext()

    async def __aenter__(self):
        await self._lock.acquire()
        await self._ensure_connection()
        return self

    async def __aexit__(self):
        self._lock.release()

    async def _ensure_connection(self):
        if self._connection is not None:
            return
        loop = self._loop or asyncio.get_running_loop()
        connector = partial(
            adb_commands.AdbCommands().ConnectDevice,
            **self._connect_kwargs
        )
        fut = loop.run_in_executor(pool, connector)
        self._connection = await fut

    async def shell(self, command):
        loop = self._loop or asyncio.get_running_loop()
        max_attempts = self._max_retry or 1
        attempts = 0
        while True:
            with self._limiter:
                try:
                    fut = loop.run_in_executor(
                        self._executor,
                        self._connection.Shell,
                        command
                    )
                    return await fut
                except _retry_exceptions as e:
                    attempts += 1
                    if attempts >= max_attempts:
                        raise
                    # re-connect on retry
                    self._connection = None
                    await self._ensure_connection()

If you then use a queue, use Future() instances to communicate results.
Pushing a job into the queue then becomes:
fut = asyncio.Future()
await queue.put((command, fut))
result = await fut

You could wrap that into a utility function or object. The await fut line only returns once the future has received a result. For commands where you don't care about a result, you only need to await if you want to make sure that the command completed.
The consumer in the worker task that manages the connection would use:
while True:
    command, fut = await self.queue.get():
    async with self.connection as conn:
        response = await conn.shell(command)
        fut.set_result(response)
    self.queue.task_done()  # optional, only needed when joining the queue

where self.connection is an AsyncADBConnection instance.",A,2
14108769,2013-01-01 07:38:52.070000+00:00,"This is a general feature of public COM interfaces. 
If you want backward compatibility, you never want to change a published interface, because that would mean all the code people wrote for, say, IE 6 stops working with IE 7, and all of their customers get mad at them, and they get mad at you.
So, if IE 5 adds new features that needed to be exposed, instead of changing IHTMLDocument, you create a new interface, and make IE5 support both (by inheritance, QueryInterface, or some more explicit mechanism). And when IE 7.0.2 or IE 8 or Win XP or whatever adds even more new features, you create another one. And so on.
While MS could have come up with descriptive suffixes instead of just sequential numbers, that would probably be more confusing than helpful. So, IHTMLDocument2, IHTMLDocument3, etc. are the names. They don't mean anything, except the order they were added.",A,17
13662443,2012-12-01 18:35:26.937000+00:00,"Just create a python dict with a contained list instead, then dump that to JSON in one go:
def json_serialize(ip, mac, nic):
    net_info = []
    for ipaddr, macaddr, nicname in zip(ip, mac, nic):
        net_info.append({
            'interface_name': nicaddr,
            'mac': macaddr,
            'ip': ipaddr
        })
    return json.dumps({'net_info': net_info})

Your desired output format seems to be missing the outer { and } brackets to mark it a proper JSON object. If you really have to produce that output (so missing those brackets), just remove them again:
print json_serialize(ip, mac, nic)[1:-1]
",A,4
29479399,2015-04-06 20:47:01.257000+00:00,"This question doesn't really make sense. Presumably LINQ, like any .NET library, can be used in any language that runs in the CLR: C#, VB, IronPython, IronRuby, etc.
The most common cross-language runtime that works on Linux is the Java VM, and you can use Java libraries - including ORMs like JDO - in any language that uses that VM: Java, Scala, Clojure, Jython, JRuby, etc.",A,0
20950487,2014-01-06 12:53:18.167000+00:00,"It appears wheel support is disabled.
Make sure that you have setuptools version 0.8 or newer installed, and that the use-wheel option is not set to false in $HOME/.pip/pip.conf.
Upgrading setuptools is easy enough if pip is already working:
pip install --upgrade setuptools

but note that older virtualenv versions can depend on older setuptools versions; you'll need to make sure that virtualenv is also up to date.",A,5
879765,2009-05-18 20:49:26.370000+00:00,"Do not use global.  It's an accident waiting to happen.
You can give your loggers any "".""-separated names that are meaningful to you.
You can control them as a hierarchy.  If you have loggers named a.b.c and 
a.b.d, you can check the logging level for a.b and alter both loggers.
You can have any number of loggers -- they're inexpensive.
The most common design pattern is one logger per module.  See Naming Python loggers
Do this.
import logging

logger= logging.getLogger( ""module_name"" )
logger_a = logger.getLogger( ""module_name.function_a"" )
logger_b = logger.getLogger( ""module_name.function_b"" )

def function_a( ... ):
    logger_a.debug( ""a message"" )

def functio_b( ... ):
    logger_b.debug( ""another message"" )

if __name__ == ""__main__"":
    logging.basicConfig( stream=sys.stderr, level=logging.DEBUG )
    logger_a.setLevel( logging.DEBUG )
    logger_b.setLevel( logging.WARN )

    ... etc ...
",A,16
11236372,2012-06-27 23:57:46.290000+00:00,"@hop's answer is wrong on the day of transition from Daylight Saving Time (DST) e.g., Apr 1, 2012. To fix it tz.localize() could be used:
tz = pytz.timezone(""Australia/Melbourne"")
today = datetime.now(tz).date()
midnight = tz.localize(datetime.combine(today, time(0, 0)), is_dst=None)
utc_dt = midnight.astimezone(pytz.utc)        

The same with comments:
#!/usr/bin/env python
from datetime import datetime, time
import pytz # pip instal pytz

tz = pytz.timezone(""Australia/Melbourne"") # choose timezone

# 1. get correct date for the midnight using given timezone.
today = datetime.now(tz).date()

# 2. get midnight in the correct timezone (taking into account DST)
#NOTE: tzinfo=None and tz.localize()
# assert that there is no dst transition at midnight (`is_dst=None`)
midnight = tz.localize(datetime.combine(today, time(0, 0)), is_dst=None)

# 3. convert to UTC (no need to call `utc.normalize()` due to UTC has no 
#    DST transitions)
fmt = '%Y-%m-%d %H:%M:%S %Z%z'
print midnight.astimezone(pytz.utc).strftime(fmt)
",A,24
36237250,2016-03-26 15:41:28.657000+00:00,"When asking yourself a question about speed differences between different operators, use the timeit module to measure. They are equally fast:
>>> import timeit
>>> timeit.timeit('a > 0', 'a = 1', number=10**7)
0.2486400604248047
>>> timeit.timeit('a > 0', 'a = 0', number=10**7)
0.2411360740661621
>>> timeit.timeit('a != 0', 'a = 1', number=10**7)
0.24765801429748535
>>> timeit.timeit('a != 0', 'a = 0', number=10**7)
0.24990510940551758

That's the comparisons repeated 10 million times, and if you re-run the above tests you'll find the timings can vary somewhat and none are clear winners.
You should be focusing on readability here, not speed. A simple integer comparison is going to be an infinitesimal part of your overall execution speed, even in a loop.",A,8
26596816,2014-10-27 21:04:31.417000+00:00,"I think you may be looking for the longest sequence of non-N characters in the input.
Otherwise, you have no rule to distinguish the last N in the prefix from the first N in the suffix. There is nothing at all different about the N you want to start after (before the ACAGAC…) and the next N (before the CATCCC), or, for that matter, the previous one (before the GN) except that it picks out the longest sequence. In fact, other than the 10 N's at the very start and the 9 at the very end, there doesn't seem to be anything special about any of the N's.
The easiest way to do that is to just grab all the sequences and keep the longest:
max(s.split('N'), key=len)

If you have some additional rule on top of this—e.g., the longest sequence whose length is divisible by three (which in this case is the same thing)—you can do the same basic thing:
max((seq for seq in s.split('N') if len(seq) % 3 == 0), key=len)
",A,5
27509471,2014-12-16 16:35:22.930000+00:00,"Use the parent class; you don't care which one of the many subclasses this is, just if it is an image file.
The second argument of assertIsInstace() must be the class itself, not a class name:
from PIL import ImageFile

self.assertIsInstance(SimpleExample.img, ImageFile.ImageFile)

or you could go one step further up and assert that it is a PIL.Image.Image instance, and not care if it came from a specific file or was produced otherwise:
from PIL import Image

self.assertIsInstance(SimpleExample.img, Image.Image)

If you do ever need to test against multiple classes, isinstance() (and by extension, assertIsInstance()) also accepts a tuple of classes as the second argument:
>>> foo = 'bar'
>>> isinstance(foo, (str, int, float))
True
",A,6
31170378,2015-07-01 19:47:48.807000+00:00,"logging.FATAL has been equal to logging.CRITICAL from the very first commit of the logging package to the Python repository. 
It is there for compatibility reasons; the Java log4j package (which was one of the key influencers for this module) uses FATAL as the highest level, but Python felt the name CRITICAL better reflected the situation.
See PEP 282 A Logging System, the Python Enhancement Proposal that added the logging package to Python:

The term CRITICAL is used in preference to FATAL, which is used by
      log4j.  The levels are conceptually the same - that of a serious,
      or very serious, error.  However, FATAL implies death, which in
      Python implies a raised and uncaught exception, traceback, and
      exit.  Since the logging module does not enforce such an outcome
      from a FATAL-level log entry, it makes sense to use CRITICAL in
      preference to FATAL.
",A,16
20497038,2013-12-10 14:26:16.330000+00:00,"You If you want to match the optional decimal value with an optional group:
re.compile(r""\b([1-2]?\d(?:,\d)?)\s[y+]\s([1-2]?\d(?:,\d)?)\shouses\b"")

where (?:,[0-9])? will match a comma followed by a digit if present. Note that I limit the digit matching to values between 0 and 29; matching an optional 1 or 2 first, followed by 0-9.
Demo:
>>> import re
>>> demo = '8 y 13 houses, 13 y 8 houses, 13 y 13 houses, 8 y 8 houses, 120 y 8 houses, 8 y 120 houses, 13,5 y 8 houses, 13,5 y 120 houses'
>>> pattern = re.compile(r""\b([1-2]?\d(?:,\d)?)\s[y+]\s([1-2]?\d(?:,\d)?)\shouses\b"")
>>> pattern.findall(demo)
[('8', '13'), ('13', '8'), ('13', '13'), ('8', '8'), ('13,5', '8')]
",A,4
15011753,2013-02-21 20:38:10.793000+00:00,"Not easily.
You could recurse through the gc.get_objects() list, testing each and every object if it has the same id() but that's not very practical.
The id() function is not intended to be dereferenceable; the fact that it is based on the memory address is a CPython implementation detail, that other Python implementations do not follow.",A,3
21929771,2014-02-21 09:03:58.933000+00:00,"l = []
entries = {}

for id, fid, title in t:
    entries[id] = entry = {'id': id, 'fid': fid, 'title': title}
    if fid == -1:
        l.append(entry)
    else:
        parent = entries[fid]
        parent.setdefault('son', []).append(entry)

Here entries keeps track of all entries that have been created so far, so that you can add 'sons' to them directly without having to search the tree.
This assumes that your list t is properly sorted by id and that sons only ever are children of lower ids.
Demo:
>>> from pprint import pprint
>>> t = (
...   (1, -1, 'python'),
...   (2, -1, 'ruby'),
...   (3, -1, 'php'),
...   (4, -1, 'lisp'),
...   (5,  1, 'flask'),
...   (6,  1, 'django'),
...   (7,  1, 'webpy'),
...   (8,  2, 'rails'),
...   (9,  3, 'zend'),
...   (10, 6, 'dblog')
... )
>>> l = []
>>> entries = {}
>>> for id, fid, title in t:
...     entries[id] = entry = {'id': id, 'fid': fid, 'title': title}
...     if fid == -1:
...         l.append(entry)
...     else:
...         parent = entries[fid]
...         parent.setdefault('son', []).append(entry)
... 
>>> pprint(l)
[{'fid': -1,
  'id': 1,
  'son': [{'fid': 1, 'id': 5, 'title': 'flask'},
          {'fid': 1,
           'id': 6,
           'son': [{'fid': 6, 'id': 10, 'title': 'dblog'}],
           'title': 'django'},
          {'fid': 1, 'id': 7, 'title': 'webpy'}],
  'title': 'python'},
 {'fid': -1,
  'id': 2,
  'son': [{'fid': 2, 'id': 8, 'title': 'rails'}],
  'title': 'ruby'},
 {'fid': -1,
  'id': 3,
  'son': [{'fid': 3, 'id': 9, 'title': 'zend'}],
  'title': 'php'},
 {'fid': -1, 'id': 4, 'title': 'lisp'}]

If the 'fid-comes-after-id' assumption doesn't hold up, you need to add a queue of child ids to be processed still:
l = []
entries = {}
queue = {}

for id, fid, title in t:
    entries[id] = entry = {'id': id, 'fid': fid, 'title': title}
    if id in queue:
        entry['sons'] = queue[id]
        del queue[id]
    if fid == -1:
        l.append(entry)
    elif fid in entries:
        parent = entries[fid]
        parent.setdefault('son', []).append(entry)
    else:
        queue.setdefault(fid, []).append(entry)

if queue:
    raise ValueError('No entries found for fid(s) {}'.format(queue.keys()))

Now the order of entries in t can be entirely random:
>>> import random
>>> t = list(t)
>>> random.shuffle(t)
>>> l = []
>>> entries = {}
>>> queue = {}
>>> for id, fid, title in t:
...     entries[id] = entry = {'id': id, 'fid': fid, 'title': title}
...     if id in queue:
...         entry['sons'] = queue[id]
...         del queue[id]
...     if fid == -1:
...         l.append(entry)
...     elif fid in entries:
...         parent = entries[fid]
...         parent.setdefault('son', []).append(entry)
...     else:
...         queue.setdefault(fid, []).append(entry)
... 
>>> if queue:
...     raise ValueError('No entries found for fid(s) {}'.format(queue.keys()))
... 
>>> pprint(l)
[{'fid': -1,
  'id': 1,
  'son': [{'fid': 1, 'id': 7, 'title': 'webpy'},
          {'fid': 1, 'id': 5, 'title': 'flask'},
          {'fid': 1,
           'id': 6,
           'sons': [{'fid': 6, 'id': 10, 'title': 'dblog'}],
           'title': 'django'}],
  'title': 'python'},
 {'fid': -1,
  'id': 2,
  'son': [{'fid': 2, 'id': 8, 'title': 'rails'}],
  'title': 'ruby'},
 {'fid': -1,
  'id': 3,
  'son': [{'fid': 3, 'id': 9, 'title': 'zend'}],
  'title': 'php'},
 {'fid': -1, 'id': 4, 'title': 'lisp'}]

The fid can be any id, as long as that id is named somewhere in sequence t or is -1 for root entries.",A,1
17515276,2013-07-07 19:00:04.757000+00:00,"The read position in a file is a simple index maintained by the OS kernel; there is no physical head movements involved.
The C code subtracting 10 from the current position or you calculating the new position in python will make very little difference.",A,3
44655928,2017-06-20 14:31:38.033000+00:00,"Practically speaking, there is no difference between except: and except BaseException:, for any current Python release.
That's because you can't just raise any type of object as an exception. The raise statement explicitly disallows raising anything else:

[...] raise evaluates the first expression as the exception object. It must be either a subclass or an instance of BaseException.

Bold emphasis mine. This has not always been the case however, in older Python releases (2.4 and before) you could use strings as exceptions too.
The advantage then is that you get to have easy access to the caught exception. In order to be able to add as targetname, you must catch a specific class of exceptions, and only BaseException is going to do that.
You can still access the currently active exception by using sys.exc_info() though:
except:
    be = sys.ext_info()[1] 

Pick what you feel is more readable for your future self and your colleagues.",A,14
37160720,2016-05-11 11:04:06.050000+00:00,"You seem to be confusing the file object returned by the open() call, for the actual process of reading from a file object.
Python decodes the contents of the file, as you read it. Opening a file object doesn't read any data from the file, it just creates a file object. No data is read from the file at that point, there are no bytes for Python to process yet.
In line 2 you actually read from the file, using the file.readlines() method. It is that method that tells the file object to fetch data from the filesystem (bytes) and decode those bytes. Only then can Python know that the data cannot actually be decoded as ASCII.",A,2
39121955,2016-08-24 11:21:21.383000+00:00,"No, that's not Base64-encoded data. It is hexadecimal. From the Wikipedia article on the RTF format:

RTF supports inclusion of JPEG, Portable Network Graphics (PNG), Enhanced Metafile (EMF), Windows Metafile (WMF), Apple PICT, Windows Device-dependent bitmap, Windows Device Independent bitmap and OS/2 Metafile picture types in hexadecimal (the default) or binary format in a RTF file.

The binascii.unhexlify() function will decode that back to binary image data for you; you have a PNG image here:
>>> # data contains the hex data from your link, newlines removed
...
>>> from binascii import unhexlify
>>> r = unhexlify(data)
>>> r[:20]
'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x01@'
>>> from imghdr import test_png
>>> test_png(r, None)
'png'

but of course the \pngblip entry was a clue there. I won't include the image here, it is a rather dull 8-bit 320x192 black rectangle.",A,5
27944224,2015-01-14 13:34:41.480000+00:00,"You are looking for the enumerate() function:
for i, e in enumerate(clist):
    log.info(u'Processing record {:,} of {:,}.'.format(i, len(clist)))

enumerate() yields tuples with (index, item) for items in the passed-in sequence.
If you needed to create counts independent of the list, you could use itertools.count(); each time you pass a count() object to the next() function it'll yield the next value in the series:
from itertools import count

i = count()

for e in clist:
    if some_condition:
        log.info(u'Processing record {:,} of {:,}.'.format(next(i), len(clist)))

Now i is only incremented each time some_condition is true.",A,4
18500087,2013-08-28 23:44:54.940000+00:00,"The problem is that you're making a copy of the previous node here:
prev = copy.copy(current)

So, when you update that copy in-place here:
prev.setNext(newNode)

… it doesn't affect the original node that's actually linked into the list. (Nor do you replace the original node with the modified copy.) So, nothing ever gets changed.
To fix it, just remove the copy.copy.

When you fix that, there's another bug in your code that will lead to an infinite loop of printing out ""absolute"", in printLinkedList:
prints.setNext(prints.nextNode())

This doesn't do anything useful—it sets prints.next to prints.next. Crucially, it doesn't update the variable prints to point to the next node. Just do this:
prints = prints.nextNode()


And with both of those changes, the output from your original example is:
absolute
crisp
daytona
demand
extra


However, notice that your new example is missing one value:
albacore
axe
hello
jupiter

I'll leave it to you to figure out where the shrimp went. (You can always post a new question if you get stuck.)

If you're wondering how I found the problem:
I added a print statement after the while loop that dumps out a bunch of information about the found previous node, including its id, and another print before and after the setNext, so I could see that I was successfully setting the next member of the first node each time through the loop, but it was always a different first node each time through.
Then I added a print to show me the id of each nodes, and it was clear that the found previous node each time wasn't any of the nodes actually in the list. At which point the copy.copy finally jumped out at me.",A,2
44790074,2017-06-27 21:17:29.350000+00:00,"In this case, you can enumerate all the attributes of the class that are uppercase; I'd use the vars() function to access the class namespace:
@classmethod
def all(cls):
    return [value for name, value in vars(cls).items() if name.isupper()]

Demo:
>>> class AwsRegion():
...     '''
...     Class to define AWS Regions
...     '''
...     OHIO = 'us-east-2'
...     NORTH_VIRGINIA = 'us-east-1'
...     NORTH_CALIFORNIA = 'us-west-1'
...     OREGON = 'us-west-2'
...     MUMBAI = 'ap-south-1'
...     SEOUL = 'ap-northeast-2'
...     SINGAPORE = 'ap-southeast-1'
...     SYDNEY = 'ap-southeast-2'
...     TOKYO = 'ap-northeast-1'
...     FRANKFURT = 'eu-central-1'
...     IRELAND = 'eu-west-1'
...     LONDON = 'eu-west-2'
...     SAO_PAULO = 'sa-east-1'
...     @classmethod
...     def all(cls):
...         return [value for name, value in vars(cls).items() if name.isupper()]
...
>>> AwsRegion.all()
['us-east-2', 'us-east-1', 'us-west-1', 'us-west-2', 'ap-south-1', 'ap-northeast-2', 'ap-southeast-1', 'ap-southeast-2', 'ap-northeast-1', 'eu-central-1', 'eu-west-1', 'eu-west-2', 'sa-east-1']
",A,2
25624966,2014-09-02 13:46:10.343000+00:00,"Split, then join again, picking larger and larger slices:
elems = x.split('-')
['-'.join(elems[:i]) for i in xrange(3, len(elems) + 1)]

This assumes you want your first part to have 3 elements, always. The loop goes through 3, 4 and 5, slicing elems to that many elements and joins them together again on the separator.
Demo:
>>> x = 'AAA-BBB-CCC-DDD-EEE'
>>> elems = x.split('-')
>>> ['-'.join(elems[:i]) for i in xrange(3, len(elems) + 1)]
['AAA-BBB-CCC', 'AAA-BBB-CCC-DDD', 'AAA-BBB-CCC-DDD-EEE']
",A,3
11022360,2012-06-13 19:54:32.510000+00:00,"As the documentation for NSDateComponents says:

Weekday units are the numbers 1 through n, where n is the number of days in the week. For example, in the Gregorian calendar, n is 7 and Sunday is represented by 1.
This value is interpreted in the context of the calendar with which it is used—see “Calendars, Date Components, and Calendar Units” in Date and Time Programming Guide.

And if you look at NSGregorianCalendar, as expected, it defines the meaning of 1–7 as Sunday–Saturday.
Calling setFirstWeekday:2 means that your weeks start on Monday, for calculating things like whether this is the second week of the month, etc. It doesn't mean your Mondays suddenly become Sundays. 1 Jun 2012 is still a Friday, not a Thursday, so you get 6, not 5.
If, instead of the weekday, you want to know how many days from the start of the week it is, that's easy:
(components.weekday - calendar.firstWeekday + 1) % 7
",A,6
29984428,2015-05-01 09:26:32.697000+00:00,"There is a quick&dirty solution that works on any language that supports monkeypatching (Python, Ruby, ObjC, etc.). I honestly can't remember ever needing it in Python, but I did it quite a bit in both SmallTalk and ObjC, so maybe it'll be useful for you.
Just dynamically wrap A.f in a function, like this:
real_A_f = A.f
def wrap_A_f(self, *args, **kwargs):
    result = real_A_f(self, *args, **kwargs)
    return result
A.f = wrap_A_f

In most scriptable debuggers, you should be able to write a script that does this automatically for a method by name. In pdb, which lets you execute normal Python code right in the debugger, it's especially simple.
Now you can put a breakpoint on that return result, and it's guaranteed to hit immediately after the real A.f returns (even if it returns in the middle or falls off the end without a return statement).
A few things you may want to add:

If you also want to catch A.f raising, put a try: and except: raise around the code, and add a breakpoint on the raise.
For Python 2.x, you may want to wrap that up with types.MethodType to make a real unbound method.
If you only want a breakpoint on a specific A instance, you can either use a conditional breakpoint that checks self is a, or use types.MethodType to create a bound instance and store that as a.f.
You may want to use functools.wraps if you want to hide the wrapper from the rest of the code (and from your debugging, except in the cases where you really want to see it).
Since pdb lets you execute dynamic code right in the live namespace, you can put a wrap_method function somewhere in your project that does this, and then, at the prompt, write p utils.wrap_method(A, 'f'). But if you wrap multiple methods this way, they're going to share the same breakpoints (inside the wrapper function defined inside wrap_method). Here I think a conditional breakpoint is the only reasonable option.
If you want access to the real A.f's locals from the wrapper's breakpoint, that's a lot harder. I can think of some very hacky options (e.g., exec(real_A_f.__code__, real_A_f.globals()), but nothing I'd be happy with.
",A,3
2387748,2010-03-05 15:18:26.797000+00:00,"Use csv.DictReader:

Create an object which operates like a regular reader but maps the information read into a dict whose keys are given by the optional fieldnames parameter. The fieldnames parameter is a sequence whose elements are associated with the fields of the input data in order. These elements become the keys of the resulting dictionary. If the fieldnames parameter is omitted, the values in the first row of the csvfile will be used as the fieldnames. If the row read has more fields than the fieldnames sequence, the remaining data is added as a sequence keyed by the value of restkey. If the row read has fewer fields than the fieldnames sequence, the remaining keys take the value of the optional restval parameter. Any other optional or keyword arguments are passed to the underlying reader instance...
",A,16
26094746,2014-09-29 07:37:01.393000+00:00,"
Has the cell_contents call for closures in python changed?

__closure__ is, and always has been, a tuple of cells (even back when it was called func_closure).
Each cell still has a cell_contents member. But the tuple, of course, does not.
So, what you want is probably one of these:
func.__closure__[0].cell_contents

[cell.cell_contents for cell in func.__closure__]

It's worth noting that the details of __closure__ are undocumented and an implementation-specific feature of the CPython implementation. While the data model defines __closure__ as:

None or a tuple of cells that contain bindings for the function’s free variables.

… it doesn't say anything about what those cells are, or that they have an attribute named cell_contents.
But in 3.3+, there's a documented way to get this information, inspect.getclosurevars:
>>> inspect.getclosurevars(func)
ClosureVars(nonlocals={'i': 1}, globals={}, builtins={}, unbound=set())

If you want to know more than what this function returns, you may want to look at how it's implemented in your favorite interpreter (which is probably CPython, given that none of the other major interpreters support 3.3 yet). inspect is one of those modules that's designed to have helpful, readable source, so the docs link directly to the source code. So you can see how it works—but it's basically what you'd expect; if __closure__ isn't None, it just creates a dict mapping each cell name in __code__.co_freevars to the corresponding cell.cell_contents from the tuple.
If you want to go even deeper, I don't know of any good explanations of the internals of closures (that would make a good blog post, and I'll bet someone's written one… but the best I could find in a quick google is Michael Foord's Nothing is Private: Python Closures (and ctypes), but CPython's source for function objects and code objects is pretty readable if you know C and the Python C API. You might also want to consider looking at PyPy, which tends to be a bit more complicated, but it's all in Python. There's also a brief implementation note in PEP 227, which added closures in Python 2.1, but it doesn't explain much.",A,10
19644952,2013-10-28 20:57:33.467000+00:00,"Python 3 switches to absolute imports by default, and disallows unqualified relative imports. The from base import Base line is such an import.
Python 3 will only look for top-level modules; you don't have a base top-level module, only model.base. Use a full module path, or use relative qualifiers:
from .base import Base

The . at the start tells Python 3 to import starting from the current package.
You can enable the same behaviour in Python 2 by adding:
from __future__ import absolute_import

This is a change introduced by PEP 328, and the from future import is available from Python 2.5 onwards.",A,8
25361914,2014-08-18 11:10:42.980000+00:00,"Function defaults are created at definition time, not when called.  The timer_compare function is an object and defaults are evaluated when it is created and stored as an attribute on that object.
Since your function is created the moment your module is imported (or when your top-level script is first loaded by Python), the time.clock() value will be very low.
Use a sentinel instead:
def timer_compare(time1, time2='', note='@', time3=None):
    if time3 is None:
        time3 = time.clock()
",A,2
2438858,2010-03-13 15:27:09.790000+00:00,"
I know that alternative to that kind of listing would be to split up big files into smaller classes/files, so that their class declaration would be self-explanatory enough.

Correct.

but some complex tasks require a lot of code

Incorrect.  While a ""lot"" of code be required, long runs of code (over 25 lines) are a really bad idea.

actually not listing functions, but code blocks inside a function

Worse.  A function that needs a table of contents must be decomposed into smaller functions.

I'm not sure is it really worth it spending your time subdividing implementation into multiple of files?

It is absolutely mandatory that you split things into smaller files.  The folks that maintain, adapt and reuse your code need all the help they can get. 

is it ok to create an index-listing additionally to the class/interface declaration?

No.  
If you have to resort to this kind of trick, it's too big.
Also, many languages have tools to generate API docs from the code.  Java, Python, C, C++ have documentation tools.  Even with Javadoc, epydoc or Doxygen you still have to design things so that they are broken into intellectually manageable pieces.

Make things simpler.
Use a tool to create an index.
",A,10
28461634,2015-02-11 18:25:18.877000+00:00,"GeoDjango does not handle converting zip codes into locations: that's simply not what it's for. You'll need a geocoding library, a Google search should reveal plenty.
The project you link to simply uses an existing set of geocoded data for cities and zip codes, and even tells you where to get it - see the relevant management command.",A,1
21835273,2014-02-17 17:15:36.500000+00:00,"Python comes with batteries included; using a collections.Counter() object:
from collections import Counter

counts = Counter(list_of_words)
",A,1
39475119,2016-09-13 16:43:20.983000+00:00,"You can't install mysql through pip; it's a database, not a Python library (and it's currently in version 5.7). You need to install the binary package for your operating system.",A,1
24456042,2014-06-27 15:55:30.993000+00:00,"Python 3 changed behaviour for the == case, see Python 3, What's New:

!= now returns the opposite of ==, unless == returns NotImplemented.

It was deemed a useful change.
The fact that the documentation has not been updated is indeed a long standing bug.
However, as a comment on the report points out, if you inherit from a class that already has defined __ne__, overriding just __eq__ is not enough and you'll also have to override the __ne__ method.",A,15
24096996,2014-06-07 12:06:55.620000+00:00,"I prefer using CSS Selectors. Select the first table, then all rows in the tbody for ease of processing; the rows are 'grouped' by tr th rows. From there you can select all next siblings that don't contain th headers and scan these for preview links:
previews = {}

table = soup.select('table.fixture')[0]
for group_header in table.select('tbody tr th'):
    date = group_header.string
    for next_sibling in group_header.parent.find_next_siblings('tr'):
        if next_sibling.th:
            # found a next group, end scan
            break
        for preview in next_sibling.select('a.preview'):
            previews.setdefault(date, []).append(
                ""http://www.afl.com.au"" + preview.get('href'))

This builds a dictionary of lists; for the current version of the page this produces:
{u'Monday, June 09': ['http://www.afl.com.au/match-centre/2014/12/melb-v-coll'],
 u'Sunday, June 08': ['http://www.afl.com.au/match-centre/2014/12/gcfc-v-syd',
                      'http://www.afl.com.au/match-centre/2014/12/fre-v-adel',
                      'http://www.afl.com.au/match-centre/2014/12/nmfc-v-rich']}
",A,0
44954134,2017-07-06 16:14:21.217000+00:00,"A list comprehension has a fixed number of (nested) loops. So must have two loops here, one over the top-level dictionaries, and one over the fields. The trick is to produce one iteration in the nested loop if there are no fields:
[d['name'] + fieldname 
 for d in results
 for fieldname in (
    ('.' + sub['name'] for sub in d['fields']) if 'fields' in d else
    ('',))
]

The for fieldname loop loops either over the names of the fields sub-directories (with '.' prefixed), or over a tuple with just a single empty string in it.
Note that this isn't actually all that readable. I'd delegate producing the fieldname loop to a helper generator function to improve that:
def fieldnames(d):
    if 'fields' in d:
        for sub in d['fields']:
            yield '.' + sub['name']
    else:
        yield ''

[d['name'] + fieldname for d in results for fieldname in fieldnames(d)]

Demo:
>>> def fieldnames(d):
...     if 'fields' in d:
...         for sub in d['fields']:
...             yield '.' + sub['name']
...     else:
...         yield ''
...
>>> [d['name'] + fieldname for d in results for fieldname in fieldnames(d)]
['input', 'status', 'entity.value', 'entity.address']
",A,1
15923221,2013-04-10 10:23:14.977000+00:00,"It's certainly fine to do this. You can pass extra kwargs by specifying them in a dictionary after the pattern:
(r'^a/foo/$', my_view, {'b': False}),
(r'^b/foo/$', my_view, {'b': True}),

Alternatively, you could capture the prefix itself as a kwarg and check that in the dispatch method.",A,1
38998030,2016-08-17 13:27:02.930000+00:00,"IPython auto-indents blocks for compound statements, so to enter the else: line you have to manually unindent again.
I can only reproduce your error if I do not un-indent properly; hitting backspace between 1 and 3 times, instead of 4.
Note the extra space before else: in this error example, where I used backspace 3 times:
In [2]: if x == True:
   ...:     print('ok')
   ...:  else:
  File ""<ipython-input-2-915b4c0eb5ea>"", line 3
    else:
         ^
IndentationError: unindent does not match any outer indentation level

IPython allows you to edit your block; just use the Up arrow key to recall the failed piece of text, and remove the extraneous spaces at the start of the else: line.
If you are not typing in code by hand, don't copy-and-paste code directly. Use the %paste command instead, and IPython will paste the code for you, avoiding any auto-indent issues:
In [3]: %paste
x = True
if x == True:
    print('ok')
else:
    print('nok')
## -- End pasted text --
ok

See %paste? for more information.",A,1
29176337,2015-03-20 21:49:20.637000+00:00,"I didn't read all of that - sorry, there's too much. But I did notice your comment that you expect to access in your modeladmin definition a variable that you set in your view. That can't possibly work.
Anything at class level is always executed when the module containing the class is first imported. That is when the server process starts up, so there is no possible way anything done in the view can have happened yet.
You almost never want to have any logic at class level. You need to put it in methods, which are called at the relevant time. In this case, you probably need to use the get_fields method. 
Edit
Looking further up at your attempt at a get_fields method, I can't see at all what you are trying to do here.  'prestressed_concrete_deck' is a literal string, and could never be None, so neither of your conditions can ever be true. And as to your question about what the parameters are, the documentation for that method explains clearly that obj is the object being edited.",A,0
41103330,2016-12-12 14:41:35.213000+00:00,"Functions are given a reference to a global namespace. That namespace is the module they are defined in.
Because both func_two and do_the_job are defined in the same module, they both 'live' in that namespace. Using the name do_the_job in the  body of func_two will be resolved by looking at the globals for func_two and that'll find the other function.
When you import a module, the whole module is always loaded; you can find it in the sys.modules mapping. Once loaded (which only needs to be done once), the importing namespace is updated to add the names you imported; these are just references to objects in the imported module namespace.
So the from my_py import func_two import ensures the my_py module is loaded (stored in sys.modules['my_py']) and then the name func_two is added to your current namespace to reference sys.modules['my_py'].func_two.
You did not, however, import do_the_job into your current namespace, so you can't use that name. It is not part of your global namespace. Either import it explicitly, or reference it via sys.modules['my_py'].do_the_job.
You may want to read up on how Python names work, I can heartily recommend you read Ned Batchelder's Facts and myths about Python names and values. It doesn't directly cover importing, but it does offer vital insights that'll help you understand this specific situation better.",A,7
53360609,2018-11-18 11:58:07.280000+00:00,"Because in Python 2, the standard open() call creates a far simpler file object than the Python 3 open() call does.  The Python 3 open call is the same thing as io.open(), and the same framework is available on Python 2.
To make this a fair comparison, you'd have to add the following line to the top of your test:
from io import open

With that change, the timings on Python 2 go from 5.5 seconds, to 37 seconds. Compared to that figure, the 11 seconds Python 3 takes on my system to run the test really is much, much faster.
So what is happening here? The io library offers much more functionality than the old Python 2 file object:

File objects returned by open() consist of up to 3 layers of composed functionality, allowing you to control buffering and text handling.
Support for non-blocking I/O streams
A consistent interface across a wide range of streams
Much more control over the universal newline translation feature.
Full Unicode support.

That extra functionality comes at a performance price. 
But your Python 2 test reads byte strings, newlines are always translated to \n, and the file object the code is working with is pretty close to the OS-supplied file primitive, with all the downsides. In Python 3, you usually want to process data from files as text, so opening a file in text mode gives you a file object that decodes the binary data to Unicode str objects.
So how can you make things go 'faster' on Python 3? That depends on your specific use case, but you have some options:

For text-mode files,  disable universal newline handling, especially when handling a file that uses line endings that differ from the platform standard. Set the newline parameter to the expected newline character sequence, like \n. Binary mode only supports \n as line separator.
Process the file as binary data, and don't decode to str. Alternatively, decode to Latin-1, a straight one-on-one mapping from byte to codepoint. This is an option when your data is ASCII-only too, where Latin-1 omits an error check on the bytes being in the range 0-127 rather than 0-255.

When using mode='rb', Python 3 can easily match the Python 2 timings, the test only takes 5.05 seconds on my system, using Python 3.7.
Using latin-1 as the codec vs. UTF-8 (the usual default) makes only a small difference; UTF-8 can be decoded very efficiently. But it could make a difference for other codecs. You generally want to set the encoding parameter explicitly, and not rely on the default encoding used.",A,5
32621868,2015-09-17 02:52:00.097000+00:00,"You don't need shell=True, to call tr command:
#!/usr/bin/env python
from subprocess import check_call

with open('input', 'rb', 0) as input_file, \
     open('output', 'wb', 0) as output_file:
    check_call(['tr', '-d', r'\000'], stdin=input_file, stdout=output_file)

Backslash is special inside Python string literals and therefore to pass the backslash, you either need to escape it: '\\000' or you should use a raw-string literal: r'\000'.
You don't need the external process here. You could remove zero bytes from a file using pure Python:
chunk_size = 1 << 15
with open('input', 'rb') as input_file, \
     open('output', 'wb') as output_file:
    while True:
        data = input_file.read(chunk_size)
        if not data: # EOF
            break
        output_file.write(data.replace(b'\0', b''))
",A,0
2482626,2010-03-20 10:20:18.977000+00:00,"It would just be:
my_customer.templates.add(my_template)

where my_customer and my_template are instances of Customer and Template respectively.
See the documentation for related objects",A,0
28198229,2015-01-28 17:08:46.427000+00:00,"You are not streaming the upload because requests can only do that if the whole body is sourced from an open file object. It'll still read all files into memory to build a multi-part post body.
For multi-part uploads, use the requests toolbelt; it includes a Streaming Multipart Data Encoder:
from requests_toolbelt import MultipartEncoder
import requests

files = {
    'md5': ('', md5hash),
    'modified': ('', now),
    'created': ('', now),
    'file': (os.path.basename(url), fileobject, 'application/octet-stream')
}
m = MultipartEncoder(fields=dict(files, **params))
headers['content-type'] = m.content_type

r = s.post(url, data=m, headers=headers)
r = requests.post('http://httpbin.org/post', data=m, headers=headers)

The first argument to MultipartEncoder is parsed with the iter_field_objects() function from the urllib3 library; this means that it can either be a dictionary of key-value pairs, or a sequence (list, tuple) of RequestField() objects.
When passing in a dictionary like I did above, each key-value pair is parsed with RequestField.from_tuples(), and you can only specify the field name, the value, and optionally the filename and the mimetype. Extra headers are not supported. I used that option in the above sample.
If you want to add the Content-Transfer-Encoding header to the file field, then we need to use a sequence of RequestField objects:
from requests.packages.urllib3.fields import RequestField

fields = [RequestField.from_tuples(*p) for p in params.iteritems()]
fields.extend([
    RequestField('md5', md5hash),
    RequestField('modified', now),
    RequestField('created', now),
    RequestField(
        'file', fileobject, 'application/octet-stream',
        {'Content-Transfer-Encoding':'binary'}),
])

Note that you cannot combine streaming requests with the request-cache project; the latter requires access to the full body of the request to produce a cache key.
You'd have to patch the requests_cache.backends.base.BaseCache.create_key method to handle MultipartEncoder objects and come up with some kind of hash-key for the body. This is outside the scope of this question however.",A,4
12179090,2012-08-29 13:28:24.300000+00:00,"Take a look at your test configuration, the url setting has an error in it:
'cache.short_term.url': '127.0.0.1.11211',

Note that there is no : colon in there. The memcached module you use, uses regular expressions to try and parse that value, and that method sets port to None when you specify that value as a host:
>>> host = '127.0.0.1.11211'
>>> re.match(r'^(?P<host>[^:]+)(:(?P<port>[0-9]+))?$', host).groupdict()
{'host': '127.0.0.1.11211', 'port': None}

which is the source of your traceback. Change the cache_opts dict to read:
'cache.short_term.url': '127.0.0.1:11211',

and things will work fine:
>>> host = '127.0.0.1:11211'
>>> re.match(r'^(?P<host>[^:]+)(:(?P<port>[0-9]+))?$', host).groupdict()
{'host': '127.0.0.1', 'port': '11211'}
",A,3
24578419,2014-07-04 16:50:32.123000+00:00,"Reducing this to the simplest method that'll work:

Put static assets into your static subfolder.
Leave Flask set to the default, don't give it a static_url_path either.
Access static content over the pre-configured /static/ to verify the file works

If you then still want to reuse a static file, use current_app.send_static_file(), and do not use leading / slashes:
from flask import Flask, current_app
app = Flask(__name__)

@app.route('/')
def hello_world():
    return current_app.send_static_file('editor.html')

This looks for the file editor.html directly inside the static folder.
This presumes that you saved the above file in a folder that has a static subfolder with a file editor.html inside that subfolder.
Some further notes:

static_url_path changes the URL static files are available at, not the location on the filesystem used to load the data from.
render_template() assumes your file is a Jinja2 template; if it is really just a static file then that is overkill and can lead to errors if there is actual executable syntax in that file that has errors or is missing context.
",A,17
11248103,2012-06-28 15:38:11.087000+00:00,"There are two general approaches ""push"" and ""poll"":

""push"": your dictionary generates an event/calls a callback on any change to itself e.g., traits library. It is useful when you need to act on any change as soon as it happens.
""poll"": you inspect the dictionary for changes when you're ready to act on them. It is useful if the changes could be processed in a batch mode and you're are not interested in individual changes only the final state matters.
",A,0
28642346,2015-02-21 05:06:27.900000+00:00,"zlib successfully decompresses data that it has compressed previously:
>>> data = b'data'
>>> import zlib
>>> compressed = zlib.compress(data)
>>> import base64
>>> original_data = base64.b64encode(compressed).decode()
>>> zlib.decompress(base64.b64decode(original_data))
b'data'

zlib fails to decompress (with the default settings) gzip data:
>>> import gzip
>>> gzipped_data = base64.b64encode(gzip.compress(data)).decode()
>>> gzipped_data != original_data
True
>>> print(zlib.decompress(base64.b64decode(gzipped_data)))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
zlib.error: Error -3 while decompressing data: incorrect header check
>>> gzip.decompress(base64.b64decode(gzipped_data))
b'data'
",A,4
629533,2009-03-10 10:05:23.227000+00:00,"Do you have a copy of ""The Data Warehouse Toolkit""?  
The suggestion there is to do the following.

Separate facts (measurable, numeric) values from the dimensions which qualify or organize those facts.  One big table isn't really the best idea.  It's a fact table that dominates the design, plus a number of small dimension tables to allow ""slicing and dicing"" the facts.
Keep the facts in simple flat files until you want to do SQL-style reporting.  Don't create and back up a database.  Create and back up files; load a data base only for the reports you must do from SQL.
Where possible create summary or extra datamarts for analysis.  In some cases, you may need to load the whole thing to a database.  If your files reflect your table design, all databases have bulk loader tools that can populate and index SQL tables from the files.
",A,8
13910009,2012-12-17 07:32:35.167000+00:00,"To round to 4 significant digits:
f = float(""%.4g"" % 8829834.123)

To print it as an integer:
print ""%.0f"" % f
# -> 8830000
",A,4
13955377,2012-12-19 15:14:34.033000+00:00,"Just add it before the command without exporting it:
cmd = ""my_password=%s python script.py"" % test
",A,3
3101510,2010-06-23 12:22:18.917000+00:00,There are various third-party projects that manage this sort of thing. Here's one review.,A,2
47991006,2017-12-27 11:02:00.263000+00:00,"Django provides you a shortcut do to this: self.get_tldate_mth_display().
(The reason your code failed is that that isn't at all what .index() does; you should just do self.MONTHS[self.tldate_mth-1][1]; but, as I say, there's no need to do that when there's a built-in way already.)",A,1
11347569,2012-07-05 15:26:57.147000+00:00,"It seems like a very fragile approach. It is enough to change any of __slots, __dc_list, __iter_cache during active iteration to put the object into an inconsistent state.
You need either to forbid changing the object during iteration or generate all cache items at once and return a copy of the list.",A,1
11528293,2012-07-17 18:04:39.703000+00:00,"I think you're looking at the problem the wrong way.
No generic framework for running scripts would be likely to help type-checking the arguments to an arbitrary perl or python script, since they're just strings (whether passed to sys.argv/$ARGV or as CGI environment variables).
But if you split this into parts, they're all pretty easy, and mostly orthogonal.
First, how do you do type-checking on the fields? Well, if you're willing to require HTML 5, just put the appropriate type and other parameters on the fields:
<input name=""foo"" type=""number"" required>

If you need to support older browsers, google ""HTML form validation"" or ""JS form validation"" and you'll find hundreds of libraries that do things, either by simplifying the JS you'd have to write, or by letting you attach stock validators by name or CSS style, e.g.:
  // ...
  <input name=""foo"" id=""foo"">
</form>
<script>
  // ...
  Validator.add(""foo"", type=""number"", required=true)
</script>

As usual jQuery is tremendously helpful. There are some solutions that let you write something identical to HTML 5; articles like this one show how easy it is to build one from scratch. Or, even simpler, just use the Validation plugin, so you only need one line of JS.
For setting up the form to trigger your scripts, any stock CGI framework will do this out of the box. 
For making the scripts return their stdout to the user, that's built-in to CGI too. However, make sure to configure your server to send content-type text/plain instead of text/html (for the directory the scripts are in, or for *.cgi or *.py/*.pl, or whatever's appropriate).
So, what's left? Do you want to generate the forms automatically from, say, some database mapping script names to type validation strings? Again, nobody's going to have a generic library that builds forms out of your type string format. But it's pretty easy to write with jQuery. Just create a form with an empty fieldset and action. In #(document).ready (or on selecting from an option menu, or whatever), add fields with the appropriate validation attributes to the empty fieldset. (Or, alternatively, if you're using a simple script-based validation library, just modify the library code to use your type descriptors directly instead of using some other format.) You can also set the action attribute here (if you're using HTML 5 or the Validation plugin, so you don't need a custom submit script), or just hit the URL from your custom submit script (if you're doing validation in a way that already needs it). 
So, yes, there's some code to be written here. But any solution is going to require some code—you have to feed the types into it somehow, right?—and using popular general-purpose libraries (e.g., jQuery with the Validation plugin) is probably going to require no more code than any more special-purpose framework would have. As well as being a lot more customizable, and maintainable (it's a lot easier to find people who have jQuery experience than people who have experience with some narrow-purpose library).",A,1
31271443,2015-07-07 14:28:06.263000+00:00,"Iteration is simply faster when using a list comprehension:
In [23]: from collections import deque

In [24]: %timeit deque((node[0] for node in pwnodes if node[1].get('pm')), maxlen=0)
1000 loops, best of 3: 305 µs per loop

In [25]: %timeit deque([node[0] for node in pwnodes if node[1].get('pm')], maxlen=0)
1000 loops, best of 3: 246 µs per loop

The deque is used to illustrate iteration speed; a deque with maxlen set to 0 discards all elements taken from the iterable so there are no memory allocation differences to skew the results.
That's because in Python 2, list comprehensions don't use a separate namespace, while a generator expression does (it has to, by necessity). That extra namespace requires a new frame on the stack, and this is expensive. The major advantage of generator expressions is their low memory footprint, not their speed.
In Python 3, list comprehensions have a separate namespace as well, and list comprehension and generator iteration speed is comparable. You also have set comprehensions, which are fastest still, even on Python 2.",A,6
27065274,2014-11-21 15:52:30.043000+00:00,"Indeed, the struct approach falls down here because it expects fields to be a fixed number of bytes wide, while your format uses a fixed number of codepoints.
I'd not use struct here at all. Your lines are already decoded to Unicode values, just use slicing to extract your data:
def ParseLine(arquivo):
    slices = [slice(1, 13), slice(14, 32), slice(33, 49)]
    for line in arquivo:
        yield [line[s].strip() for s in slices]

This deals entirely in characters in an already decoded line, rather than bytes. If you have field widths instead of indices, the slice() objects could also be generated:
def widths_to_slices(widths):
    pos = 0
    for width in widths:
        pos += 1  # delimiter
        yield slice(pos, pos + width)
        pos += width

def ParseLine(arquivo):
    widths = (12, 18, 16)
    for line in arquivo:
        yield [line[s].strip() for s in widths_to_slices(widths)]

Demo:
>>> sample = '''\
... |     field 1|      field 2     |     field 3    |
... | sreaodrsa  | raesodaso t.thl o| .tdosadot. osa |
... | resaodra   | rôn. 2x  17/220V | sreao.tttra v  |
... | esarod sê  | raesodaso t.thl o| .tdosadot. osa |
... | esarod sa í| raesodaso t.thl o| .tdosadot. osa |
... '''.splitlines()
>>> def ParseLine(arquivo):
...     slices = [slice(1, 13), slice(14, 32), slice(33, 49)]
...     for line in arquivo:
...         yield [line[s].strip() for s in slices]
... 
>>> for line in ParseLine(sample):
...     print(line)
... 
['field 1', 'field 2', 'field 3']
['sreaodrsa', 'raesodaso t.thl o', '.tdosadot. osa']
['resaodra', 'rôn. 2x  17/220V', 'sreao.tttra v']
['esarod sê', 'raesodaso t.thl o', '.tdosadot. osa']
['esarod sa í', 'raesodaso t.thl o', '.tdosadot. osa']
",A,0
27003543,2014-11-18 20:36:22.417000+00:00,"First, the reason for that error is that you're trying to use idx, a list, as a key in the dictionary, d. You can't use mutable values like lists as dictionary keys. (Otherwise, you could just mutate the list, and your dictionary would suddenly be invalid.) The usual solution is to create a tuple with the same elements as the list (because tuples are immutable, and therefore hashable if their elements are hashable), like d[tuple(idx)].
But in this case, that wouldn't make any sense. Doing that would just add each val onto the same dictionary value. (Actually, it wouldn't even do that, because list.add doesn't exist. You probably wanted list.append here. But since, again, it doesn't make sense or do anything useful, it's not worth putting too much effort into figuring out how to do it right…) If you wanted that, you could just skip the loop and do d[tuple(idx)] += june, but I can't imagine why you'd want that.
I think what you're trying to do is use the corresponding idx entry for each june entry. You could do that by zipping the two lists together, but really, there's a much easier way: Just use them immediately as you generate them, instead of storing them and then trying to figure out how to use them later:
for line in mets:
    fields = line.split(""\t"")
    idx = fields[0]
    june = fields[1:4]
    july = fields[4::]
    d[idx] += june

However, even this doesn't really make sense. Assuming you're going to do the same thing for july you did with june, this is just going to just add fields[1:] back together.
From your intermediate, it looks like what you actually want is to use the idx and the june-vs.-july-ness together as a key. Which is easy:
d = {}
for line in mets:
    fields = line.split(""\t"")
    idx = fields[0]
    june = fields[1:4]
    july = fields[4::]
    d[idx, 'a'] = june
    d[idx, 'b'] = july

Or maybe you don't even want a dict at all, just a list:
d = []
for line in mets:
    fields = line.split(""\t"")
    idx = fields[0]
    june = fields[1:4]
    july = fields[4::]
    d.append([idx, 'a'] + june)
    d.append([idx, 'b'] + july)
",A,1
22137785,2014-03-03 02:43:24.630000+00:00,"If input is already grouped by id and time as in the example in your question then you could use itertools.groupby() to compute the statistics on the fly:
#!/usr/bin/env python
import sys
from itertools import groupby

file = sys.stdin
next(file) # skip header line

lines = (line.split() for line in file if line.strip())
for id, same_id in groupby(lines, key=lambda x: x[0]): # by id
    max_time, max_count = None, 0
    for time, same_time in groupby(same_id, key=lambda x: x[1]): # by time
        count = sum(int(c) for _, _, c in same_time)
        if count > max_count:
            max_time, max_count = time, count
    print(""{} {} {}"".format(id, max_time, max_count))

Output
100000458 18 5
100005361 10 3
",A,3
24959252,2014-07-25 15:28:40.853000+00:00,"You are writing encoded byte strings to your CSV file. There is little point in doing this when you are expecting Unicode objects.
Don't encode, decode:
cells = [""hello"".decode(""utf-8""), ""nǐ hǎo"".decode(""utf-8""), ""你好"".decode(""utf-8"")]

or use u'...' unicode string literals:
cells = [u""hello"", u""nǐ hǎo"", u""你好""]

You cannot use a codecs.open() file object with the Python 2 csv module. Either use the UnicodeWriter approach (with a regular file object) and pass in Unicode objects, or encode your cells to byte strings and use the csv.writer() object directly (again with a regular file object), as that's what the UnicodeWriter does; pass encoded byte strings to the csv.writer() object.",A,3
21149185,2014-01-15 22:02:52.407000+00:00,"This is almost certainly a very bad idea—the whole point of keys is that they're a stable way to refer to records (in another take with a foreign key, or in an application, etc.).
But if you really want to, just iterate over the rows in ID order and set them to the ""right"" values, like this:
cursor.execute('SELECT id FROM events ORDER BY id')
inner_cursor = db.cursor()
for i, row in enumerate(cursor):
    inner.execute('UPDATE events SET id = ? WHERE id = ?', i, row[0])

You can optimize this in two ways:

Don't do it after every modification, but only after the last one in a batch.
If you do it after every modification, just modify the ones after ending_index, and use a single UPDATE to set each one to id - gap.
",A,0
26919614,2014-11-13 22:24:33.460000+00:00,"Escape the dollar and the dot:
re.search(r'\$\.', inputstring)

Rules of thumb:

Use a raw string literal, so you don't have to double slashes (both Python and regular expressions derive meaning from the backslash)
When in doubt, escape the character to make it match a literal character.
",A,1
13053226,2012-10-24 16:11:55.417000+00:00,"The urllib package is just that, a package. It's __init__.py does not import urllib.request and thus you cannot simply reach urllib.request by only importing urllib. It is intended as a namespace only.
Import urllib.request instead.",A,2
235761,2008-10-25 01:53:09.093000+00:00,"The summary has 14 items.  You can't do them all.  So, what's the first step?
Here's what you have to do first -- get one thing improved.

You've got fundamental quality issues.  (#2-5)
You've got architecture and component issues. (#6, 7)
You've got schedule problems.  (#1, 8, 9)

You can tackle quality.  Formal unit testing, heading toward TDD can help.  This might be hard because architecture issues slow testing down.
You can tackle architecture.  This might be harder because it will probably involve rework that will not appear deliverable.  But it may fix quality issues.  Or, it may be compounded by fundamental testing problems.
You can tackle schedule.  Without other corrections (i.e., quality or architecture) you may not get any traction with fixing schedule issues.
I think that overall improvements in people's attitudes come from starting with one success -- any success -- as early as possible.  What's the lowest-hanging fruit?  

One long-standing bug?  One unit test suite to find and fix that bug?
One major architectural feature?  Would a diagram that everyone can post in their cube help?  How about a presentation clarify things?
One new use case?  One new feature that actually works?
",A,1
17128758,2013-06-15 23:09:12.633000+00:00,"The github3 library supports iteration over all users:
import github3

for user in github3.iter_all_users():
    user.refresh()
    print user.location

github3.iter_all_users() produces User objects.
You need the .refresh() call here, because the /users endpoint only returns a smaller subset of user information, and location isn't included in that. This requires another API request, so you may want to pace your script to avoid hitting the GitHub rate limits.
A future version of github3 (newer than 0.7.0) adds support to specify the page (batch) size  to reduce the number of API requests you need to make; the GitHub API defaults to 30 results per page but lets you load up to 100 results per page instead:
for user in github3.iter_all_users(per_page=100):
    user.refresh()
    print user.location
",A,2
27230516,2014-12-01 14:28:10.683000+00:00,"You are creating integers, checking their id(), then discarding the integers again. Python then reuses the memory address for the next integer you created, so their id() values match.
From the id() function documentation:

Two objects with non-overlapping lifetimes may have the same id() value.

Python object lifetimes are determined by their reference count; if there are more than 0 references to an object, it remains in memory, otherwise it is removed. The inner id() call in the expression id(id(our_user)) creates an integer that is only referenced by the stack; it is passed to the outer id() call and then removed from the stack, after which there are no references left to it and it is discarded again.
Had you created references to the produced integer values first, you'd have found they are not the same:
>>> a = b = []
>>> id(a) == id(b)
True
>>> a is b
True
>>> id_of_a, id_of_b = id(a), id(b)
>>> id_of_a == id_of_b
True
>>> id(id_of_a) == id(id_of_b)
False

But wait! For small integers (from -5 through to 256) Python only ever creates one copy (interning), so there the identity of two integers would be the same again:
>>> small = 200
>>> small is 200
True

All in all, unless you absolutely know you have to test for a singleton object (object identity), stick to equality tests instead. This goes for SQLAlchemy as well.",A,4
14985430,2013-02-20 16:52:47.610000+00:00,"Both Dog and sparky are referred to by the global namespace that forms your module, keeping both in memory.
If you were to run del Dog, sparky would still refer to the class (through it's __class__ reference) keeping it alive. The class refers to the two attributes that are part of it's definition, so they are kept alive as well. This is all independent of the get_dog_info function.
CPython keeps objects in memory based on reference counts; if anything in Python starts refering to an object somethere, that object's reference count is increased by 1, and decreased again when the reference is removed. When the count drops to 0, the object is removed from memory, and a garbage collection process breaks up circular references as needed to facilitate this process.
Note that because sparky is a global, the function code does not directly reference anything; globals are looked up at runtime. If you were to delete sparky too, all references would be cleared up. Because sparky in get_dog_info() is looked up in the global namespace, calling get_dog_info() would then result in a NameError.
If you did have a closure (reference to a variable in a parent function scope), the same rules would apply, except that the closure reference counts as another reference to the instance, thus indirectly to the class and the contained attributes.
So, considering the following example, where we do create a closure:
class Dog():
    breed = 'electronic dog'
    collar_type = 'microsoft'

def foo():
    sparky = Dog()
    def bar():
        return sparky.breed
    return bar

bar = foo()
del Dog

In the above example the Dog class remains in memory, because the bar closure still refers to an instance of that class:
>>> bar.__closure__
(<cell at 0x1012b2280: Dog object at 0x1012b5110>,)
>>> bar.__closure__[0].cell_contents
<__main__.Dog object at 0x1012b5110>
>>> bar()
'electronic dog'
",A,3
45504443,2017-08-04 10:29:41.127000+00:00,"Your URL is expecting the keyword album_id but you are passing pk.
return reverse('cart:details', kwargs={'album_id': self.pk})
",A,2
5367884,2011-03-20 10:25:13.100000+00:00,"First of all: there is a big difference between the concepts of providing and implementing an interface.
Basically, classes implement an interface, instances of those classes provide that interface. After all, classes are the blueprints for instances, detailing their implementations.
Now, an interface describes the implementation provided by instances, but the __init__ method is not a part of instances! It is part of the interface directly provided by classes instead (a classmethod in Python terminology). If you were to define an __init__ method in your interface, you are declaring that your instances have (provide) a __init__ method as well (as an instance method).
So interfaces describe what kind of instances you get, not how you get them.
Now, interfaces can be used for more than just describing what functionality an instance provides. You can also use interfaces for any kind object in Python, including modules and classes. You'll have to use the directlyProvides method to assign an interface to these, as you won't be calling these to create an instance. You can also use the @provider() class decorator, or the classProvides or moduleProvides functions from within a class or module declaration to get the same results.
What you want in this case is a factory definition; classes are factories that when called, produce an instance, so a factory interface must provide a __call__ method to indicate they are callable. Here is your example set up with a factory interface:
from zope import interface

class ITest(interface.Interface):
    required_attribute = interface.Attribute(
        """"""A required attribute for classes implementing this interface."""""")
    def required_method():
        """"""A required method for classes implementing this interface.""""""

class ITestFactory(interface.Interface):
    """"""Creates objects providing the ITest interface""""""
    def __call__(a, b):
        """"""Takes two parameters""""""

@interface.implementer(ITest)
@interface.provider(ITestFactory)
class Test(object):
    def __init__(self, a, b):
        self.required_attribute = a*b

    def required_method():
        return self.required_attribute

The zope.component package provides you with a convenience class and interface for factories, adding a getInterfaces method and a title and description to make discovery and introspection a little easier. You can then just subclass the IFactory interface to document your __init__ parameters a little better:
from zope import component

class ITestFactory(component.interfaces.IFactory):
    """"""Creates objects providing the ITest interface""""""
    def __call__(a, b):
        """"""Takes two parameters""""""

testFactory = component.Factory(Test, 'ITest Factory', ITestFactory.__doc__)
interface.directlyProvides(testFactory, ITestFactory)

You could now register that factory as a zope.component utility, for example, allowing other code to find all ITestFactory providers.
I used zope.interface.directlyProvides here to mark the factory instance with your subclassed ITestFactory interface, as zope.component.Factory instances normally only provide the IFactory interface.",A,8
19038862,2013-09-26 21:17:21.793000+00:00,"Your line parsing code doesn't match the format of the file.
You are trying to interpret the whole line as an integer, but the line contains more.
Perhaps you wanted to split the line first? That one line contains all elements of the record:
parts = line.strip().split(':')
record.idNum = int(parts[0])
record.firstName = parts[1]
record.lastName = parts[2]
record.classCode = parts[3]
record.gpa = float(parts[4])

You can override the original StudentFileReader.fetchRecord()) method by subclassing the class in your own code:
class MyStudentFileReader(StudentFileReader):
    def fetchRecord(self):
        line = self._inputFile.readline()
        if not line:
            return None

        record = StudentRecord()

        parts = line.strip().split(':')
        record.idNum = int(parts[0])
        record.firstName = parts[1]
        record.lastName = parts[2]
        record.classCode = parts[3]
        record.gpa = float(parts[4])

        return record

Then use MyStudentFileReader() instead of StudentFileReader().",A,3
35749242,2016-03-02 13:57:21.773000+00:00,"Your tuple contains only one element, a dictionary, with one key-value pair. If you wanted to extract that pair, you'd need to address it:
(x1, x2), = q[0].items()

The above expression extracts the first element from the tuple (the dictionary), and calls the dict.items() method on that. The resulting sequence of (key, value) pairs is then assigned to the (key, value), left-hand target, which can only take one such pair.
Demo:
>>> q = ({'sum(total)': Decimal('89')},)
>>> (x1, x2), = q[0].items()
>>> x1
'sum(total)'
>>> x2
Decimal('89')

You could also just iterate over all key-value pairs in the tuple, or you could use the key name. The latter, for example, would look like this:
decimal_value = q[0]['sum(total)']

You can still use unpacking in the assignment of course:
contained_dictionary, = q
decimal_value = contained_dictionary['sum(total)']

It all depends on what you are trying to achieve, and if the dictionary has different keys or should only ever contain one key-value pair.",A,2
11678704,2012-07-26 22:01:06.467000+00:00,"If it works for 100 files but fails for 10000, then check that arcpy.AddJoin_management closes csvfile after it is done with it.
There is a limit on the number of open files that a process may have at any one time (which you can check by running ulimit -n).",A,1
51200854,2018-07-05 22:47:19.053000+00:00,"As I understand it, the problem is that you have to write a bunch of functions like this:
def allAlpha():
     for eachVar in alphaCbVars:
          eachVar.set(allAlphaVar.get())

def allBravo():
     for eachVar in bravoCbVars:
          eachVar.set(allBravoVar.get())

… and so on.
What you want is a function that takes the list of vars and the all-var as parameters:
def allGroup(listOfVars, groupVar):
     for eachVar in listOfVars:
          eachVar.set(groupVar.get())

And now, you can use partial to create all of the individual partial functions:
from functools import partial

allAlphaCb = tk.Checkbutton(root, text='Alpha', variable=allAlphaCbVar,
                            command=partial(allGroup, alphaCbVars, allAlphaVar)
allBetaCb = tk.Checkbutton(root, text='Beta', variable=allBetaCbVar,
                            command=partial(allGroup, betaCbVars, allBetaVar)

However, you can reduce the duplication even further by writing a function that does all the stuff for building a group together in one place. It's hard to show that without more of your code to use, but it would look something like this:
def makeGroup(name): # maybe some more params needed for the individual cbs
    allCbVars = []
    allCbs = []
    # some loop that generates the individual variables and checkboxes
    # and puts them in those lists
    cbVar = tk.IntVar()
    cb = tk.Checkbutton(root, text=name, variable=cbVar, 
                        command=partial(allGroup, allCbVars)
    return allCbVars, allCbs, cbVar, cb

alphaCbVars, alphaCbs, allAlphaVar, allAlphaCb = makeGroup('Alpha')

Or, alternatively, maybe just a function to group a bunch of cbs together:
def groupify(name, cbVars):
    cbVar = tk.IntVar()
    cb = tk.Checkbutton(root, text=name, variable=cbVar, 
                        command=partial(allGroup, cbVars)
    return cbVar, cb

# now build all your individual alpha cbs the same as in your current code

alphaCbVar, alphaCb = groupify('Alpha', alphaCbVars)

And maybe you don't even want to store alphaCbVars, betaCbVars, etc. as separate variables. Maybe it makes more sense to have a list of groups, or a dict mapping names to groups, or some kind of object with attributes, or… whatever.",A,2
29930622,2015-04-28 21:53:03.867000+00:00,"Everything in Python is an object; the tuple and list objects are no exception.
The function simply returns a reference to either the tuple or the list object, based on the immutable flag:
>>> def sequence_class(immutable):
...     return tuple if immutable else list
... 
>>> sequence_class(True)
<class 'tuple'>
>>> sequence_class(False)
<class 'list'>

You then bind seq to that object, and calling seq then calls the referenced type:
>>> seq = sequence_class(True)
>>> seq
<class 'tuple'>
>>> seq('abc')
('a', 'b', 'c')

You can do the same by directly assigning tuple or list to a variable, it just creates another reference to the same object:
>>> foo = list
>>> foo
<class 'list'>
>>> foo('abc')
['a', 'b', 'c']
",A,5
49474291,2018-03-25 09:19:09.210000+00:00,"One way would be to use a dict:
choices = { ""BCM"": GPIO.BCM, ""BOARD"": GPIO.BOARD }
mode_chosen = choices[mode]
",A,2
43090610,2017-03-29 10:13:51.500000+00:00,"You can find all names defined on the class itself in the __dict__ attribute of a class; you can use the vars() function to list those more succinctly:
from types import FunctionType

for name, object in vars(Ci).items():
    if isinstance(object, FunctionType):
        print(name, 'is defined on Ci directly and not inherited')

You may need to vary the test for functions; I'm assuming here you only want plain function objects (which are bound into methods when looked up on the instance), but you may need to test for class or static methods or other callables too, depending on your use-case.
This will include names that have been re-defined; so these exist on the parent too but Ci provides a new implementation that masks that of a parent.
If you must find all names that uniquely belong to Ci and no parent provides, you'll have to filter on names that exist on the bases:
parent_defined = set().union(*(dir(b) for b in Ci.__bases__))

for name, object in vars(Ci).items():
    if name not in parent_defined and isinstance(object, FunctionType):
        print(name, 'is defined on Ci directly and not inherited')
",A,3
5563966,2011-04-06 09:10:10.893000+00:00,"Presumably you've hit a case where parem1 is not 'p' and parem2 is not 'APR'. In this case, you haven't made any calls to execute.
Perhaps you should restrict the if/else to just defining the string to be executed, with a catch-all default, and do the execute call outside the if block.",A,3
15295555,2013-03-08 13:54:34.400000+00:00,"Ask yourself why you needed that readonly value in the first place. Presumably, it was your code that generated it, when the user first requested the form. So, what was available to your code when the user requested the form that is not available when the user submits it back? There shouldn't be anything, which should lead you to the conclusion that that field can just as easily be generated on submit, without it needing to appear in the form at all.",A,1
21047622,2014-01-10 15:05:47.210000+00:00,"Python separates the right-hand side expression from the left-hand side assignment. First the right-hand side is evaluated, and the result is stored on the stack, and then the left-hand side names are assigned using opcodes that take values from the stack again.
For tuple assignments with 2 or 3 items, Python just uses the stack directly:
>>> import dis
>>> def foo(a, b):
...     a, b = b, a
... 
>>> dis.dis(foo)
  2           0 LOAD_FAST                1 (b)
              3 LOAD_FAST                0 (a)
              6 ROT_TWO             
              7 STORE_FAST               0 (a)
             10 STORE_FAST               1 (b)
             13 LOAD_CONST               0 (None)
             16 RETURN_VALUE        

After the two LOAD_FAST opcodes (which push a value from a variable onto the stack), the top of stack holds [a, b]. The ROT_TWO opcode swaps the top two positions on the stack so the stack now has [b, a] at the top. The two STORE_FAST opcodes then takes those two values and store them in the names on the left-hand side of the assignment. The first STORE_FAST pops a value of the top of the stack and puts it into a, the next pops again, storing the value in b. The rotation is needed because Python guarantees that assignments in a target list on the left-hand side are done from left to right.
For a 3-name assignment, ROT_THREE followed by ROT_TWO is executed to reverse the top three items on the stack.
For longer left-hand-side assignments, an explicit tuple is built:
>>> def bar(a, b, c, d):
...     d, c, b, a = a, b, c, d
... 
>>> dis.dis(bar)
  2           0 LOAD_FAST                0 (a)
              3 LOAD_FAST                1 (b)
              6 LOAD_FAST                2 (c)
              9 LOAD_FAST                3 (d)
             12 BUILD_TUPLE              4
             15 UNPACK_SEQUENCE          4
             18 STORE_FAST               3 (d)
             21 STORE_FAST               2 (c)
             24 STORE_FAST               1 (b)
             27 STORE_FAST               0 (a)
             30 LOAD_CONST               0 (None)
             33 RETURN_VALUE        

Here the stack with [d, c, b, a] is used to build a tuple (in reverse order, BUILD_TUPLE pops from the stack again, pushing the resulting tuple onto the stack), and then UNPACK_SEQUENCE pops the tuple from the stack again, pushes all elements back from the tuple back onto the stack again for the STORE_FAST operations.
The latter may seem like a wasteful operation, but the right-hand side of an assignment may be something entirely different, a function call that produces a tuple perhaps, so the Python interpreter makes no assumptions and uses the UNPACK_SEQUENCE opcode always. It does so even for the two and three-name assignment operations, but a later (peephole) optimization step replaces a BUILD_TUPLE / UNPACK_SEQUENCE combination with 2 or 3 arguments with the above ROT_TWO and ROT_THREE opcodes for efficiency.",A,64
25759661,2014-09-10 07:29:19.450000+00:00,"I don't quite know what you're trying to do here. Angular templates and Django ones are quite different, even if they superficially share the same {{ var }} syntax. You can't inherit an Angular template from a Django one, or vice versa.
But there's no reason to want to do that anyway. Angular is already loading the template as a partial, which means it will be inserted into the div in base.html. There's no need for inheritance here at all.",A,0
18083374,2013-08-06 14:44:58.730000+00:00,This can't work. You seem to have failed to understand the difference between server-side template code and client-side JavaScript. Template code is fully evaluated on the server side before it is passed to the browser. JS that is triggered by changes to client-side controls cannot possibly evaluate a template tag.,A,3
22122908,2014-03-02 01:54:19.440000+00:00,"You are not building one list, you are continually replacing your list with a new one:
for i in otp:
    important_list = [base_dict[i]]

Here the [...] syntax creates a new list object with just one element.
You could create the object just once, empty, then use list.append() to add elements:
important_list = []
for i in otp:
    important_list.append(base_dict[i])

or use a list comprehension:
important_list = [base_dict[i] for i in otp]

The dictionary is a little overkill here though; you could use ord() to get the ASCII codepoint instead and subtract 64:
important_list = [ord(i) - 64 for i in otp]

as ord('A') is 65.",A,0
22621876,2014-03-24 22:25:43.393000+00:00,Your file is empty; you cannot seek beyond the start of the file.,A,1
26241745,2014-10-07 17:28:23.783000+00:00,"Global variables aren't bad in themselves. What's bad is modifying global variables in your functions. Reading global constants, which is what you're proposing here, is fine - you don't even need the global keyword, since Python will look up the var in surrounding scopes until it finds it. You might want to put your variable in ALL_CAPS to signify that it is indeed a constant.
However, you still might consider whether your code would be better structured as a class, with your variable as an instance attribute.",A,2
23114131,2014-04-16 15:43:47.753000+00:00,"There are at least two issues:

to use double quotes inside a Python string literal, you should escape them or use single quotes for the string literal:
'stamp=$(date ""+%H%M%S"")'

it is pointless to set stamp shell variable because check_output() call spawns its own shell i.e., stamp won't be defined in the second command where you are trying to use it

You could emulate date ""+%H%M%S"" in Python:
import time
from subprocess import check_output, STDOUT

timestamp = time.strftime('%H%M%S')
path = '/home/pi/photobooth_images/Test{stamp}.jpg'.format(stamp=timestamp)
gpout = check_output([""raspistill"", ""-t"", ""1"", ""--output"", path], stderr=STDOUT)

Note: shell=True is not used. 
You could also format an existing datetime object:
from datetime import datetime

path = '/path/to/Test{now:%H%M%S}.jpg'.format(now=datetime.now())
",A,1
1562268,2009-10-13 19:00:26.593000+00:00," (setq viper-mode t)
 (require 'viper)
",A,1
38943841,2016-08-14 15:44:03.743000+00:00,"Your code relies on the ordering of dictionaries. Dictionary order depends on insertion and deletion history, varies between Python interpreter runs thanks to hash randomisation and should not be relied upon.
If your dictionaries are not nested, you can store them in sets as tuples of their key-value pairs, sorted:
set_1 = set(tuple(sorted(x.items())) for x in list1)
set_2 = set(tuple(sorted(x.items())) for x in list2)

This creates an immutable representation that retains the original key-value pairing but avoids any issues with ordering. These tuples can trivially be fed back into the dict() type to re-create the dictionary.",A,2
25275383,2014-08-12 22:51:48.597000+00:00,"You've defined database_license as a list of tuples (pairs), so there's no way a string like 35 can be in it.
What you want to check is if any of the members have a matching id. Like this:
if any(product_license_id == license_id
       for product, license_id in database_license):

… or:
if product_license_id in map(itemgetter(1), database_license):

But maybe it would be better to store database_license as a dict, mapping license IDs to products:
database_license = {2: 'Veeam Backup', 1: 'VMWare VM'}

Then your existing code would work just fine.",A,0
17167841,2013-06-18 11:36:16.163000+00:00,"Python uses closures to capture references to the original variable. The lambda objects retain a reference to the i name, through which the value can be accessed. This means that the i variable continues to live on after f completes.
You can introspect this closure in the .__closure__ tuple on the lambda objects; functions have the same attribute:
>>> L[0].__closure__
(<cell at 0x1077f8b78: int object at 0x107465880>,)
>>> L[0].__closure__[0]
<cell at 0x1077f8b78: int object at 0x107465880>
>>> L[0].__closure__[0].cell_contents
4

This is also why all lambdas in your list L refer to the value 4, and not to the numbers 0 through to 4. They all refer to the same closure:
>>> L[0].__closure__[0] is L[1].__closure__[0]
True

The closure refers to the variable, not to the value of that variable at the time the closure was defined. At the end of the loop i was last set to 4, so when looking up i in the lambda closure 4 will be found, for all lambdas in your list.
If you want your lambdas to refer to the value of i during the loop, capture it in a keyword argument:
def f():
    L = []
    for i in range(5): 
        L.append(lambda x, i=i: i ** x) 
    return L

Now i is a local variable to the lambda, not a closure.
Alternatively, create an entirely new scope from which to draw the closure:
def create_lambda(i):
    return lambda x: i ** x

def f():
    return [create_lambda(i) for i in range(5)]

Now create_lambda() is a new scope with it's own local i for the lambda closure to refer to. The lambdas then each have their own closures:
>>> L[0].__closure__[0] is L[1].__closure__[0]
False

Closures refer to a variable in a specific namespace; each time you call a function a new local namespace is created, so each closure refers to i in create_lambda in a separate namespace from other calls to create_lambda.",A,9
29196252,2015-03-22 15:35:01.063000+00:00,"You should not mix UTF-8 and regular expressions. Process all your text as Unicode. Make sure you decoded both the regex and the input string to unicode values first:
>>> import re
>>> columnString = '\xe5\xbd\x93\xe4\xbb\xa3\xe9\xaa\xa8\xe4\xbc\xa4\xe7\xa7\x91\xe5\xa6\x99\xe6\x96\xb9(\xe7\xac\xac\xe5\x9b\x9b\xe7\x89\x88)'
>>> regex = '[\\(\xef\xbc\x88\\[{\xe3\x80\x90]+(\\w+|\\s+|\\S+|\\W+)?[\xef\xbc\x89\\)\\]}\xe3\x80\x91]+'
>>> utf8_compiled = re.compile(regex, flags=re.I)
>>> utf8_compiled.sub('', columnString)
'\xe5\xbd\x93\xe4\xbb\xa3\xe9\xaa\xa8\xe4'
>>> print utf8_compiled.sub('', columnString).decode('utf8', 'replace')
当代骨�
>>> unicode_compiled = re.compile(regex.decode('utf8'), flags=re.I | re.U)
>>> unicode_compiled.sub('', columnString.decode('utf8'))
u'\u5f53\u4ee3\u9aa8\u4f24\u79d1\u5999\u65b9'
>>> print unicode_compiled.sub('', columnString.decode('utf8'))
当代骨伤科妙方
>>> print unicode_compiled.sub('', u'物理化学名校考研真题详解 (理工科考研辅导系列(化学生物类))')
物理化学名校考研真题详解 

When using UTF-8 in your pattern consists of separate bytes for the 【 codepoint:
>>> '【'
'\xe3\x80\x90'

which means your character class matches any of those bytes; \xe3, or \x80 or \x90 are each separately valid bytes in that character class.",A,1
54421867,2019-01-29 13:10:52.833000+00:00,"Just detach your TextIOWrapper() object before letting it be garbage collected:
def mangle(x):
    wrapper = io.TextIOWrapper(x)
    wrapper.detach()

The TextIOWrapper() object only closes streams it is attached to. If you can't alter the code where the object goes out of scope, then simply keep a reference to the TextIOWrapper() object locally and detach at that point.
If you must subclass TextIOWrapper(), then just call detach() in the __del__ hook:
class DetachingTextIOWrapper(io.TextIOWrapper):
    def __del__(self):
        self.detach()
",A,3
51285431,2018-07-11 12:24:49.623000+00:00,"Just put the ForeignKey on Comment, pointing to Company. This does exactly what you want.",A,0
52776031,2018-10-12 09:06:13.097000+00:00,"The problem is a simple typo: you have a space between the { and the % in your url tag. This is causing Django to not recognise it as a tag, so the Ajax is using the literal string ""{ % url ... }"" as the URL which explains the mess you see in the terminal. Remove the space.
(Note, you still might not get the result you expect, since your Ajax function returns a complete HTML page but you are inserting that result inside a div in an existing page; you probably either want to replace the whole page or return a template fragment from your view.)",A,0
37197913,2016-05-12 21:41:46.310000+00:00,"Add User-Agent http header as @Alik suggested. The question you've linked shows how. Here's an adaptation for Python 3:
#!/usr/bin/env python3
import urllib.request

opener = urllib.request.build_opener()
opener.addheaders = [('User-Agent', 'CERN-LineMode/2.15 libwww/2.17b3')]
urllib.request.install_opener(opener) #NOTE: global for the process

urllib.request.urlretrieve(url, filename)

See the specification for User-Agent. If the site rejects your custom User-Agent, you could send User-Agent produced by ordinary browsers such as fake_useragent.UserAgent().random.",A,0
6669054,2011-07-12 18:18:11.670000+00:00,"
Don't follow random directions on the internet. Follow the actual documentation on Django's website.
Don't use mod_python - it's deprecated. Use mod_wsgi.
Don't even think about PSP.
",A,0
21185485,2014-01-17 12:00:26.503000+00:00,"You are looking for the os.path module:
path = os.path.join(parentDir, filename)
",A,1
21329389,2014-01-24 09:49:28.270000+00:00,"You can use a for loop and break out:
counter = 0
for elem in MyList:
    if isinstance(elem, tuple):
        break
    counter += 1

or, using enumerate():
for counter, elem in enumerate(MyList):
    if isinstance(elem, tuple):
        break

# counter *could* be unbound if `MyList` is empty

or you could use itertools.takewhile():
from itertools import takewhile

counter = sum(1 for elem in takewhile(lambda e: not isinstance(e, tuple), MyList))

Demo of the latter approach:
>>> from itertools import takewhile
>>> MyList = [2,3,(1,2),5]
>>> sum(1 for elem in takewhile(lambda e: not isinstance(e, tuple), MyList))
2
",A,10
52495399,2018-09-25 09:57:32.943000+00:00,"Your JSON does not have a top-level result key. It has a response key, which itself contains a result. So:
{% for product in products.response.result %}
",A,1
26181176,2014-10-03 14:45:01.957000+00:00,"You'll need to store the output of the random.randint() functions first, not just print these:
 die1 = random.randint(MIN, MAX)
 die2 = random.randint(MIN, MAX)
 total = die1 + die2
 print(die1, '+', die2, '=', total)

You'll also have a problem with ending the loop, because you set the local name again, while testing the name Reroll instead. Because Reroll never changes, the loop will go on forever. Replace again with Reroll:
Reroll = input('Try for a higher roll? (y=yes)')

This needs to be indented to match the other lines in the loop!",A,1
51136524,2018-07-02 12:42:18.207000+00:00,"The Friend model is a bit strange and is causing you some difficulty. Really, Friend should be the through table in the many-to-many relationship between User and itself. Presumably you are using the built-in User model, which is why you have used an external model; but you can still simulate a through table with two foreign keys:
class Friend(models.Model):
    from_user = models.ForeignKey(User, related_name='users_from', on_delete=models.CASCADE)
    to_user = models.ForeignKey(User, related_name='users_to', on_delete=models.CASCADE)

Now your query can be:
Post.objects.filter(creator__users_to__from_user__username='admin')

that is, give me all Posts whose creator is on the ""to_user"" side of the friend relationship where the from_user's username is ""admin"".",A,0
39222221,2016-08-30 08:17:27.443000+00:00,"Django 1.11 is not released. You should use the actual released version, 1.10. Also, you should use the actual tutorial for your version.
However, the problem is that both of your files should define urlpatterns with an s, not urlpattern.",A,2
27080954,2014-11-22 18:31:29.777000+00:00,"Simply join your nested lists with the '\t' string:
newlist = ['\t'.join(map(str, sublist)) for sublist in outerlist]

The map(str, ...) is needed to convert your counts to strings first.
The numbers won't necessarily be centered, however; tab characters move the 'cursor' to the next tab position, which may be earlier than you expect if the preceding text is not long enough.
You could simply use string formatting to pad out the first column with spaces; you could base this on the maximum length of the first column:
max_team_width = max(len(item[0]) for item in outerlist)
max_count_width = max(len(str(item[1])) for item in outerlist)
for item in outerlist:
    print('{item[0]:{team_width}} {item[1]:{count_width}d}'.format(
        team_width=max_team_width, count_width=max_count_width, item=item))

This prints the items, but you could also just append the result to a list still.
The latter produces:
>>> team_counts = [
...     ['New York Giants', 5],
...     ['Anaheim Angels', 1],
...     ['Detroit Tigers', 4],
...     ['St. Louis Cardinals', 11],
...     ['Minnesota Twins', 2],
... ]
>>> max_team_width = max(len(item[0]) for item in team_counts)
>>> max_count_width = max(len(str(item[1])) for item in team_counts)
>>> for item in team_counts:
...     print('{item[0]:{team_width}} {item[1]:{count_width}d}'.format(
...         team_width=max_team_width, count_width=max_count_width, item=item))
... 
New York Giants      5
Anaheim Angels       1
Detroit Tigers       4
St. Louis Cardinals 11
Minnesota Twins      2

while the tab-separate output could be produced with:
>>> for item in team_counts:
...     print('\t'.join(map(str, item)))
... 
New York Giants 5
Anaheim Angels  1
Detroit Tigers  4
St. Louis Cardinals 11
Minnesota Twins 2
",A,0
42213924,2017-02-13 21:29:57.977000+00:00,"This isn't a thing you can do. Your subprocess call creates a subshell and sets the env var there, but doesn't affect the current process, let alone the calling shell.",A,1
16865988,2013-05-31 21:06:11.363000+00:00,"Call it in the .__init__() initializer then:
class Something(object):
    def __init__(self):
        self.our_random = Something.random_thing()

or call the static method after you defined it, but are still defining the class; because it is a static method, you'd have to access it through the __func__ attribute:
class Something(object):
    @staticmethod
    def random_thing():
        return 4

    our_random = random_thing.__func__()

If you didn't mean to call it, just create a copy of the method with a different name, just do so after you defined it:
class Something(object):
    @staticmethod
    def random_thing():
        return 4

    our_random = random_thing   # our_random as an alias for random_thing

The class body is executed as a function, with the local namespace of the function then forming the class attributes. So, like a function, if you want to refer to other objects you need to make sure they are defined first.",A,3
32559452,2015-09-14 07:36:55.633000+00:00,"You seem quite confused about how a JS frontend and a REST backend should work. There is no need for your TemplateView here. Your JS should be making the request directly to the DRF endpoint, which will return JSON directly.",A,1
47593440,2017-12-01 12:29:24.773000+00:00,"The first executes two more bytecodes, one for storing the result, the other for retrieving it again:
>>> def aplusb_local(a, b):
...     result = a + b
...     return result
...
>>> def aplusb_return(a, b):
...     return a + b
...
>>> import dis
>>> dis.dis(aplusb_local)
  2           0 LOAD_FAST                0 (a)
              2 LOAD_FAST                1 (b)
              4 BINARY_ADD
              6 STORE_FAST               2 (result)

  3           8 LOAD_FAST                2 (result)
             10 RETURN_VALUE    
>>> dis.dis(aplusb_return)
  2           0 LOAD_FAST                0 (a)
              2 LOAD_FAST                1 (b)
              4 BINARY_ADD
              6 RETURN_VALUE

You also need a little more local variable space in the function frame object:
>>> aplusb_local.__code__.co_nlocals
3
>>> aplusb_return.__code__.co_nlocals
2

The differences here are extremely marginal; a 0.038 second difference on 10 million calls on my Macbook Pro:
>>> from timeit import timeit
>>> timeit('aplusb(42, 81)', 'from __main__ import aplusb_local as aplusb', number=10**7)
1.0834476329619065
>>> timeit('aplusb(42, 81)', 'from __main__ import aplusb_return as aplusb', number=10**7)
1.045598166063428

This is not something you should be optimising for.
Optimise for readability instead.",A,4
14985574,2013-02-20 16:59:42.520000+00:00,"o is an instance of B, not A. When it is initialised, it sets the value of its test attribute to ""b"". At no point do you even call the superclass __init__, so it never gets set to ""a"" - but even if you did, it can only have one of the values ""a"" or ""b"", depending on whether you called super first or second. 
No idea why you think this breaks the Python object model. ",A,6
14497953,2013-01-24 09:30:14.753000+00:00,"You are looking at the representation of the Fault object; the string message itself is contained in the .faultString attribute:
print fault.faultString

The __repr__ of the Fault class otherwise represents that value using repr(); you cannot get around that without changing the xmlrpclib.Fault class itself (by replacing it's __repr__ method or adding a __str__ method to it).
You could monkey patch that into the class:
from xmlrpclib import Fault

def fault_repr(self):
    return ""<Fault %s: %s>"" % (self.faultCode, self.faultString)

Fault.__repr__ = fault_repr
",A,1
19697689,2013-10-31 02:54:53.087000+00:00,"You have two independent Python 2.7 installations, one in /usr and one in /usr/local. (And that's on top of the Python 3.x installation you also have.)
This is bound to cause confusion, especially for novices. And it has caused exactly the kind of consuion it was bound to cause.
You've installed PySide into the /usr installation, so it ended up in /usr/lib/python2.7/dist-packages. If you run /usr/bin/python, that import PySide will probably work fine. (If not, see below.)
But the default thing called python and python2.7 on your PATH is the /usr/local installation, hence which python says /usr/local/bin/python, so it can't see PySide at all. So you need to get it installed for the other Python as well.

Unless you know that you need a second Python 2.7 in /usr/local for some reason, the simplest thing to do would be to scrap it. Don't uninstall it and reinstall it; just uninstall it. You've already got a Python 2.7 in /usr, and you don't need two of them.

If you really need to get PySide working with the second 2.7…
Since you still haven't explained how you've been installing PySide despite being asked repeatedly, I can't tell you exactly how to do that. But generally, the key is to make sure to use explicit paths for all Python programs (python itself, python-config, pip, easy_install, etc.) that you have to run. For example, if the docs or blog or voices in your head tell you to run easy_install at some step, run /usr/local/bin/easy_install instead. If there is no such program, then you need to install that. The fact that you already have /usr/bin/easy_install doesn't help—in fact, it hurts.

If you can get rid of the second Python, but that doesn't fix PySide yet, uninstall, rebuild, and reinstall PySide. Or, even simpler… PySide has pre-made, working binary Ubuntu packages for all of the major Python versions that have Ubuntu packages. Just install it that way.",A,1
26047022,2014-09-25 19:56:46.553000+00:00,"It depends on what you mean by ""more efficient"".
Setting the variables to None, assuming they're the only references to their values, will allow the garbage collector to collect them. And in CPython (which uses ref counting for its garbage collector), it will even do so right away.
But on the other hand, you're also adding more bytecodes to the function that have to be executed by the interpreter, and that make the code object harder to keep in cache, and so on.
And keep in mind that freeing up memory almost never means actually freeing memory to the OS. Most Python implementations have multiple levels of free lists, and it usually sits on top of something like malloc that does as well. So, if you were about to allocate enough additional memory to increase your peak memory size, having a lot of stuff on the free list may prevent that; if you've already hit your peak, releasing values is unlikely to make any difference. (And that's assuming peak memory usage is what matters to your app—just because it's by far the easiest thing to measure doesn't mean it's what's most relevant to every problem.)
In almost all real-life code, this is unlikely to make any difference either way. If it does, you'll need to test, and to understand how things like memory pressure and cache locality are affecting your application. You may be making your code better, you may be making it worse (at least assuming that some particular memory measurement is not the only thing you care about optimizing), most likely you're having no effect but to make it longer and therefore less readable. This is a perfect example of the maxim ""premature optimization is the root of all evil"".


Does the answer change if we do this at the end of the function (since I assume python itself would do it at that point)?

You're right that Python frees local variables when the function returns. So yes, in that case, you're still getting almost all of the negatives with almost none of the positives, which probably changes the answer.

But, all those caveats aside, there are cases where this could improve things.* So, if you've profiled your app and discovered that holding onto that memory too long is causing a real problem, by all means, fix it!
Still, note that del temp_1 will have the same effect you're looking for, and it's a lot more explicit in what you're doing and why. And in most cases, it would probably be better to refactor your code into smaller functions, so that temp_1 and friends go out of scope as soon as you're done with them naturally, without the need for any extra work.
* For example, imagine that the rest of the function is just an exact copy of the first half, with three new values. Having a perfect set of candidates at the top of the free lists is probably better than having to search the free lists more deeply—and definitely better than having to allocate more memory and possibly trigger swapping…",A,14
17836219,2013-07-24 13:59:32.290000+00:00,"No; copying over collections.py may work, but is likely only the tip of the iceberg.
Django 1.5 requires Python 2.6 or newer. Do not try to run in on Python 2.5. You could downgrade to Django 1.4 instead, which does support Python 2.5.",A,2
38072052,2016-06-28 09:18:20.287000+00:00,"I have absolutely no idea what you are asking here. As Rajesh comments, if v1 is always calculated from the other elements, why are you accepting it in the form at all? 
It's possible that you are after this:
class F1Model(models.Model):
    ...
    def calculate(self):
        self.v1 = self.u1 + self.a1

class F1Form(forms.ModelForm):
    class Meta:
        exclude = ('v1',)

def f1(request):
    if request.method == 'POST':
        form = F1Form(request.POST)
        if form.is_valid():
            f1instance = form.save(commit=False)
            f1instance.calculate()
            f1instance.save()
            return redirect('question_list')
    else:
        form = F1Form()
    return render(request, 'question_list.html', {'form': form})

but it's impossible to tell for sure.",A,0
20479216,2013-12-09 19:40:56.620000+00:00,"Use a list comprehension:
splitstring = [float(s) for s in splitstring]

or, on Python 2, for speed, use map():
splitstring = map(float, splitstring)

When you loop over a list in Python, you don't get indexes, you get the values themselves, so c is not an integer but a string value ('500' in the first iteration).
You'd have to use enumerate() to generate indices for you, together with the actual values:
for i, value in enumerate(splitstring):
    splitstring[i] = float(value)

or use for c in range(len(splitstring)): to only produce indices. But the list comprehension and map() options are better anyway.",A,6
25835733,2014-09-14 16:59:42.490000+00:00,"GMT-4 is ambiguous: is it time in America/New_Your (-0400 utc offset) or in Europe/Moscow (+0400)?
$ TZ=GMT-4 date +%Z%z
GMT+0400
$ TZ=UTC-4 date +%Z%z
UTC+0400
$ TZ=America/New_York date +%Z%z
EDT-0400
$ TZ=Europe/Moscow date +%Z%z
MSK+0400

Your comment suggests that you need the sign of the utc offset reversed.
Python 2.6 has no fixed-offset timezones in stdlib. You could use the example implementation from the datetime docs:
from datetime import tzinfo, timedelta, datetime

ZERO = timedelta(0)

class FixedOffset(tzinfo):
    """"""Fixed UTC offset: `local = utc + offset`.""""""

    def __init__(self, offset, name):
        self.__offset = timedelta(hours=offset)
        self.__name = name

    def utcoffset(self, dt):
        return self.__offset

    def tzname(self, dt):
        return self.__name

    def dst(self, dt):
        return ZERO

utc = FixedOffset(0, ""UTC"")

Then to parse the time string, you could use strptime():
dt = datetime.strptime(""2014/08/19 03:38:46 GMT-4"", ""%Y/%m/%d %H:%M:%S GMT-4"")
aware = dt.replace(tzinfo=FixedOffset(-4, ""GMT-4""))
print(aware)                 # -> 2014-08-19 03:38:46-04:00
print(aware.astimezone(utc)) # -> 2014-08-19 07:38:46+00:00
",A,1
2421235,2010-03-10 22:46:02.273000+00:00,"You're right in your comment to Ignacio that select_related works in the opposite direction.
I've written about a technique to do it in this direction on my blog (sorry about the plug).",A,-1
13602429,2012-11-28 10:01:44.817000+00:00,"It'll be easier just to filter your string for digits and picking out the last 10:
''.join([c for c in mobilePhone if c.isdigit()][-10:])

Result:
>>> mobilePhone = ""07870 622103""
>>> ''.join([c for c in mobilePhone if c.isdigit()][-10:])
'7870622103'
>>> mobilePhone = ""(0) 07543 876545""
>>> ''.join([c for c in mobilePhone if c.isdigit()][-10:])
'7543876545'
>>> mobilePhone = ""07321 786543 - not working""
>>> ''.join([c for c in mobilePhone if c.isdigit()][-10:])
'7321786543'

The regular expression approach (filtering everything but digits), is faster though:
$ python -m timeit -s ""mobilenum='07321 786543 - not working'"" ""''.join([c for c in mobilenum if c.isdigit()][-10:])""
100000 loops, best of 3: 6.68 usec per loop
$ python -m timeit -s ""import re; notnum=re.compile(r'\D'); mobilenum='07321 786543 - not working'"" ""notnum.sub(mobilenum, '')[-10:]""
1000000 loops, best of 3: 0.472 usec per loop
",A,3
32774741,2015-09-25 04:18:07.910000+00:00,"chardet detects BOM_UTF8 automatically since 2.3.0 version released on Oct 7, 2014:
#!/usr/bin/env python
import chardet # $ pip install chardet

# detect file encoding
with open(filename, 'rb') as file:
    raw = file.read(32) # at most 32 bytes are returned
    encoding = chardet.detect(raw)['encoding']

with open(filename, encoding=encoding) as file:
    text = file.read()
print(text)

Note: chardet may return 'UTF-XXLE', 'UTF-XXBE' encodings that leave the BOM in the text. 'LE', 'BE' should be stripped to avoid it -- though it is easier to detect BOM yourself at this point e.g., as in @ivan_pozdeev's answer.
To avoid UnicodeEncodeError while printing Unicode text to Windows console, see Python, Unicode, and the Windows console.",A,7
44415943,2017-06-07 14:46:57.157000+00:00,"You can use aiohttp; you'll need to translate your http.client code to use aiohttp client API. You can re-use your ssl.SSLContext() object; pass it to a TCPConnector() instance, then create a client from that:
import aiohttp
import ssl


SSL_CONTEXT = ssl.SSLContext(protocol=ssl.PROTOCOL_TLS)
SSL_CONTEXT.load_cert_chain(certfile='foo', keyfile='bar')


async def fetch_url(url):
    conn = aiohttp.TCPConnector(ssl_context=SSL_CONTEXT)
    async with aiohttp.ClientSession(conn=connector) as client:
        async with client.get(url) as response:
            print(resp.status)
            print(await resp.text())

Note that you can't share the connector between client sessions; if you want to re-use, reuse the client.",A,1
34029135,2015-12-01 20:04:43.093000+00:00,"When using etree.parse(), simply call .getroot() to get the root element; the .attrib attribute is a dictionary of all attributes, use that to get the value:
>>> from lxml import etree
>>> tree = etree.parse('test.xml')
>>> tree.getroot().attrib['id']
'123'

If you used etree.fromstring() the object returned is the root object already, so no .getroot() call is needed:
>>> tree = etree.fromstring('''\
... <?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?>
... <item id=""123"">
...     <sub>ABC</sub>
... </item>
... ''')
>>> tree.attrib['id']
'123'
",A,2
19457404,2013-10-18 19:15:11.643000+00:00,"You are comparing a string to an integer:
age = int(input())

if age >= ""13"":

In Python 2, a string is always larger than a number. Use numbers instead:
if age >= 13:

In Python 3, comparing strings with integers like that raises an error instead:
>>> 13 >= ""13""
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unorderable types: int() >= str()

giving you a clearer message about what you are doing wrong.",A,6
1300789,2009-08-19 15:37:20.347000+00:00,"Your view doesn't have to do much.
tasks = Tasks.objects.all()

Provide this to your template.
Your template can then do something like the following.
{% for t in tasks %}
    name: {{t.name}}
    description: {{t.description}}
    priority: **{{t.priority.name}}**
{% endfor %}
",A,2
40201951,2016-10-23 10:33:57.567000+00:00,"You are passing in a string here:
sentence = ""All good\tthings come to those who wait.""
# ...
print_first_word(sentence)

You are meant to pass in a list; presumably you meant to pass in words instead; that's a list (the result of the str.split() call):
words = sentence.split()
",A,0
18325111,2013-08-20 00:28:25.440000+00:00,"There's no exactly-compatible class, but ExecutorService gives you everything you need to implement it.
In particular, there's no function to map a Callable over a Collection and wait on the results, but you can easily build a Collection<Callable<T>> out of a Callable<T> and Collection<T>, then just call invokeAll, which returns you a List<Future<T>>.
(If you want to emulate some of the other functions from multiprocessing.Pool, you will need to loop around submit instead and build your own collection of things to wait on. But map is simple.)",A,4
4353642,2010-12-04 13:11:08.170000+00:00,"
Is this still viable with modern browsers? 

Yes.  And browsers are not the issue.  
Non-browser applications can make HTTP requests, also.  Applications like curl can probably be used for this kind of thing.  Or you can write something in Python using urllib2 to do CSRF's.  You can easily fake out responses of all kinds if the framework doesn't properly include CSRF tokens.

Should I use tokens to prevent this?

No.  
You should find a framework that provides you the support for handling this.",A,-4
52041672,2018-08-27 14:38:18.497000+00:00,"Your code needs to be inside the handle method. As it is, it's at module level, so is executed when the script is imported, before the settings have been properly activated.
The fact that you had to put a pass in that handle method should have given you a clue that somethng was wrong. Put the rest of the code there instead.",A,0
43864651,2017-05-09 08:20:33.940000+00:00,"No, it's no safer than using exec(). Both importing and exec() compile the text to bytecode and then execute the bytecode. The generated bytecode will be exactly the same.
The only difference will be that a .pyc bytecode cache file can be created for a file on disk, while exec() will re-compile the text each and every time. If you generate the file just once, then import it across Python invocations, then writing the file to disk is going to benefit from that cache.",A,3
35943888,2016-03-11 15:22:21.060000+00:00,"Use a nested defaultdict.
myDict = defaultdict(lambda: defaultdict(list))
",A,3
26004400,2014-09-23 20:43:31.710000+00:00,"But that's the point, you don't need to know. A super call must always use the class that you define it in. Python resolves the reference using the MRO.
class RedirectMixin(object)
    def response_add(...):
        ...
        return super(RedirectMixin, self).response_add(...)
",A,1
28085133,2015-01-22 09:25:33.090000+00:00,"To avoid bunzip2 dependency, you could pump the input manually:
import subprocess
import threading
from contextlib import closing

p = subprocess.Popen(['tcpdump', '-lnr', '-s', '0', '-'],
                     stdin=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=-1)
threading.Thread(target=pump, args=[filename, p.stdin]).start()
with closing(p.stdout):
     for line in iter(p.stdout.readline, b''):
         print line,
p.wait()

where pump() is:
from shutil import copyfileobj

def pump(filename, pipe):
    """"""Decompress *filename* and write it to *pipe*.""""""
    with closing(pipe), bz2.BZ2File(filename) as input_file:
         copyfileobj(input_file, pipe)
",A,2
25588471,2014-08-31 02:26:50.740000+00:00,"You are using two assignment statements. Perhaps you should give dict.get() a second argument:
day_num = weekdays.get(day, -1)

That second argument is a default to return if day is not present in the dictionary; the default default is None but here I return -1 instead.
This saves you a test for is None and a second assignment.",A,1
19103336,2013-09-30 20:35:17.183000+00:00,"Use a simple loop:
for y in range(1, 9):
    for x in range(4):
        base = 20 if y % 2 == 0 else 10
        canvas.drawRect(base + (x * 20), y * 10, 10, 10)

Here y ranges from 1 to 8 inclusive, and multiplying that by 10 gives us 10, 20, 30, 40, etc. For every y, x ranges from 0 to 3 inclusive, and multiplying that by 20 gives 0, 20, 40, and 60. Adding 10 or 20 gives you your range of 10 - 70 or 20 - 80.
Using itertools.product() would let you eliminate one loop:
from itertools import product

for y, x in product(range(1, 9), range(4)):
    base = 20 if y % 2 == 0 else 10
    canvas.drawRect(base + (x * 20), y * 10, 10, 10)

Quick demo just printing the coordinates used:
>>> from itertools import product
>>> for y, x in product(range(1, 9), range(4)):
...     base = 20 if y % 2 == 0 else 10
...     print (base + (x * 20), y * 10)
... 
(10, 10)
(30, 10)
(50, 10)
(70, 10)
(20, 20)
(40, 20)
(60, 20)
(80, 20)
(10, 30)
(30, 30)
(50, 30)
(70, 30)
(20, 40)
(40, 40)
(60, 40)
(80, 40)
(10, 50)
(30, 50)
(50, 50)
(70, 50)
(20, 60)
(40, 60)
(60, 60)
(80, 60)
(10, 70)
(30, 70)
(50, 70)
(70, 70)
(20, 80)
(40, 80)
(60, 80)
(80, 80)
",A,0
1317093,2009-08-22 21:34:37.337000+00:00,"Use this.
BigDecimal truncated= number.setScale(0,BigDecimal.ROUND_DOWN);
",A,38
16387501,2013-05-05 18:08:57.447000+00:00,"Two options:

Choose to interpret it as JSON; that format uses the same escape codes. The input does need to have quotes around it to be seen as a string.
Encode to latin 1 (to preserve bytes), then decode with the unicode_escape codec:
>>> urllib.parse.unquote(sig).encode('latin1').decode('unicode_escape')
'45C482D2486105B02211ED4A0E3163A9F7095E81.4DDB3B3A13C77FE508DCFB7C6CC68957096A406C&type=video/3gpp;+codecs=""mp4v.20.3,+mp4a.40.2""&quality=small&itag=17&url=http://r6---sn-cx5h-itql.c.youtube.com/videoplayback?source=youtube&mt=1367776467&expire=1367797699&itag=17&factor=1.25&upn=pkX9erXUHx4&cp=U0hVTFdUVV9OU0NONV9PTllHOnhGdTVLUThqUWJW&key=yt1&id=ab9b0e2f311eaf00&mv=m&newshard=yes&ms=au&ip=49.205.30.138&sparams=algorithm%2Cburst%2Ccp%2Cfactor%2Cid%2Cip%2Cipbits%2Citag%2Csource%2Cupn%2Cexpire&burst=40&algorithm=throttle-factor&ipbits=8&fexp=917000%2C919366%2C916626%2C902533%2C932000%2C932004%2C906383%2C904479%2C901208%2C925714%2C929119%2C931202%2C900821%2C900823%2C912518%2C911416%2C930807%2C919373%2C906836%2C926403%2C900824%2C912711%2C929606%2C910075&sver=3&fallback_host=tc.v19.cache2.c.youtube.com'


This interprets \u escape codes just like it Python would do when reading string literals in Python source code.",A,4
22641615,2014-03-25 17:06:22.790000+00:00,"You could use stdout parameter to redirect output from a subprocess to a file:
import shlex
from subprocess import check_call

with open('outputfilename', 'wb', 0) as outputfile:
    check_call(shlex.split(cmd), stdout=outputfile)


Do you know what I should add to the command to prevent the subprocess from printing warning/errors to the shell ?

Just set stderr=subprocess.STDOUT to merge stdout/stderr. Or redirect stderr to os.devnull to discard the output.",A,2
18308594,2013-08-19 07:33:55.140000+00:00,"requests does not handle parsing XML responses, no. XML responses are much more complex in nature than JSON responses, how you'd serialize XML data into Python structures is not nearly as straightforward.
Python comes with built-in XML parsers. I recommend you use the ElementTree API:
import requests
from xml.etree import ElementTree

response = requests.get(url)

tree = ElementTree.fromstring(response.content)

or, if the response is particularly large, use an incremental approach:
response = requests.get(url, stream=True)
# if the server sent a Gzip or Deflate compressed response, decompress
# as we read the raw stream:
response.raw.decode_content = True

events = ElementTree.iterparse(response.raw)
for event, elem in events:
    # do something with `elem`

The external lxml project builds on the same API to give you more features and power still.",A,149
936622,2009-06-01 20:31:54.887000+00:00,"There are two ways of populating a Django form.
The first is to pass a dictionary as the first argument when you instantiate it (or pass it as the data kwarg, which is the same thing). This is what you do when you want to use POST data to populate and validate the form.
data_dict = {'charfield1': 'data1', 'charfield2': 'data2', 'choicefield': 3}
form = MyForm(data_dict)

However, this will trigger validation on the form, so only works if you are actually passing in valid and complete data to begin with - otherwise you will start off with errors.
The other way to populate a form is to use the initial parameter (documented here). This gives initial values for the form fields, but does not trigger validation. It's therefore suitable if you're not filling in all values, for example.
form = MyForm(initial=data_dict)

To populate a choicefield via initial, use the pk value. ",A,25
21695915,2014-02-11 07:43:01.293000+00:00,"The datastore is not a relational database and does not allow you to do things like aggregation. If you need to calculate things like averages across the whole data set, you need to use the mapreduce framework, which uses offline tasks to run a mapper over each entity in a kind. ",A,1
13352245,2012-11-12 21:56:25.307000+00:00,"Use a sequence of two-value tuples instead:
payload = (('key1', 'value1'), ('key2', 'value2'))
r = requests.get(""http://httpbin.org/get"", params=payload)

The reason requests doesn't retain ordering when using a dictionary is because python dictionaries do not have ordering. A tuple or a list, on the other hand, does, so the ordering can be retained.
Demo:
>>> payload = (('key1', 'value1'), ('key2', 'value2'), ('key3', 'value3'))
>>> r = requests.get(""http://httpbin.org/get"", params=payload)
>>> print r.json
{u'url': u'http://httpbin.org/get?key1=value1&key2=value2&key3=value3', u'headers': {u'Content-Length': u'', u'Accept-Encoding': u'gzip, deflate, compress', u'Connection': u'keep-alive', u'Accept': u'*/*', u'User-Agent': u'python-requests/0.14.1 CPython/2.7.3 Darwin/11.4.2', u'Host': u'httpbin.org', u'Content-Type': u''}, u'args': {u'key3': u'value3', u'key2': u'value2', u'key1': u'value1'}, u'origin': u'109.247.40.35'}
",A,7
9487271,2012-02-28 18:01:42.987000+00:00,"There's a secret to this.  A Redirect should points to a view which gets the meeting AND the participants.
Often, that's a simple view function that handles simple GET requests and returns the meeting and the list of participants. 
If you're going to redirect back to this addMeeting view function, then the GET processing needs to query meetings AND the participants.
That means that the render_to_response must include the meeting AND the participants.
return render_to_response('MeetingHub/addmeeting.html', 
    {'meetingform': meetingform, 'message':message,
    'meeting': meeting, 'participants': participants,
    #... and anything else that might be helpful to show on the page
    },
    context_instance=RequestContext(request))
",A,3
48151187,2018-01-08 13:20:34.817000+00:00,"Prefix your command with ! to disable the pdb magic.
ipdb> !args
",A,2
16464125,2013-05-09 14:24:54.607000+00:00,"If you want to insert values into a row, you need to pass those values along as SQL parameters to the .execute() call:
with con:
    for i in xrange(0,5):
        tmp1 = array[i, 0]
        tmp2 = array[i, 1]
        cur.execute(""""""INSERT INTO TESTTABLE VALUES(?, ?)"""""", (tmp1, tmp2))

The ? characters are parameters, and they are filled, in order, by values takes from the second argument to .execute(), a tuple. The above code will insert the numbers 0 through to 4 as pairs into the database.
Names in the SQL code have no correlation to names you define in Python, values can only be passed in explicitly.",A,2
828978,2009-05-06 10:25:10.817000+00:00,"Yes.
C++ with a smart pointer implementation will garbage collect as the smart pointer reference counts go to zero.
You have garbage collection.  You did not build it yourself.",A,0
29087870,2015-03-16 22:09:59.753000+00:00,"Your CSV file is encoded to CP-1252, you'd have to re-code that to UTF-8:
r = r.decode('cp1252').encode('utf8')

Your plain Python code was using UTF-8 bytes; provided your code editor indeed saved the data as UTF-8 as your coding: utf-8 header implies.
Just putting a PEP 263 header in your Python source file doesn't magically make all data you read from a file UTF-8 data too; it'll still need to be decoded with the correct codec for that file.",A,2
31222316,2015-07-04 15:32:29.990000+00:00,"Your output is incorrect; you print an empty line before the stairs that should not be there. Your range() loop starts at 0, so you print n spaces and zero # characters on the first line.
Start your range() at 1, and n should start at num_stairs - 2 (as the , in print adds a space:
def staircase(num_stairs):
    n = num_stairs - 2
    for stairs in range(1, num_stairs):
        print ' ' * n, '#' * stairs
        n -= 1
    print '#' * num_stairs

You can simplify this to one loop:
def staircase(num_stairs):
    for stairs in range(1, num_stairs + 1):
        print ' ' * (num_stairs - stairs) + '#' * stairs

Note that I use concatenation now to combine spaces and # characters, so that in the last iteration of the loop zero spaces are printed and num_stairs # characters.",A,11
51199413,2018-07-05 20:30:55.933000+00:00,"You can use threads here, and it'll work fine, but it will require a few changes. First, the scheduler on your background thread is going to try to kick off a new recvfrom every second, no matter how long the last one took. Second, since both threads are apparently trying to call the same writeToBuffer function, you're probably going to need a Lock or something else to synchronize them.
Rewriting the whole program around an asynchronous event loop is almost certainly overkill here.
Just changing the socket to be nonblocking and doing a hybrid is probably the simplest change, e.g., by using settimeout:
# wherever you create your socket
sock.settimeout(0.8)

# ...

def readDataFromUDP():
    udpData = []
    try:
        rcvData, addr = sock.recvfrom(256)
    except socket.timeout:
        return
    udpData.append(rcvData.decode('ascii'))

    if(len(udpData)>0):
        writeToBuffer(udpData)

Now, every time you call recvfrom, if there's data available, you'll handle it immediately; if not, it'll wait up to 0.8 seconds, and then raise an exception, which means you have no data to process, so go back and wait for the next loop. (There's nothing magical about that 0.8; I just figured something a little less than 1 second would be a good idea, so there's time left to do all the other work before the next schedule time hits.)
Under the covers, this works by setting the OS-level socket to non-blocking mode and doing some implementation-specific thing to wait with a timeout. You could do the same yourself by using setblocking(False) and using the select or selectors module to wait up to 0.8 seconds for the socket to be ready, but it's easier to just let Python take care of that for you.",A,1
4658218,2011-01-11 13:41:49.227000+00:00,"The way you're doing this, places is a standard list, not a QuerySet, and collect is a method that only exists on GeoDjango QuerySets.
You should be able to do the whole query in one go by following the relations with the double-underscore syntax:
places = Place.objects.filter(manor__lord=person)

Note that your use of related_name=""place"" on the Manor.place field is very confusing - this is what sets the reverse attribute from Place back to Manor, so it should be called manors.",A,1
14349707,2013-01-16 00:51:10.490000+00:00,"You have your patterns and text-to-search mixed up.
You are looking for testing in text test, and the latter is not nearly long enough. :-)
If you reversed the two (pattern test, text testing) things work:
>>> import re
>>> pattern = '''
... test
... '''
>>> print(re.search(pattern, 'testing', re.VERBOSE))
<_sre.SRE_Match object at 0x1062f4c60>
",A,4
45042773,2017-07-11 19:11:17.257000+00:00,This has nothing to do with boolean fields. The error is telling you that your specific User does not have a related entry in the UserProfile table.,A,0
17432500,2013-07-02 17:55:09.013000+00:00,"It looks like you've run into a bug in requests 0.10.0 (or, really, in urllib3) with HTTPS in Python 2.5 with the ssl module installed.
If you trace through the 0.10.0 source, you can see that if ssl is installed and you make an HTTPS requests, you are going to get to the VerifiedHTTPSConnection.connect method. This is also explained in comments in the HTTPSConnectionPool source. But you don't really even need to trace through the source, because you already saw that from your tracebacks.
And if you look at the source to that method, it unconditionally calls socket.create_connection, which is guaranteed to fail in 2.5.
The odds that anyone is ever going to fix this bug are pretty minimal. It looks like it was introduced in 0.10.0, and 0.10.1 resolved it by just dropping 2.5 support. (I'm not positive about that, because I can't find it in the bug tracker.)
So, what can you do about it?
First, note that, while create_connection is ""higher level"" than connect, there are only real advantage is that it does the name lookup before deciding what kind of socket to create. If you know you only ever care about IPv4, you can replace it with this:
self.sock = socket.socket()
self.sock.settimeout(self.timeout)
self.sock.connect((self.host, self.port))

If you care about IPv6, you can just borrow the 2.6 code for create_connection instead.
So, you have a few options:

Fork the source and patch urllib3.connectionpool.VerifiedHTTPConnection.connect to use the workaround instead of create_connection.
Monkeypatch urllib3.connectionpool.VerifiedHTTPConnection.connect at runtime.
Monkeypatch socket at runtime to add a create_connection implementation.

However, I wouldn't want to guarantee that 0.10.0 won't have further problems with Python 2.5, given the history.",A,1
7064808,2011-08-15 12:19:33.367000+00:00,The database is rolled back at the end of every test.,A,2
23740961,2014-05-19 14:55:04.217000+00:00,"Python's for statement iterates over the items of any iterable object, not just sequences:

The for statement is used to iterate over the elements of a sequence (such as a string, tuple or list) or other iterable object

Iterables are objects that either have an __iter__ method or a __getitem__ method.
file objects implement an __iter__ method; they also are iterator objects, in that they have a __next__ method (Python 3) or next method (Python 2), so all the __iter__ method of a file object has to do is return self.
For Python 3, also see the io.IOBase documentation:

IOBase (and its subclasses) supports the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream.
",A,1
26197696,2014-10-04 22:10:52.360000+00:00,"You are looking for the Longest Common Subsequence algorithm; the following uses dynamic programming to find the elements in O(NM) time (for sequences of length N and M):
def lcs(a, b):
    tbl = [[0 for _ in range(len(b) + 1)] for _ in range(len(a) + 1)]
    for i, x in enumerate(a):
        for j, y in enumerate(b):
            tbl[i + 1][j + 1] = tbl[i][j] + 1 if x == y else max(
                tbl[i + 1][j], tbl[i][j + 1])
    res = []
    i, j = len(a), len(b)
    while i and j:
        if tbl[i][j] == tbl[i - 1][j]:
            i -= 1
        elif tbl[i][j] == tbl[i][j - 1]:
            j -= 1
        else:
            res.append(a[i - 1])
            i -= 1
            j -= 1
    return res[::-1]

Demo:
>>> def lcs(a, b):
...     tbl = [[0 for _ in range(len(b) + 1)] for _ in range(len(a) + 1)]
...     for i, x in enumerate(a):
...         for j, y in enumerate(b):
...             tbl[i + 1][j + 1] = tbl[i][j] + 1 if x == y else max(
...                 tbl[i + 1][j], tbl[i][j + 1])
...     res = []
...     i, j = len(a), len(b)
...     while i and j:
...         if tbl[i][j] == tbl[i - 1][j]:
...             i -= 1
...         elif tbl[i][j] == tbl[i][j - 1]:
...             j -= 1
...         else:
...             res.append(a[i - 1])
...             i -= 1
...             j -= 1
...     return res[::-1]
... 
>>> list1 = [1, 2, 3, 4, 5, 6, 7]
>>> list2 = [7, 6, 3, 4, 5, 8]
>>> lcs(list1, list2)
[3, 4, 5]

This will find the subsequence regardless of location and if other elements are mixed in between:
>>> lcs([1, 2, 3, 4, 5, 6, 7], [7, 3, 6, 4, 8, 5])
[3, 4, 5]
",A,3
14287995,2013-01-11 22:55:09.820000+00:00,"def make_generic_getter(name, maxlen):
    def getter(self):
        value = getattr(self, name)
        r_str = """"
        if len(value) > maxlen:
            r_str = value[:maxlen]
        else:
            r_str = value.strip()
        return r_str.strip()
    return getter

Now, you can do this:
class Foo(object):
    def __init__(self):
        self._Bar = 'abc'
        self._Baz = 'def'
    GetBar = make_generic_getter('_Bar', 5)
    GetBaz = make_generic_getter('_Baz', 2)

Then:
>>> f = Foo()
>>> f.GetBar()
'abc'
>>> f.GetBaz()
'de'

Clearly, there's also a lot of repetitive and unnecessary stuff in the original function. (And it would be much better to use PEP8-style names for your properties.) But obviously it's much easier to refactor first, then improve, than the other way around. (In other words, start here, but don't stop here.)
From the comments:

How does the method maker get the ""self"" reference?

The method maker doesn't actually get the self reference. There is no self reference to get at the time the method maker is being called. But there also is no self reference to get for a normal method at the time the class is being defined. In either case, you're just defining a function that takes self as its first parameter, and it somehow magically gets the appropriate self when you call it.
To really understand how this actually works, you need to know about descriptors. See Implementing Descriptors and Invoking Descriptors (or the 3.3 version), read it over a few times, look at how the @property decorator is implemented, play around in the interactive interpreter, give up, go to sleep, and try again tomorrow, and it should all click. But it's easier if you learn the magic version first, so let's do that, using a simpler example:
>>> def func(self): pass
>>> class C(object):
...     def meth(self): pass
...     fake1 = func
>>> C.fake2 = func
>>> func, C.meth, C.fake1, C.fake2
(<function __main__.func>, <unbound method C.meth>, <unbound method C.func>, <unbound method C.func>)

An unbound method is just a thing with an im_class holding its class, an im_func holding a normal function, and an im_self holding None. And when you do fake1 = func in the class definition, or C.fake2 = func after the fact, you don't actually end up with func itself as the value of fake1 or fake2, but with an unbound method wrapped around func, its im_class pointing at C.
>>> c = C()
>>> c.meth, c.fake1
(<bound method C.meth of <__main__.C object at 0x111ebb0d0>>, <bound method C.meth of <__main__.C object at 0x111ebb0d0>>)

When you take an instance of a class, all of its unbound methods become bound methods. If you look at the bound methods' attributes, they're the same as the unbound methods, except that im_self is c instead of None. And when you call c.fake1(), that's how it works—Python sees that c.fake1 is a bound method, so, in effect, it calls c.fake1.im_func(c.fake1.im_self). And that's how fake gets its self parameter.
(This all becomes simpler in Python 3, because there's no such thing as unbound methods anymore, but I assume you care more about Python 2 given that you're dealing with a huge mess of legacy code.)",A,11
19748505,2013-11-03 00:24:49.657000+00:00,"Use key assignment:
people = {}

for line in f:
    tmp = l.rstrip('\n').split(',')
    people[tmp[2]] = tmp[0]

This loops over the file object directly, no need for .readline() calls here, and removes the newline.
You appear to have CSV data; you could also use the csv module here:
import csv

people = {}

with open(""people.in"", ""rb"") as f:
    reader = csv.reader(f)
    for row in reader:
        people[row[2]] = row[0]

or even a dict comprehension:
import csv

with open(""people.in"", ""rb"") as f:
    reader = csv.reader(f)
    people = {r[2]: r[0] for r in reader}

Here the csv module takes care of the splitting and removing newlines.
The syntax error stems from trying close the opening { with a ) instead of }:
people = {tmp[2] : tmp[0])  # should be }

If you need to collect multiple entries per row[2] value, collect these in a list; a collections.defaultdict instance makes that easier:
import csv
from collections import defaultdict

people = defaultdict(list)

with open(""people.in"", ""rb"") as f:
    reader = csv.reader(f)
    for row in reader:
        people[row[2]].append(row[0])
",A,3
7985014,2011-11-02 17:53:35.937000+00:00,"Just pass Unicode string to json.loads():
>>> badstr = ""qualité""[:-1]+""..""
>>> badstr
'qualit\xc3..'
>>> json_str = '[""%s""]' % badstr
>>> import json
>>> json.loads(json_str)
Traceback (most recent call last):
 ...
UnicodeDecodeError: 'utf8' codec can't decode byte 0xc3 in position 6: invalid \
continuation byte
>>> json.loads(json_str.decode('utf-8','ignore'))
[u'qualit..']
",A,8
25518383,2014-08-27 03:14:39.843000+00:00,"The u is not part of the string, any more than the quotes are. The u'go' is just how Python represents a Unicode string whose value is go. You do not need to ""concatenate the u"" or anything like that.
The raw_input will return 'go', rather than u'go', because it's reading in an encoded byte string. But in Python 2.x, if you compare those two strings, they're still equal. Try it:
>>> 'go' == u'go'
True

So, there is no problem here.
However, as soon as you start dealing with non-ASCII usernames or passwords, then you are going to have a problem. You will need to call decode on the values you got from the user, using the input's encoding, like this:
>>> self.username = raw_input(""Enter Username> "").decode(sys.stdin.encoding)

This is a bit clumsy, but hey, Unicode is clumsy in Python 2.x, that's why Python 3.x was invented.

There is, however, a bug in your code that may be causing whatever problem you're seeing:
for k, v in self.file:

When you loop over a dictionary, you loop over its keys, not its key-value pairs. So, each username will be unpacked into k and v. If you have any username that aren't exactly 2 characters long, you will get ValueError: too many values to unpack. But because you happen to have only one username, and it happens to be exactly 2 characters long, that u'go' gets unpacked into u'g' and u'o'. So, instead of comparing the username to go and the password to go, you end up comparing the username to g and the password to o, which doesn't match.
If you want to iterate over key-value pairs, use for k, v in self.file.items():.

But you usually don't want to iterate through a dict's items to search it either. The whole point of a dict is that you can look things up instantly. Instead of this:
for k, v in self.file:
    if k == self.username and v == self.password:
        print ""It worked""
    else:
        print ""Fail""

… just do this:
if self.password == self.file.get(self.username):
    print ""It worked""
else:
    print ""Fail""

Or, if you want to distinguish ""wrong password"" from ""unknown user"":
try:
    if self.password == self.file[self.username]:
        print ""It worked""
    else:
        print ""That's the wrong password, you evil hacker""
except KeyError:
    print ""I've never heard of you""
",A,3
22305086,2014-03-10 15:58:28.123000+00:00,"You need to tell Django not to create the standard auth.User model. You do that by specifying AUTH_USER_MODEL in settings.py:
AUTH_USER_MODEL = 'bot_data.Employee'
",A,2
13691273,2012-12-03 20:36:37.590000+00:00,"Jochen Ritzel's answer is the right way to do this 99.9999% of the time:
variable = d.get(""the key"", ""something"")

As he notes, it doesn't allow you to short-circuit the evaluation of the default value. You usually don't care. It the default value is ""something"", you certainly don't. It only matters if the default is either dangerous, or very expensive, to generate.
Only in that case, you can and should use your idea. Except that you want in instead of has_key (because it's more readable, and faster, and not deprecated):
variable = d[""the key""] if ""the key"" in d else expensiveComputation()

However, it's probably worth using an if statement instead of a ternary expression, because the fact that you're avoiding the expensiveComputation is important, and you want it to be more visible:
if ""the_key"" in d:
    variable = d[""the key""]
else:
    variable = expensiveComputation()

If you expect the default case to be rare, this is better:
try:
    variable = d[""the key""]
except KeyError:
    variable = expensiveComputation()

If for some reason you need to avoid looking the key up twice, and you also can't deal with exceptions, and you need to short-circuit the default, all at once:
sentinel = object()
variable = d.get(sentinel)
if variable == sentinel:
    variable = expensiveComputation()

And yes, you could wrap that all up in a function so it's a one-liner, but you almost certainly don't want to hide the fact that you're doing three rare things all at once.
Or, of course, you could just make do this:
d = collections.defaultdict(expensiveComputation)

Then, it's just:
variable = d[""the key""]

This has the side-effect of setting d[""the key""] to expensiveComputation() before returning it to you, so a later call to d[""the key""] will return the same value. If that sounds appropriate, this is the best answer; if you're never going to use the same key twice, or these things are huge and wasteful to keep around, etc., this is a bad idea.
Or, alternatively, you can override the dict.__missing__ method instead of using defaultdict:
class MyDict(dict):
    def __missing__(self, key):
        return expensiveComputation()

Then, again, it's just:
variable = d[""the key""]

This one is appropriate when you want to generate the value separately each time, and not keep it around for later.",A,6
29548794,2015-04-09 20:54:17.783000+00:00,"You are passing your values to JavaScript, and it is ignoring the leading zeros on the integer literal you gave it. This is hardly Python or Flask's fault.
Make it a string literal by wrapping the value in quotes:
{% extends ""maptemplate.html"" %}
{% block mapcustom %}
console.log( ""hello"" );
console.log( ""{{propnris}}"" );
addSingleSiteLayer(map, ""{{propnris}}"" );
{% endblock %}

or better still, format it as JSON to have any quoting issues handled for you:
{% extends ""maptemplate.html"" %}
{% block mapcustom %}
console.log( ""hello"" );
console.log( {{propnris|tojson|safe}} );
addSingleSiteLayer(map, {{propnris|tojson|safe}} );
{% endblock %}

as Flask's tojson filter produces valid JavaScript-compatible JSON output this can be interpreted directly as JavaScript literals, and a Python string is rendered properly quoted.",A,3
15690947,2013-03-28 19:54:14.647000+00:00,"First, this is a variant dialect of CSV, and can be parsed with the csv module instead of trying to do it manually. For example:
with open('TEXT.txt') as fd:
    rows = csv.reader(fd, delimiter='|')
    to_search = {row[1]:row for row in rows}
    print('\n'.join(to_search[name]))

You might also prefer to use DictReader, so each row is a dict (keyed off the names in the header row, or manually-specified column names if you don't have one):
with open('TEXT.txt') as fd:
    rows = csv.DictReader(fd, delimiter='|')
    to_search = {row['Name']:row for row in rows}
    print('\n'.join(to_search[name]))

Then, to select a specific attribute:
with open('TEXT.txt') as fd:
    rows = csv.DictReader(fd, delimiter='|')
    to_search = {row['Name']:row for row in rows}
    print(to_search[name][attribute])

However… I'm not sure this is a good design in the first place. Do you really want to re-read the entire file for each lookup? I think it makes more sense to read it into memory once, into a general-purpose structure that you can use repeatedly. And in fact, you've almost got such a structure:
with open('TEXT.txt') as fd:
    monsters = list(csv.DictReader(fd, delimiter='|'))
monsters_by_name = {monster['Name']: monster for monster in monsters}

Then you can build additional indexes, like a multi-map of monsters by location, etc., if you need them.

All this being said, your original code can almost handle what you want already. to_search[name] is a list. If you just build a map from attribute names to indices, you can do this:
attributes = ['Name', 'NUM 1', 'DESC 1', 'TYPE', 'LOCATION', 'STARS', 'ATK', 'DEF', 'DESCRIPTION']
attributes_by_name = {value: idx for idx, value in enumerate(attributes)}
# ...
with open('TEXT.txt') as fd:
     input=[x.strip('|').split('|') for x in fd.readlines()]
     to_search={x[0]:x for x in input}
     attribute_index = attributes_by_name[attributes]
     print(to_search[name][attribute_index])
",A,5
1592340,2009-10-20 03:51:26.777000+00:00,">>> data
'\x00\x00\x00\x01\x00\x04AMTD\x00\x00\x00\x00\x02A\x9733A\x99\\)A\x90=qA\x91\xd7\nG\x0f\xc6\x14\x00\x00\x01\x16j\xe0h\x80A\x93\xb4\x05A\x97\x1e\xb8A\x90z\xe1A\x96\x8fWF\xe6.\x80\x00\x00\x01\x16zS|\x80\xff\xff'
>>> from struct import unpack, calcsize
>>> scount, slength = unpack(""!IH"", data[:6])
>>> assert scount == 1
>>> symbol, error_code = unpack(""!%dsb"" % slength, data[6:6+slength+1])
>>> assert error_code == 0
>>> symbol
'AMTD'
>>> bar_count = unpack(""!I"", data[6+slength+1:6+slength+1+4])
>>> bar_count
(2,)
>>> bar_format = ""!5fQ""                                                         
>>> from collections import namedtuple
>>> Bar = namedtuple(""Bar"", ""Close High Low Open Volume Timestamp"")             
>>> b = Bar(*unpack(bar_format, data[6+slength+1+4:6+slength+1+4+calcsize(bar_format)]))
>>> b
Bar(Close=18.899999618530273, High=19.170000076293945, Low=18.030000686645508, Open=18.229999542236328, Volume=36806.078125, Timestamp=1195794000000L)
>>> import time
>>> time.ctime(b.Timestamp//1000)
'Fri Nov 23 08:00:00 2007'
>>> int(b.Volume*100 + 0.5)
3680608
",A,6
39574842,2016-09-19 13:48:47.060000+00:00,MySQLdb is not compatible with Python 3. Use mysql-client or mysql-connect.,A,2
53742230,2018-12-12 11:38:49.713000+00:00,"That shouldn't be done in form_valid. You should do that in the form itself. Instead of letting CreateView automatically create a form for you, do it explicitly and overwrite the clean method.
class MyForm(forms.ModelForm):
   class Meta:
      model = MyModel
      fields = ('list', 'of', 'fields')

   def clean(self):
       for field, value in self.cleaned_data.items():
           self.cleaned_data['field'] = value.lower()

...
class MyCreateView(views.CreateView):
    form_class = MyForm
",A,2
45086395,2017-07-13 16:30:25.540000+00:00,"It doesn't matter how many dicts or other values you have.  Whatever the number, they are all passed in the third parameter to render, which is itself a dict.",A,1
51829082,2018-08-13 19:12:09.923000+00:00,"ProcessPoolExecutor runs each of your workers in its own separate child process.
ThreadPoolExecutor runs each of your workers in separate threads within the main process.
The Global Interpreter Lock (GIL) doesn't just lock a variable or function; it locks the entire interpreter. This means that every builtin operation, including things like listodicts[3]['spam'] = eggs, is automatically thread-safe.
But it also means that if your code is CPU-bound (that is, it spends its time doing calculations rather than, e.g., waiting on network responses), and not spending most of its time in an external library designed to release the GIL (like NumPy), only one thread can own the GIL at a time. So, if you've got 4 threads, even if you have 4 or even 16 cores, most of the time, 3 of them will be sitting around waiting for the GIL. So, instead of getting 4x faster, your code gets a bit slower.
Again, for I/O-bound code (e.g., waiting on a bunch of servers to respond to a bunch of HTTP requests you made), threads are just fine; it's only for CPU-bound code that this is an issue.
Each separate child process has its own separate GIL, so this problem goes away—even if your code is CPU-bound, using 4 child processes can still make it run almost 4x as fast.
But child processes don't share any variables. Normally, this is a good thing—you pass (copies of) values in as the arguments to your function, and return (copies of) values back, and the process isolation guarantees that you're doing this safely. But occasionally (usually for performance reasons, but also sometimes because you're passing around objects that can't be copied via pickle), this is not acceptable, so you either need to use threads, or use the more complicated explicit shared data wrappers in the multiprocessing module.",A,2
37775355,2016-06-12 14:27:14.390000+00:00,"Counter objects are just dictionaries with extra behaviour, so you can also use dict.items(). However, I'd use a different method here unique to Counter objects.
Loop over the Counter.most_common() method and format the tuples this yields into a string:
for key, value in list1:
    print key, value['count'], '|||', ', '.join(['%s %d' % kv for kv in value['value1'].most_common()])

You may want to use string formatting with str.format() to create the column formatting:
for key, value in list1:
    print '{}  {:<2d} ||| {}'.format(
        key, value['count'],
        ', '.join(['%s %d' % kv for kv in value['value1'].most_common()]))

Counter.most_common() produces key-value pairs in order of highest count to lowest.
Demo of the latter:
>>> for key, value in list1:
...     print '{}  {:<2d} ||| {}'.format(
...         key, value['count'],
...         ', '.join(['%s %d' % kv for kv in value['value1'].most_common()]))
...
key1  10 ||| set1 5, set2 4, set3 1
key2  8  ||| set1 6, set2 2
key3  7  ||| set1 5, set2 2, set3 1
",A,2
23457248,2014-05-04 14:14:20.440000+00:00,"Don't use interpolation; use SQL parameters:
def searchForExactEnglishToCzechTranslation(inputPattern,db):
    result = db.execute(""SELECT czech FROM translations WHERE english=?"",
                        (inputPattern,)).fetchall()

This takes care of quoting (the ? is not surrounded by quotes) and escaping.
Quoting the sqlite3 module documentation:

Usually your SQL operations will need to use values from Python variables. You shouldn’t assemble your query using Python’s string operations because doing so is insecure; it makes your program vulnerable to an SQL injection attack (see http://xkcd.com/327/ for humorous example of what can go wrong).
Instead, use the DB-API’s parameter substitution. Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor’s execute() method. (Other database modules may use a different placeholder, such as %s or :1.)
",A,5
29996068,2015-05-01 22:27:58.863000+00:00,"You are not correct in your assessment of the problem.
You can check the results and see that there's a </html> right near the end. That means you've got the whole page.
And requests.text always grabs the whole page; if you want to stream it a bit at a time, you have to do so explicitly.
Your problem is that the table doesn't actually exist in the HTML; it's build dynamically by client-side JavaScript. You can see that by actually reading the HTML that's returned. So, unless you run that JavaScript, you don't have the information.
There are a number of general solutions to that. For example:

Use selenium or similar to drive an actual browser to download the page.
Manually work out what the JavaScript code does and do equivalent work in Python.
Run a headless JavaScript interpreter against a DOM that you've built up.
",A,6
1496622,2009-09-30 07:43:53.297000+00:00,"I presume your error is caused by a circular import - that is, models.py imports spam_analyzer.py, and vice versa, so Python gets into a muddle. However with the code you've posted, there's no need to import SharedLink in spam_analyzer, since you pass it in as a parameter (although note that you're passing an instance, not the class itself, so you should really call the parameter shared_link for the sake of clarity).
An alternative would be to make the spam_checker function a method of the SharedLink class, so you could just call self.spam_checker() in your save() method.",A,1
37109183,2016-05-09 06:36:22.990000+00:00,"Python has you covered here, just use the csv module:
import csv

def readFile(filename):
    with open(filename, 'rb') as f:
        reader = csv.reader(f)
        return list(reader)

Your code makes several classical errors:

str.split() returns a list; you are trying to assign that list 4 times to indices of another list. Just use the list returned by str.split() directly.
Take into account that lines from a file come with the line-separator (\n) included; you probably want to strip that off first.
You started with an empty list. You can't assign to indices that are not there, use list.append() instead to add elements.
You don't need to test for len(line) != 0; just if line: is enough because empty strings are considered 'false' in a truth test. See Truth Value Testing.
You don't need to use file.readline() each time; just use a for line in f: loop and you'll get each line one by one, because file objects are iterable.
If you use your file as a context manager (by using the with statement), Python will close the file for you. 

So, without the csv module, you could write your code like this:
def readFile(fileName):
    rows = []
    with open(fileName, 'r') as f:
        for line in f:
            columns = line.strip().split(',')
            rows.append(columns)
    return rows
",A,2
2488508,2010-03-21 20:03:35.123000+00:00,"Using django-rest-interface
Still true.
It's quite trivial to roll your own.  Each REST URI maps to a view function.  Each REST method (GET, POST, PUT, DELETE) is a simple condition in the view function.
Done.",A,4
1818977,2009-11-30 10:22:25.693000+00:00,"As others have noted, you can't use Django's ORM on AppEngine. However it's obvious that you are following some instructions to import django.db in order to disconnect some signals. The error message shows you the problem: the sqlite3 library is not installed on your system. 
Usually this comes along with Python versions 2.5 onwards, so you should have it as part of your 2.6 installation, but perhaps you have a minimal install for some reason. Try installing one of the full Python versions, from python.org or ActiveState, or you could try just installing the pysqlite2 library.",A,2
35813451,2016-03-05 11:21:10.557000+00:00,You haven't added anything to your local repo; there is nothing to push.,A,0
47854992,2017-12-17 12:13:30.160000+00:00,Your barn method overwrites itself by setting self.barn to a list. Use a different name for the list.,A,0
5515449,2011-04-01 15:26:00.197000+00:00,"I'm presuming that you would only add nocall: to conditions that already test on items that are not callable in the first place, and that perhaps avoiding the python builtin callable test might be give you a performance boost.
The short answer to that question is no, that would not help you. On my Macbook Pro laptop, running callable(True) a 1000 times clocks in at 119ns per loop, vs. 71ns per loop for the plain True statement. So for simple python objects, the callable test takes a mere 48ns. Adding the nocall: to a TALES statement on the other hand, requires extra processing that almost certainly will exceed the 48ns overhead of the callable test you just saved.
Thus, adding nocall: for the sake of improving performance would backfire. You'd be better off implementing proper caching (look at plone.app.caching in combination with Varnish), or take a look if Chameleon could work for your use-case.",A,1
14425153,2013-01-20 13:50:57.113000+00:00,"No, you can't do that. For your usecase, use a tuple instead:
key = (2013, 1)

Since you don't need to do date manipulations on the value a tuple more than suffices.",A,13
38785188,2016-08-05 08:55:56.540000+00:00,"You are using the Sphinx project notation, which incidentally was rejected for inclusion in the PEP 484 -- Type Hints proposal. 
:type is an info field list, and there isn't really all that much of a formal specification for these. The documentation example uses or:
:type priority: integer or None

but note that integer isn't a formal type, nor is None (it is a singleton object).
These are documentation constructs, not type hints, really. That PyCharm supports these at all is nice, but these are not a Python standard.
I'd stick with proper type annotations instead. That means using a Union type:
Union[type1, type2]

You can put these in a # type: comment if you need to support Python 2.",A,1
16347188,2013-05-02 20:31:59.883000+00:00,"While Python 3 deals in Unicode, the Windows console or POSIX tty that you're running inside does not. So, whenever you print, or otherwise send Unicode strings to stdout, and it's attached to a console/tty, Python has to encode it.
The error message indirectly tells you what character set Python was trying to use:
  File ""C:\Python32\lib\encodings\cp850.py"", line 19, in encode

This means the charset is cp850.
You can test or yourself that this charset doesn't have the appropriate character just by doing '\u2013'.encode('cp850'). Or you can look up cp850 online (e.g., at Wikipedia).
It's possible that Python is guessing wrong, and your console is really set for, say UTF-8. (In that case, just manually set sys.stdout.encoding='utf-8'.) It's also possible that you intended your console to be set for UTF-8 but did something wrong. (In that case, you probably want to follow up at superuser.com.)
But if nothing is wrong, you just can't print that character. You will have to manually encode it with one of the non-strict error-handlers. For example:
>>> '\u2013'.encode('cp850')
UnicodeEncodeError: 'charmap' codec can't encode character '\u2013' in position 0: character maps to <undefined>
>>> '\u2013'.encode('cp850', errors='replace')
b'?'


So, how do you print a string that won't print on your console?
You can replace every print function with something like this:
>>> print(r['body'].encode('cp850', errors='replace').decode('cp850'))
?

… but that's going to get pretty tedious pretty fast.
The simple thing to do is to just set the error handler on sys.stdout:
>>> sys.stdout.errors = 'replace'
>>> print(r['body'])
?


For printing to a file, things are pretty much the same, except that you don't have to set f.errors after the fact, you can set it at construction time. Instead of this:
with open('path', 'w', encoding='cp850') as f:

Do this:
with open('path', 'w', encoding='cp850', errors='replace') as f:

… Or, of course, if you can use UTF-8 files, just do that, as Mark Ransom's answer shows:
with open('path', 'w', encoding='utf-8') as f:
",A,35
24496914,2014-06-30 18:49:25.180000+00:00,"You should be passing the UserProfile object as the instance, not the user.
new_api_form = ChangeApiForm(request.POST, instance=request.user.userprofile)
",A,2
40464415,2016-11-07 11:45:23.573000+00:00,"The data descriptor lives in the class namespace, while the instance attribute lives in the instance namespace (so instance.__dict__). These are two separate dictionaries, so there is no conflict here.  
So for any given attribute lookup for the name foo on an instance bar, Python also looks at it's class (type(bar), named C below), in the following order:

C.foo is looked up. If it is a data descriptor, this is where the lookup ends. C.foo.__get__(bar, C) is returned. Otherwise, Python will store this result for step 3 (no point in looking this up twice).
If C.foo did not exist or is a regular attribute, then Python looks for bar.__dict__['foo']. If it exists, it is returned. Note that this part is never reached if C.foo is a data descriptor!
If bar.__dict__['foo'] does not exist, but C.foo exists, then C.foo is used. If C.foo is a (non-data) descriptor, thet C.foo.__get__(bar, C) is returned.

(Note that C.foo is really C.__dict__['foo'], but for simplicity sake I've ignored descriptor access on classes in the above).
Perhaps a concrete example helps; here are two descriptors, one is a data descriptor (there is a __set__ method), and the other is not a data descriptor:
>>> class DataDesc(object):
...     def __get__(self, inst, type_):
...         print('Accessed the data descriptor')
...         return 'datadesc value'
...     def __set__(self, inst, value):
...         pass   # just here to make this a data descriptor
...
>>> class OtherDesc(object):
...     def __get__(self, inst, type_):
...         print('Accessed the other, non-data descriptor')
...         return 'otherdesc value'
...
>>> class C(object):
...     def __init__(self):
...         # set two instance attributes, direct access to not
...         # trigger descriptors
...         self.__dict__.update({
...             'datadesc': 'instance value for datadesc',
...             'otherdesc': 'instance value for otherdesc',
...         })
...     datadesc = DataDesc()
...     otherdesc = OtherDesc()
...
>>> bar = C()
>>> bar.otherdesc  # non-data descriptor, the instance wins
'instance value for otherdesc'
>>> bar.datadesc  # data descriptor, the descriptor wins
Accessed the data descriptor
'datadesc value'
",A,3
23137921,2014-04-17 15:56:37.203000+00:00,"To add headers to every outgoing response, use the @Flask.after_request hook instead:
@app.after_request
def add_ua_compat(response):
    response.headers['X-UA-Compatible'] = 'IE=Edge'
    return response

There is a Flask extension that does exactly this; register the hook and add a header.",A,7
24887945,2014-07-22 12:51:52.180000+00:00,"You can reference the current class with type(self) or self.__class__. Take into account that that value changes as a class is subclassed.
So:
class turn(object):
    def __init__(self): 
        cls = type(self)
        self.currentTurn = cls.TURNS[cls.curr]

    def getTurn(self):
        cls = type(self)
        return cls.TURNS[cls.curr]

    def switch(self):
        cls = type(self)
        cls.curr = 1 - cls.curr

        if cls.curr == 0: 
            print ""It's your move! \n""

        self.currentTurn = cls.TURNS[cls.curr]

but if you were to subclass turns, then setting cls.curr will be done on the subclass, not the turn parent class when you call subclass().switch().
You can also use the self.__class__ attribute, but using a built-in function lets you keep the code more readable.
However, note that as long as there are no instance attributes with the same names, you can refer to your class attributes on self as well; the following works too:
class turn(object):
    def __init__(self): 
        self.currentTurn = self.TURNS[self.curr]

    def getTurn(self):
        return self.TURNS[self.curr]

    def switch(self):
        cls = type(self)
        cls.curr = 1 - cls.curr

        if self.curr == 0: 
            print ""It's your move! \n""

        self.currentTurn = self.TURNS[self.curr]

Setting class attributes still requires that you do so directly on the class, not self, however.",A,1
24547541,2014-07-03 07:27:13.587000+00:00,"The linebreaks template tag is just a wrapper around the line breaks function from django.utils.html, so you can just use that directly.",A,7
19957214,2013-11-13 15:07:43.647000+00:00,"pip is a command line tool, not Python syntax.
In other words, run the command in your console, not in the Python interpreter:
pip install beautifulsoup4

You may have to use the full path:
C:\Python27\Scripts\pip install beautifulsoup4

or even
C:\Python27\Scripts\pip.exe install beautifulsoup4

Windows will then execute the pip program and that will use Python to install the package.
Another option is to use the Python -m command-line switch to run the pip module, which then operates exactly like the pip command:
python -m pip install beautifulsoup4

or
python.exe -m pip install beautifulsoup4
",A,125
19774003,2013-11-04 17:56:21.087000+00:00,"You can:

Use a flag variable; you are already using one, just reuse it here:
running = True
while running:
    # ...
    if roll == first_roll:
        running = False
    else:
        # ...
        if answer.lower() in ('n', 'no'):
            running = False
        # ...

Return from a function:
def game():
    while True:
        # ...
        if roll == first_roll:
            return
        # ...
        if answer.lower() in ('n', 'no'):
            return
        # ...

Raise an exception:
class GameExit(Exception):
    pass

try:
    while True:
        # ...
        if roll == first_roll:
            raise GameExit()
        # ...
        if answer.lower() in ('n', 'no'):
            raise GameExit()
        # ...
except GameExit:
    # exited the loop
    pass

",A,4
20553592,2013-12-12 20:29:34.480000+00:00,"This is due to eventual consistency. Queries without an ancestor are not guaranteed to be strongly consistent (the dev server exaggerates this effect to make you aware of it, in production you rarely see inconsistencies).
You should ensure that your query uses an ancestor so that it supplies to an entity group. Alternatively, you could decrement the memcache counter explicitly, rather than refreshing it from a query.",A,1
19192988,2013-10-05 01:19:17.250000+00:00,"All of the exceptions in the standard library that are expected to be ""generally usable"" are built-ins, and are documented in the Built-in Exceptions part of the library reference. 
In 3.3, that includes this one:

exception ConnectionError
A base class for connection-related issues.
Subclasses are BrokenPipeError, ConnectionAbortedError, ConnectionRefusedError and ConnectionResetError.

But this is a built-in. So this should work:
except ConnectionError:


In 3.0-3.2, there is no such exception as ConnectionError. Nothing in the stdlib raises anything of that name. So there's no point in trying to handle it. (See PEP 3151 for an explanation of how OSError and IOError were reorganized between 3.2 and 3.3.)
The 3.2 equivalent of ConnectionError is OSError with certain errno values. So, what you want is something like:
except OSError as e:
    if e.errno not in (EPIPE, ESHUTDOWN, ECONNABORTED, ECONNREFUSED, ECONNRESET):
        raise
    # whatever you wanted to do for ConnectionError.


Meanwhile, in the future, when you don't know what kind of exception you need to handle, it's pretty easy to test. First, write some test code that handles any exception by logging the qualified name of the exception type. Then take the type out of the log and use that in your real code.
try:
    code_that_raises()
except Exception as e:
    print(type(e), type(e).__qualname__, whatever_else_looks_useful(e))
",A,1
1465505,2009-09-23 11:49:52.903000+00:00,"First, look at modules on pypi.  Download several that are related to what you're doing so you can see exactly what the state of the art is.  
For example, look at easy_install for an example of something like what you're proposing.
After looking at other modules, write yours to look like theirs.
Then publish information on your blog.
When people show an interest, post it to SourceForge or something similar.  This will allow you to get started slowly.
When people start using it, you'll know exactly what kind of maintenance you need to do.
Then, when demand ramps up, you can create the pypi information required to publish it on pypi.
Finally, when it becomes so popular that people demand it be added to Python as a standard part of the library, many other folks will be involved in helping you mature your offering.",A,2
45266220,2017-07-23 14:31:59.270000+00:00,"I'd not use a list comprehension here; you are building two lists, not one, and that's just easier with a regular for loop:
listC = []
listD_filtered = []
for a, b, d in zip(listA, listB, listD):
    if a > b:
        listC.append(a)
        listD_filtered.append(d)

Using zip() you can iterate over lists in parallel, removing the need to track indices altogether.
You can still use a comprehension here, but then you'd build a sequence with (a, d) values, and then use zip() again to split that result into two separate lists:
listC, listD = zip(*((a, d) for a, b, d in zip(listA, listB, listD) if a > b))

This however becomes harder to follow and explain, and you actually have tuples, not lists, in that case (depending on your usecase you'd have to convert those to lists again).
Demo:
>>> listA = [230, 232, 230, 229, 237, 212, 245, 233, 220, 230]
>>> listB = [232, 231, 234, 230, 234, 228, 244, 236, 227, 229]
>>> listD = [1,2,3,4,5,6,7,8,9,10]
>>> listC = []
>>> listD_filtered = []
>>> for a, b, d in zip(listA, listB, listD):
...     if a > b:
...         listC.append(a)
...         listD_filtered.append(d)
...
>>> listC
[232, 237, 245, 230]
>>> listD_filtered
[2, 5, 7, 10]
>>>
>>> listC, listD = zip(*((a, d) for a, b, d in zip(listA, listB, listD) if a > b))
>>> listC
(232, 237, 245, 230)
>>> listD
(2, 5, 7, 10)
",A,1
32134458,2015-08-21 07:16:08.533000+00:00,"You are using the current requests master branch taken from GitHub. Don't, using the current development version can easily lead to problems as it is not yet release ready. The revision you are using contains a bug in the dictionary header merging code only present on Python 3; this change doesn't take into account that dict.items() is a live dictionary view on Python 3:
>>> requests.get('http://httpbin.org/get', headers={'foo': None})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/api.py"", line 69, in get
    return request('get', url, params=params, **kwargs)
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/api.py"", line 50, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/sessions.py"", line 455, in request
    prep = self.prepare_request(req)
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/sessions.py"", line 382, in prepare_request
    headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/sessions.py"", line 66, in merge_setting
    for (k, v) in merged_setting.items():
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python3.4/_collections_abc.py"", line 503, in __iter__
    for key in self._mapping:
  File ""/Users/mj/Development/venvs/stackoverflow-3.4/requests/requests/structures.py"", line 60, in <genexpr>
    return (casedkey for casedkey, mappedvalue in self._store.values())
RuntimeError: dictionary changed size during iteration

I have submitted a pull request to fix the issue properly (now merged).
Your best bet is to stick with the latest release from PyPI (currently that is  version 2.7.0 which is tried and tested.
If you can't use the PyPI version nor can you check out the current tip of the master branch, one work-around is to filter out None values from the dictionary before using the headers:
user_headers = {k: v for k, v in Config.get_headers().items() if v is not None}
",A,0
15719917,2013-03-30 15:00:28.743000+00:00,"You'd need to process this recursively:
from operator import itemgetter

def sortLinesPoints(data):
    if isinstance(data, dict):
        if 'children' in data:
            sortLinesPoints(data['children'])
    else:
        for elem in data:
            sortLinesPoints(elem)
        data.sort(key=itemgetter('type'))
",A,2
1352939,2009-08-30 02:36:53.577000+00:00,"From wikipedia

Shebangs specify absolute paths to system executables; this can cause
  problems on systems which have non-standard file system layouts
Often, the program /usr/bin/env can be used to circumvent this
  limitation
",A,25
27072677,2014-11-22 00:34:32.063000+00:00,"
I was wondering if it's possible to create a text entry dialog input bar

You seem to be a bit confused here.
A text entry input bar isn't a dialog. It's just a widget (aka control)—in particular, a TextCtrl.
A TextEntryDialog includes a TextCtrl, and a title bar, and a caption, and some buttons; you don't want any of that except maybe a button or two. And it has a panel with some sizers to lay things out, but you don't want that; it doesn't even place the buttons alongside the entry bar. It's also a top-level window, which you again don't want. More importantly, it has a bunch of code to act like a dialog—to modally take over the entire interface and not return until the user has typed something and clicked a button—which you definitely don't want.
So, the only thing you want out of the dialog is that you want a TextCtrl, and a Button, probably organized together in a Panel with a Sizer so you can control the layout. So, just create those. Add a handler for the button that read's the text input's value and does something with it, and you're done.
(What you're specifically asking for, a way to place a dialog within the window, isn't possible; you can embed top-level windows into child windows, or you can create something that acts just like a dialog but isn't top-level. But neither of those is easy, and neither is what you want here.)",A,0
9648447,2012-03-10 17:14:59.903000+00:00,"You're trying to insert the string ""id"", rather than the value of the variable id.
It should be:
cursor.execute(sql, (id,))

Of course, there's no reason to be using raw SQL for this at all.",A,2
23561924,2014-05-09 10:09:39.217000+00:00,"The page uses AJAX to fetch player details. The response is actually in JSON, so you could just replicate the behaviour in Python.

The table cell has a data-playerid attribute:
<a data-playerid=""CD_I271072"" href=""javascript:void(0);"">Daniel Rich</a></td>

The player ID is loaded with AJAX:
http://www.afl.com.au/api/cfs/afl/playerProfile/CD_I271072

and
http://www.afl.com.au/api/cfs/afl/playerRatings?playerId=CD_I271072&pageSize=100

The responses contains JSON data:
{
  ""playerProfile"" : {
    ""id"" : ""CD_I271072"",
    ""position"" : ""Left Half Back"",
    ""surname"" : ""Rich"",
    ""jumperNumber"" : 10,
    ""milestones"" : null,
    ""careerAverages"" : {
      ""goals"" : 0.7,
      ""behinds"" : 0.7,
      ""superGoals"" : null,
      ""kicks"" : 11.3,
      ""handballs"" : 7.8,
      ""disposals"" : 19.1,
      ""marks"" : 2.8,
      ""bounces"" : 0.1,
      ""tackles"" : 4.0,
      ""contestedPossessions"" : 8.6,
      ""uncontestedPossessions"" : 10.5,
      ""totalPossessions"" : 19.1,
      ""inside50s"" : 4.5,
      ""marksInside50"" : 0.1,
      ""contestedMarks"" : 0.2,
      ""hitouts"" : 0.1,
      ""onePercenters"" : 1.5,
      ""disposalEfficiency"" : null,
      ""clangers"" : 2.3,
      ""freesFor"" : 0.8,
      ""freesAgainst"" : 1.0,
      ""dreamTeamPoints"" : 76.4,
      ""clearances"" : {
        ""centreClearances"" : 1.4,
        ""stoppageClearances"" : 2.3,
        ""totalClearances"" : 3.7
      },
      ""rebound50s"" : 1.6,
      ""goalAssists"" : 0.6,
      ""goalAccuracy"" : null,
      ""ratingPoints"" : null,
      ""ranking"" : null,
      ""interchangeCounts"" : null
    },
    ""firstName"" : ""Daniel"",
    ""bio"" : ""<p>Daniel Rich is a high possession-winning in-and-under midfielder with a penetrating left foot and quality skills. The high-profile West Australian recruit received the AFL Rising Star Award in his debut season with the Lions and is now&nbsp;widely regarded as one of the most damaging midfielders in the AFL competition.</p>"",
    ""photoUrl"" : ""http://m.afl.com.au/staticfile/AFL Tenant/BrisbaneLions/Player Profiles/2014 - Profiles/RICH Daniel.png"",
    ""aflAwards"" : null,
    ""clubAwards"" : null,
    ""qa"" : null,
    ""sponsor"" : null,
    ""basicStats"" : {
      ""dateOfBirth"" : ""1990-06-07T02:00:00.000+0000"",
      ""draftYear"" : ""2008"",
      ""heightInCm"" : 183,
      ""weightInKg"" : 84,
      ""recruitedFrom"" : ""Subiaco (WA)"",
      ""debutYear"" : ""2009""
    },
    ""careerStats"" : {
      ""goals"" : 67.0,
      ""behinds"" : 66.0,
      ""superGoals"" : null,
      ""kicks"" : 1139.0,
      ""handballs"" : 787.0,
      ""disposals"" : 1926.0,
      ""marks"" : 285.0,
      ""bounces"" : 8.0,
      ""tackles"" : 403.0,
      ""contestedPossessions"" : 867.0,
      ""uncontestedPossessions"" : 1060.0,
      ""totalPossessions"" : 1927.0,
      ""inside50s"" : 452.0,
      ""marksInside50"" : 14.0,
      ""contestedMarks"" : 24.0,
      ""hitouts"" : 8.0,
      ""onePercenters"" : 156.0,
      ""disposalEfficiency"" : 69.2,
      ""clangers"" : 237.0,
      ""freesFor"" : 85.0,
      ""freesAgainst"" : 101.0,
      ""dreamTeamPoints"" : 7716.0,
      ""clearances"" : {
        ""centreClearances"" : 141.0,
        ""stoppageClearances"" : 233.0,
        ""totalClearances"" : 374.0
      },
      ""rebound50s"" : 166.0,
      ""goalAssists"" : 59.0,
      ""goalAccuracy"" : 44.4,
      ""ratingPoints"" : null,
      ""ranking"" : null,
      ""interchangeCounts"" : null
    },
    ""yearlySeasonStats"" : [ {
      ""year"" : ""2014"",
      ""seasonId"" : ""CD_S2014014"",
      ""totalsAndAverages"" : {
        ""averages"" : {
          ""stats"" : {
            ""goals"" : 0.0,
            ""behinds"" : 0.3,
            ""superGoals"" : null,
            ""kicks"" : 8.0,
            ""handballs"" : 7.7,
            ""disposals"" : 15.7,
            ""marks"" : 3.7,
            ""bounces"" : 0.0,
            ""tackles"" : 2.7,
            ""contestedPossessions"" : 9.7,
            ""uncontestedPossessions"" : 6.0,
            ""totalPossessions"" : 15.7,
            ""inside50s"" : 0.7,
            ""marksInside50"" : 0.0,
            ""contestedMarks"" : 0.3,
            ""hitouts"" : 0.0,
            ""onePercenters"" : 2.3,
            ""disposalEfficiency"" : null,
            ""clangers"" : 1.7,
            ""freesFor"" : 0.7,
            ""freesAgainst"" : 0.7,
            ""dreamTeamPoints"" : 60.0,
            ""clearances"" : {
              ""centreClearances"" : 0.7,
              ""stoppageClearances"" : 1.7,
              ""totalClearances"" : 2.3
            },
            ""rebound50s"" : 3.0,
            ""goalAssists"" : 0.0,
            ""goalAccuracy"" : null,
            ""ratingPoints"" : null,
            ""ranking"" : null,
            ""interchangeCounts"" : null
          },
          ""player"" : {
            ""playerId"" : ""CD_I271072"",
            ""playerName"" : {
              ""givenName"" : ""Daniel"",
              ""surname"" : ""Rich""
            },
            ""captain"" : false,
            ""playerJumperNumber"" : null
          },
          ""teamId"" : ""CD_T20"",
          ""gamesPlayed"" : 3.0,
          ""timeOnGroundPercentage"" : null
        },
        ""totals"" : {
          ""stats"" : {
            ""goals"" : 0.0,
            ""behinds"" : 1.0,
            ""superGoals"" : null,
            ""kicks"" : 24.0,
            ""handballs"" : 23.0,
            ""disposals"" : 47.0,
            ""marks"" : 11.0,
            ""bounces"" : 0.0,
            ""tackles"" : 8.0,
            ""contestedPossessions"" : 29.0,
            ""uncontestedPossessions"" : 18.0,
            ""totalPossessions"" : 47.0,
            ""inside50s"" : 2.0,
            ""marksInside50"" : 0.0,
            ""contestedMarks"" : 1.0,
            ""hitouts"" : 0.0,
            ""onePercenters"" : 7.0,
            ""disposalEfficiency"" : 72.3,
            ""clangers"" : 5.0,
            ""freesFor"" : 2.0,
            ""freesAgainst"" : 2.0,
            ""dreamTeamPoints"" : 180.0,
            ""clearances"" : {
              ""centreClearances"" : 2.0,
              ""stoppageClearances"" : 5.0,
              ""totalClearances"" : 7.0
            },
            ""rebound50s"" : 9.0,
            ""goalAssists"" : 0.0,
            ""goalAccuracy"" : 0.0,
            ""ratingPoints"" : 495.3,
            ""ranking"" : 22.0,
            ""interchangeCounts"" : null
          },
          ""player"" : {
            ""playerId"" : ""CD_I271072"",
            ""playerName"" : {
              ""givenName"" : ""Daniel"",
              ""surname"" : ""Rich""
            },
            ""captain"" : false,
            ""playerJumperNumber"" : null
          },
          ""teamId"" : ""CD_T20"",
          ""gamesPlayed"" : 3.0,
          ""timeOnGroundPercentage"" : 63.3
        }
      }
    },  // etc. 

    ],
    ""seasonStats"" : {
      ""goals"" : 0.0,
      ""behinds"" : 1.0,
      ""superGoals"" : null,
      ""kicks"" : 24.0,
      ""handballs"" : 23.0,
      ""disposals"" : 47.0,
      ""marks"" : 11.0,
      ""bounces"" : 0.0,
      ""tackles"" : 8.0,
      ""contestedPossessions"" : 29.0,
      ""uncontestedPossessions"" : 18.0,
      ""totalPossessions"" : 47.0,
      ""inside50s"" : 2.0,
      ""marksInside50"" : 0.0,
      ""contestedMarks"" : 1.0,
      ""hitouts"" : 0.0,
      ""onePercenters"" : 7.0,
      ""disposalEfficiency"" : 72.3,
      ""clangers"" : 5.0,
      ""freesFor"" : 2.0,
      ""freesAgainst"" : 2.0,
      ""dreamTeamPoints"" : 180.0,
      ""clearances"" : {
        ""centreClearances"" : 2.0,
        ""stoppageClearances"" : 5.0,
        ""totalClearances"" : 7.0
      },
      ""rebound50s"" : 9.0,
      ""goalAssists"" : 0.0,
      ""goalAccuracy"" : 0.0,
      ""ratingPoints"" : 495.3,
      ""ranking"" : 22.0,
      ""interchangeCounts"" : null
    },
    ""latestPlayerRating"" : {
      ""position"" : ""MIDFIELDER"",
      ""roundId"" : ""CD_R201401407"",
      ""player"" : {
        ""playerId"" : ""CD_I271072"",
        ""playerName"" : {
          ""givenName"" : ""Daniel"",
          ""surname"" : ""Rich""
        },
        ""captain"" : false,
        ""playerJumperNumber"" : null
      },
      ""team"" : {
        ""teamId"" : ""CD_T20"",
        ""teamAbbr"" : ""BL"",
        ""teamName"" : ""Brisbane Lions"",
        ""teamNickname"" : ""Lions""
      },
      ""detailedRatings"" : [ {
        ""ratingPoints"" : 478,
        ""ranking"" : 28,
        ""ratingType"" : ""OVERALL"",
        ""trend"" : ""FALLING_FAST""
      }, {
        ""ratingPoints"" : 478,
        ""ranking"" : 1,
        ""ratingType"" : ""TEAM"",
        ""trend"" : ""NO_CHANGE""
      }, {
        ""ratingPoints"" : 478,
        ""ranking"" : 24,
        ""ratingType"" : ""POSITION"",
        ""trend"" : ""FALLING_FAST""
      } ]
    },
    ""careerGamesPlayed"" : 101
  }
}

and
{
  ""playerRatings"" : [
    {
        ""position"": ""MIDFIELDER"",
        ""roundId"": ""CD_R201401407"",
        ""player"": {
            ""playerId"": ""CD_I271072"",
            ""playerName"": {
                ""givenName"": ""Daniel"",
                ""surname"": ""Rich""
            },
            ""captain"": false,
            ""playerJumperNumber"": null
        },
        ""team"": {
            ""teamId"": ""CD_T20"",
            ""teamAbbr"": ""BL"",
            ""teamName"": ""Brisbane Lions"",
            ""teamNickname"": ""Lions""
        },
        ""detailedRatings"": [
            {
                ""ratingPoints"": 478,
                ""ranking"": 28,
                ""ratingType"": ""OVERALL"",
                ""trend"": ""FALLING_FAST""
            },
            {
                ""ratingPoints"": 478,
                ""ranking"": 1,
                ""ratingType"": ""TEAM"",
                ""trend"": ""NO_CHANGE""
            },
            {
                ""ratingPoints"": 478,
                ""ranking"": 24,
                ""ratingType"": ""POSITION"",
                ""trend"": ""FALLING_FAST""
            }
        ]
    },
    // etc.
  ],
  ""pageNum"" : 1,
  ""pageSize"" : 100,
  ""pagesTotal"" : 1,
  ""ratingsTotal"" : 61
}     


Use this to your advantage. AJAX Requests do require a X-media-mis-token token set in the request header; this is obtained by using a session (to track cookies) and POSTing to an API URL.
A sample script using requests library with BeautifulSoup would look like:
import requests
from bs4 import BeautifulSoup

page_url = 'http://www.afl.com.au/afl/stats/player-ratings/overall-standings'
token_url = 'http://www.afl.com.au/api/cfs/afl/WMCTok'
player_url = 'http://www.afl.com.au/api/cfs/afl/playerProfile/'

session = requests.Session()
r = session.get(page_url)
soup = BeautifulSoup(r.content)
token = session.post(token_url).json()['token']

for player in soup.find_all('a', {'data-playerid': True}):
    playerid = player['data-playerid']
    data_r = session.get(player_url + playerid, headers={
        'X-media-mis-token': token})
    profile = data_r.json()['playerProfile']
    print profile['firstName'], profile['surname'], profile['position']

Last but not least, note that the token POST response contains a disclaimer:
>>> print session.post(token_url).json()['disclaimer']
All content and material contained within this site is protected by copyright owned by or licensed to Telstra. Unauthorised reproduction, publishing, transmission, distribution, copying or other use is prohibited.

Take that into account when you start using this data.",A,2
39427972,2016-09-10 16:05:45.840000+00:00,"You can't use six on Python 2.5; it requires Python 2.6 or newer.
From the six project homepage:

Six supports every Python version since 2.6.

Trying to install six on Python 2.5 anyway fails as the included setup.py tries to import the six module, which tries to access objects not available in Python 2.5:
Traceback (most recent call last):    
  File ""<string>"", line 16, in <module>
  File ""/private/tmp/test/build/six/setup.py"", line 8, in <module>
    import six
  File ""six.py"", line 604, in <module>
    viewkeys = operator.methodcaller(""viewkeys"")
AttributeError: 'module' object has no attribute 'methodcaller'
",A,0
35412160,2016-02-15 14:40:24.577000+00:00,"The reason that created_by is still required is because you have not excluded it from the form itself; you need to do that in PostForm.
class PostForm(forms.ModelForm):
    class Meta:
        exclude = ['created_by']

As for assigning it automatically, the docs on the editing views have a specific section describing exactly how to do this; you override form_valid():
def form_valid(self, form):
    form.instance.created_by = self.request.user
    return super(PostCreate, self).form_valid(form)
",A,1
20198823,2013-11-25 17:00:03.940000+00:00,"You are importing _thread instead of thread, remove the underscore. Remove the underscore from the _thread.start_new_thread() line too.
In general, do not use modules that start with an underscore, they are an internal implementation detail.",A,2
21004454,2014-01-08 19:10:20.757000+00:00,"Really, this is the same as any case where you want to refactor commonly-used complex expressions or statements: just turn the expression or statement into a function. The fact that your expression happens to be a composition of function calls doesn't make any difference (but see below).
So, the obvious thing to do is to write a wrapper function that composes the functions together in one place, so everywhere else you can make a simple call to the wrapper:
def get_page_list(url):
    return make_list(cleanup(get_text(get_page(url))))

things = get_page_list(url)
stuff = get_page_list(another_url)
spam = get_page_list(eggs)


If you don't always call the exact same chain of functions, you can always factor out into the pieces that you frequently call. For example:
def get_clean_text(page):
    return cleanup(get_text(page))
def get_clean_page(url):
    return get_clean_text(get_page(url))


This refactoring also opens the door to making the code a bit more verbose but a lot easier to debug, since it only appears once instead of multiple times:
def get_page_list(url):
    page = get_page(url)
    text = get_text(page)
    cleantext = cleanup(text)
    return make_list(cleantext)


If you find yourself needing to do exactly this kind of refactoring of composed functions very often, you can always write a helper that generates the refactored functions. For example:
def compose1(*funcs):
    @wraps(funcs[0])
    def composed(arg):
        for func in reversed(funcs):
            arg = func(arg)
        return arg
    return composed

get_page_list = compose1(make_list, cleanup, get_text, get_page)

If you want a more complicated compose function (that, e.g., allows passing multiple args/return values around), it can get a bit complicated to design, so you might want to look around on PyPI and ActiveState for the various existing implementations.",A,5
17203123,2013-06-19 23:42:50.117000+00:00,"Python 2.x does not have a reliable way to count seconds since launch.
time.clock() is very wrong—on any platform other than Windows, it's counting CPU time instead of wall clock time.
time.time() is closer, in that it's wall clock on every platform. It's not guaranteed to have sub-second precision, but when you're only checking every 10 seconds, that's no problem. What is a problem is that it doesn't handle changes to the system clock very nicely. For example, if you set your clock ahead an hour, the timer won't fire for 3610 seconds instead of just 10.
Fortunately, PyGame has its pygame.time module, with features designed specifically to deal with cases like this.
The easiest thing to do is to just use pygame.game.set_timer. Instead of checking the time every frame or every idle tick or whatever, just add an event handler and ask PyGame to fire that event every 10 seconds.
If that isn't appropriate, pygame.time.get_ticks gives you the number of milliseconds of wall clock time since the game started, which is probably what you'd want to use with Elazar's code.
But it's worth reading the linked page, and some of the examples, before deciding what you want to do.",A,3
24368034,2014-06-23 14:05:16.667000+00:00,"For user-defined functions, in Python 2 unbound and bound methods are created on demand, through the descriptor protocol; OrderedDict.__repr__ is such a method object, as the wrapped function is implemented as a pure-Python function.
The descriptor protocol will call the __get__ method on objects that support it, so __repr__.__get__() is called whenever you try to access OrderedDict.__repr__; for classes None (no instance) and the class object itself are passed in. Because you get a new method object each time the function __get__ method is invoked, is fails. It is not the same method object.
dict.__repr__ is not a custom Python function but a C function, and its __get__ descriptor method essentially just returns self when accessed on the class. Accessing the attribute gives you the same object each time, so is works:
>>> dict.__repr__.__get__(None, dict) is dict.__repr__  # None means no instance
True

Methods have a __func__ attribute referencing the wrapped function, use that to test for identity:
>>> OrderedDict.__repr__
<unbound method OrderedDict.__repr__>
>>> OrderedDict.__repr__.__func__
<function __repr__ at 0x102c2f1b8>
>>> OrderedDict.__repr__.__func__.__get__(None, OrderedDict)
<unbound method OrderedDict.__repr__>
>>> OrderedDict.__repr__.__func__ is OrderedDict.__repr__.__func__
True

Python 3 does away with unbound methods, function.__get__(None, classobj) returns the function object itself (so it behaves like dict.__repr__ does). But you will see the same behaviour with bound methods, methods retrieved from an instance.",A,56
17783981,2013-07-22 09:26:03.030000+00:00,"You'll have to use a function match:
def input_not_type_hidden(tag):
    return tag.name == 'input' and tag.get('type') != 'hidden'

soup.find_all(input_not_type_hidden)
",A,3
12493278,2012-09-19 10:52:29.190000+00:00,"Methods are functions that are associated with a class. Methods are only created when you retrieve them from an already defined class; a method is a wrapper around a function, with a reference to the class as well (and optionally a reference to the instance).
What happens in the first case is: Python compiles your class definition Adder. It finds the decorator definition and a function. The decorator is passed the function, returning a new function. That function is added to the class definition (stored in the class __dict__). All this time you are dealing with a python function, not a method. That happens later.
When you then call a(1), a lookup reveals that the instance doesn't have a __call__ but the Adder class does, so it is retrieved using __getattribute__(). This finds a function (your deco decorator), which is a descriptor so it's __get__() method is called (so Adder.__call__.__get__(a, Adder)), returning a bound method, which is then called and passed in the 1 value. The method is bound because instance is not None when __get__() is called. Your decorator, which wrapped a function at class building time, prints False because it was passed an unwrapped function to start with.
In the second case, however, you retrieve a method (again via __getattribute__() calling __get__() on the undecorated Adder2.__call__ function), this time unbound (as there is no instance, only a class passed to __get__() (the full call is Adder2.__call__.__get__(None, Adder2)), and you then decorate that method. Now ismethod() prints True.
Note that in Python 3, the latter case changes. In Python 3 there no longer is a concept of an unbound method, only functions and bound methods. The term 'bound' is thus dropped altogether. Your second case would also print False as Adder2.__call__.__get__(None, Adder2) returns a function in that case.",A,14
49395608,2018-03-20 23:11:13.680000+00:00,"The issue here is that, while you're not modifying the variable (which you can't do without global), you are modifying the value.
This can be a bit confusing at first. These two look like they do the same thing:
>>> a = [1, 2, 3]
>>> a = a + [1]
>>> a
[1, 2, 3, 1]
>>> a = [1, 2, 3]
>>> a.append(1)
>>> a
[1, 2, 3, 1]

But they're not. The first one is making a new list out of a and [1] and then binding that new list back to the name a. If there was some other reference to that same list, it would be unchanged. The second one, on the other hand, is changing the actual list object that a names. If there was some other reference to that same list, it would change.
>>> a = [1, 2, 3]
>>> b = a
>>> a = a + [1]
>>> b
[1, 2, 3]
>>> a = [1, 2, 3]
>>> b = a
>>> a.append(1)
>>> b
[1, 2, 3, 1]

What you're doing in your code is the second one, changing the value of the list object. Which means that every reference to that list object, even a name you don't have access to, will see the change.
But you're not modifying A itself, you're modifying list(A). Shouldn't that be a new copy of the list?
Yes, it is—but, even if Ab is a different list from A, Ab[0] is still the same list as A[0]. You've made a new list that references the same sublists as the original one. So when you mutate those sublists… same problem.
If you want to change this, you can do one of two things:

Copy all of the sublists, as well as the list, at the top of the function, so you mutate the copy instead of the original.
Don't use any mutating operations on the lists; build a new one.

The first one is a simple change: instead of Ab = list(A), do Ab = copy.deepcopy(A) or Ab = [list(sub) for sub in Ab].
The second one requires rewriting your code a bit:
def make_Ab(A,b):
    n = len(A)
    Ab = []        
    for index in range(0,n):
        Ab.append(A[n] + [b[index][0]])
    return Ab

(Or, of course, you can rewrite that as a list comprehension.)",A,3
23963881,2014-05-30 21:50:34.723000+00:00,"You are sharing one list object, and only are altering the contents. wrapper() returns a list with 6 references to that one list:
>>> res = wrapper([1, 2, 3])
>>> res[0] is res[1]
True
>>> res[0] is res[2]
True
>>> res[0][0] = 42
>>> res
[[42, 2, 3], [42, 2, 3], [42, 2, 3], [42, 2, 3], [42, 2, 3], [42, 2, 3]]

You could yield copies instead:
def permu(arr, start = 0):
    if start == len(arr):
        yield arr[:]
    for i in range(start, len(arr)):
        arr[start], arr[i] = arr[i], arr[start]
        for x in permu(arr, start + 1):
            yield x
        arr[start], arr[i] = arr[i], arr[start]

or you could use itertools.permutations():
>>> from itertools import permutations
>>> print list(permutations([1, 2, 3]))
[(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]
",A,3
22378375,2014-03-13 12:16:29.667000+00:00,"Use the fileinput module to create a script that can handle 1 or more files, replacing lines as needed.
Use regular expressions to find your hex RGB value, and take into account that there are two formats; #fff and #ffffff. Replace each format:
import fileinput
import sys
import re

_hex_colour = re.compile(r'#([0-9a-fA-F]{3}|[0-9a-fA-F]{6})\b')

def replace(match):
    value = match.group(1)
    if len(value) == 3:  # short group
        value = [str(int(c + c, 16)) for c in value]
    else:
        value = [str(int(c1 + c2, 16)) for c1, c2 in zip(value[::2], value[1::2])]
    return 'rgb({})'.format(', '.join(value))

for line in fileinput.input(inplace=True):
    line = _hex_colour.sub(replace, line)
    sys.stdout.write(line)

The regular expression looks for a # followed by either 3 or 6 hexadecimal digits, followed by a word boundary (meaning what follows must not be a character, digit or underscore character); this makes sure we don't accidentally match on a longer hexadecimal value somewhere.
The #hhh (3 digit) patterns are converted by doubling each hex digit; #abc is equivalent to #aabbcc. The hex digits are converted to integers, then to strings for easier formatting, then put into a rgb() string and returned for replacement.
The fileinput module will take filenames from the command line; if you save this as a Python script, then:
python scriptname.py filename1 filename2

will convert both files. Without a filename stdin is used.",A,10
35631727,2016-02-25 15:37:10.313000+00:00,"You don't have a name attribute on your select element. That is the attribute that browsers use to send information in forms; without it no data will be sent.
Note also that your pop handler does not do anything if the method is POST, even though you explicitly say you accept that method.",A,0
14456952,2013-01-22 10:55:52.507000+00:00,"Printing unicode objects requires Python to guess the output encoding and encoding the Unicode codepoints to that encoding.
On your VPS server, the output encoding appears to be ASCII, which is the default when no encoding could be detected (such as when using a pipe). If you run the same code on a terminal, the terminal encoding is usually detected and the encoding succeeds.
The solution is to encode explicitly depending on your script requirements.
Please do read the Python Unicode HOWTO to understand how Python does this detection and why it needs to encode for you.",A,9
40579318,2016-11-13 22:04:42.103000+00:00,"The Python equivalent is Latin-1; all Unicode codepoints from 0x00 through to 0xFF are mapped one-on-one to bytes with the same value. 
Note that this would encode Unicode to a bytestring. In Python 2.7, str is already a bytestring and doesn't need encoding.",A,2
2479894,2010-03-19 19:00:36.830000+00:00,"""It may get difficult to allocate new tasks to them during sprint as mostly all projects are tied up at that point with planned work""
Project management smell.
If you have ""pre-planned"" all the work, that's not very Agile, is it?  Agile == Flexible.  You should be able to rearrange without breaking anything.  
If your plan ""breaks"" you have too much plan and too little Agile.
You shouldn't over-plan this kind of thing.",A,0
2882342,2010-05-21 13:15:29.710000+00:00,"It's not really a question of how big the Thing object is, but rather of how many you have in your database. That's because for a ForeignKey, by default Django's admin gives you a drop-down list containing all the existing items. If you've got lots and lots, then Django will load them all in order to populate that list. The same is true here of User.
The best way round this is to add the offending field to the raw_id_fields in your ModelAdmin subclass. That will change the representation to a simple textfield for the id, with a pop-up lookup window. ",A,5
19920626,2013-11-12 04:00:55.907000+00:00,"First, you have to turn your code into actual valid Python code. For example, your functions have to be defined with a list of arguments.
Then, you have to do is return values rather than just printing them.
Also, you don't want a string representation of a tuple of frequencies, but an actual tuple of them that you can use.
And finally, you're going to have to put the two collections into some kind of format that can be compared. ENGLISH is just a sequence of 26 frequencies; the value computed by functOne is a sequence of up to 26 (letter, count) pairs in descending order of frequency. But really, we don't need the counts or the frequencies at all; we just need the letters in descending order of frequency.
In fact, if you look at it, functTwo is completely unnecessary—it's effectively computing a constant, so you might as well just do that at module level.
While we're at it, I'd reorganize functOne so it takes the input as an argument. And close the file instead of leaking it. And give the functions meaningful names.
def count_letters(data):
    data = data.upper()
    char_counter = collections.Counter(data)
    return [char for char, count in char_counter.most_common()]

english_freqs = (0.0749, 0.0129, 0.0354, 0.0362, 0.1400, 0.0218, 0.0174, 
                 0.0422, 0.0665, 0.0027, 0.0047, 0.0357, 0.0339, 0.0674, 
                 0.0737, 0.0243, 0.0026, 0.0614, 0.0695, 0.0985, 0.0300, 
                 0.0116, 0.0169, 0.0028, 0.0164, 0.0004)
pairs = zip(english_freqs, string.ascii_uppercase)
english_letters = [char for count, char in sorted(pairs, reversed=True)]

def decrypt(data):
    input_letters = count_letters(data)
    return {input_letter: english_letter
           for input_datum, english_datum in zip(input_letters, english_letters)}

crypt = input(""what file would you like to select? "")
with open(crypt, ""r"") as f:
    data = f.read()
mapping = decrypt(data)

For the editing feature… you'll have to design what you want the interface to be, before you can implement it. But presumably you're going to edit the english_freqs object (which means you may want to use a list instead of a tuple) and rebuild english_letters from it (which means you may want that in a function after all).",A,2
19212067,2013-10-06 18:03:25.903000+00:00,"You want self.tags.all(), as it is the instance that has the relationship, not the class.",A,3
19334399,2013-10-12 12:52:46.637000+00:00,"Split on commas, then map to integers:
map(int, example_string.split(','))

Or use a list comprehension:
[int(s) for s in example_string.split(',')]

The latter works better on Python 3 if you want a list result.
This works because int() tolerates whitespace:
>>> example_string = '0, 0, 0, 11, 0, 0, 0, 0, 0, 19, 0, 9, 0, 0, 0, 0, 0, 0, 11'
>>> map(int, example_string.split(','))  # Python 2, in Python 3 returns iterator object
[0, 0, 0, 11, 0, 0, 0, 0, 0, 19, 0, 9, 0, 0, 0, 0, 0, 0, 11]
>>> [int(s) for s in example_string.split(',')]
[0, 0, 0, 11, 0, 0, 0, 0, 0, 19, 0, 9, 0, 0, 0, 0, 0, 0, 11]

Splitting on just a comma also is more tolerant of variable input; it doesn't matter if 0, 1 or 10 spaces are used between values.",A,50
11757548,2012-08-01 10:44:18.150000+00:00,Use os.path.dirname(filename).,A,72
19343269,2013-10-13 08:33:26.433000+00:00,"The logging.config module only supports the keys format, datefmt and class; your converter key is entirely ignored.
The class key does let you specify a custom formatter, so your best option is to subclass logging.Formatter() and have that subclass set the converter:
from logging import Formatter
import time

class GMTFormatter(Formatter):
    converter = time.gmtime

then refer to that class in your YAML:
formatters:
  simple:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    class: ext://your.module.GMTFormatter
",A,7
10699380,2012-05-22 09:33:58.730000+00:00,"Include the db_array[k][0] value in your dict generator expression:
dv = ['', 0]
result_array = {k: [
        db_array.get(k, result_array.get(k))[0],
        db_array.get(k, dv)[1] + result_array.get(k, dv)[1]
    ] for k in set(db_array) | set(result_array)}

Note that I updated the default value to ['', 0] (and used a variable for that to increase readability) if the key is not already present in either dict. Note that for the first item in the list we fall back to result_array if the key was not present in db_array; the key is always present in at least one of the two dicts, this way you do not end up with empty string values.
If the key were not in the result_array dict, then your original default would cause problems, as you use an int 0 to then index as an array:
>>> result_array.get('foobar', 0)[1]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'int' object is not subscriptable

(Updated to reflect your comment showing your original code).",A,3
26722218,2014-11-03 20:05:14.087000+00:00,"You're iterating the whole list once with that in, then iterating it again with that for, the iterating it again with each index call. That's not just wasteful performance, it's also overcomplicated and easier to get wrong.
In fact, almost every time you think you want list.index, you actually don't, because you will have trouble with duplicate values—exactly the problem you're running into. If you ask for the index of e in elephant, there's no way the list can know whether you want 2 instead of 0. The only way to know that is to keep track of the index as you go along—e.g., with the enumerate function.
So, a better solution is to just loop once:
for i, letter in enumerate(chosen_word):
    if letter == letter_input:
        list_01[i] = letter_input
        print(""The letter is in the word"")
        print(""Letters matched so far:"","""".join(list_01))

Once you've simplified it like this, you'll notice a second problem: you're printing ""The letter is in the word"" once for each time it appears, instead of just once, and you're also printing ""Letters matched so far"" once for each time it appears—which means not at all, if it doesn't appear at all. But now it should be a lot more obvious how to fix that problems.
If you understand list comprehensions and zip, it might be even clearer to simplify it farther:
list_01 = [letter_input if letter_input == letter else oldval
           for oldval, letter in zip(list_01, chosen_word)]
",A,0
17489536,2013-07-05 12:59:51.330000+00:00,"You assigned the conversion to float to the wrong name:
mais_mais = input(""Quanto mais deseja? \n"")
total_float = float(mais_mais)
total_total = quantidade_restante + mais_mais

mais_mais is still a string. Assign the conversion to mais_mais instead:
mais_mais = input(""Quanto mais deseja? \n"")
mais_mais = float(mais_mais)
total_total = quantidade_restante + mais_mais
",A,0
16023976,2013-04-15 20:14:56.367000+00:00,"Use a list comprehension:
return [[int(i) for i in row] for row in matrix]
",A,4
11208201,2012-06-26 13:19:33.473000+00:00,"Use the any builtin:
if any(map(lambda x: float(x) >= 95, fields[3:23])):

We do several things:

retrieve the correct columns with a slice operator
map all these values to a boolean (testing the float() against 95)
check that all results of the test are True.
",A,1
14303320,2013-01-13 12:14:18.780000+00:00,"You are looking at the repr() representation of a custom class, which by default include the id() (== memory address in CPython).
This is the default used when printing a list, any contents are included using the representation:
>>> class CustomObject(object):
...     def __str__(self):
...         return ""I am a custom object""
... 
>>> custom = CustomObject()
>>> print(custom)
I am a custom object
>>> print(repr(custom))
<__main__.CustomObject object at 0x10552ff10>
>>> print([custom])
[<__main__.CustomObject object at 0x10552ff10>]
",A,2
11648678,2012-07-25 11:27:42.250000+00:00,"If you really need an object whose members are accessible via dot notation but you still want to iterate through them, you need a namedtuple.
Reaction = collections.namedtuple('Reaction', ['Var1', 'Var2', 'Var3'])
reaction = Reaction('lorem', 'ipsum', 'dolor')
for value in reaction:
    print value
",A,3
18722561,2013-09-10 15:09:01.117000+00:00,"Because you should not rely on the implementation details of another module. If the other module stops using sys, then your first module is now broken.
Importing merely creates a reference in the current namespace. You are not loading the module into memory twice when using the import, so importing a module in two different locations doesn't cost you anything.",A,11
17967466,2013-07-31 10:06:46.503000+00:00,"If that handler takes keyword arguments, then use a dictionary for the third element:
handlers = [(1, 2, {}), (3, 4, {'keyword': 5), (6, 7, {})]

for route, handler, kwargs in handlers:
    some_method(route, handler, **kwargs)

Or you can apply the arguments using *args syntax; in that case just catch all values in the loop:
for args in handlers:
    some_method(*args)

If you have to unpack into at least 2 arguments, do so in a separate step:
for handler in handlers:
    route, handler, args = (handler[0], handler[1], handler[2:])

where args would be a tuple of 0 or more elements.
In Python 3, you can handle arbitrary width unpacking with a splat (*) target:
for route, handlers, *args in handlers:

where *args captures 0 or more extra values in the unpack.
The other route, to elements in handlers to a minimal length could be done with:
[(h + (None,) * 3)[:3] for h in handlers]

Demo:
>>> handlers = [(1, 2), (3, 4, 5), (6, 7)]
>>> [(h + (None,) * 3)[:3] for h in handlers]
[(1, 2, None), (3, 4, 5), (6, 7, None)]
",A,4
36487131,2016-04-07 21:01:50.887000+00:00,"You are printing individual bytes. GB2312 is a multi-byte encoding, and each codepoint uses 2 bytes. Printing those bytes individually won't produce valid output, no.
The solution is to not encode from Unicode to bytes when printing. Loop over the Unicode string instead:
# deleting {}
s = re.sub('{[^}]+}', '', s)
# deleting every line that does not start with a chinese char
s = re.sub(r'(?m)^[A-Z0-9a-z].*\n?', '', s)
# delete non chinese chars
s = re.sub(r'[\s\.A-Za-z0-9\?\!\\/\-\""\,\*]', '', s)
#print s

# No `s.encode()`!
for char in s:
    print char

You could encode each char chararter individually:
for char in s:
    print char

But if you have your console / IDE / terminal correctly configured you should be able to print directly without errors, especially since your print s.encode('gb2312)` produces correct output.
You also appear to be confusing UTF-8 (an encoding) with the Unicode standard. UTF-8 can be used to represent Unicode data in bytes. GB2312 is an encoding too, and can be used to represent a (subset of) Unicode text in bytes.
You may want to read up on Python and Unicode:

The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
",A,0
12661272,2012-09-30 12:34:05.457000+00:00,"You could use a different Python runtime:

if you run your script using Jython; you can exploit Java's permission system
with Pypy's sandboxed version you can choose what is allowed to run in your controller script
",A,1
15342333,2013-03-11 15:24:37.863000+00:00,"You need to bind the value of e to the scope of the lambda you create. Or rather, you want to bind the str(e) result to the lambda:
error = str(e)
for fn in names:
    setattr(sys.modules[__name__], fn, lambda error=error: error

Now each lambda has a keyword parameter storing the then-current value of error. Because the error parameter is a keyword parameter with a default value, your lambdas still work when called with arguments.
The alternative would be to create a new function scope:
def errorfunction(error):
    return lambda: error

error = str(e)
for fn in names:
    setattr(sys.modules[__name__], fn, errorfunction(error)
",A,1
5432310,2011-03-25 12:16:58.630000+00:00,"Remove some code duplication:
def site_exists(site):
    """"""
    returns the sitebean if it exists,
    else returns None
    """"""
    vpadmin_service = _get_vpadmin_service(site)
    all_sites = VpAdminServiceUtil.getSites(vpadmin_service)

    for site_listing in all_sites:
        if (site.getName() == site_listing.getName() and
            site.getCustomer().getId() == site_listing.getCustomer().getId()):
            if site.getId(): # if id is set; it should be the same
                if site.getId() != site_listing.getId(): continue
            else: # no id; consider it the same site
                site.setId(site_listing.getId()) #XXX side-effect
            return site_listing

Note: it is unexpected that site_exists() might modify its argument (via .setId()). Consider to refactor it:
def same_site(site, other):
    if site.getId() and site.getId() != other.getId(): 
       # if id is set; it should be the same
       return False 
    return (site.getName() == other.getName() and
            site.getCustomer().getId() == other.getCustomer().getId())

def get_site_listing(site):
    """"""
    returns the sitebean corresponding to `site`,
    returns None if there is none
    """"""
    vpadmin_service = _get_vpadmin_service(site)
    all_sites = VpAdminServiceUtil.getSites(vpadmin_service)

    return next((s for s in all_sites if same_site(site, s)), None)

Note: the code doesn't modify the site object. Use the return value from get_site_listing() instead.
If next() is unavailable then use:
for site_listing in all_sites:
    if same_site(site, site_listing):
       return site_listing
return None

btw, jython should provide property wrappers for you; so you could write:
def same_site(site, other):
    if site.id and site.id != other.id: 
       # if id is set; it should be the same
       return False 
    return site.name == other.name and site.customer.id == other.customer.id

and site.id = id instead of site.setId(id).",A,0
21755280,2014-02-13 13:13:34.413000+00:00,"Don't use a list of tuples. Use a dictionary:
ref_map = dict(ref_set)

for line in x:
    line = ' '.join([ref_map.get(word, word) for word in line.split()])

otherwise you have a NxM loop; for every extra word in your text or in your ref_set you double the number of iterations you need to do.
Your code only rebinds word, not replace the word in the line; the list comprehension above produces a new line value instead. This doesn't replace the line in x though, you need another list comprehension for that:
x = [' '.join([ref_map.get(word, word) for word in line.split()]) for line in x]

It appears from the comments that x is not a list of sentences but rather one sentence. In which case you use just process that one line with one list comprehension, as in the loop iteration over x above:
def corrected(line):
    return ' '.join([ref_map.get(word, word) for word in line.split()])
",A,4
36107808,2016-03-19 22:09:37+00:00,"JSON requires strings, and encoding to JSON produces a string, but b64ecode() requires bytes. Encode the output of json.dumps():
encoded_json = json.dumps({'username': username})
r = uuid4().hex + b64encode(encoded_json.encode('utf8')).decode('ascii')

Note that the b64encode() function produces bytes too, so you'd have to decode those if you wanted to concatenate this with the UUID hex.",A,2
9244230,2012-02-11 21:30:27.233000+00:00,"The .run() method finishes after processing a single item from the queue. Change it to:
while True:
    job = q.get()
    # ...

There are many other problems in your code.",A,2
8578828,2011-12-20 16:41:44.443000+00:00,"a1.classvar = 'aaa'

This is not a ""reference"" to a class variable.
That is a new instance variable in the object 'a1'. 
An expression like  A.classvar is the class variable.  The class object (and it's superclasses) all have a class level dictionary (A.__dict__) with class-level objects defined in it.  Name resolution works by checking the class, then all the super-classes in Method Resolution Order (MRO).
An expression like a.classvar is resolved by a search through the object's namespace.  When this is a ""reading"" reference, the object and the class (and the superclasses) are searched.
When this appears on the left side of assignment, the instance variable (""classvar"") is simply created on the referenced object (""a"").  No searching through parent namespaces to resolve a name, since there's nothing to resolve.  It's being created.",A,3
54934052,2019-02-28 20:50:19.560000+00:00,"You are passing the MultipartEncoder.to_string() result to the files parameter. You are now asking requests to encode the result of the multipart encoder to a multipart component. That's one time too many.
You don't need to replicate every byte here, just post the file, and perhaps set the user agent, referer, and origin:
files = {
    'file': (""blob"", open(""image_path"", 'rb'), ""image/jpeg"")
}

url = ""https://translate.yandex.net:443/ocr/v1.1/recognize?srv=tr-image&sid=9b58493f.5c781bd4.7215c0a0&lang=en%2Cru""
headers = {
    ""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0"", 
    ""Referer"": ""https://translate.yandex.com/"",
    ""Origin"": ""https://translate.yandex.com"",
}

response = requests.post(url, headers=headers, files=files)
print(response.status)
print(response.json())

The Connection header is best left to requests, it can control when a connection should be kept alive just fine. The Accept* headers are there to tell the server what your client can handle, and requests sets those automatically too.
I get a 200 OK response with that code:
200
{'data': {'blocks': []}, 'status': 'success'}

However, if you don't set additional headers (remove the headers=headers argument), the request also works, so Yandex doesn't appear to be filtering for robots here.",A,2
1288313,2009-08-17 14:37:52.733000+00:00,"You should make your actual command script the minimum possible, so that it just calls a function elsewhere. The function can then be tested via unit tests or doctests as normal. ",A,17
11995752,2012-08-16 21:32:27.353000+00:00,"You need to specify that the parts are alternatives of one another, e.g. the multipart/alternative mime type:
msg = MIMEMultipart('alternative')

The default is mixed; see the email library examples.
Note that to create an email with both attachments and an alternative (HTML / CSS) option you'll need to have a top-level multipart/related container that contains the alternative parts as the first entry.",A,5
53388520,2018-11-20 07:57:15.560000+00:00,"You can't make it much faster, but I certainly would use __slots__ to save on memory allocations. Also make it possible to pass in the attribute values when creating the instance:
class Edge:
    __slots__ = ('node1', 'node2', 'weight')
    def __init__(self, node1=0, node2=0, weight=0):
        self.node1 = node1
        self.node2 = node2
        self.weight = weight

With the updated __init__ you can use a list comprehension:
edges = [Edge(*args) for args in ijw]

Together these can shave off a decent amount of time creating the objects, roughly halve the time needed. 
Comparison creating 1 million objects; the setup:
>>> from random import randrange
>>> ijw = [(randrange(100), randrange(100), randrange(1000)) for _ in range(10 ** 6)]
>>> class OrigEdge:
...     def __init__(self):
...         self.node1 = 0
...         self.node2 = 0
...         self.weight = 0
...
>>> origloop = '''\
... edges= []
... for (i,j,w) in ijw:
...     edge = Edge()
...     edge.node1 = i
...     edge.node2 = j
...     edge.weight = w
...     edges.append(edge)
... '''
>>> class SlotsEdge:
...     __slots__ = ('node1', 'node2', 'weight')
...     def __init__(self, node1=0, node2=0, weight=0):
...         self.node1 = node1
...         self.node2 = node2
...         self.weight = weight
...
>>> listcomploop = '''[Edge(*args) for args in ijw]'''

and the timings:
>>> from timeit import Timer
>>> count, total = Timer(origloop, 'from __main__ import OrigEdge as Edge, ijw').autorange()
>>> (total / count) * 1000 # milliseconds
722.1121070033405
>>> count, total = Timer(listcomploop, 'from __main__ import SlotsEdge as Edge, ijw').autorange()
>>> (total / count) * 1000 # milliseconds
386.6706900007557

That's nearly 2 times as fast.
Increasing the random input list to 10^7 items, and the timing difference holds:
>>> ijw = [(randrange(100), randrange(100), randrange(1000)) for _ in range(10 ** 7)]
>>> count, total = Timer(origloop, 'from __main__ import OrigEdge as Edge, ijw').autorange()
>>> (total / count)
7.183759553998243
>>> count, total = Timer(listcomploop, 'from __main__ import SlotsEdge as Edge, ijw').autorange()
>>> (total / count)
3.8709938440006226
",A,8
22937147,2014-04-08 12:33:37.713000+00:00,"You are passing in rid as a positional argument, so can be found in args instead:
if self._token_valid(args[0]) is not None:

If your decorator always needs access to that argument, just name it explicitly:
def safe_forcecall(fn):
    def _safe_forcecall(self, rid, *args, **kwargs):
        if self._token_valid(rid) is not None:
            return fn(self, rid, *args, **kwargs)
    return _safe_forcecall

You won't find the value in the kwargs dictionary; only if you were to call add_booking with a rid=rid argument would it be placed there. By using an explicit rid argument in your decorator wrapper function, you make sure it is always bound to the rid name.",A,4
38484321,2016-07-20 14:39:27.323000+00:00,"Just add a filter to your list comprehension:
out = [b.text.title() + ""##"" +  b.xpath(""./following::text()[1]"")[0].lstrip("","")
       for b in div.xpath("".//b[@class='black']"")
       if b.text]

The if b.text ensures there is a truthy (non-empty or not None) b.text attribute value.",A,0
12538204,2012-09-21 21:11:05.443000+00:00,"You call the function recursively when t < 50, but don't do anything with the return value:
if t<50:
    t+=1
    iteratepal(b+n)
else:                          
    t=0                        
    print(""yea"")               
    return True

The else: branch is never executed then, so None is returned instead. You probably want to return the result of the recursive call:
if t<50:
    t+=1
    return iteratepal(b+n)
else:                          
    t=0                        
    print(""yea"")               
    return True

Some further tips:

There is no need to test for ==True in an if statement, the following will work just fine:
if iteratepal(i):

You can return the test in def ispal(n) is itself a boolean result, just return that without testing:
def ispal(n):
    return n == int(''.join(reversed(str(n))))

",A,4
49287940,2018-03-14 21:31:06.830000+00:00,"I'm not sure exactly what you mean by ""not being recognized"", but our_list is a local variable inside main, so it can't be used anywhere but inside main. 
So, if you try to use it elsewhere, you should get a NameError.
If your code actually has a global variable with the same name as the local variable that we aren't seeing here, things can be more confusing—you won't get a NameError, you'll get the value of the global variable, which isn't what you want.
The best solution here is to return the value from the function, and then have the caller use the returned value. For example:
def main():
    our_list = []
    ne = int(input('How many numbers do you wish to enter?  '))
    for i in range(0, (ne)): # set up loop to run user specified number of time
        number=int(input('Choose a number:-  '))
        our_list.append(number) # append to our_list
    print ('The list of numbers you have entered is ') 
    print (our_list) 
    return our_list

the_list = main()

while True:
    op = input ('For the mean type <1>, for the median type <2>, for the mode type <3>, to enter a new set of numbers type <4> or 5 to exit')
    import statistics
    if op == ""1"":
        mn = statistics.mean(the_list)
        print (""The mean of the values you have entered is:- "",mn)
    if op == ""2"":
        me = statistics.median(the_list)
        print (""The median of the values you have entered is:- "",me)
    if op == ""3"":
        mo = statistics.mode(the_list)
        print (""The mode of the values you have entered is:- "",mo)
    if op == ""5"":
        the_list = main()
    else:
        print(""Goodbye"")
        break

There are other options—you could pass in an empty list for main to fill, or use a global variable (or, better, a more restricted equivalent like an attribute on a class instance or a closure variable), or refactor your code so everyone who needs to access our_list is inside the same function… but I think this is the cleanest way to do what you're trying to do here.

By the way, this isn't quite the last hurdle—but you're very close:

After any mean, median, or mode, it's going to hit the ""Goodbye"" and exit instead of going back through the loop. Do you know about elif?
You mixed up '5' and '4' in the menu.
If the user enters 2 and 3 and asks for the mode, your code will dump a ValueError traceback to the screen; probably not what you want. Do you know try/except?

That's all I noticed, and they're all pretty simple things to add, so congrats in advance.",A,1
28430694,2015-02-10 11:36:46.223000+00:00,"Don't remove so much information. Leave in the : colons (your regex removes them but I bet there were there before you clobbered them); your dates parse fine if there are time separators:
>>> from dateutil.parser import parse
>>> date1 = 'Tue Feb 10  2015 12 52pm IST'
>>> parse(date1)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mpieters/Development/venvs/stackoverflow-2.7/lib/python2.7/site-packages/dateutil/parser.py"", line 743, in parse
    return DEFAULTPARSER.parse(timestr, **kwargs)
  File ""/Users/mpieters/Development/venvs/stackoverflow-2.7/lib/python2.7/site-packages/dateutil/parser.py"", line 310, in parse
    ret = default.replace(**repl)
ValueError: hour must be in 0..23
>>> date1_with_colon = 'Tue Feb 10  2015 12:52pm IST'
>>> parse(date1_with_colon)
datetime.datetime(2015, 2, 10, 12, 52)
>>> date2_with_colon = '10 February  15  08:35am'
>>> parse(date2_with_colon)
datetime.datetime(2015, 2, 10, 8, 35)
>>> date3_with_colon = '2015 02 10 08:24:26 UTC'
>>> parse(date3_with_colon)
datetime.datetime(2015, 2, 10, 8, 24, 26, tzinfo=tzutc())
",A,1
27204111,2014-11-29 15:56:36.013000+00:00,"Take into account that distutils.util.get_platform() returns the minimal version on which the binary will run:

For Mac OS X systems the OS version reflects the minimal version on which
     binaries will run (that is, the value of MACOSX_DEPLOYMENT_TARGET
     during the build of Python), not the OS version of the current system.

Emphasis mine. That said, for Python 3.4 you can reasonably expect the value to be 10.10 if you built Python on that platform.
For Python to handle MACOSX_DEPLOYMENT_TARGET to work correctly throughout, you need to upgrade to Python 3.4.2; 3.4.1 is not ready for Mac OS X 10.10 or higher. See issue #21811:

There are a number of places within the cpython code base where decisions are made based on either the running system version or the OS X ABI (e.g. the value of MACOSX_DEPLOYMENT_TARGET) that the interpreter was built with or is being built with.  Most of the current tests do string comparisons of these values which will not work properly with a two-digit version number ('10.10' < '10.9' --> True). 

3.4.2 includes the required fixes. The same applies to Python 2.7 up to version 2.7.7; if you see this same problem in Python 2.7 upgrade to 2.7.8 or up.
Without the fixes, intel is mixed up with x86_64, as is the case with your setup:


When running current 3.4.1 and 2.7.7 binary installers on 10.10, building C extension modules will likely result in an incorrect universal platform name, for example, ""x86_64"" instead of ""intel"", and that could affect extension module file names and wheel or egg names.


For reference, on my OS X 10.10 system, on Python 3.4.2 the result of get_platform() is:
>>> import distutils.util
>>> distutils.util.get_platform()
'macosx-10.10-x86_64'

and for Python 2.7.8 I get:
>>> import distutils.util
>>> distutils.util.get_platform()
'macosx-10.4-x86_64'
",A,4
25231061,2014-08-10 17:09:44.460000+00:00,"
So, is there any way to make it so, for example, when you're level 5 your max health is set to 15, rather than ten?

Yes. You need to store a ""max health"" for the player, in addition to his ""current health"":
if player1.getHealth() > player.getMaxHealth()
    player1.setHealth(player.getMaxHealth())


As Padraic explains in a comment, you don't need getters and setters like this in Python, and it's generally considered bad style. So it would be better to write it like this:
if player1.health > player1.max_health:
    player1.health = player1.max_health

You can write this a little more concisely using min:
player1.health = min(player1.health, player1.max_health)


Also, if max health is something you can compute on the fly (maybe 10 + level?), you can write a function for that, and call that function whenever you need it, instead of storing it as an extra attribute.
One of the neat things about Python is that you can start with an attribute, and change it to a computed value without having to turn it into a function, by using @property:
@property
def max_health(self):
    return 10 + self.level
",A,1
44256810,2017-05-30 08:06:59.597000+00:00,"You've misunderstood the logic flow here. There are no errors until after the form is submitted, and the view has re-rendered the template to show those errors. At the point the form is first submitted, there will never be any errors at all so your if statement is always true.
As Raja says in the comments, the only way to do this is to submit the form via Ajax, and then also show the gif via JS.",A,1
33424692,2015-10-29 21:12:28.580000+00:00,"If your input is in the local timezone:
from datetime import date

then = datetime.strptime('10/12/68', '%d/%m/%y').date()
if date.today() <= then: # *then* must be in the past
    then = then.replace(year=then.year - 100)

It should work ok until 2100 (excluding). See here for more details on arithmetic with calendar years.",A,1
37316094,2016-05-19 07:02:51.893000+00:00,"Literals like your h and m characters in the datetime.strptime() format string work just fine.
Your input has no date component however. Remove the %d/%m/%Y portion and parsing works (albeit that the result has a standard date of 1900-01-01):
>>> from datetime import datetime
>>> datetime.strptime('21h03m', '%Hh%Mm')
datetime.datetime(1900, 1, 1, 21, 3)
",A,0
14206016,2013-01-07 23:40:13.587000+00:00,All your hrefs should begin with an initial slash.,A,5
15165858,2013-03-01 20:00:12.173000+00:00,"You are comparing an integer with a string. Convert the input value:
if equation == int(answer):
",A,3
50322725,2018-05-14 03:27:03.530000+00:00,"What you're trying to do is impossible without ugly hacks. You either take *args and get a sequence of parameter values that you can use as args:
def ff(*args):
    return list(map(myfunc, args))

… or you take three explicit parameters and use them by name:
def ff(a, b, c):
    return list(map(myfunc, (a, b, c)))

… but it's one or the other, not both.
Of course you can put those values in a sequence yourself if you want:
def ff(a, b, c):
    args = a, b, c
    return list(map(myfunc, args))

… but I'm not sure what that buys you.

If you really want to know how to write a getArgValueList function anyway, I'll explain how to do it. However, if you're looking to make your code more readable, more efficient, more idiomatic, easier to understand, more concise, or almost anything else, it will have the exact opposite effect. The only reason I could imagine doing something like this is if you had to generate functions dynamically or something—and even then, I can't think of a reason you couldn't just use *args. But, if you insist:
def getArgValueList():
    frame = inspect.currentframe().f_back
    code = frame.f_code
    vars = code.co_varnames[:code.co_argcount]
    return [frame.f_locals[var] for var in vars]

If you want to know how it works, most of it's in the inspect module docs:

currentframe() gets the current frame—the frame of getArgValueList.
f_back gets the parent frame—the frame of whoever called getArgValueList.
f_code gets the code object compiled from the function body of whoever called getArgValueList.
co_varnames is a list of all local variables in that body, starting with the parameters.
co_argcount is a count of explicit positional-or-keyword parameters.
f_locals is a dict with a copy of the locals() environment of the frame.

This of course only works for a function that takes no *args, keyword-only args, or **kwargs, but you can extend it to work for them as well with a bit of work. (See co_kwonlyargcount, co_flags, CO_VARARGS, and CO_VARKEYWORDS for details.)
Also, this only works for CPython, not most other interpreters. and it could break in some future version, because it's pretty blatantly relying on implementation details of the interpreter.",A,2
32728587,2015-09-22 23:20:52.323000+00:00,"In Python 3.7+:
>>> from datetime import datetime
>>> datetime.fromisoformat('2014-03-05 07:22:26.976637+00:00').timestamp()
1394004146.976637


There are two steps:
Convert input rfc-3339 time string into datetime object
#!/usr/bin/env python
from datetime import datetime

time_str = '2014-03-05 07:22:26.976637+00:00'
utc_time = datetime.strptime(time_str[:26], '%Y-%m-%d %H:%M:%S.%f')
assert time_str[-6:] == '+00:00'

Find number of the microseconds since Epoch for given datetime
from datetime import datetime, timedelta

epoch = datetime(1970, 1, 1)

def timestamp_microsecond(utc_time):
    td = utc_time - epoch
    assert td.resolution == timedelta(microseconds=1)
    return (td.days * 86400 + td.seconds) * 10**6 + td.microseconds

print(timestamp_microsecond(utc_time))
# -> 1394004146976637

The advantage is that you could convert this unique number back to the corresponding UTC time:
utc_time = epoch + timedelta(microseconds=1394004146976637)
# -> datetime.datetime(2014, 3, 5, 7, 22, 26, 976637)

Follow the links if you need to support arbitrary utc offsets (not just UTC time).
If you need to accept a leap second as the input time; see Python - Datetime not accounting for leap second properly?",A,3
25630152,2014-09-02 18:42:39.553000+00:00,"The problem is that you can't put a statement in the middle of an expression.

For simple cases, there is an if expression that, being an expression, can be used in the middle of an expression. In your case:
(rlcore.str2float(d['ABSQuantityForCalcCurrentFace']) * rlcore.str2float(d['Price']) / 100
 if str(d['_Investment_InvestmentGroup_']) == ""AssetBacked""
 else rlcore.str2float(d['NetCashAmount']))


For more complicated cases, move the if statement upward and store a temporary value in a variable, then use that variable in the expression (exactly as you're already doing for, each, trade_dt):
if str(d['_Investment_InvestmentGroup_']) == ""AssetBacked"":
    priceval = rlcore.str2float(d['ABSQuantityForCalcCurrentFace']) * rlcore.str2float(d['Price']) / 100
else:
    priceval = rlcore.str2float(d['NetCashAmount'])

… then just use priceval in the yield.

However, no matter how you solve this, your code is a huge unreadable mess. You have at least three different conversion methods you're using repeatedly; if you discover that you're formatting dates or strings or whatever wrong, you'll need to change that in dozens of places. You'd probably do better off with a mapping of column names to either types, or converters, and then just generate the values by dynamically looking up each converter. For example:
_COLUMNS = {'_UDF_SGCP_ID_': str,
            '_UDF_Execution_ID_': str,
            # ...
            'EventDate': datetime.datetime,
            # ...
            }

_CONVERTERS = {str: str, 
               datetime.datetime: lambda val: rlcore.str2dt(val).isoformat(),
               # ...}

def _converted(d, col):
    val = d[col]
    converter = _CONVERTERS[_COLUMNS[col]]
    return converter(val)

And now you can just do this:
yield(_converted(d, col) for col in (        
    '_UDF_SGCP_ID_',
    '_UDF_Execution_ID_',
    # ...
)
",A,1
6925937,2011-08-03 11:47:25.230000+00:00,"urlparse.urlparse will split the URL into protocol, location, port, etc. You can then split the location by . to get the subdomain.
url = urlparse.urlparse(address)
subdomain = url.hostname.split('.')[0]
",A,14
50747581,2018-06-07 18:05:14.633000+00:00,"You can simplify this best function by factoring out the code to score a word. I'm not sure exactly what you're trying to do, so this might be oversimplified for your actual problem, but it should be enough to get you going:
def score(word):
    dic = {'D':2, 'C':2, 'L':2, 'P':2, 'B':3, 'N':3, 'F':4, 'G':4, 'H':4, 'V':4, 'J':5, 'Q':6, 'X':8, 'Y':8, 'Z':8}
    total = 0
    for char in word:
        total += dic.get(char.upper(), 0)
    return total

Now, if you have a list of words, you can use that score function as a key function and just pass it to max:
def best(lista):
    return max(lista, key=score)
",A,1
49504752,2018-03-27 05:07:43.147000+00:00,"Your first problem is that you set q=0 inside the loop instead of just once before it, so every time you do q=q+1, it gets forgotten the next time around. But you can simplify this away instead of fixing it. First, q is supposed to be the same as i, so just use i:
x = len(a)
for i in range(0, x):
    z = a[i]
    list1.pop(z)

Or, better, just iterate directly over a:
for z in a:
    list1.pop(z)

However, that just gets you to the actual problem you're asking about. In fact, you usually don't want to delete from or insert into list in a loop, for exactly this reason. But if you think about it, if you make sure to work right to left, the only values you're shifting are the ones you don't want to touch again. So:
for z in sorted(a, reverse=True):
    list1.pop(z)

However, unless you really need to mutate your original list in place, it's a lot simpler to just make a copy, filtering out the ones you don't want:
list2 = []
for i, z in enumerate(list1):
    if i not in a:
        list2.append(z)

And you can convert this into a list comprehension easily:
list2 = [z for i, z in enumerate(list1) if i not in a]
",A,0
4168866,2010-11-12 20:38:40.880000+00:00,"Why are you doing this?
 reader = csv.reader(open(filepath, ""rU""))

The docs are pretty clear that you must do this:
with open(filepath, ""rb"") as src:
    reader= csv.reader( src )

The mode must be ""rb"" to read.
http://docs.python.org/library/csv.html#csv.reader

If csvfile is a file object, it must be opened with the ‘b’ flag on platforms where that makes a difference. 
",A,2
17632824,2013-07-13 18:03:39.480000+00:00,"When you call list(str) you split the string into individual characters. This happens because strings are sequences too.
To split a string into separate lines use the str.splitlines() method:
for line in somestring.splitlines():
    print line[0]  # print first character

To print the first word of each line, use str.split() to spilt on whitespace:
for line in somestring.splitlines():
    print line.split()[0]  # print first word

Or a little more efficiently by splitting only once:
for line in somestring.splitlines():
    print line.split(None, 1)[0]  # print first word
",A,4
25283830,2014-08-13 10:29:01.657000+00:00,"In your comparison, you made two big mistakes. First, you neglected to even consider the idiomatic ""don't delete anything, copy half the dict"" option. Second, you didn't realize that deleting half the entries in a hash table at 2/3 load leaves you with a hash table of the exact same size at 1/3 load.
So, let's compare the actual choices (I'll ignore the 2/3 load to be consistent with your n/2 measures). For each one, there's the peak space, the final space, and the time:

2.0n, 1.0n, 1.5n: Copy, delete half the original
2.0n, 1.0n, 1.5n: Copy, delete half the copy
1.5n, 1.0n, 1.5n: Built a deletions set then delete
1.0n, 1.0n, 0.5n: Delete half in-place
1.5n, 0.5n, 1.0n: Delete half in-place, then compact
1.5n, 0.5n, 0.5n: Copy half

So, your proposed design would be worse than what we already do idiomatically. Either you're doubling the final (permanent) space just to save an equivalent amount of transient space, or you're taking twice as long for the same space.

And meanwhile, building a new dictionary, especially if you use a comprehension, means:

Effectively non-mutating (automatic thread/process safety, referential transparency, etc.).
Fewer places to make ""small"" mistakes that are hard to detect and debug.
Generally more compact and more readable.
Semantically restricted looping, dict building, and exception handling provides opportunities for optimization (which CPython takes; typically a comprehension is about 40% faster than an explicit loop).


For more information on how dictionaries are implemented in CPython, look at the source, which is comprehensively documented, and mostly pretty readable even if you're not a C expert.
If you think about how things work, some of the choices you assumed should obviously go the other way—e.g., consider that Python only stores references in containers, not actual values, and avoids malloc overhead wherever possible, so what are the odds that it would use chaining instead of open addressing?
You may also want to look at the PyPy implementation, which is in Python and has more clever tricks.

Before I respond to all of your comments, you should keep in mind that StackOverflow is not where Python changes get considered or made. If you really think something should be changed, you should post it on python-ideas, python-dev, and/or the bugs site. But before you do: You're pretty clearly still using 2.x; if you're not willing to learn 3.x to get any of the improvements or optimizations made over the past half-decade, nobody over there is going to take you seriously when you suggest additional changes. Also, familiarize yourself with the constructs you want to change; as soon as you start arguing on the basis of Python dicts probably using chaining, the only replies you're going to get will be corrections. Anyway:


Please explain to me how 'Delete half in place' takes 1.0n space and adds 1.0n space to the final space. 

I can't explain something I didn't say and that isn't true. There's no ""adds"" anywhere. My numbers are total peak space and total final space. You're algorithm is clearly 1.0n for each. Which sounds great, until you compare it to the last two options, which have 0.5n total final space.

As your arguments in favor of not providing to the programmer the option of delete in place,

The argument not to make a change is never ""that change is impossible"", and rarely ""that change is inherently bad"", but usually ""the costs of that change outweigh the benefits"". The costs are obvious: there's the work involved; the added complexity of the language and each implementation; more differences between Python versions; potential TOOWTDI violations or attractive nuisances; etc. None of those things mean no change can go in; almost every change ever made to Python had almost all of those costs. But if the benefits of a change aren't worth the cost, it's not worth changing. And if the benefits are less than they initially appear because your hoped-for optimization (a) is actually a pessimization, and (b) would require giving up other benefits to use even if it weren't, that puts you a lot farther from the bar.
Also, I'm not sure, but it sounds like you believe that the idea of there being an obvious, one way to do things, and having a language designed to encourage that obvious way when possible, constitutes Python being a ""nanny"". If so, then you're seriously using the wrong language. There are people who hate Python for trying to get them to do things the Pythonic way, but those people are smart enough not to use Python, much less try to change it.

Your fourth point, which echoes the one presented in the mailing list about the issue, could easily be fixed … by simply providing a 'for (a,b) in mydict.iteritems() as iter', in the same way as it is currently done for file handles in a 'with open(...) as filehandle' context.

How would that ""fix"" anything? It sounds like the exact same semantics you could get by writing it = iter(mydict.items()) then for (a, b) in it:. But whatever the semantics are, how would they provide the same, or equivalent, easy opportunities for compiler optimization that comprehensions provide? In a comprehension, there is only one place in the scope that you can return from. It always returns the top value already on the stack. There is guaranteed to be no exception handling in the current scope except a stereotyped StopIteration handler. There is a very specific sequence of events in building the list/set/dict that makes it safe to use generally-unsafe and inflexible opcodes that short-circuit the usual behavior. How are you expecting to get any of those optimizations, much less all of them?

""Either you're doubling the final (permanent) space just to save an equivalent amount of transient space, or you're taking twice as long for the same space."" Please explain how you think this works.

This works because 1.0 is double 0.5. More concretely, a hash table that's expanded to n elements and is now at about 1/3 load is twice as big as a hash table that's expanded to n/2 elements and is now at about 2/3 load. How is this not clear?

Delete in place takes O(1) space

OK, if you want to count extra final space instead of total final space, then yes, we can say that delete in place takes 0.0n space, and copying half takes -0.5n. Shifting the zero point doesn't change the comparison.

and none of the options can take less than 1.0n time

Sorry, this was probably unclear, because here I was talking about added cost, and probably shouldn't have been, and didn't mention it. But again, changing the scale or the zero point doesn't make any difference. It clearly takes just as much time to delete 0.5n keys from one dict as it does to add 0.5n keys to another one, and all of the other steps are identical, so there is no time difference. Whether you call them both 0.5n or both 1.0n, they're still equal.

The reason I didn't consider only copying half the dictionary, is that the requirement is to actually modify the dictionary, as is clearly stated.

No, it isn't clearly stated. All you said is ""I need to remove about half the elements in a set/dictionary"". In 99% of use cases, d = {k: v for k, v in d.items() if pred(k)} is the way to write that. And many of the cases people come up with where that isn't true (""but I need the background thread to see the changes immediately"") are actively bad ideas. Of course there are some counterexamples, but you can't expect people to just assume you had one when you didn't even give a hint that you might.

But also, the final space of that is 1.5n, not .5n

No it isn't. The original hash table is garbage, so it gets cleaned up, so the final space is just the new, half-sized hash table. (If that isn't true, then you actually still need the original dict alongside the new one, in which case you had no choice but to copy in the first place.)
And if you're going to say, ""Yeah, but until it gets cleaned up""—yes, that's why the peak space is 1.5n instead of 1.0n, because there is some non-zero time that both hash tables are alive.",A,3
14179855,2013-01-06 06:18:47.707000+00:00,"You should define what do you mean by ""word"". Different definitions can produce different results. The general template is:
from collections import Counter

count = Counter(getwords(normalize(text)))
words = [w for w, c in count.items() if c >= 5]

Where you could use various definition of getwords(), normalize() e.g.:
import re

def normalize(text): # remove non-ascii, convert to lowercase
    return text.encode('ascii', 'ignore').lower().decode()

def getwords(text): # allow only a-z
    return re.findall(ur""[a-z]+"", text)

Or nltk-based tokenizer:
from nltk.tokenize import sent_tokenize, word_tokenize

def getwords(text):
    return (w for sent in sent_tokenizer(text) for w in word_tokenize(sent))
",A,4
19175768,2013-10-04 07:35:30.297000+00:00,"Use str.split():
for email in user_domain_array:
    domain = email.split('@', 1)[-1]

This splits the string once on @ then takes the last element of the resulting list. This has the advantage that you still get a result even if no at-sign was present in the input string.
Demo:
>>> 'rohit@ghrix.com'.split('@', 1)[-1]
'ghrix.com'
>>> 'rohit.ghrix.com'.split('@', 1)[-1]
'rohit.ghrix.com'
",A,3
28695239,2015-02-24 11:59:08.747000+00:00,"I don't think partial is the right tool here. Instead, you could use a lambda:
func = lambda s: s.startswith('#')
lines = filter(func, lines)

Although note that Python isn't really a functional language, and a more Pythonic way of doing this would be with a list comprehension:
lines = [line for line in lines if line.startswith('#')]
",A,1
411925,2009-01-04 23:42:46.310000+00:00,"
model = django.authx.models.User
? Django returns an error, ""global
  name django is not defined.""

Django does not return the error.  Python does.
First, you MUST import the model.  You must import it with
from django.authx.models import User

Second, if you get an error that django is not defined, then Django is not installed correctly.  You must have Django on your PYTHONPATH or installed in your Python lib/site-packages.
To install Django correctly, see http://docs.djangoproject.com/en/dev/intro/install/#intro-install",A,1
3415893,2010-08-05 14:46:51.453000+00:00,"
is that considered bad design since it's extremely tightly coupled to the original class? 

No, that's not a problem.  Subclass-superclass design is also tightly coupled.

it just feels weird mixing the processing logic and the reporting generating logic in one class.

Delegation of reporting/summary often makes sense.  Reporting changes and expands more quickly than the processing does.",A,4
31408374,2015-07-14 13:40:59.040000+00:00,"You cannot iterate over a file twice without rewinding it to the start. Doing so is not very efficient either; better to not use dictionary comprehensions here:
mydict1, mydict2 = {}, {}
for row in csv.reader(g):
    mydict1[row[0]] = row[5]
    mydict2[row[0]] = row[6]

If you really insist, you can use a file.seek(0) to put the file pointer back to the start; you don't need to re-create the reader here:
reader = csv.reader(g)
mydict1 = {row[0]:row[5] for row in reader}
g.seek(0)
mydict2 = {row[0]:row[6] for row in reader}
",A,3
1599907,2009-10-21 10:19:55.213000+00:00,"We limit the Django Admin -- unmodified -- for ""back-office"" access by our admins and support people.  Not by users or customers.  Some stylesheet changes to make the colors consistent with the rest of the site, but that's it.
For the users (our customers), we provide proper view functions to do the various transactions.  Even with heavily tailored forms, there are still a few things that we need to check and control.
Django update transactions are very simple to write, and trying to customize admin seems more work that writing the transaction itself.
Our transactions are not much more complex than shown in http://docs.djangoproject.com/en/dev/topics/forms/#using-a-form-in-a-view.  
Generally, our pages that have transactions almost always include workflow elements (or related content) that make them slightly more complex than the built-in admin interface.  We'll have a half-dozen or so additional lines of code beyond the boilerplate. 
Our use cases aren't simple add/change/delete, so we need more functionality than the default admin app provides.",A,1
3517356,2010-08-18 22:47:20.677000+00:00,"The question is really one of what the ""branches"" are.  
If there are multiple steps there must be user choices at each step.  There should be multiple ""When""'s.    This should form a rich tree with lots of user-selected alternatives at each branch.  Each possible outcome should have it's own test to make the various choices and arrive at that outcome.
A three step sequence with two user choices is 8 possible paths.  Different paths may arrive at the same outcome (or may not).  But you should have multiple paths through this.
If it's just sequential (because someone felt like writing sequential steps) and the user has no choices, then it's not really driven by consideration of the user's behavior, is it?
I don't see the choices.  No choices == bad smell.  But easy to test since there's only one outcome with a sequence of captive steps where the user has few or no choices.
If you work out the choices properly, then each step has multiple outcomes and each step should be tested independently.",A,1
18689649,2013-09-08 23:24:14.800000+00:00,"d is rebound; the variable is updated to point to val in each loop.
For each key in keys, either the key is found (val = d[key] succeeds) or the default_factory() is used to create a new value for that key.
If the key was found but the value was not a MutableMapping type, the found value is replaced with a new default_factory() result.
Once the new value has been determined for this level, d is told to forget about the old dictionary and pointed to the new instead.
Rebinding does not change the old value. It merely stops referring to that old value.
Let's use a simple example:
>>> d = {'foo': {}}
>>> keys = ['foo']
>>> newkey = 'bar'
>>> newval = 'eggs'
>>> original = d

At the start, original and d are the same object. Think of names here as paper labels, and their values as balloons. The labels are tied with string to the balloons. In the above example, the d and original labels are both tied to the same dictionary balloon.
When we enter the for key in keys loop, the d[key] lookup succeeds and val is tied to the result of d['foo'], an empty dictionary:
>>> key = keys[0]
>>> key
'foo'
>>> val = d[key]
>>> val
{}

This is a regular Python dictionary, and isinstance(val, MutableMapping) is True. Next line rebinds the d label to that dictionary. The string is simply untied from the original dictionary and now attached to the same balloon val is tied to:
>>> d = val
>>> d
{}
>>> original
{'foo': {}}
>>> d is val
True
>>> d is original
False

The original dictionary was not altered by the rebinding!
Having run out of keys (there was only one in keys), the next part then assigns newval to d[newkey]:
>>> d[newkey] = newval
>>> d
{'bar': 'eggs'}

However, d is not the only label attached to this dictionary balloon. Dictionaries themselves contain keys and values, both of which are labels that are tied to balloons too! The original label is still tied to the outer dictionary balloon, and it has a foo key associated value, which was tied to a nested dictionary, and it is this nested dictionary we just changed:
>>> original
{'foo': {'bar': 'eggs'}}

The algorithm merely followed along labels via strings to new dictionaries.
Using more complex key combinations just means more strings are being followed, with perhaps an extra dictionary being pumped up to be tied in.",A,2
1923976,2009-12-17 19:22:24.123000+00:00,">>> 'True' is not True
True

'True' is a string
True is a boolean
They have nothing to do with each other, except coincidentally.  The string value happens to have the same letters as the boolean literal.  But that's just a coincidence.",A,0
26573522,2014-10-26 13:30:51.830000+00:00,"What you've done here doesn't make sense: you're just asking for tags whose names are in the list consisting of the literal text "" request.slug"". 
You need to override get_queryset so that it queries on the actual value of the slug, which is in self.kwargs.
def get_queryset(self, *args, **kwargs):
    return  Link.objects.filter(tags__name=self.kwargs ['slug'])

Also, I don't know what that if statement is doing in your template, but you haven't defined ""link"" do it will never evaluate to true, so no links will show.",A,0
47680111,2017-12-06 17:25:45.833000+00:00,You need to use slug instead of pk in the URL pattern to tell Django you want to look up by the slug.,A,0
40100901,2016-10-18 06:19:33.980000+00:00,"There is no point in looping if you just need to update one key:
_dict['b'] = max(_dict['b'], n)

The above sets 'b' to the highest value of the two.",A,13
27696780,2014-12-29 22:09:49.673000+00:00,"You can create one ore more .pth files in site-packages; their contents will be added to sys.path when present.
From the site module documentation:

A path configuration file is a file whose name has the form name.pth and exists in one of the four directories mentioned above; its contents are additional items (one per line) to be added to sys.path. Non-existing items are never added to sys.path, and no check is made that the item refers to a directory rather than a file. No item is added to sys.path more than once. Blank lines and lines beginning with # are skipped. Lines starting with import (followed by space or tab) are executed.

Use it to add the full or relative paths to package directories to sys.path.
For example, for your MyPackage-1.10.0 package, all you need is a MyPackage-1.10.0.pth file with the contents MyPackage-1.10.0 on a line by itself for Python to add the full directory path to sys.path.",A,2
418445,2009-01-06 22:27:59.097000+00:00,"From within cmd.exe:
>>> print '\x0b'
♂
>>> print '\x0c'
♀
>>> print '\f' # form feed
♀
>>> print '\v' # vertical tab
♂
>>>

Inside Emacs:
>>> print '\f\v'
^L^K

Here's an excerpt from formats(5)' man page:

| Sequence | Character    | Terminal Action                             |
|----------+--------------+---------------------------------------------|
| \f       | form-feed    | Moves the printing position to the initial  |
|          |              | printing position of the next logical page. |
| \v       | vertical-tab | Moves the printing position to the start of |
|          |              | the next vertical tab position. If there    |
|          |              | are no more vertical tab positions left on  |
|          |              | the page, the behavior is undefined.        |
",A,6
29843106,2015-04-24 09:03:12.963000+00:00,"First, the reason your code isn't working is that those NaT values aren't the string 'NaT', they're the value pd.NaT. But since I don't think fixing that would actually give you what you want, let's ignore that for the moment.
A Pandas DataFrame, like the NumPy ndarray it's built on, is a compact array of typed, low-level values. That's what makes it small, fast, and type-safe. But that inherently means it can only store values of the specified type. And None is not a value of the datetime64[ns] type.
More specifically, a datetime64[ns] can only hold 64-bit integers that represent datetimes as nanoseconds since the epoch, and None is not a 64-bit integer.
Pandas does have a special value to deal with this, called NaT, for ""Not a Time""; that's the closest thing to None that you can get in any kind of datetime field (just like the more familiar NaN for floats). And that's what you've already got.

Meanwhile, Pandas has special support for None in various different places that tries to do what you often want—storing NaN/NaT/0, or repeating the last value, or various other things. But when that's not what you want, that isn't much help.

If you actually need to have None, the only way to do that is to store normal boxed-up Python objects instead of typed low-level values, which you do by using dtype=object. Then you can stick any Python value into any element, including None, of course. 
But doing that largely defeats the purpose of using Pandas and NumPy. You're probably better off using NaT, and changing the rest of your code—either to expect NaT where you're currently expecting None, or to wrap the DataFrame in something that converts NaT values to None during extraction or printout.

For completeness, if you want to get crazy, there's nothing stopping you from defining an optionaldatetime64[ns] dtype that's just like datetime64[ns] except that it uses the special value reserved for NaT to mean None instead. Or it could even reserve another special value, or a whole bit, to mean None, while leaving NaT alone. Either way, this would be a lot of work, and it would completely break any operations that depend on datetime arithmetic (d - NaT == NaT for any d, but d - None is a TypeError for any d…), and ultimately it's no better than the wrapper solution for any purpose I can think of…",A,5
47423864,2017-11-21 22:31:16.187000+00:00,Because the individual field clean methods provide the data to populate cleaned_data - that's what they're for. So you can't access data for fields which haven't had their clean methods called yet.,A,2
31275786,2015-07-07 17:45:23.943000+00:00,"You need to disambiguate the tuples in the expression:
tmp = ((seq, '-', ref) if seq == ref else (seq, ' ', ref)
       for seq, ref in zip(sequence, reference))

or even
tmp = ((seq, '-' if seq == ref else ' ', ref)
       for seq, ref in zip(sequence, reference))

Demo:
>>> sequence = 'CTGAGTAGC'
>>> reference = 'ATGACTGCC'
>>> tmp = ((seq, '-' if seq == ref else ' ', ref)
...        for seq, ref in zip(sequence, reference))
>>> for tup in tmp:
...     print tup
... 
('C', ' ', 'A')
('T', '-', 'T')
('G', '-', 'G')
('A', '-', 'A')
('G', ' ', 'C')
('T', '-', 'T')
('A', ' ', 'G')
('G', ' ', 'C')
('C', '-', 'C')
",A,2
20201394,2013-11-25 19:19:58.693000+00:00,"Just loop and print:
for item, freq in items:
    print(item, freq)

Strictly speaking, because you return wc.items(), you do not have a dictionary but a list of tuples instead.",A,1
29947893,2015-04-29 15:15:44.137000+00:00,"You are looking for the symmetric difference; all elements that appear only in set a or in set b, but not both:
a.symmetric_difference(b)

From the set.symmetric_difference() method documentation:

Return a new set with elements in either the set or other but not both.

You can use the ^ operator too, if both a and b are sets:
a ^ b

while set.symmetric_difference() takes any iterable for the other argument.
The output is the equivalent of (a | b) - (a & b), the union of both sets minus the intersection of both sets.",A,64
21909189,2014-02-20 13:25:28.600000+00:00,"The common placeholder for stdin or stdout is -:
./myscript.py - - volumeid

and:
if filename == '-':
    input_file = sys.stdin
else:
    input_file = open(filename, 'rU')

etc.
In addition, you could default filename and filename_out to - when there are fewer than 3 command line arguments. You should consider using a dedicated command-line argument parser such as argparse, which can handle these cases for you, including defaulting to stdin and stdout, and using -.
As a side note, I'd not use print to write to a file; I'd just use:
file_new.write(x)

which removes the need to strip off the newlines as well.
You appear to read from the input file twice; once to parse the XML tree, once again to call change_class() with the open file object. What are you trying to do there? You'll have problems replicating that with sys.stdin as you cannot re-read the data from a stream the way you can from a file on disk.
You'd have to read all the data into memory first, then parse the XML from it, then read it it again for change_class(). It'd be better if you used the parsed XML tree for this instead, if possible (e.g. read the file only once, then use the parsed structure from there on out). ",A,3
27932212,2015-01-13 21:55:38.300000+00:00,"Python stores results in multiple targets from left to right, executing the assignment target expression in that order.
So your second version essentially comes down to this:
temp = list[list.index(max(list))],list[0]
list[0] = temp[0]
list[list.index(max(list))] = temp[1]

Note that the list.index(max(list)) is executed after list[0] was altered, and that's where you just stored the maximum value.
This is documented in the Assignment statements documenation:


If the target list is a comma-separated list of targets: The object must be an iterable with the same number of items as there are targets in the target list, and the items are assigned, from left to right, to the corresponding targets.


From there on out it is as if each target is a single target, so the documentation that follows applies from left to right to each target:

Assignment of an object to a single target is recursively defined as follows.
[...]

If the target is a subscription: The primary expression in the reference is evaluated. It should yield either a mutable sequence object (such as a list) or a mapping object (such as a dictionary). Next, the subscript expression is evaluated.


If you changed the order of assignments your code would work:
list[list.index(max(list))], list[0] = list[0], list[list.index(max(list))]

because now list[list.index(max(list))] is assigned to first.",A,5
32041238,2015-08-17 00:22:59.300000+00:00,"You could abuse the url quoting, to get both readable and easy to decode in other languages format that passes your validation function:
#!/usr/bin/env python3
import urllib.parse

def alnum_encode(text):
    return urllib.parse.quote(text, safe='')\
        .replace('-', '%2d').replace('.', '%2e').replace('_', '%5f')\
        .replace('%', '_')

def alnum_decode(underscore_encoded):
    return urllib.parse.unquote(underscore_encoded.replace('_','%'), errors='strict')

s = alnum_encode(""Random  UTF-8 String ☑⚠⚡"")
print(s)
print(alnum_decode(s))

Output
Random_20_F0_9F_90_8D_20UTF_2d8_20String_20_E2_98_91_E2_9A_A0_E2_9A_A1
Random  UTF-8 String ☑⚠⚡

Here's an implementation using a bytearray() (to move it to C later if necessary):
#!/usr/bin/env python3.5
from string import ascii_letters, digits

def alnum_encode(text, alnum=bytearray(ascii_letters+digits, 'ascii')):
    result = bytearray()
    for byte in bytearray(text, 'utf-8'):
        if byte in alnum:
            result.append(byte)
        else:
            result += b'_%02x' % byte
    return result.decode('ascii')
",A,1
17655123,2013-07-15 13:13:36.030000+00:00,"Bram worked against an older version of Python, one where the urllib2 code did not add .msg and .code attributes to addinfourl objects. Specifically, the Python version he developed with did not have this change applied.
The workaround is to copy those attributes yourself from the original addinfourl object in the HTTPContentEncodingHandler class found in the original zurllib.py file:
class HTTPContentEncodingHandler(HTTPHandler):
    """"""Inherit and add gzip/deflate/etc support to HTTP gets.""""""
    def http_open(self, req):
        # add the Accept-Encoding header to the request
        # support gzip encoding (identity is assumed)
        req.add_header(""Accept-Encoding"",""gzip"")
        req.add_header('User-Agent', 'BitTorrent/' + version)
        if DEBUG: 
            print ""Sending:"" 
            print req.headers
            print ""\n""
        fp = HTTPHandler.http_open(self,req)
        headers = fp.headers
        if DEBUG: 
             pprint.pprint(headers.dict)
        url = fp.url
        resp = addinfourldecompress(fp, headers, url)
        resp.code = fp.code
        resp.msg = fp.msg
        return resp
",A,1
37059431,2016-05-05 20:12:51.550000+00:00,"NLTK classifiers work with feature sets; these are always given as dictionaries with feature names mapping to a value. You are passing in a list instead, so you are not producing features as per the NLTK documentation. The code simply expects a Python dictionary, and Python dictionaries have a .copy() method.
See the NLTK tutorial chapter on Learning to Classify Text:

The returned dictionary, known as a feature set, maps from feature names to their values. Feature names are case-sensitive strings that typically provide a short human-readable description of the feature, as in the example 'last_letter'. Feature values are values with simple types, such as booleans, numbers, and strings.

Also see the Featuresets section of the NLTK Classify API documentation:

The features describing a token are encoded using a “featureset”, which is a dictionary that maps from “feature names” to “feature values”. Feature names are unique strings that indicate what aspect of the token is encoded by the feature.

You haven't shared what kind of objects the train_data list contains; if those are feature set dictionaries, you want to use classify_many() instead:
results = classifier.classify_many(test_data)

That method does take a list, but each element must still be a valid feature set.",A,2
39039255,2016-08-19 12:37:43.783000+00:00,"Use the textwrap.fill() function; this takes care of splitting your string, figuring out how many words fit per line, and rejoining the lines with newlines:
import textwrap

for elem, string in dictionary.items():
    dictionary[elem] = textwrap.fill(string)

Adjust the width as needed with the width keyword argument (defaults to 70). See the module documentation for what other options there are.",A,3
36755232,2016-04-20 21:09:18.730000+00:00,"You need to group the both the expression under the division bar in parentheses:
>>> (
...     (.310 * .290) / .260
...     /
...     (
...         (.310 * .290) / .260
...         +
...         ((1 - .310) * (1 - .290) / (1 - .260))
...     )
... )
0.3430943785456421

This ensures that the addition of the two parts under the bar takes place before dividing the upper expression result.
You didn't do that in your attempt, so you essentially get this instead:
(
    ((.310 * .290) / .260)
    /
    (.310 * .290)
    /
    (
        .260
        + 
        (
            (1 - .310) * (1 - .290) / 1
            -
            .260
        )
    )
)

I split both my and your attempts out according to precedence rules. Notice the difference?
The result of (.310 * .290) should first be divided .260 and have the rest of the expression under the bar summed to it; your version divides the outcome from above the bar by that much first, then that result is divided by the outcome of .260 + .... That's not the same expression.
Some of the parentheses even in the original expression are redundant. Multiplication and division goes before addition and subtraction, so we can remove parentheses around multiplication and division:
(
     .310 * .290 / .260
     /
     (.310 * .290 / .260 + (1 - .310) * (1 - .290) / (1 - .260))
)

or all on one line:
>>> .310 * .290 / .260 / (.310 * .290 / .260 + (1 - .310) * (1 - .290) / (1 - .260))
0.3430943785456421
",A,1
24974567,2014-07-26 19:08:33.007000+00:00,"Use a raw string literal instead:
r'\xb9\xfe'

or print the output of repr() of your string:
print repr('\xb9\xfe')
",A,4
4840863,2011-01-30 02:54:33.447000+00:00,"
whether it's theoretically possible to allow changes to Python code 

Yes.  It's possible to update a code object ""on the fly"".  Nothing in the Python VM seems to prevent this.  There's not an ongoing verification of the checksums or anything.

somehow update all the references to the objects that changed, 

This doesn't even make sense.  Once the class-level method definition is changed, ""updating"" all the references isn't even necessary or sensible.  The code is changed in the one and only place it exists.
This is why debuggers are bad.  They seem to lead to murky, unclear thinking.  Thinking too much about the debugger means thinking too little about the real problem at hand.  
TDD is a far, far better investment.  Small, manageable unit tests run quickly and provide enduring evidence that things  work.  
http://en.wikipedia.org/wiki/Test-driven_development",A,0
13517408,2012-11-22 17:15:16.817000+00:00,"Sort on your grouping criteria, then use itertools.groupby():
from itertools import groupby

key = lambda el: el[1]
list2 = [[[el] for el in grouped] for group, grouped in groupby(sorted(input, key=key), key)]

Demonstration:
>>> from itertools import groupby
>>> input = ['A0', 'A1', 'A2', 'B0', 'B1', 'B2', 'C1', 'C0', 'C2']
>>> key = lambda el: el[1]
>>> [[[el] for el in grouped] for group, grouped in groupby(sorted(input, key=key), key)]
[[['A0'], ['B0'], ['C0']], [['A1'], ['B1'], ['C1']], [['A2'], ['B2'], ['C2']]]

Your output requirement is a little convoluted; if returning lists of the grouped elements is enough, calling list() on grouped is enough:
>>> [list(grouped) for group, grouped in groupby(sorted(input, key=key), key)]
[['A0', 'B0', 'C0'], ['A1', 'B1', 'C1'], ['A2', 'B2', 'C2']]
",A,3
650254,2009-03-16 12:38:54.460000+00:00,"The expression:
(k for k in (j for j in (i for i in xrange(10))))

is equivalent to:
(i for i in xrange(10))

that is almost the same:
xrange(10)

The last variant is more elegant than the first one.",A,0
23232483,2014-04-23 00:04:54.927000+00:00,"Unrelated to your .flush() issue, you could pass one file via stdin instead of writing the content on disk:
from tempfile import NamedTemporaryFile
from subprocess import Popen, PIPE

with NamedTemporaryFile() as file:
    file.write(prod_notes)
    file.flush()
    p = Popen(['diff', '-', file.name], stdin=PIPE)
    p.communicate(stage_notes) # diff reads the first file from stdin

if p.returncode == 0:
    print('the same')
elif p.returncode == 1:
    print('different')
else:
    print('error %s' % p.returncode)

diff reads from stdin if input filename is -.
If you use a named pipe then you don't need to write the content on the disk at all:
from subprocess import Popen, PIPE
from threading import Thread

with named_pipe() as path:
    p = Popen(['diff', '-', path], stdin=PIPE)
    # use thread, to support content larger than the pipe buffer
    Thread(target=p.communicate, args=[stage_notes]).start()
    with open(path, 'wb') as pipe:
        pipe.write(prod_notes)

if p.wait() == 0:
    print('the same')
elif p.returncode == 1:
    print('different')
else:
    print('error %s' % p.returncode)

where named_pipe() context manager is defined as:
import os
import tempfile
from contextlib import contextmanager
from shutil import rmtree

@contextmanager
def named_pipe(name='named_pipe'):
    dirname = tempfile.mkdtemp()
    try:
        path = os.path.join(dirname, name)
        os.mkfifo(path)
        yield path
    finally:
        rmtree(dirname)

The content of a named pipe doesn't touch the disk.",A,2
16670205,2013-05-21 12:37:31.233000+00:00,"You'll have to use the 'n' locale-aware number format instead, and set your locale to one that uses a space as a thousands separator. Yes, this is painful.

'n' - Number. This is the same as 'd', except that it uses the current locale setting to insert the appropriate number separator characters.
",A,11
21181418,2014-01-17 08:46:13.373000+00:00,"The .string attribute gives you just the tag text:
uni_name = soup.title.string

If you want to use only the first part, split on the | pipe:
uni_name = soup.title.string.partition('|')[0].strip()

This uses str.partition() to split just once (for efficiency), takes the first part of the result, and removes any extra whitespace around that result.
Demo:
>>> soup.title
<title>College of Agriculture &amp; Life Sciences | The University of Arizona, Tucson, Arizona</title>
>>> soup.title.string
u'College of Agriculture & Life Sciences | The University of Arizona, Tucson, Arizona'
>>> soup.title.string.partition('|')[0].strip()
u'College of Agriculture & Life Sciences'
",A,2
2439676,2010-03-13 19:21:35.823000+00:00,"You don't make it absolutely clear what you want to do, but I think there are two possibilities here.
The general way to override the database query is to define a custom Manager, and override get_query_set method. You can add extra filtering criteria here.
However, if I understand your question properly, you are trying to change the query for a particular field only. In this case, I think the better answer is to define a custom Field. Here you can override get_db_prep_lookup which allows you to customise the value that is used in the database lookup.",A,3
39296492,2016-09-02 15:56:32.337000+00:00,"lsof will only show the child processes which are actually binding to the port. You need to kill the master process. If you start gunicorn with the --pid option you can give it a filename to store the PID of that process in, then you can kill it directly; if not you can get it from ps|grep gunicorn.
Even better, as elethan suggests in the comments, set up gunicorn as a service using whatever process manager exists on your system - systemd, upstart, supervisor, or whatever - and use that to start and stop it.",A,2
41999160,2017-02-02 10:01:34.933000+00:00,"If you're using Postgres, you could use an ArrayField, which maps to the db's built-in array support, to store a list of the selected language codes.
If not, it's relatively simple to write a custom field that stores a comma-separated list of codes and translates it to/from a list - there are probably third-party implementations out there already.",A,0
640467,2009-03-12 21:02:20.420000+00:00,"Put in the file ~/.gitignore:
*.pyc

Run:
git config --global core.excludesfile ~/.gitignore

I agree with @David Wolever this option should be used with caution if ever.
The ~/.gitignore (""%HOME%\.gitignore"" on Windows) is just a convention you can use any filename you like e.g., ""c:\docs\gitignore.txt"".",A,57
611124,2009-03-04 15:41:51.253000+00:00,"First, you need some sense of Location.  Your various objects occupy some amount of coordinate space.
You have to decide how regular these various things are.  In the trivial case, you can drop them into your coordinate space as simple rectangles (or rectangular solids) to make locations simpler to plan out.
If the things are irregular -- and densely packed -- life is somewhat more complex.
Define a Map to contain locations.  Each location has a span of coordinates; if you work with simple rectangles, then each location  can have a (left, top, right, bottom) tuple.  
Your Map will need methods to determine who is residing in a given space, and what's adjacent to the space, etc.
You can then unit test this with a fixed set of locations you've worked out that can all be dropped into the map and pass some basic sanity checks for non-conflicting, adjacent, and the like.

Second, you need a kind of ""maze generator"".  A simply-connected maze is easily generated as a tree structure that's folded up into the given space.
The maze/tree has a ""root"" node that will be the center of the maze. Not necessarily the physical center of your space, but the root node will be the middle of the maze structure. 
Ideally, one branch from this node contains one ""entrance"" to the entire space.
The other branch from this node contains one ""exit"" from the entire space.  
Someone can wander from entrance to exit, visiting a lot of ""dead-end"" locations along the way.
Pick a kind of space for the root node.  Drop it into your Map space.
This will have 1 - n entrances, each of which is a sub-tree with a root node and 1 - n entrances.  It's this multiple-entrance business that makes a tree a natural fit for this structure.  Also a proper tree is always well-connected in that you never have isolated sections that can't be reached.
You'll -- recursively -- fan out from the root node, picking locations and dropping them into the available space.  
Unit test this to be sure it fills space reasonably well.

The rest of your requirements are fine-tuning on the way the maze generator picks locations.  
The easiest is to have a table of weights and random choices.  Choose a random number, compare it with the weights to see which kind of location gets identified.

Your definition of space can be 2D or 3D -- both are pretty rational.  For bonus credit, consider how you'd implement a 2D-space tiled with hexagons instead of squares.
This ""geometry"" can be a Strategy plug-in to the various algorithms.  If you can replace square 2D with hexagonal 2D, you've done a good job of OO Design.",A,7
38514155,2016-07-21 20:51:19.230000+00:00,"The .hg directory cannot be removed. It contains everything Mercurial knows about your project. Deleting that directory effectively removes all source control.
Add files you don't want to be tracked to the .hgignore file in the root of your repository instead. See the hgignore documentation for the format of this file.",A,3
18205869,2013-08-13 09:50:03.107000+00:00,"os.path.isfile() and os.path.isdir() only work on local filenames. 
I'd use the sftp.listdir_attr() function instead and load full SFTPAttributes objects, and inspect their st_mode attribute with the stat module utility functions:
import stat

def downLoadFile(sftp, remotePath, localPath):
    for fileattr in sftp.listdir_attr(remotePath):  
        if stat.S_ISDIR(fileattr.st_mode):
            sftp.get(fileattr.filename, os.path.join(localPath, fileattr.filename))
",A,24
25554602,2014-08-28 17:24:33.907000+00:00,"The time.time() function does not take any arguments, because it is a function that simply returns the current time (in the form of seconds since a specific point in time, the UNIX epoch). You cannot use it to compare two function runtimes like that. You could use it to measure time passing by storing the value before running a function then comparing that to the time.time() value afterwards, but that's a poor method of measuring performance.
The timeit.timeit() function does let you measure how much a test function takes, by repeatedly executing it and by making sure other factors that may impede accurate measurement are at least minimized. You can only test one such function at a time however.
To test one function, pass in Python source to run the function, and another function to set up the test. That setup should include importing the function:
timeit.timeit(""normal(sound)"", 'from __main__ import normal, sound')

Do this again for the other function and compare the outcomes against one another.
Take into account that the function will be executed many times (you can adjust how many), so if the function changes global state you'd have to reset that state each time. That'll change how you can measure the performance as well.",A,0
44672082,2017-06-21 09:29:08.390000+00:00,"You are using a very old (and unsupported) version of Django. In that version, you need to pass a Context instance to template.render, not a dict.
from django.template import Context
...
context = Context({...})
html = template.render(context)

But, you must upgrade. There is no reason to be using Django 1.5 at this point.",A,1
28522826,2015-02-15 03:51:13.330000+00:00,"There are two major string types in Python: bytestrings (a sequence of bytes) that represent binary data and Unicode strings (a sequence of Unicode codepoints) that represent human-readable text. It is simple to convert one into another (☯):
unicode_text = bytestring.decode(character_encoding)
bytestring = unicode_text.encode(character_encoding)

If you open a file in binary mode e.g., 'rb' then file.read() returns a bytestring (bytes type):
>>> b'A' == b'\x41' == chr(0b1000001).encode()
True


There are several methods that can be used to classify bytes:

string methods such as bytes.isdigit():
>>> b'1'.isdigit()
True

string constants such as string.printable
>>> import string
>>> b'!' in string.printable.encode()
True

regular expressions such as \d
>>> import re
>>> bool(re.match(br'\d+$', b'123'))
True

classification functions in curses.ascii module e.g., curses.ascii.isprint()
>>> from curses import ascii
>>> bytearray(filter(ascii.isprint, b'123'))
bytearray(b'123')


bytearray is a mutable sequence of bytes — unlike a bytestring you can change it inplace e.g., to lowercase every 3rd byte that is uppercase:
>>> import string
>>> a = bytearray(b'ABCDEF_')
>>> uppercase = string.ascii_uppercase.encode()
>>> a[::3] = [b | 0b0100000 if b in uppercase else b 
...           for b in a[::3]]
>>> a
bytearray(b'aBCdEF_')

Notice: b'ad' are lowercase but b'_' remained the same.

To modify a binary file inplace, you could use mmap module e.g., to lowercase 4th column in every other line in 'file':
#!/usr/bin/env python3
import mmap
import string

uppercase = string.ascii_uppercase.encode()
ncolumn = 3 # select 4th column
with open('file', 'r+b') as file, \
     mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_WRITE) as mm:
    while True:
        mm.readline()   # ignore every other line
        pos = mm.tell() # remember current position
        if not mm.readline(): # EOF
            break
        if mm[pos + ncolumn] in uppercase:
            mm[pos + ncolumn] |= 0b0100000 # lowercase

Note: Python 2 and 3 APIs differ in this case. The code uses Python 3.
Input
ABCDE1
FGHIJ
ABCDE
FGHI

Output
ABCDE1
FGHiJ
ABCDE
FGHi

Notice: 4th column became lowercase on 2nd and 4h lines.

Typically if you want to change a file: you read from the file, write modifications to a temporary file, and on success you move the temporary file inplace of the original file:
#!/usr/bin/env python3
import os
import string
from tempfile import NamedTemporaryFile

caesar_shift = 3
filename = 'file'

def caesar_bytes(plaintext, shift, alphabet=string.ascii_lowercase.encode()):
    shifted_alphabet = alphabet[shift:] + alphabet[:shift]
    return plaintext.translate(plaintext.maketrans(alphabet, shifted_alphabet))

dest_dir = os.path.dirname(filename)
chunksize = 1 << 15
with open(filename, 'rb') as file, \
     NamedTemporaryFile('wb', dir=dest_dir, delete=False) as tmp_file:
    while True: # encrypt
        chunk = file.read(chunksize)
        if not chunk: # EOF
            break
        tmp_file.write(caesar_bytes(chunk, caesar_shift))
os.replace(tmp_file.name, filename)

Input
abc
def
ABC
DEF

Output
def
ghi
ABC
DEF

To convert the output back, set caesar_shift = -3.",A,3
49788116,2018-04-12 04:41:16.707000+00:00,"The first problem is that you're confusing raw string literals with strings. A string literal is the way you enter a string in your Python source code, like ""abc"". You can use an r prefix to make this a raw string literal, like r""a\b\c"". That doesn't change what kind of string it is, it just prevents the usual Python source code rules from being applied, so you get actual backslashes and letters instead of special characters like a backspace. So, you can't turn user input into a raw string, but you don't have to—the string is already exactly the letters the user typed.
(This can be a bit confusing, because when you print out a regular expression, you see something like re.compile(r'\.', re.UNICODE). That r isn't really part of the object; it's showing you how you could create exactly the same regular expression object in your source code.)

The re.escape function is sort of similar, but it's not the same thing. What it does is take a regex pattern and turn it into another pattern with all the regex special characters escaped. So, for example, re.escape('.') gives you \., meaning it will only match an actual . character, rather than matching anything. Since user input can easily contain characters like ., and the user probably isn't asking you to strip every character, you were right to use re.escape here.
So:
re.compile(re.escape(thing2))


When you tested this code with the input \d and tried to search the string 123, it didn't find anything. But that's exactly what you want. If the user types in \d, they're not asking to strip off any digit, they're asking to strip off \ and d.
Of course for some programs, you really do want to take regular expressions from the user. (For example, you might want to write something similar to grep.) In that case, you wouldn't call re.escape.

One last thing: When you call '1234'.strip('14'), that doesn't strip off the string '14' from both sides, it strips off any characters that are in the string '14'—in order words, you'll get back 23. To make this work with a regular expression, you want to turn that '14' into '1|4'. In other words, you want to escape each character, and then join those characters up with '|', to get the pattern.",A,1
45185815,2017-07-19 09:04:23.843000+00:00,"__init__ is not responsible for creating a instance. It is a hook method that Python calls for you after the instance is already created. You can't prevent instance creation from there. Besides, you don't want to prevent all instance creation, even your classmethod has to create an instance at some point.
Since all you want to do is raise an exception when your factory method is not used to create the instance, it's still fine to raise an exception in __init__ method. That'll prevent the new instance from being assigned anywhere. What you need to do then is distinguish between direct access, and your factory method being used.
You could achieve this is several different ways. You could use a ""secret"" token that only the factory method passes in:
_token = object()  # unique token to flag factory use

class A:
    def __init__(self, data, _from_factory=None):
        if _from_factory is not _token:
            raise TypeError(f""Can't create {type(self).__name__!r} objects directly"")
        self._data = data

    @classmethod
    def create_from_file(cls, file):
        data = file.read()
        return cls(data, _from_factory=_token)

The classmethod still creates an instance, the __init__ is still called for that instance, and no exception is raised because the right token was passed in.
You could make your class an implementation detail of the module and only provide a public factory function:
def create_from_file(cls, file):
    data = file.read()
    return _A(data)

class _A:
    def __init__(self, data):
        self._data = data

Now the public API only gives you create_from_file(), the leading underscore tells developers that _A() is an internal name and should not be relied on outside of the module.
Actual instance creation is the responsibility of the object.__new__ method; you could also use that method to prevent new instances to be created. You could use the same token approach as I showed above, or you could bypass it altogether by using super() to call the original overridden implementation:
class A:
    def __new__(cls, *args, **kwargs):
        raise TypeError(f""Can't create {cls.__name__!r} objects directly"")

    def __init__(self, data):
        self._data = data

    @classmethod
    def create_from_file(cls, file):
        data = file.read()
        # Don't use __new__ *on this class*, but on the next one in the
        # MRO. We'll have to manually apply __init__ now.
        instance = super().__new__(cls)
        instance.__init__(data)
        return instance

Here a direct call to A() will raise an exception, but by using super().__new__ in the classmethod we bypass the A.__new__ implementation.
Note: __new__ is implicitly made a staticmethod, so we have to manually pass in the cls argument when we call it from the classmethod.",A,1
35182775,2016-02-03 16:42:18.960000+00:00,"You are right that int.__lt__(12, x) won't work; that method returns the NotImplemented singleton in that case:
>>> int.__lt__(12, x)
NotImplemented

However, Python then calls type(x).__gt__(), not __lt__, as the inverse:
>>> class Demo:
...     def __lt__(self, other):
...         print('__lt__ called')
...         return NotImplemented
...     def __gt__(self, other):
...         print('__gt__ called')
...         return NotImplemented
...
>>> 12 < Demo()
__gt__ called
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unorderable types: int() < Demo()

That's because you can still determine if 12 is smaller than the other object by asking the other object if 12 is bigger than it. If you asked for the inverse (12 > x) then your C.__lt__() is called:
>>> 12 > Demo()
__lt__ called
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unorderable types: int() > Demo()

Note that Python raises the TypeError exception, because Demo.__gt__() and Demo.__lt__() both returned NotImplemented too. You don't have to raise the exception yourself.",A,1
23257776,2014-04-24 00:40:14.747000+00:00,"The two requests are independent:
r = requests.get(url, auth=HTTPDigestAuth('user', 'pass')) 
response = requests.get(url) #XXX <-- DROP IT

The second request does not send any credentials. Therefore it is not surprising that it receives 401 Unauthorized http response status.
To fix it:

Use the same url as you use in your browser. Drop digest-auth/auth/user/pass at the end. It is just an example in the requests docs
Print r.status_code instead of response.status_code to see whether it's succeeded.


Why would you use username/password in the url and in auth parameter? Drop username/password from the url. To see the request that is sent and the response headers, you could enable logging/debugging:
import logging
import requests
from requests.auth import HTTPDigestAuth

# these two lines enable debugging at httplib level (requests->urllib3->httplib)
# you will see the REQUEST, including HEADERS and DATA, 
# and RESPONSE with HEADERS but without DATA.
# the only thing missing will be the response.body which is not logged.
try:
    import httplib
except ImportError:
    import http.client as httplib

httplib.HTTPConnection.debuglevel = 1

logging.basicConfig(level=logging.DEBUG) # you need to initialize logging, 
                      # otherwise you will not see anything from requests

# make request
url = 'https://example.com/cgi/metadata.cgi?template=html'
r = requests.get(url, auth=HTTPDigestAuth('myUsername', 'myPassword'),
                 timeout=10)
print(r.status_code)
print(r.headers)
",A,3
327548,2008-11-29 13:46:42.053000+00:00,"Let's look at
d[t].append(c)

What is the value of d[t]?  Try it.
d = {}
t = 0
d[t]

What do you get?  Oh.  There's nothing in d that has a key of t.
Now try this.
d[t] = []
d[t]

Ahh.  Now there's something in d with a key of t.
There are several things you can do.  

Use example 2.
Use setdefault.  d.setdefault(t,[]).append(c).
Use collections.defaultdict.  You'd use a defaultdict(list) instead of a simple dictionary, {}.


Edit 1.  Optimization
Given input lines from a file in the above form: ts, x, y, the grouping process is needless.  There's no reason to go from a simple list of ( ts, x, y ) to a more complex
list of ( ts, (x,y), (x,y), (x,y), ... ).  The original list can be processed exactly as it arrived.
d= collections.defaultdict(list)
for ts, x, y in someFileOrListOrQueryOrWhatever:
    d[ts].append( (x,y) )


Edit 2.  Answer Question
""when initialising a dictionary, you need to tell the dictionary what the key-value data structure will look like?""
I'm not sure what the question means.  Since, all dictionaries are key-value structures, the question's not very clear.  So, I'll review the three alternatives, which may answer the question.
Example 2.
Initialization
d= {}

Use
if t not in d:
    d[t] = list()
d[t].append( c )

Each dictionary value must be initialized to some useful structure.  In this case, we check to see if the key is present; when the key is missing, we create the key and assign an empty list.
Setdefault
Initialization
d= {}

Use
d.setdefault(t,list()).append( c )

In this case, we exploit the setdefault method to either fetch a value associated with a key or create a new value associated with a missing key.
default dict
Initialization
import collections
d = collections.defaultdict(list)

Use
d[t].append( c )

The defaultdict uses an initializer function for missing keys.  In this case, we provide the list function so that a new, empty list is created for a missing key.",A,72
19014545,2013-09-25 20:25:08.680000+00:00,"Trying to interact with a CLI script—send commands, wait for responses and prompts, etc.—is a pain in the neck. There are libraries like pexpect that make it easier for you, but it's still not easy.

But fortunately, you don't have to. respotify is clearly usable as a module instead of a script, so, rather than try to generate input to feed it so it can parse it, and so on, just import it and use it directly.
Or, even more simply, all of the code in respotify is simple—at least as simple as trying to interact with it over a CLI. So just use the Spotify modules directly.",A,3
26243727,2014-10-07 19:30:19.630000+00:00,"There are a few problems here.
First, there's no good reason to iterate through all the numbers just to see if one of them matches selection; you already know that will be true if 1 <= selection <= len(response), and you can then just do response[selection-1] to get the v. (If you know anything about dicts, you might be able to see an even more convenient way to write this whole thing… but if not, don't worry about it.)
But if you really want to do this exhaustive search, you shouldn't print out There is an error man after any mismatch, because then you're always going to print it at least twice. Instead, you want to only print it if all of them failed to match. You can do this by keeping track of a ""matched"" flag, or by using a break and an else: clause on your for loop, whichever seems simpler, but you have to do something. See break and continue Statements, and else Clauses on Loops in the tutorial for more details.
But the biggest problem is that raw_input returns a string, and there's no way a string is ever going to be equal to a number. For example, try '1' == 1 in your interactive interpreter, and it'll say False. So, what you need to do is convert the user's input into a number so you can compare it. You can do that like this:
try:
    selection = int(selection)
except ValueError:
    print ""That's not a number!""
    continue
",A,1
25015442,2014-07-29 12:34:00.927000+00:00,"Split the string on the period:
third_part = inputstring.split('.', 3)[2]

I've used str.split() with a limit here for efficiency; no point in splitting the dd.ee part here, for example.
The [2] index then picks out the third result from the split, your cccc string:
>>> ""aa.bb.cccc.dd.ee"".split('.', 3)[2]
'cccc'
",A,0
35094626,2016-01-29 21:47:51.740000+00:00,"Related objects are querysets. So you can order by any field you like:
f.b.order_by('name')
",A,3
17182098,2013-06-19 02:45:31.690000+00:00,"Your question is basically meaningless, because you're misunderstanding some fundamental things and creating problems that don't exist. I tried answering in comments, but there's a limit to what you can do that way, so…


I want to do everything without using file-like objects, because App Engine cannot provide conventional Python filenames for dynamic files.

You don't need filenames, or files, for file-like objects. That's the whole idea behind file-like objects.


App Engine does not provide a way to open a file-like object (unless the file was uploaded as part of the app).

No, you are still confusing file objects and file-like objects. A file object represents an actual file on disk. GAE restricts those. A file-like object is any object with the same API—that is, an object that acts like a file, without (necessarily) actually being one. GAE does not do anything to prevent you from creating file-like objects.
--
The paradigm example of a file-like object, a StringIO.StringIO, acts like a file object, but instead of actually reading and writing a file, it reads and writes an in-memory string buffer.
So, you can open a file-like object for writing like this:
my_file_like_obj = StringIO()

Or, if you've got a buffer in memory and you want to be able to read from it like a file:
my_file_like_obj = StringIO(buffer)


However, there are many cases where Python/GAE already gives you a file-like object that you can just use as-is, without reading it into a buffer and wrapping it up in another file-like object. Many networking APIs give you back file-like objects, but not all.
For example, if you call urllib2.urlopen, the result is a file-like object; if you call urlfetch.fetch, it's not, so you'll have to use StringIO(response.content) if you need one.


So if GAE gets compressed data from the Linux box, I don't know of a way to uncompress it if the compression module insists that I go through a file-like object.

If it insists on a file-like object, give it a file-like object. Creating an actual file is one way to do it, but not the only way. If you've got a urllib2.urlopen response, just pass that. If you've got a buffer in memory, just wrap it in a StringIO. And so on.


The gzip module, for example, insists on having a filename: http://docs.python.org/2/library/gzip.html

No it doesn't. Read the documentation you linked to:

class gzip.GzipFile([filename[, mode[, compress level[, fileobj[, mtime]]]]])

Notice that there's a fileobj parameter as well as a filename parameter? And the very first line of the documentation says:

… At least one of fileobj and filename must be given a non-trivial value …

So, it doesn't insist on having a filename, unless fileobj is None. To get around that, just… don't pass None for fileobj.
Does fileobj have to be a real file object, or can it be another file-like object? Well, the very next paragraph says:

… The new class instance is based on fileobj, which can be a regular file, a StringIO object, or any other object which simulates a file.

So, there you go.

Unfortunately, Python 2.x isn't 100% consistent about what counts as a file-like object, and the documentation doesn't always make it clear. (This is cleaned up a lot in 3.x, but that doesn't do you any good if you're using GAE.)
If some API doesn't like your file-like object because it doesn't simulate enough of the API, you will find out by getting an AttributeError. For example, you may get an error saying that the object you got back from urllib2.urlopen doesn't have a seek attribute.
The workaround is simple: read it into memory and create a StringIO. In other words, justt change fileobj=my_file_obj to fileobj=StringIO(my_file_obj.read()).

Also note that a GzipFile is itself a file-like object. This is important, because it means you can chain things together—you can make a GzipFile out of a StringIO, and then make a TarFile out of a GzipFile, and so on.


""Thread-safe"" on linux = Application will be behind a webserver, and so separate threads will be likely be called upon to compress and/or decompress at the same time.

That's not a problem. Again, read the documentation you linked to:

if you need to use a single LZMAFile instance from multiple threads, it is necessary to protect it with a lock.

Compressing and/or decompressing multiple independent LZMAFile instances is not a problem. It's only if you want to share the same instance across threads. And there is almost never a good reason to do so.


The Linux app starts by reading a few thousand (out of many million) semi-random chunks of compressed data from disk, then uncompressing each, then altering each uncompressed chunk, then compressing the altered chunks, then sending to GAE.

All of the compressors you're talking about are stream compressors. You cannot decompress an arbitrary chunk out of the middle of a file without compressing the file up to that point.
What that implies to me is that what you actually have is a bunch of independently-compressed chunks (whether in separate files, or concatenated together into a single file isn't clear, but doesn't really matter).
This means that you do not need to share decompressors or compressors anywhere. For example:
with lzma.LZMAFile(chunk_path) as f:
    decompressed_chunk = f.read()
new_chunk = alter(decompressed_chunk)
sio = StringIO.StringIO()
with lzma.LZMAFile(fileobj=sio) as f:
    f.write(new_chunk)
compressed_chunk = sio.getvalue()
send_to_gae(compressed_chunk)

There is nothing to share here between threads. Even if 200 threads are doing this at the same time, even if 100 of them are trying to process the same chunk file, there still won't be a problem. The only thing that needs to be sequenced is that send_to_gae at the end.


Right now, the app uses zlib and runs flawlessly under light load within cherrypy, but raises zlib errors as soon as requests start happening in parallel.

Without knowing more about your code, it's very hard to debug it, but I've got a good guess: You're doing the compression by writing to a temporary file, and rather than using the safe APIs in tempfile, you've reinvented the wheel, with unique bugs, meaning that you end up with threads overwriting each others' temporary files.


what does the comment about individual locking mean with respect to bz2

Admittedly, it's a bit confusing. All it says is:

Thread safety uses individual locking mechanism.

This clearly means it's thread safe, but why do you care what locking mechanism they use? And what's an ""individual locking mechanism"" anyway?
You can only tell by looking at the source).
What they mean is that each BZ2Compressor (and BZ2Decompressor) object has its own separate lock, so one of them can lock without affecting the others.
If you haven't dealt with threading in Python C extensions, you may not understand what this is all about. Normally, in Python, every thread needs to hold the GIL to do any work, which means only one thread can run at a time. But a C extension module can release the GIL while doing CPU-heavy work with non-Python objects (like, say, compressing a big buffer). If N threads release the GIL, up to N+1 threads can be running in parallel, meaning you get a big advantage out of that 8-core CPU of yours without running multiple processes. However, you can't touch any Python objects while the GIL is released unless you protect them with a lock.
Many modules that release the GIL for speedup create a single module lock (sometimes because it's not so easy to figure out what objects the code might touch). That means you can run one thread doing that module's stuff in parallel with threads doing other things, but not more than one thread doing that module's stuff. 
But if each thread only needs to touch a single object, you can just use a different lock for each object, which means you can run as many threads as you want in parallel, as long as they're all working on different objects.
If you do try to use the same object in two threads at the same time, it won't break anything; you'll just end up with one thread waiting to acquire the lock until the other's done (which is no better or worse than waiting on the GIL).",A,3
26475899,2014-10-20 22:20:47.543000+00:00,"The first point is that you need some rule that tells you which values are integers and which ones aren't. In a data set that includes things like 32s, I'm not sure it makes sense to just treat anything that could be an integer as if it were.
But, for simplicity, let's assume that is the rule you want: anything that could be an integer is. So, int(a) is already pretty close; the only issue is that it can fail. What do you do with that?
Python is designed around EAFP: it's Easier to Ask Forgiveness than Permission. Try something, and then deal with the fact that it might fail. As Cyber suggests, with a try statement:
try:
    intvalue = int(a)
except ValueError:
    # Oops, it wasn't an int, and that's fine
    pass
else:
    # It was an int, and now we have the int value
    maxvalue = max(maxvalue, intvalue)
",A,4
8011214,2011-11-04 14:57:21.357000+00:00,"You don't need to put it in extra_context. It's already captured in the URL, so is present in the params dictionary in the template: {{ params.user_id }}. 
See the documentation - and also note that these old function-based generic views are deprecated, and you should be using the class-based TemplateView.",A,6
8401406,2011-12-06 14:22:19.607000+00:00,"
What I don't like about this approach is that there is a lot of repeated information. 

That's the point of a warehouse.  Repeat the information to represent (a) historical facts as they occurred and (b) reduce the number of joins.

If Attribute1 changes once a week and Attribute2 only once a year, you end up repeating Attribute2 every week redundantly. If you have a lot of attributes that can add up.

Wrong.  It doesn't add up very quickly at all.
You appear to be talking about dimensions in a star schema.  They're relatively small. The storage is irrelevant compared to the fact tables.  Don't normalize or optimize.  Consider this a ""pre-joined"", ""high-speed"", ""denormalized"", ""reporting-only"" table.  Be happy with non-normalized data: it's faster.
If you're talking about a Fact Table, then these changes have different time granularity and never should have been in the same fact table.",A,7
24062027,2014-06-05 13:49:53.407000+00:00,"The request.POST dict is for form-encoded values. You're not sending those: you're sending a single JSON blob. You can use request.body to get that JSON, and then parse it to get a Python dict.
usersV = json.loads(request.body)
",A,0
48102186,2018-01-04 19:26:02.453000+00:00,"You haven't accounted for the case where column_exclude_list is empty. An empty list is a false-y value:
>>> type_ = 'list'
>>> column_exclude_list = []
>>> column_exclude_edit = 'column_exclude_edit'  # for demo purposes
>>> type_ == 'list' and column_exclude_list or column_exclude_edit
'column_exclude_edit'

Because column_exclude_list is a false value, the and operator produces an empty list and the or operator produces the other operand.
This doesn't mean that and and or are broken or work differently in Python than they do in other languages. You just haven't considered that in a <test> and <true expression> or <false expression> construct, that the <true expression> is still used to determine the outcome of the whole. This applies to all boolean logic, not just Python.
Don't use and and or for a conditional expression. Use an actual conditional expression, so <true expression> if <test> else <false expression>:
column_exclude_list if type_ == 'list' else column_exclude_edit

Like and and or, this lazily evaluates either one of the two operand expressions based on the outcome of the if test. If column_exclude_list is empty, it'll still be returned, because it's value is not used to determine the outcome of further boolean testing.",A,5
41183853,2016-12-16 11:55:30.467000+00:00,"You should look at what the response actually is. When I try that URL with those parameters, I get a mediawiki help page; reading through, it seems that you need to supply an action parameter with the value of lyrics.
query = urllib.urlencode(dict(action='lyrics', artist=artist, song=song, fmt=""realjson""))
",A,1
21234629,2014-01-20 12:50:17.310000+00:00,"You forgot to call the list() type:
list_of_tuples = list()
#    ----------------^ You didn't do this.

Your exception (as posted in the comments) shows that you tried to call .append on the type object instead:
>>> list.append(())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: descriptor 'append' requires a 'list' object but received a 'tuple'
>>> list().append(())

Better use [] to produce an empty list in any case:
list_of_tuples = []
",A,4
32328488,2015-09-01 09:48:42.423000+00:00,"To redirect all standard output of an external process to a file in Python:
#!/usr/bin/env python
from subprocess import check_call, STDOUT

with open('log', 'ab', 0) as file:
    check_call(['program', 'arg 1', 'arg 2'], stdout=file, stderr=STDOUT)

The output is redirected as is. To make it conform to the logging format, you might need it to pass through your program explicitly:
#!/usr/bin/env python3
import logging
from subprocess import Popen, PIPE, STDOUT

logging.basicConfig(filename='log',level=logging.DEBUG)
with Popen(['program', 'arg 1', 'arg 2'], stdout=PIPE, stderr=STDOUT, bufsize=1,
           universal_newlines=True) as process:
    for line in process.stdout:
        logging.debug('program output %s', line.rstrip('\n'))

The code decodes program's standard output using locale.getpreferredencoding(False) and appends lines to the log file using logging module (you could configure whatever logging format you like using standard logging tools).",A,0
26226257,2014-10-06 23:11:04.350000+00:00,"The first step is obviously to split each key into components:
target = {}
for key, value in source.items():
    components = key.split('_')

Now, what do you do with those components? Walk each one from left to right, keeping track of the current collection and the index or key to the next one, creating any missing sub-collections along the way.
You haven't specified your rule for what counts as an index vs. a key, what happens if indices are skipped or appear out of order, what happens if you get both an index and a key under the same prefix, etc., so I'll just pick some arbitrary rules for each—just treat every component as a key, because that makes for the shortest code. That's obviously not the rule you want, but if you understand the code, you should be able to adapt it as appropriate; if not, you need to understand it before you try to use it.
    subtarget = target
    for component in components[:-1]:
        subtarget = subtarget.setdefault(component, dict())
    subtarget[components[-1]] = value

If you understand the code but don't know how to adapt it, you probably have to look up the int function, the try statement, or the list.insert method.",A,2
39549096,2016-09-17 16:25:35.643000+00:00,"Yes, the associated code object accounts for all local names in the co_nlocals attribute:
foo.__code__.co_nlocals

Demo:
>>> def foo():
...     a = 1
...     b = 2
...
>>> foo.__code__.co_nlocals
2

See the Datamodel documentation:

User-defined functions
[...]
__code__   The code object representing the compiled function body. 
Code objects
[...]
Special read-only attributes: [...] co_nlocals is the number of local variables used by the function (including arguments); [...]
",A,8
1456147,2009-09-21 18:46:25.693000+00:00,"""Is this the current state of affairs"" 
Yes.
This is the open-source tradeoff.
A closed-source framework (i.e., .Net) will solve all of this for you.
An open source solution means you have to solve it (and resolve it) all the time.
You might be able to find some pre-built configurations and pay someone to keep those up-to-date.  For example, you could elect to use Red Hat Enterprise Linux.  If you stick to precisely what they support (and nothing more) then configuration is solved.
Odds are good, however, that no packaged configuration meets your requirements.  ",A,-5
36056133,2016-03-17 09:16:46.647000+00:00,"Yes; see the full documentation on ndb caching. Basically, every write is cached both in a request-local in-context cache, and in the main memcached store; a get by key will look up in both caches first before falling back to the real datastore.
Edit I can't understand why you think your example would work. You defined a model with a content property, but then try to set name and value properties on it; naturally that will fail.
You should go through the ndb documentation, which gives a good introduction to using the model class.",A,1
1691460,2009-11-07 00:41:39.667000+00:00,"Are you aware of Sphinx's documentation?  http://sphinx.pocoo.org/contents.html
Specifically, read about the toctree directive: http://sphinx.pocoo.org/concepts.html
You can have as many files as you want.  Via toctree you can create a single document from many parts.

Please actually read this: http://sphinx.pocoo.org/concepts.html#document-names

Since the reST source files can have
  different extensions (some people like
  .txt, some like .rst – the extension
  can be configured with source_suffix)
  and different OSes have different path
  separators, Sphinx abstracts them: all
  “document names” are relative to the
  source directory, the extension is
  stripped, and path separators are
  converted to slashes. All values,
  parameters and suchlike referring to
  “documents” expect such a document
  name.
Examples for document names are index,
  library/zipfile, or
  reference/datamodel/types. Note that
  there is no leading slash.

Since you globbed with *, you do not need to list your files.
If you want to list your files, please actually read and follow the above rules.",A,6
50439201,2018-05-20 20:45:53.687000+00:00,"The documentation just says:

See also the docstring of .evalf() for information on the options.

You can do help(sympy.evalf) in an interactive session, or you can click source on N and scroll up a bit to the docstring. Either way:

   subs=<dict>
       Substitute numerical values for symbols, e.g.
       subs={x:3, y:1+pi}. The substitutions must be given as a
       dictionary.


If you look at the source for evalf itself, what this ultimately does is:
x = x.subs(evalf_subs(prec, options['subs']))

What evalf_subs does is:
def evalf_subs(prec, subs):
    """""" Change all Float entries in `subs` to have precision prec. """"""
    newsubs = {}
    for a, b in subs.items():
        b = S(b)
        if b.is_Float:
            b = b._eval_evalf(prec)
        newsubs[a] = b
    return newsubs

So, a you can see, it's not quite the same as just calling subs before calling eval. (And of course both of them are quite different from calling subs after eval, even though in some simple examples they end up doing the same thing.)
The intention seems to be that subs= can apply the other evalf options while the substitution is happening. That ""other evalf options"" appears, from the source, to only be prec at present, but presumably that could conceivably change in the future.
At any rate, applying the prec to all of the substitutions is a good idea if you're trying to do a numerical approximation with a given precision. 
But it might be a bad idea if you're trying to stay with symbolic computation. And I suspect that may be what you're seeing here, although this part is just a guess. Of course cos(omega*0) is going to be 1 no matter what t is. But cos(omega*t) where t is within one ulp of 0 at some specific precision is probably 1 plus or minus a few ulps depending on the value of omega. So, it can't be reduced.",A,2
38854368,2016-08-09 15:06:03.787000+00:00,"Union typing is only needed when you have a statically typed language, as you need to declare that an object can return one of multiple types (in your case an int or str, or in the other example str or NoneType).
Python deals in objects only, so there is never a need to even consider 'union types'. Python functions return what they return, if the programmer wants to return different types for different results then that's their choice. The choice is then an architecture choice, and makes no difference to the Python interpreter (so there is nothing to 'benchmark' here).
Python 3.5 does introduce a standard for creating optional type hints, and that standard includes Union[...] and Optional[...] annotations.",A,13
33518300,2015-11-04 09:38:00.107000+00:00,"You're not sending either Login or Register in your post data, so neither of the conditions will ever be true on post and the variables remain undefined.
You have given your submit buttons values, but not names, so they cannot be included in the post data. Do this:
<input type=""submit"" name=""Login"" value=""Login""/>
...
<input type=""submit"" name=""Register"" value=""Register""/>
",A,0
23922733,2014-05-28 22:10:52.907000+00:00,"Don't use string interpolation. Use SQL parameters; these are placeholders in the query where your database will insert values:
SQL = '''\
    SELECT Top 1 cr.ReleaseLabel
    FROM [ODS].[v000001].[ComponentRevisions] cr
    WHERE cr.ComponentId = ?
    ORDER BY cr.CreatedOn DESC
'''
resp_rows_obj = db_cursor.execute(SQL, (component_id,))

Values for the ? placeholders are sourced from the second argument to the cursor.execute() function, a sequence of values. Here you only have one value, so I used a one-element tuple.
Note that you probably want argv[1], not argv[0]; the latter is the script name, not the first argument.",A,4
11180907,2012-06-24 20:16:46.963000+00:00,"Python comes with batteries included, and a logging module is part of the stdlib:
from logging import getLogger

log = getLogger('my.module')

log.debug('Debug level messages')
log.warning('Warning!')
log.info('Informative message')
log.error('Error messages')
log.exception('Use this in an exception handler, the exception will be included automatically')

The above set of methods are shortcuts for the log.log(level, msg) method, which takes arbitrary (integer) levels, and the logging module defines DEBUG, WARNING and other levels.
The methods support lazy evaluation of python string formatting templates; extra arguments are interpolated only when the log level of the message actually exceeds the logging level being recorded:
log.warning('Warning message: the %s is missing %i frobnars', systemname, count)

The above message will be logged with the equivalent of 'Warning message: the %s is missing %i frobnars' % (systemname, count) only if the log message actually reaches a handler.",A,9
16339213,2013-05-02 13:25:24.440000+00:00,"Unescape before inserting:
from xml.sax.saxutils import unescape

text = doc.createTextNode(unescape(text))

The escaping takes place when writing and cannot be disabled, nor should it be. Sometimes you want to include literal &gt; text in your XML, and that should be escaped properly for you if you do. If your input is XML escaped, simply unescape it before inserting.",A,2
20158124,2013-11-23 03:24:45.560000+00:00,"The problem is that you have a Word .doc file, not a text file.
A Word file is essentially a sequence of runs of formatted text. (It's actually more complicated than that, it's a tree of all kinds of things, some of which are runs of text, but let's keep it simple for now.)
In ""classic"" Word format, each of these runs is a string of bytes. In modern (DOCX, aka Office 2007, aka Office Open XML, aka WordML) Word format, each of them is an XML node in a tree. But either way, if you have one run that ends with : and another that starts with 1, you won't found :1 in the file; you'll find : followed by a bunch of cruft (the end of one binary object and the start of the next, or the end of one XML node and the start of the next, possibly with other objects/nodes in between), followed by the 1.
There's no good way to deal with this without actually parsing the Word format.
So, the big question is, which format do you have? 

If it's DOCX, that's basically just an XML file, or a ZIP file with XML files inside of it, which you can parse natively in Python—or, better, use a module like docx that does all the hard work for you.
If it's classic DOC, the only way to parse it is to read the reverse-engineered documentation that people have written up over the years and write some nasty code to deal with it. Or, of course, you can use some code that someone's already written. In this case, I don't know of any python modules that will help, but you can control the antiword program pretty easily via subprocess.

Or, alternatively, if you have a program that can read the files on your machine, like Word or Wordpad/Write on Windows, or iWork Pages on Mac, or OpenOffice.org/Libre Office on any platform, you can script that. Python has nice wrappers to talk to COM interfaces on Windows and AppleScript interfaces on Mac, and OO.o/Libre is built to be scriptable.
This blog post is a nice example of using Word on Windows via pywin32 to do things to doc files. You can use this as a starting point for your own code to extract the text from each file, or to make Word to the searching for you, or to just save a plain text copy of each file which you can then do whatever you want with. There are hundreds of other such examples all over the web, as well as similar examples for using appscript or ScriptingBridge to do the equivalent on the Mac, or using VBA instead of Python to script from inside Word, etc. To find out what functions are available when scripting Word, see the Word 2013 developer reference, or the similar docs for earlier versions if you don't have 2013, or just ""Open Dictionary"" in AppleScript Editor and look at Word's dictionary if you have a Mac.",A,0
11225447,2012-06-27 11:45:56+00:00,"Instead of using a event handler, you could customize the delete_confirmation actions; these can be altered through the web even, and can be customized per type. The delete_confirmation script is a CMF Form Controller script and there are several options to alter it's behaviour.
Currently, the actions are defined as such:
[actions]
action.success=redirect_to:python:object.aq_inner.aq_parent.absolute_url()
action.confirm=traverse_to:string:delete_confirmation_page

You could add a type specific action by defining action.success.TypeName, for example.
To do so through-the-web, visit the ZMI and find the portal_form_controller tool, then click the Actions tab:

As you can see in this screenshot there is also documentation on the tool available here.
On the actions tab there is a form to add new actions:

As you can see, the context type is a drop-down with all existing type registrations to make it easier to specify a type-specific action. I've copied in the regular action (a redirect_to action specified by a python: expression and added an extra .aq_parent to select the container parent.
You could also add such an action with the .addFormAction method on the tool:
fctool = getToolByName(context, 'portal_form_controller')
fctool.addFormAction('delete_confirmation', 'success', 'Event', None,
     'redirect_to',
     'python:object.aq_inner.aq_parent.aq_parent.absolute_url()')

Last, but not least, you can specify such custom actions in the cmfformcontroller.xml file in a GenericSetup profile; here is an example based on the above action:
<?xml version=""1.0"" ?>
<cmfformcontroller>
  <action
      object_id=""delete_confirmation"" 
      status=""success""
      context_type=""Event""
      action_type=""redirect_to""
      action_arg=""python:object.aq_inner.aq_parent.aq_parent.absolute_url()""
      />
</cmfformcontroller>

This format is one of those under-documented things in Plone; I got this from the CMFFormController sourcecode for the GS import and export code.",A,1
27514630,2014-12-16 21:53:31.240000+00:00,"You are getting confused by the names in alpha; it calls beta() with the arguments swapped.
Pay close attention to the x and y in alpha(). If it helps, replace the arguments with longer names:
def alpha(first, second):
    return first + beta(second, first)

Filling in the values everywhere gives you:
alpha(2, 3)
-> 2 + beta(3, 2)
-> 2 + (2 - 3)
-> 2 + -1
-> 1
",A,2
37754280,2016-06-10 17:43:22.553000+00:00,"Your code deadlocks as soon as cat's stdout OS pipe buffer is full. If you use stdout=PIPE; you have to consume it in time  otherwise the deadlock as in your case may happen.
If you don't need the output while the process is running; you could redirect it to a temporary file:
#!/usr/bin/env python3
import subprocess
import tempfile

with tempfile.TemporaryFile('r+') as output_file:
    with subprocess.Popen(['cat'],
                          stdin=subprocess.PIPE,
                          stdout=output_file,
                          universal_newlines=True) as process:
        for i in range(100000):
            print(i, file=process.stdin)
    output_file.seek(0)  # rewind (and sync with the disk)
    print(output_file.readline(), end='')  # get  the first line of the output

If the input/output are small (fit in memory); you could pass the input  all at once and get the output all at once using .communicate() that reads/writes concurrently for you:
#!/usr/bin/env python3
import subprocess

cp = subprocess.run(['cat'], input='\n'.join(['%d' % i for i in range(100000)]),
                    stdout=subprocess.PIPE, universal_newlines=True)
print(cp.stdout.splitlines()[-1]) # print the last line

To read/write concurrently manually, you could use threads, asyncio, fcntl, etc. @Jed provided a simple thread-based solution. Here's asyncio-based solution:
#!/usr/bin/env python3
import asyncio
import sys
from subprocess import PIPE

async def pump_input(writer):
     try:
         for i in range(100000):
             writer.write(b'%d\n' % i)
             await writer.drain()
     finally:
         writer.close()

async def run():
    # start child process
    # NOTE: universal_newlines parameter is not supported
    process = await asyncio.create_subprocess_exec('cat', stdin=PIPE, stdout=PIPE)
    asyncio.ensure_future(pump_input(process.stdin)) # write input
    async for line in process.stdout: # consume output
        print(int(line)**2) # print squares
    return await process.wait()  # wait for the child process to exit


if sys.platform.startswith('win'):
    loop = asyncio.ProactorEventLoop() # for subprocess' pipes on Windows
    asyncio.set_event_loop(loop)
else:
    loop = asyncio.get_event_loop()
loop.run_until_complete(run())
loop.close()

On Unix, you could use fcntl-based solution:
#!/usr/bin/env python3
import sys
from fcntl import fcntl, F_GETFL, F_SETFL
from os import O_NONBLOCK
from shutil import copyfileobj
from subprocess import Popen, PIPE, _PIPE_BUF as PIPE_BUF

def make_blocking(pipe, blocking=True):
    fd = pipe.fileno()
    if not blocking:
        fcntl(fd, F_SETFL, fcntl(fd, F_GETFL) | O_NONBLOCK) # set O_NONBLOCK
    else:
        fcntl(fd, F_SETFL, fcntl(fd, F_GETFL) & ~O_NONBLOCK) # clear it


with Popen(['cat'], stdin=PIPE, stdout=PIPE) as process:
    make_blocking(process.stdout, blocking=False)
    with process.stdin:
        for i in range(100000):
            #NOTE: the mode is block-buffered (default) and therefore
            # `cat` won't see it immidiately
            process.stdin.write(b'%d\n' % i)
            # a deadblock may happen here with a *blocking* pipe
            output = process.stdout.read(PIPE_BUF)
            if output is not None:
                sys.stdout.buffer.write(output)
    # read the rest
    make_blocking(process.stdout)
    copyfileobj(process.stdout, sys.stdout.buffer)
",A,1
11299068,2012-07-02 18:21:01.833000+00:00,"Many cross-platform tools use the same path on OS X as on linux (and other POSIX/non-Windows platforms). The main advantage of using the POSIX locations isn't saving a few lines of code, but saving the need for Mac-specific instructions, and allowing Mac users to get help from the linux users in the community (without any need to translate their suggestions). 
The other alternative is to put them in the ""Mac-friendly"" locations under ~/Library instead. The main advantage of using the Mac locations is basically ""Apple says so""—unless you plan to sandbox your code, in which case the main advantage is that you can do so.
If you choose to use the Library locations, you should read About the OS X File System and OS X Library Directory Details in the File System Programming Guide, but here's the short version:

Almost everything: Create a subdirectory with your app's name or bundle ID (unless you're going out of your way to set a bundle ID, you'll get org.python.python, which you don't want…) under ~/Library/Application Support. Ideally you should use APIs like -[NSFileManager URLForDirectory:inDomain:appropriateForURL:create:error:] to get the path; if not, you have to deal with things like localization, sandbox containers, etc. manually.
Anything that can be easily re-created (so it doesn't need to be backed up, migrated, etc.): An identically-named subdirectory of ~/Library/Caches.
Preferences: Use the NSUserDefaults or CFPreferences APIs instead. If you use your own format, the ""old"" way of doing things is to create a subdirectory under ~/Library/Preferences named with your app's name or bundle ID, and put your files in that. Apple no longer recommends that, but doesn't really recommend an alternative (short of ""use CFPreferences, damnit!""); many apps (e.g., Aquamacs) still do it the old way, but others instead pretend they're not preferences and store them under Application Support.

In Python, this works as follows (leaving out the error handling, and assuming you're going by name instead of setting a bundle ID for yourself):
from Foundation import *
fm = NSFileManager.defaultManager()
appsupport = (fm.URLForDirectory_inDomain_appropriateForURL_create_error_(
  NSApplicationSupportDirectory, NSUserDomainMask, None, True, None)[0].
  URLByAppendingPathComponent_isDirectory_(
  appname, True))
caches = (fm.URLForDirectory_inDomain_appropriateForURL_create_error_(
  NSCachesDirectory, NSUserDomainMask, None, True, None)[0].
  URLByAppendingPathComponent_isDirectory_(
  appname, True))
prefs = NSUserDefaults.persistentDomainForName_(appname)
",A,1
43981222,2017-05-15 13:47:15.630000+00:00,"These properties don't apply to a model class, they apply to individual instances of that model. An instance represents an entity in the datastore, and it is this instance that has a list in its ""liked_user"" field. It makes no sense to ask if a value is in a class property.
It's not quite clear what you are trying to do here. If you just want to know if a user has already liked another user, then it shouldn't be a classmethod at all; just a normal instance method, which takes the instance.
# no decorator
def likePost(self, user_key):
    if user_key not in self.liked_user:
        self.liked_user.append(user_key)
        return True
    else:
        return False

If however you are trying to find out if any instance has that key in its liked_user, then you need to query the database:
@classmethod
def likePost(cls,user_key):
    users = cls.query(cls.liked_user == user_key).get()
    if users:
        ...
",A,3
659922,2009-03-18 20:17:11.247000+00:00,"Here's what we do.
urls.py has patterns like this
url(r'^(?P< datarealm >.*?)/json/someClass/(?P<object_id>.*?)/$', 'json_someClass_resource', ),

views.py as reverse calls like this
    object = SomeModel.objects.get(...)
    url= reverse('json_someClass_resource', kwargs={'object_id':object.id,'datarealm':object.datarealm.name})
",A,7
4937435,2011-02-08 19:33:29.237000+00:00,"Django automatically provides a way to access the related objects in a ManyToMany relationship:
events = my_calendar.events.all()

See the docs on many-to-many relationships.
If you don't already have a calendar instance, but just an ID or name, you can do the whole thing in one query:
events = Event.objects.filter(calendar__id=my_id)
",A,1
32684336,2015-09-20 21:10:06.170000+00:00,"You could check what happens without risking to fill all available memory. You could set the memory limit explicitly:
#!/usr/bin/env python
import contextlib
import resource

@contextlib.contextmanager
def limit(limit, type=resource.RLIMIT_AS):
    soft_limit, hard_limit = resource.getrlimit(type)
    resource.setrlimit(type, (limit, hard_limit)) # set soft limit
    try:
        yield
    finally:
        resource.setrlimit(type, (soft_limit, hard_limit)) # restore

with limit(100 * (1 << 20)): # 100MiB
    # do the thing that might try to consume all memory
    i = 1
    while True:
        i <<= 1

This code consumes 100% CPU (on a single core) and the consumed memory grows very very slowly.
In principle, you should get MemoryError at some point whether it happens before your computer turns to dust is unclear. CPython uses a continuous block of memory to store the digits and therefore you may get the error even if there is RAM available but fragmented.
Your specific code shouldn't trigger it but in general you could also get OverflowError if you try to construct an integer larger than sys.maxsize bytes.",A,1
52792917,2018-10-13 12:28:22.173000+00:00,"You don't have lists, you have strings, as you are processing a single line from your input file at a time. Your mistake then is to use zip() on the left and right strings. You don't want to pair up the characters of the two strings, you just want to concatenate the strings, with a space in between.
You could use +, or ' '.join(), or for just two values, use string formatting:
for line in lines[1:]:
    left, right, value = line.rstrip('\n').split('|')
    z = f'{left} {right}'
    data[z] = value
",A,0
51146645,2018-07-03 04:23:22.577000+00:00,"It looks like all you need to do, at least for this example, is strip all whitespace off the edges of the brand and title values. You don't need a regex for that, just call the strip method.
However, your brand isn't a single string; it's a list of strings (even if there's only one string in the list). So, if you try to just strip it, or run a regex on it, you're going to get an AttributeError or TypeError from trying to treat that list as a string.
To fix this, you need to map the strip over all of the strings, with either the map function or a list comprehension:
item['brand'] = [brand.strip() for brand in item['brand']]
item['title'] = map(str.strip, item['title'])

… whichever of the two is easier for you to understand.

If you have other examples that have embedded runs of whitespace, and you want to turn every such run into exactly one space character, you need to use the sub method with your regex:
item['brand'] = [re.sub(ur'\s+', u' ', brand.strip() for brand in item['brand']]

Notice the u prefixes. In Python 2, you need a u prefix to make a unicode literal instead of a str (encoded bytes) literal. And it's important to use Unicode patterns against Unicode strings, even if the pattern itself doesn't care about any non-ASCII characters. (If all of this seems like a pointless pain and a bug magnet—well, it is; that's the main reason Python 3 exists.)

As for the retail_price, the same basic observations apply. Again, it's a list of strings, not just a string. And again, you probably don't need regex. Assuming the price is always a $ (or other single-character currency marker) followed by a number, just slice off the $ and call float or Decimal on it:
item['retail_price'] = [float(price[1:]) for price in item['retail_price']]

… but if you have examples that look different, with arbitrary extra characters on both sides of the price, you can use re.search here, but you'll still need to map it, and to use a Unicode pattern.
You also need to grab the matching group out of the search, and to handle empty/invalid strings in some way (they'll return None for the search, and you can't convert that to a float). You have to decide what to do about it, but from your attempt with filter it looks like you just want to skip them. This is complicated enough that I'd do it in multiple steps:
prices = item['price']
matches = (re.search(r'[0-9.]+', price) for price in prices)
groups = (match.group() for match in matches if match)
item['price'] = map(float, validmatches)

… or maybe wrap that in a function.",A,2
13827596,2012-12-11 20:02:16.153000+00:00,"No, you cannot know what the name was of the local variable used to pass a value to your function.
This is an impossible task in any case. What would be the variable name in the following example?
arguments = ('a', 1, 10)
somefunction(*(arguments[:2] + [10]))

Here we pass in 3 arguments, two taken from a tuple we defined earlier, and one literal value, and all three are passed in using the variable argument list syntax.",A,4
12087777,2012-08-23 08:36:21.637000+00:00,".text() returns the text within a QListWidgetItem. Note that you need to call .item(index) on the original QListWidget instance to get the items contained in the list widget:
items = []
for index in xrange(self.ui.QListWidget.count()):
     items.append(self.ui.QListWidget.item(index))
labels = [i.text() for i in items]
",A,7
3102581,2010-06-23 14:35:46.250000+00:00,"
the leading edge of technology

Always a problem.  Avoid it.
It's really quite simple.  You adopt new technology when you're forced to by your customers.
Unless you have a huge market segment already, you cannot adopt a technology because it's cool.  
You adopt a technology because folks willing to use that technology are your target market.",A,7
19511807,2013-10-22 07:36:56.810000+00:00,"A Python module is an object. hasattr() works just fine on that.
Demo:
>>> import os
>>> type(os)
<type 'module'>
>>> os
<module 'os' from '/Users/mj/Development/venvs/stackoverflow-2.7/lib/python2.7/os.pyc'>
>>> hasattr(os, 'sep')
True
>>> hasattr(os, 'foobar')
False

If you have a string with the module name, then you can look up the module object in the sys.modules mapping:
>>> import sys
>>> sys.modules['os']
<module 'os' from '/Users/mj/Development/venvs/stackoverflow-2.7/lib/python2.7/os.pyc'>
 >>> hasattr(sys.modules['os'], 'sep')
True
",A,10
24736757,2014-07-14 12:40:07.563000+00:00,"Just loop over the values then:
with open('log.txt','w') as log:
    for value in log_disk.values():
        log.write('{}\n'.format(value))
",A,5
17529948,2013-07-08 15:02:19.880000+00:00,"The input() function takes only one string argument (and not multiple, like print()). Use string formatting:
answer = int(input('{} + {} = '.format(oneTerm, twoTerm)))

The {} placeholders are replaced by the two arguments to .format(), creating essentially the same string to prompt the user for input.
See the str.format() method documentation for more details.",A,1
51712105,2018-08-06 16:35:38.040000+00:00,"You missed that this change applies to Python 3.7 and newer. You won't see the conversion in Python 3.6 or older, unless you enable the feature with a from __future__ import first (available as of Python 3.5).
From the same page you linked:

Changed in version 3.5: Introduced the RuntimeError transformation via from __future__ import generator_stop, see PEP 479.
Changed in version 3.7: Enable PEP 479 for all code by default: a StopIteration error raised in a generator is transformed into a RuntimeError.

PEP 479 -- Change StopIteration handling inside generators further details why this change was made and how it applies. For your code, running on Python 3.7, the output becomes:
>>> import sys
>>> sys.version_info
sys.version_info(major=3, minor=7, micro=0, releaselevel='final', serial=0)
>>> def gen1():
...     yield from [1, 2, 3]
...     raise StopIteration
...
>>> def gen2():
...     yield 42  # make this an actual generator
...     raise StopIteration
...
>>> try:
...     a = list(gen1())
... except RuntimeError:
...     print(""Caught"")
...
Caught
>>> try:
...     a = gen1()
...     next(a), next(a), next(a), next(a), next(a)
... except RuntimeError:
...     print(""Caught"")
...
Caught
>>> try:
...     a = list(gen2())
... except RuntimeError:
...     print(""Caught"")
...
Caught

Note that I added a yield 42 line to gen2() to make it a generator. Without yield or yield from in the body, you get a regular function instead. Calling a generator function produces a generator object and the function body starts out paused, while calling a normal function executes the body immediately:
>>> def normal():
...     raise StopIteration
...
>>> def generator():
...     raise StopIteration
...     yield  # never reached, but this is now a generator
...
>>> normal()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 2, in normal
StopIteration
>>> generator()
<generator object generator at 0x105831ed0>
>>> next(generator())
Traceback (most recent call last):
  File ""<stdin>"", line 2, in generator
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: generator raised StopIteration

For Python 3.6, you'd use the from __future__ import generator_stop compiler switch (use it at the top of your code when writing a script or module):
>>> import sys
>>> sys.version_info
sys.version_info(major=3, minor=6, micro=5, releaselevel='final', serial=0)
>>> def generator():
...     raise StopIteration
...     yield
...
>>> next(generator())
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 2, in generator
StopIteration
>>> from __future__ import generator_stop
>>> def generator():  # re-define it so it is compiled anew
...     raise StopIteration
...     yield
...
>>> next(generator())
Traceback (most recent call last):
  File ""<stdin>"", line 2, in generator
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: generator raised StopIteration
",A,6
21961680,2014-02-22 22:47:45.493000+00:00,"Using partition() function:
import itertools

def partition(items, predicate=bool):
    a, b = itertools.tee((predicate(item), item) for item in items)
    return ((item for pred, item in a if not pred),
            (item for pred, item in b if pred))

odd, even = partition(numbers, lambda n: n % 2 == 0)

If odd, even are not consumed nearly in sync or if the predicate is not expensive then your list version (from the question) should be faster.",A,3
51295988,2018-07-12 01:00:33.627000+00:00,"Imports are absolute by default. So, import Next is looking for a top-level module named Next, and that's not what you want.
But when you're trying to import another module in the same package, you usually don't want an absolute import, you want an import relative to the current package:
from . import Next

If you do want an absolute import, you need to specify the top-level package (or, for more deeply-nested modules, the whole path) to import it from:
from suite_syracuse import Next

Or, of course, you can do this:
import suite_syracuse.Next

… but then you'll have to also change your code to call suite_syracuse.Next.suiv instead of just Next.suiv.

This is all explained under Modules in the official tutorial, but it's kind of scattered around all over the chapter. (Most of it is in Intra-package References, but you wouldn't find that, or maybe even understand it, without reading half the rest of the chapter first.)
You might find it more useful to read PEP 328, the language change proposal that added the current relative import syntax, which gives a lot of background on the design, and on when and why you'd want to use absolute vs. relative imports.",A,0
51601885,2018-07-30 20:32:54.067000+00:00,"When you just run ""${SRC_DIR}/systemd_client.py"", you're telling it to run that script the same way it would be run by the shell: by looking at the #! line and running it with whatever interpreter is specified there. Which is probably something like #! /usr/bin/python or #! /usr/bin/env python.
If you want to run your script with a particular interpreter, you have to run that interpreter and pass it the script—just as you would at the shell. I'm pretty rusty with CMake, but I'd assume you do that like this:
""${PYTHON_EXECUTABLE}"" ""${SRC_DIR}/amyconfig_service.py""

Alternatively, since this is your code, maybe you want to use setuptools to programmatically generate scripts for your entry-points, which means it would create a #! line for them that runs whichever Python version was used to run setup.py.",A,0
13538687,2012-11-24 05:51:06.843000+00:00,"setup.py develop creates a python egg, in-place; it does not [modify the] python environment so package can be imported from it's current location. You still have to either add it's location to the python search path or use the directory it is placed in as the current directory.
It is the job of the develop command to create an in-place egg, which may include compiling C extensions, running the 2to3 python conversion process to create Python3 compatible code, and to provide metadata other python code may be relying on. When you install the package as an egg in your site-packages directory, the same metadata is included there as well. The data is certainly not temporary (it is extracted from your setup.py file for easy parsing by other tools).
The intent is that you can then rely on that metadata when using your package in a wider system that relies on the metadata being present, while still developing the package. For example, in a buildout development deployment, we often use mr.developer to automate the process of fetching the source code for a given package when we need to work on it, which builds it as a develop egg and ties it into the deployment while we work on the code.
Note that the .egg-info directory serves a specific purpose: to signal to other tools in the setuptools eco-system that your package is installed and available. If your package is a dependency of another egg in your setup, then that dependency is satisfied. pip and easy_install and buildout will not try and fetch the egg from somewhere else instead.
Apart from creating the .egg-info directory, the only other thing the command does, is to build extensions, in-place. So the command you are looking for instead is:
setup.py build_ext --inplace

This will do the exact same thing as setup.py develop but leave out the .egg-info directory. It also won't generate the .pth file.
There is no way of generating only the .pth file and leave out the .egg-info directory generation.
Technically speaking, setup.py develop will also check if you have the setuptools site.py file installed to support namespaced packages, but that's not relevant here.",A,16
25611194,2014-09-01 18:22:42.993000+00:00,"The error can be ignored, your gunicorn package installed successfully.
The error is thrown by a bit of code that'd only work on Python 3.3 or newer, but isn't used by older Python versions that Gunicorn supports.
See https://github.com/benoitc/gunicorn/issues/788:

The error is a syntax error happening during install. It is harmless.

During installation the setup.py script tries to collect all files to be installed, and compiles them to .pyc bytecache files. One file that is used only on Python 3.3 or up is included in this and the compilation for that one file fails.
The file in question adds support for the aiohttp http client/server package, which only works on Python 3.3 and up anyway. As such you can ignore this error entirely.",A,74
31077763,2015-06-26 16:13:52.300000+00:00,"Python compiles your source into bytecode and runs that bytecode. Once the source file has been read it is no longer needed to run that bytecode. Changes to the source file won't then affect already running code.
However, if an exception is raised and Python tries to format a traceback for display, it'll reach back to the source code, mapping line markers in the bytecode back to source lines. If the source file changed after compilation, this could mean the wrong lines are being displayed. That can create confusion.
You can easily give your program command line parameters to vary how it behaves. Take a look at the sys.argv list, and perhaps the argparse module for more complex command-line option handling. That way your code remains stable and flexible.",A,6
15166484,2013-03-01 20:38:54.287000+00:00,"This is explained in the docs under Programming Guidelines for Windows. 
Depending on your platform, each process may have to start a brand-new interpreter and import your module to get that f function to call. (On Windows, it always has to do so.) When it imports your module, all top-level code is run, which includes the line computed_result=computeInfoPooled(G), which creates a whole new pool of 4 processes, etc.
You get around this the same way you deal with any other case where you want the same file to be `both importable as a module and runnable as a script:
def computeInfoPooled(G,num_list):
    pool=Pool(processes=4)

    def f(k):
        curr_stat={}
        curr_stat[k]=slow_function(k,G)
        return curr_stat

    result = pool.map(f,num_list)

    return result

if __name__ == '__main__':
    computed_result=computeInfoPooled(G)


From your edit, and your comments, you seem to be expecting that doing the call to computeInfoPooled(G) from the interactive interpreter will solve that problem. The same linked docs section explains in detail why that don't work, and a big note at the very top of the Introduction directly says:

Functionality within this package requires that the main module be importable by the children. This is covered in Programming guidelines however it is worth pointing out here. This means that some examples, such as the multiprocessing.Pool examples will not work in the interactive interpreter.

If you want to understand why this is true, you need to read the linked docs (and you will also need to understand a little about how import, pickle, and multiprocessing all work).",A,0
49621688,2018-04-03 03:17:35.017000+00:00,"subprocess works just fine on Windows.
But on any platform, subprocess can't call programs that don't exist. And there is no program named mkdir (or MKDIR.EXE) on Windows. When you type that into a command prompt, it's handled as a special built-in command by cmd.exe.
And, even if it did exist, you wouldn't be calling it, because from the error message you clearly passed ' mkdir', with a space before the m, and there's no program with than name on any system.
If you read the docs for subprocess, you can see that there are two ways around this:

Pass shell=True, so the command will be executed by the shell (cmd.exe), similar to os.system but better in a variety of ways. Usually when you use this flag, you want to use a string rather than a list of arguments with this like run(f'mkdir {fold}', shell=True)
Explicitly call cmd.exe, like run(['cmd', '/C', 'mkdir', fold])

However, there's really no reason to use either os.system or subprocess here. If you want to make a directory, just call os.mkdir (or, if you want to do it recursively, os.makedirs. That's cross-platform, and about as simple as possible. And similarly, instead of running cp on Unix and copy on Windows, you can just shutil.copyfile or its variants.",A,2
17086789,2013-06-13 12:12:40.390000+00:00,"
It is not being called; you are inheriting from it instead; MyHTMLParser is called to create an instance of the class.
handle_data() is called by the HTMLParser code; your subclass overrides it to handle textual data in the document. By calling .feed() on your MyHTMLParser instance the process is started.
See the html.parser module documentation for the full API. You could try to understand the full html.parser source code if you really want to know how this all works.
The example you found is doing things quite wrong, actually. The global y refers to a different y variable, one that lives in the module scope. That line shows that the original developer of that example did not understand what they were doing. The global y line is not the only example; the code could be made much more Pythonic in other ways.
The y definition in the class scope otherwise unused. The handle_data() method should refer to self.y instead:
class MyHTMLParser(HTMLParser):
    y = None

    def handle_data(self, data):
        if '\\' not in data:
            if data == str1:
                print(""Name of professor:"", data)
                print(""Name of course:"", self.y) 
            self.y = x

This stores the element text in self.y for each element, and if a next element matches your str1 global variable, then it is printed as well, together with the professor name.

You may want to look into using BeautifulSoup instead for a more intuitive HTML parser API.",A,1
28135567,2015-01-25 10:53:14.440000+00:00,"The square brackets denote optional arguments. The function takes at least a url argument, with data, proxies and context being optional.
The notation is somewhat misleading. You can pass in a proxies argument without providing a data argument, if you use it as a keyword argument, for example. The actual function signature is:
def urlopen(url, data=None, proxies=None, context=None):

so url is required` the other arguments can either be passed in as positional arguments or keyword arguments.",A,1
12772093,2012-10-07 19:27:24.020000+00:00,"You could use the datetime module:
datetime.datetime.strptime(str_date, '%m/%d/%Y').date().isoformat()

or, as running code:
>>> import datetime
>>> str_date = '2/28/1987'
>>> datetime.datetime.strptime(str_date, '%m/%d/%Y').date().isoformat()
'1987-02-28'
",A,17
39301095,2016-09-02 21:44:00.273000+00:00,"You already exhausted the file, you can't then read from the file again without seeking back to 0. As a result your for i, k in enumerate(f, 1): loop exits immediately. The same then applies to every future iteration of your while 1: loop; the file is still at the end and all access with next() will raise a StopIteration immediately.
You already know how many lines you have read, just set count = len(lines). There is no need to read the whole file again just to set count.
It'd be better if you used itertools.islice() to get your 1000 lines:
from itertools import islice

count = 10000
with open('mobydick_ch1.txt') as f:
    lines = list(islice(f, count))  # list of up to count lines
if len(lines) < count:
    input(""File does not contain that many lines, press enter to continue printing maximum lines:"")
    count = len(lines)  # set count to actual number of lines

If you are trying to wait until a file contains at least count lines, you'll have to re-open the file each time and seek to the last recorded location:
lines = []
pos = 0
while len(lines) < count:
    with open('mobydick_ch1.txt') as f:
        f.seek(pos)
        lines.extend(islice(f, count - len(lines)))
        pos = f.tell()
    if len(lines) < count:
        input(""File does not contain that many lines, press enter to continue printing maximum lines:"")
",A,2
31655746,2015-07-27 14:23:17.417000+00:00,"Just store the current numbers in the loop; the next iteration they'll be the previous values until you assign again:
previous = None
for i, (low,high) in enumerate(exons[name]):
    if low -3 <= ppos1 <= high + 6:
        exonnumber = i+1
        print previous
        previous = low, high
",A,2
37685965,2016-06-07 17:44:50.070000+00:00,"You have a logic problem. Your while statement will always be true; if random_disease is ""Y"" it will not be ""N"", and vice versa. You need and there, not or.",A,0
42460309,2017-02-25 18:59:52.380000+00:00,"Type hints are there for other tools to check your code, they are not enforced at runtime. The goal is enable static analysis tools to detect invalid argument use.
Use an IDE like PyCharm, or the commandline code checker mypy to be told that 2.0 is not a valid argument type.
From the Type Hinting PEP (484):

This PEP aims to provide a standard syntax for type annotations, opening up Python code to easier static analysis and refactoring, potential runtime type checking, and (perhaps, in some contexts) code generation utilizing type information.

Emphasis mine. Runtime type checking is left to third-party tools. Note that such runtime checks would come with a performance downside, your code will likely run slower if you were to check for types on every call.",A,3
22789044,2014-04-01 14:34:59.767000+00:00,"Use the string.Formatter() class:
from string import Formatter

def no_curly_brackets(fmt):
    try:
        parsed = Formatter().parse(fmt)
        return not any('{' in lt or '}' in lt for lt, _, _, _ in parsed)
    except ValueError:
        return False

Basically anything that's parsable as a format and doesn't contain a curly brace in the parsed literal text would be True.
This matches all your test cases:
>>> for test in tests:
...     print test, no_curly_brackets(test)
... 
word True
{word False
{}word True
{word} True
{{word}} False
{{{word}}} False
{word}{{}} False

plus a few of my own:
>>> no_curly_brackets('word}}')
False
>>> no_curly_brackets('{{word')
False
>>> no_curly_brackets('word{{}}')
False
",A,6
48516994,2018-01-30 08:29:18.857000+00:00,"Python's booleans are simply subclasses of int. True is equivalent to 1, and False to 0.",A,0
12934023,2012-10-17 12:10:17.763000+00:00,"If you just want to print the label for each object, you could use a loop or a list comprehension:
print [vertex.label for vertex in x]

But to answer your original question, you need to define the __repr__ method to get the list output right. It could be something as simple as this:
def __repr__(self):
    return str(self)
",A,51
14251152,2013-01-10 05:04:39.790000+00:00,"Do it in the reverse order: first unquote then .decode('utf-8')
Do not mix bytes and Unicode strings.
Example
import urllib

q = ""Eptisa+Servicios+de+Ingenier%C3%ADa+S.L.""
b = urllib.unquote_plus(q)
u = b.decode(""utf-8"")
print u

Note: print u might produce UnicodeEncodeError. To fix it:
print u.encode(character_encoding_your_console_understands)

Or set PYTHONIOENCODING environment variable.
On Unix you could try locale.getpreferredencoding() as character encoding, on Windows see output of chcp",A,1
35957810,2016-03-12 12:47:01.780000+00:00,"The fact that you are struggling with this should be an indication that your data structure is wrong. 
Instead of getting the user to write a single string, get them to enter individual elements and store them separately in a list.",A,0
3683764,2010-09-10 10:09:39.943000+00:00,"
So how to solve it? 

Well, you have to get the raw bytes.  Once you have downloaded the raw bytes, you can actually print them and actually look at them to see what the problem actually is.

Is there any lib that would check what encoding page is and convert so it would be readable?

The page itself says what it's encoding is.  You can assume it's UTF-8, but that's not always true.  
If the page is XML or XHTML, the <?xml at the beginning includes the encoding.
The page has a content-type header Content-Type: text/plain; charset=""UTF-8"" which has the encoding.
It's quite easy to properly decode a page.  
Step 1.  Don't assume the page is UTF-8.
Step 2.  Get the content, read the headers.
Step 3.  Use the encoding specified in the header, not an assumed encoding of UTF-8.",A,2
39817126,2016-10-02 12:43:19.963000+00:00,"Yes, there is a difference. Although in Python 3, all objects are instances of object, including object itself, only Any documents that the return value should be disregarded by the typechecker.
The Any type docstring states that object is a subclass of Any and vice-versa:
>>> import typing
>>> print(typing.Any.__doc__)
Special type indicating an unconstrained type.

    - Any object is an instance of Any.
    - Any class is a subclass of Any.
    - As a special case, Any and object are subclasses of each other.

However, a proper typechecker (one that goes beyond isinstance() checks, and which inspects how the object is actually used in the function) can readily object to object where Any is always accepted.
From the Any type documentation:

Notice that no typechecking is performed when assigning a value of type Any to a more precise type.

and

Contrast the behavior of Any with the behavior of object. Similar to Any, every type is a subtype of object. However, unlike Any, the reverse is not true: object is not a subtype of every other type.
That means when the type of a value is object, a type checker will reject almost all operations on it, and assigning it to a variable (or using it as a return value) of a more specialized type is a type error.

and from the mypy documentation section Any vs. object:

The type object is another type that can have an instance of arbitrary type as a value. Unlike Any, object is an ordinary static type (it is similar to Object in Java), and only operations valid for all types are accepted for object values.

object can be cast to a more specific type, while Any really means anything goes and a type checker disengages from any use of the object (even if you later assign such an object to a name that is typechecked).
You already painted your function into a an un-typed corner by accepting list, which comes down to being the same thing as List[Any]. The typechecker disengaged there and the return value no longer matters, but since your function accepts a list containing Any objects, the proper return value would be Any here.
To properly participate in type-checked code, you need to mark your input as List[T] (a genericly typed container) for a typechecker to then be able to care about the return value. Which in your case would be T since you are retrieving a value from the list. Create T from a TypeVar:
from typing import TypeVar, List

T = TypeVar('T')

def get_item(L: List[T], i: int) -> T:
    return L[i]
",A,16
18991731,2013-09-24 20:47:16.143000+00:00,"Using the datetime module:
import datetime

today = datetime.date.today()
future = today + datetime.timedelta(days=24)
print future

Demo:
>>> import datetime
>>> today = datetime.date.today()
>>> future = today + datetime.timedelta(days=24)
>>> print future
2013-10-18

Adding the time is trivial; use datetime.datetime.now() instead of datetime.date.today().",A,7
24941789,2014-07-24 18:55:09.657000+00:00,"
I've also read that people get pointed to dict(), vars(), or dir(), but just can't understand how to use them. 

Have you tried looking at the docs for these function? For example, vars:

… Without an argument, vars() acts like locals(). Note, the locals dictionary is only useful for reads since updates to the locals dictionary are ignored.

OK, so what does locals do? Look at the docs: it gives you a dictionary that maps each local name in your function to its value. So, if you don't want to pass both names and values, pass the locals() dictionary, and the names:
def foo_function(n):
    x = 1
    y = 2
    z = 'string'
    debugFunction(locals(), ['x', 'y', 'z'])
    return x + y

def _debugFunction(localdict, var_list):
    global debug_flag
    if debug_flag:
        print 'START DEBUGGING:'
        for name in var_list:
            print('{}: {}'.format(name, localdict[name]))
        print 'END DEBUGGING'

That's it. Except I might change the interface a little to either use *var_list or, even simpler, a string that I can split, and, to make it simpler to use in simple cases, default to printing all the locals:
def _debugFunction(localdict, var_list=''):
    var_list = var_list.split()
    if not var_list:
        var_list = localdict.keys()

Now you can do this:
_debugFunction(locals(), 'x y z')

Or just:
_debugFunction(locals())
",A,3
3788523,2010-09-24 15:23:17.847000+00:00,"Writing and reading from a socket are separate.  You can try to close a socket for writing and leave it open for reading.
See http://docs.python.org/library/socket.html#socket.socket.shutdown
Also, what FTP does is use two sockets: one for data, and one for this ""confirmation"".
You'd be happier using a second socket for this additional metadata.",A,2
34376188,2015-12-19 22:47:04.360000+00:00,"TL;DR: sudo does not forward signals sent by a process in the command's process group since 28 May 2014 commit released in sudo 1.8.11 -- the python process (sudo's parent) and the tcpdump process (grandchild) are in the same process group by default and therefore sudo does not forward SIGTERM signal sent by .terminate() to the tcpdump process.


It shows the same behaviour when running that code while being the root user and while being a regular user + sudo

Running as a regular user raises OSError: [Errno 1] Operation not permitted exception on .terminate() (as expected).
Running as root reproduces the issue: sudo and tcpdump processes are not killed on .terminate() and the code is stuck on .communicate() on Ubuntu 15.10. 
The same code kills both processes on Ubuntu 12.04.
tcpdump_process name is misleading because the variable refers to the sudo process (the child process), not tcpdump (grandchild):
python
└─ sudo tcpdump -w example.pcap -i eth0 -n icmp
   └─ tcpdump -w example.pcap -i eth0 -n icmp          

As @Mr.E pointed out in the comments, you don't need sudo here: you're root already (though you shouldn't be -- you can sniff the network without root). If you drop sudo; .terminate() works.
In general, .terminate() does not kill the whole process tree recursively and therefore it is expected that a grandchild process survives. Though sudo is a special case, from sudo(8) man page:

When the command is run as a child of the sudo process, sudo will
  relay signals it receives to the command.emphasis is mine

i.e., sudo should relay SIGTERM to tcpdump and tcpdump should stop capturing packets on SIGTERM, from tcpdump(8) man page:

Tcpdump will, ..., continue  capturing  packets until  it is
  interrupted by a SIGINT signal (generated, for example, by typing your
  interrupt character,  typically  control-C)  or  a  SIGTERM signal 
  (typically generated with the kill(1) command);

i.e., the expected behavior is: tcpdump_process.terminate() sends SIGTERM to sudo which relays the signal to tcpdump which should stop capturing and both processes exit and .communicate() returns tcpdump's stderr output to the python script.
Note: in principle the command may be run without creating a child process, from the same sudo(8) man page:

As a special case, if the policy plugin does not define a close
  function and no pty is required, sudo will execute the command
  directly instead of calling fork(2) first

and therefore .terminate() may send SIGTERM to the tcpdump process directly -- though it is not the explanation: sudo tcpdump creates two processes on both Ubuntu 12.04 and 15.10 in my tests.
If I run sudo tcpdump -w example.pcap -i eth0 -n icmp in the shell then kill -SIGTERM terminates both processes. It does not look like Python issue (Python 2.7.3 (used on Ubuntu 12.04) behaves the same on Ubuntu 15.10. Python 3 also fails here).
It is related to process groups (job control): passing preexec_fn=os.setpgrp to subprocess.Popen() so that sudo will be in a new process group (job) where it is the leader as in the shell makes tcpdump_process.terminate() work in this case.

What happened? It works on previous versions.

The explanation is in the sudo's source code:

Do not forward signals sent by a process in the command's process
  group, do not forward it as we don't want the child to indirectly kill
  itself.  For example, this can happen with some versions of reboot
  that call kill(-1, SIGTERM) to kill all other processes.emphasis is mine

preexec_fn=os.setpgrp changes sudo's process group. sudo's descendants such as tcpdump process inherit the group. python and tcpdump are no longer in the same process group and therefore the signal sent by .terminate() is relayed by sudo to tcpdump and it exits.
Ubuntu 15.04 uses Sudo version 1.8.9p5 where the code from the question works as is.
Ubuntu 15.10 uses Sudo version 1.8.12 that contains the commit.
sudo(8) man page in wily (15.10) still talks only about the child process itself -- no mention of the process group:

As a special case, sudo will not relay signals that were sent by the 
  command it is running.

It should be instead:

As a special case, sudo will not relay signals that were sent by a process in the process group of the command  it is running.

You could open a documentation issue on Ubuntu's bug tracker and/or on the upstream bug tracker.",A,10
47733149,2017-12-09 20:59:29.980000+00:00,"This isn't a thing you can do. <td> elements are not form fields; you can't ""submit"" them.
If you want values to be submitted to the backend, you either have to use actual form inputs, or use Ajax to send the data.",A,1
18727316,2013-09-10 19:29:53.620000+00:00,"Wrap the file object in io.TextIOWrapper() to decode the pipe on the fly:
import io

reader = io.TextIOWrapper(p.stdout, encoding='utf8')
while something:
    char = reader.read(1)
",A,4
20993771,2014-01-08 11:04:38.533000+00:00,"BeautifulSoup splits out classes for you into a list; the regular expression won't match on multiple classes. class is one of a set of such attributes, see Multi-valued attributes.
You'll have to use a custom function to filter on multiple classes using regular expressions:
def underscored_class_span(elem, numbered=re.compile(u'_\d').match):
    if elem.name != 'span': return False
    classes = elem.get('class', [])
    return u'_' in classes and any(numbered(c) for c in classes)

for span in soup.find_all(underscored_class_span):
    span.decompose()
",A,2
23033272,2014-04-12 16:43:03.850000+00:00,There is experimental Python 3 support since py2app 0.5,A,1
26853561,2014-11-10 21:45:57.420000+00:00,"When I run your actual program, either the original version or the edited one, in either Python 2.5 or 2.7, the syntax error I get is:
SyntaxError: 'return' outside function

And the meaning of that should be pretty obvious: You can't return from a function if you aren't in a function. If you want to ""return"" from the entire program, you can do that with exit:
import sys

# ...

except IndexError:
    sys.exit()

(Note that you can give a value to exit, but it has to be a small integer, not an arbitrary Python value. Most shells have some way to use that return value, normally expecting 0 to mean success, a positive number to mean an error.)

In your updated version, if you fix that (whether by moving this whole thing into a function and then calling it, or by using exit instead of return) you will get an IndentationError. The lines starting with outfile = … have to be either indented to the same level as the return None above (in which case they're part of the except clause, and will never get run), or dedented back to the same level as the try and except lines (in which case they will always run, unless you've done a continue, return, break, exit, unhandled raise, etc.).
If you fix that, there are no more syntax errors in the code you showed us.

I suspect that your edited code still isn't your real code, and you may have other syntax errors in your real code. One common hard-to-diagnose error is a missing ) (or, less often, ] or }) at the end of a line, which usually causes the next line to report a SyntaxError, often at some odd location like a colon that looks (and would be, without the previous line) perfectly valid. But without seeing your real code (or, better, a real verifiable example), it's impossible to diagnose any further.

That being said, I don't think you want to return (or exit) here at all. You're trying to continue on to the next iteration of the loop. You do that with the continue statement. The return statement breaks out of the loop, and the entire function, which means none of the remaining URLs will ever get processed.

Finally, while it's not illegal, it's pointless to have extra statements after a return, continue, etc., because those statements can never get run. And similarly, while it's not illegal to have two except clauses with the same exception, it's pointless; the second one can only run in the case where the exception isn't an IndexError but is an IndexError, which means never.
I suspect you may have wanted a separate try/except around each of the two indexing statements, instead of one around the entire loop. While that isn't at all necessary here, it can sometimes make things clearer. If that's what you're going for, you want to write it like this:
page = urllib2.urlopen(url)
pagecontent = page.read() # get a file-like object at this url

soup = BeautifulSoup(pagecontent)

title = soup.find_all('title')
article = soup.find_all('article')

try:
    title = str(title[0].get_text().encode('utf-8'))
except IndexError:
    continue

try:
    article = str(article[0].get_text().encode('utf-8'))
except IndexError:
    return continue

outfile = open(output_files_pathname + new_filename,'w')
outfile.write(title)
outfile.write(""\n"")
outfile.write(article)
outfile.close()

print ""%r added as a text file"" % title
",A,2
49163623,2018-03-08 00:43:17.853000+00:00,"If you actually want to stop reading exactly as soon as you've reached the nth word, you pretty much have to read a byte at a time. But that's going to be slow, and complicated. Plus, it's still not really going to stop reading after the nth word, unless you're reading in binary mode and decoding manually, and you disable buffering.
As long as the text file has line breaks (as opposed to being one giant 80MB line), and it's acceptable to read a few bytes past the nth word, a very simple solution will still be pretty efficient: just read and split line by line:
import sys
f = open(sys.argv[1], ""r"")
contents = []
for line in f:
    contents += line.split()
    if len(contents) >= n:
        del contents[n:]
        break
f.close()
",A,0
32997584,2015-10-07 16:25:48.383000+00:00,"Adding 30 minutes and truncating the result to an hour is the correct method if you want to use ""round half up"" tie-breaking rule for positive timestamps (input_epoch > 0):
d = datetime.utcfromtimestamp(3600 * ((input_epoch + 1800) // 3600))
",A,1
35380410,2016-02-13 13:20:36.927000+00:00,"A child process created by Popen() may inherit open file descriptors (a finite resource) from the parent. Use close_fds=True on POSIX (default since Python 3.2), to avoid it. Also, ""PEP 0446 -- Make newly created file descriptors non-inheritable"" deals with some remaining issues (since Python 3.4).",A,4
16173919,2013-04-23 15:54:21.020000+00:00,"A Counter is a subclass of the standard dict class; you can loop over the items in the dictionary with .iteritems() (python 2) or just .items() (python 3):
for key, count in your_counter.iteritems():
    location, lat, long = key
    writer.writerow([location, lat, long, count])

This writes four columns, with 2 separate columns for the latitude and longitude. If you want to combine those into one column, say, as  a string with a slash between the two coordinates, just use string formatting:
for key, count in your_counter.iteritems():
    location, lat, long = key
    writer.writerow([location, '{}/{}'.format(lat, long), count])
",A,8
18541306,2013-08-30 20:51:58.927000+00:00,"The actual problem in your code is that you're not using format correctly. In fact, as written, it won't even compile:
os.system('/sw/bin/python2.7 program1.py -i Users/Steve/Desktop/{}   -o  Users/Steve/Desktop/{}'.format(inputFile),.format(outputFile))

For the first argument to system, you're trying to format a string with two placeholders and only feeding it one value. Then, for the second argument, you're passing .format(outputFile), which is an error.
You can fix all that by writing:
os.system('/sw/bin/python2.7 program1.py -i Users/Steve/Desktop/{}   -o  Users/Steve/Desktop/{}'.format(inputFile, outputFile))


However, this is a bad way to do things. Usually, you can just trivially rewrite your code to import the functionality into your script and call it. If the only reason you're not doing that is to force a child process, multiprocessing is still usually a better way to do that.

But sometimes you do need to do something like this. In that case, you should still not use os.system. First, to run a new script using the same interpreter as your script, use sys.executable rather than hardcoding it. Second, as the os.system docs or help will tell you, you should almost always be using functions in subprocess instead. So:
subprocess.call([sys.executable, 'program1.py',
                 '-i', 'Users/Steve/Desktop/{}'.format(inputfile),
                 '-o', 'Users/Steve/Desktop/{}'.format(outputfile)])

This takes care of all kinds of problems for you—e.g., you don't have to figure out how to quote the filenames in case they have spaces—and it's safer and more efficient to not use the shell if you don't need it for anything.

However, this still probably won't work for you, unless your current working directory happens to be '/'. If you want to use an absolute path, you have to start it with a /.
While we're at it, it's generally better to use path functions instead of string functions to deal with paths, as in os.path.join('/Users/Steve/Desktop', inputfile).

Even better, you might want to look up the current user's desktop instead of hardcoding that path. The way Apple recommends doing this requires installing pyobjc (which comes built-in with Apple's Python installation, but probably doesn't with the Fink installation you're using instead). But then it's as simple as:
desktop = AppKit.NSSearchPathForDirectoriesInDomains(AppKit.NSDesktopDirectory,
                                                     AppKit.NSUserDomainMask, 
                                                     True)[0]
",A,4
19233398,2013-10-07 19:50:37.357000+00:00,"The querystring is not part of the route. The route is just /search, and the query string can be accessed from request.query.",A,3
45706797,2017-08-16 06:49:02.143000+00:00,"Your MLE.fit() method returns None because it has no explicit return statement:
def fit(self):
    self.mle.fit()

Perhaps you wanted to pass through the result from the self.mle.fit() call?
def fit(self):
    return self.mle.fit()

With that change, the output changes to:
Initialize NormMLE
1
2
2
Initialize BinMLE
1
2
3
6

Other than that, yes, you are not understanding OOP correctly yet. You do not have inheritance here. You have containment. Instances of MLE contain an instance of a different class, one that you expect implements specific methods (often called an interface). You have implemented the Delegation Pattern; a facade class that delegates the actual work to a specialised contained object.
You could create an abstract base class to document and verify the interface you expect:
from abc import ABCMeta, abstractmethod

class MLEImplBase(object):
    __metaclass__ = ABCMeta

    @abstractmethod
    def set_param(self, param):
        pass

    @abstractmethod
    def fit(self):
        pass

class NormMLE(MLEImplBase):
    # ...

class BinMLE(MLEImplBase):
    # ...

This is not strictly necessary, but helpful when you add more implementations and accidentally misspell or forget one of the abstract methods.
However, the above does use inheritance. MLEImplBase is the superclass, with NormMLE and BinMLE being subclasses.",A,1
45833271,2017-08-23 07:27:52.300000+00:00,"You're thinking of this in the wrong way. If they're coming to your website, then they're using your web app. It's only that that needs to run under the virtual environment, which you would configure in its own startup script (eg the wsgi script).",A,0
18758739,2013-09-12 08:03:45.950000+00:00,"You've just finished writing into stream, so its file pointer is at the end of the file.
readlines returns all lines from the current file pointer until the end of the file. Since there are no lines from the end of the file to the end of the file, it returns no lines.
If you want to move the file pointer back to the start of the file, use the seek method:
print 'readlines:' # will print nothing
stream.seek(0)
for line in stream.readlines():
    print line


A few side notes:
First, there is almost never a good reason to use readlines(), and especially not in this case. A file is already an iterable of the lines in the file; there's no reason to create a list of the same lines just to iterate it. This will give you the exact same result:
stream.seek(0)
for line in stream:
    print line

… but simpler, faster, and without wasting memory.
Second, your readstream function is more complicated than it needs to be. Normally, generators yielding values have advantages over lists—they let your caller start working on the values as soon as each one is available instead of waiting until they're all done, they don't waste memory building a list just to iterate over it, etc. But in this case, you're already building a list by calling split, so you might as well just return it:
def readstream(s):
    c = s.getvalue()
    return c.split('\n')
",A,3
21033881,2014-01-10 00:12:39.767000+00:00,"A string in Python 3 is a Unicode string (sequence of Unicode codepoints). bytes is a sequence of bytes (an integer 0..255).
expected bytes, not str error message means that initialize_auth_token() method should be changed to encode a Unicode text into bytes before calling b64encode() function.",A,0
43170093,2017-04-02 15:28:48.047000+00:00,"Your error is looking at connection is True. This is always going to be False. is tests for identity, two expressions resulting in a reference to the same object in memory.
If you want to print the truth value of an object, use the bool() function:
>>> bool(1)
True
>>> bool(0)
False

That 1 == True works at all is because in Python, the bool type is a subclass of int:
>>> issubclass(bool, int)
True

The integer value of True is 1, and False has an integer value of 0. Testing for equality won't work for other integer values:
>>> 1 == True
True
>>> 2 == True
False
>>> bool(2)
True

Note that your if test is otherwise meaningless; the only thing that is executed or not is the line player, which is a no-op expression that just returns the reference to player, but since there is nothing else there the reference is unused.
If you wanted the print statement to be executed only when if is true, you need to indent it to be further to the right from if and make it part of the nested block:
def validplayers(players):
    for player, connection in players:
        if connection:
            print player, connection, bool(connection)

This then only prints something for player a:
>>> def validplayers(players):
...     for player, connection in players:
...         if connection:
...             print player, connection, bool(connection)
...
>>> validplayers([('a', 1), ('b', 0)])
a 1 True
",A,3
17144502,2013-06-17 09:38:08.953000+00:00,"You are using boolean logic or and and, which short-circuit (return the first operand for which the outcome of the operator is fixed).
You are looking for the binary bitwise operators instead, | and &:
>>> 0b10 & 0b1
0
>>> 0b10 | 0b1
3

The or operator returns the first operand if it is true-y (not empty or numeric 0), the second operand otherwise, the and operator returns the first if it is false-y, the second operator otherwise. This is why you see 3 and 2 return 2, and 3 or 2 return 3. Both 2 and 3 are non-zero, so true in a boolean context.
Using 0 as a false value you'd see:
>>> 3 and 0
0
>>> 3 or 0
3
>>> 0 and 3
0
>>> 0 or 3
3
",A,10
48623631,2018-02-05 13:26:08.007000+00:00,"You can't use in on a file and have it work twice.
What happens is that in moves the file reader position forward. A file object is an iterable and in on such an object takes each object (lines from the file in this case) to test if one is equal. When you find an equal object, iteration stops and True is returned. If no equal object is found iteration continues until you reach the end.
For example, if t2 contains just one extra line, then trying to find that line in t1 will cause all of t1 to be read to try to find it. Any subsequent line read from t2 will now fail, because t1 has reached the end and no more lines will be returned.
At no point in this process will the iterable be 'rewound' to the start. The file read pointer only moves forward, never back to the start.
If using in worked in one scenario, then that means that each line read from t1 also happens to be present in the same order in t2. The reverse is not true, if t2 contains extra lines that are present in a different order in the first file.
If your files are small, reach one of the files into a set object:
with open('textfile1.txt','r') as t1, open('textfile2.txt', 'r') as t2:
    t1_lines = set(t1)
    for line in t2:
         if line in t1_lines:
             print(line)

A set lets you test for the presence of an object in constant time; regardless of how many lines there are in t1, on average it'll always take the same amount of time to test for one of those lines.
If you work with large files, you could force the file pointer back to the start, by using fileobj.seek(0). However, reading across files (each line is reach when scanning for an in test) is going to be slow. You could use memory mapping to mitigate this a little, but you probably would want to ensure your file lines were sorted and then use an index or bisection to speed up the process of finding a matching line.",A,4
45041017,2017-07-11 17:23:53.143000+00:00,"You have an attribute on your instance named move:
class Player:
    def __init__(self, name, health, bag, room_name, move):
        # ...
        self.move = move

This masks the method. Attribute look-up on an instance first looks at the instance attributes before attributes (including methods) on the class are found.
Rename that attribute or the method.",A,2
44457490,2017-06-09 12:09:59.223000+00:00,"Your replacement value must be a bytes object too:
re.sub(b""\x1b.*\x07"", b'', test)
#                     ^^^

You can't replace matching bytes with a str object, even if that is an empty string object.
Demo:
>>> import re
>>> test = b'\x1babc\x07123'
>>> re.sub(b""\x1b.*\x07"", b'', test)
b'123'
",A,1
13222776,2012-11-04 20:51:11.813000+00:00,"You need to print it with some formatting:
print(""Your code was {0:04d}"".format(count))

which will zero-pad your number up to 4 digits. See the string formatting documentation for more details.
Alternatively, you can use the str.zfill() method to add the extra zeros after string conversion:
print(""Your code was"", str(count).zfill(4))
",A,3
5651642,2011-04-13 15:20:41.830000+00:00,"Your diagnosis is incorrect; the portal_factory tool is stateless and thus does not require any session affinity.
Your error message is also very vague and looks incomplete. Have you checked the instance log for complete tracebacks?",A,1
27077342,2014-11-22 12:23:59.817000+00:00,"From that exact tutorial page you linked to, about three paragraphs down:

You can use an optional first positional argument to a Field to designate a human-readable name. That’s used in a couple of introspective parts of Django, and it doubles as documentation.
",A,3
18655122,2013-09-06 10:03:44.103000+00:00,"You'd have to loop through the values:
if any('B' == value.upper() for value in d.itervalues()):
    print 'Yup'

For Python 3, replace .itervalues() with .values(). This tests the minimum number of values; no intermediary list is created, and the any() loop terminates the moment a match is found.
Demo:
>>> d = {3:'a',2:'b'}
>>> if any('B' == value.upper() for value in d.itervalues()):
...     print 'Yup'
... 
Yup
",A,3
23839626,2014-05-23 23:17:13.380000+00:00,"If ./autogen is killed by a signal then self.process.returncode (after .wait() or .communicate()) is less than zero and its absolute value reports the signal e.g., returncode == -11 for SIGSERV.",A,0
52539738,2018-09-27 14:51:14.210000+00:00,"
BeeBole is accepting HTTP POST resquests in a json-doc format to the following URL: https://beebole-apps.com/api/v2

The JSON document format here is the part you missed; you need to pass the information as a JSON encoded body of the request. The params argument you used only sets the URL query string (the ?... part in a URL).
Use
import requests

payload = {
    ""service"": ""absence.get"",
    ""id"": ""absence_id""
}

auth = (""API_token"", ""x"")    
url = ""https://beebole-apps.com/api/v2""

req = requests.get(url, json=payload, auth=auth).json()

The json= part ensures that the payload dictionary is encoded to JSON and sent as a POST body. This also sets the Content-Type header of the request.
I've also updated the API authentication, all that the auth keyword needs here is a tuple of the username and password. See the Basic Authentication section.
You may want to wait with calling .json() on the response; check if the response was successful first:
req = requests.get(url, json=payload, auth=auth)
if not req.ok:
    print('Request not OK, status:', req.status_code, req.reason)
    if req.content:
        print(req.text)
else:
    data = req.json()
    if data['status'] == 'error':
        print('Request error:', data['message'])

This uses the documented error responses.",A,1
20950686,2014-01-06 13:05:18.577000+00:00,"Use the Counter.most_common() method, it'll sort the items for you:
>>> from collections import Counter
>>> x = Counter({'a':5, 'b':3, 'c':7})
>>> x.most_common()
[('c', 7), ('a', 5), ('b', 3)]

It'll do so in the most efficient manner possible; if you ask for a Top N instead of all values, a heapq is used instead of a straight sort:
>>> x.most_common(1)
[('c', 7)]

Outside of counters, sorting can always be adjusted based on a key function; .sort() and sorted() both take callable that lets you specify a value on which to sort the input sequence; sorted(x, key=x.get, reverse=True) would give you the same sorting as x.most_common(), but only return the keys, for example:
>>> sorted(x, key=x.get, reverse=True)
['c', 'a', 'b']

or you can sort on only the value given (key, value) pairs:
>>> sorted(x.items(), key=lambda pair: pair[1], reverse=True)
[('c', 7), ('a', 5), ('b', 3)]

See the Python sorting howto for more information.",A,151
13701396,2012-12-04 11:01:33.523000+00:00,"No, you cannot assign in an if statement. Your initial code is fine.",A,2
1643709,2009-10-29 13:23:47.147000+00:00,"We use this.
import os
os._exit(3)

To crash in a 'controlled' way.",A,2
6943478,2011-08-04 14:44:26.863000+00:00,"Register it for a more specific interface. Say zope.interface.Interface, or Products.Archetypes.interfaces.IBaseContent instead.",A,2
12841196,2012-10-11 13:53:25.197000+00:00,"You want to use a Validator, not a rexexp object:
vpass = form.Validator('Must be more than 5 characters', lambda x:len(x)>5)

A rexexp object expects a python regular expression as the first argument, and an error message as the second argument. A password validating regular expression would be something like:
vpass = form.regexp('.{6,}', 'Must be at least 6 characters')
",A,0
39571619,2016-09-19 11:03:07.253000+00:00,"Your question isn't totally clear, but I think you want a defaultdict which itself contains a defaultdict of lists. So:
class_details = defaultdict(lambda: defaultdict(list))

Alternatively, you may not need a nested dict at all; you could instead use the original defauldict with key that is a tuple:
class_details[(cls.day, cls.period)].append(cls)
",A,1
1045368,2009-06-25 18:04:54.500000+00:00,"First, use Uppercase Names for Classes.  lowercase names for attributes.
class Resource( object ):
    class_counter= 0
    def __init__(self, name, position, type, active):
        self.name = name
        self.position = position
        self.type = type
        self.active = active
        self.id= Resource.class_counter
        Resource.class_counter += 1
",A,17
23151703,2014-04-18 09:56:24.040000+00:00,"The browser sends only the submit button used over to the server; you can test for that name in the request.form object:
if ""Civil Services Prep Combo Pack"" in request.form:
    # ...

or you could look for the form keys; there should be just one key in the form, which will be the name attribute of the submit button used:
book = form.keys()[0]

In other words, the name attribute of a submit button is the key of the submitted field; the associated value is always going to be 'Add to Cart' for your forms.
It'll be much easier to test for what form was used if you added a hidden field to each form:
<form action=""addtocart"" method=""post"">
<input type=""hidden"" name=""book"" value=""Civil Services Prep Combo Pack"" />
<tr><td>Civil Services Prep Combo Pack</td><td><input type=""submit"" name=""add"" value=""Add to Cart""></td></tr>
</form>    

and use request.form['book'] to determine what form was used.",A,3
13302057,2012-11-09 04:19:25.827000+00:00,"There are multiple ways to do this (e.g., you could load both arrays then concatenate them via x = numpy.concatenate((x1, x2)), etc.), but what I'd do is concatenate the files on the fly, and pass the result to loadtxt.
As the documentation says, fname can be:

File, filename, or generator to read. If the filename extension is .gz or .bz2, the file is first decompressed. Note that generators should return byte strings for Python 3k.

So, you just need a generator that generates myfile1, then myfile2.
The obvious way to do this is with itertools:
with open(myfile1, 'rb') as f1, open(myfile2, 'rb') as f2:
  x, y, rho, phi = numpy.loadtxt(itertools.chain(f1, f2))

You may notice that I left off the unpack=True. That's because unpack only works if you pass a filename, not a file object or generator. (It looks at the extension, not the file magic or anything fancy.)
If you know the files will always be gzip or bzip2 files, you can just substitute gzip.GzipFile or bz2.BZ2File for open above. 
But if you need to deal with possibly compressed files, you have to do the same extension-checking that numpy does, then create the appropriate object (write a open_compressed function that wraps that up), at which point this becomes stubbornly sticking to the wrong solution. So, if that's an issue, I'd probably go with loading them separately and then doing numpy.concatenate.",A,4
49246144,2018-03-12 23:47:43.580000+00:00,"If data[5] is anything, then data is probably a list (or some other sequence, like a tuple, or even a dict or other mapping that happens to take integers as keys—but at any rate, you can give it a list, and this code will accept it).
Meanwhile, if data[5] is being compared to a dict with ==, it's almost certainly supposed to be a dict. So, data is not just any list, but a list whose 5th element is a dict. We have no idea what the first 5 elements (or any elements beyond 5) may be, but since the assert isn't checking them, we can put anything we want there.
As for the members of that data[5] dict—well, you almost got them right. They are all lists—two lists of strings, and one list of ints—but notice that you wrote tline = 3. A list of one int is not the same value as that int; you need the list.
So, let's build it inside-out:
tone = ['line', 'five', 'Test', 'one', '.']
ttwo = ['line', 'five', 'Test', 'two', '.']
tline = [3]
data5 = {'tone': tone, 'ttwo': ttwo, 'tline': tline}
data = [data5 for _ in range(6)]

And now, it'll pass.",A,3
8850584,2012-01-13 12:27:07.443000+00:00,"You may want this code
for i in range(len(numberArray)):
    print text + str( numberArray[i] )
",A,1
22661871,2014-03-26 13:01:44.443000+00:00,"From the warnings module documentation:

Repetitions of a particular warning for the same source location are typically suppressed.

This is by design.
I'd not use the warnings module if you want your message to be printed always; you could reset the filters (using warnings.resetwarnings(), but that's very much not recommended as you'd discard any user-configured filters too. You could add an explicit filter that always allows the message with the warnings.simplefilter() function:
warnings.simplefilter('always', UserWarning)

but I'd just write to sys.stderr directly instead.",A,6
25234052,2014-08-10 23:36:22.700000+00:00,"The problem is that elif: is a syntax error.
The whole point of elif is that it means else if. Just like you can't write if: without a condition, you can't write elif: without a condition.
Meanwhile, the statement after the : is clearly supposed to be the condition expression, not a statement. So you almost certainly wanted this:
elif chosenCave == str(goldCave):
",A,3
30003317,2015-05-02 14:15:19.543000+00:00,"You're not passing any of the parameters up to the superclass. That means they'll never be set when the object is created, which includes when it's retrieved from the database.
You should do:
def __init__(self, *args, **kwargs):
    super(Admin, self).__init__(*args, **kwargs)
",A,0
4825374,2011-01-28 06:07:15.237000+00:00,"If you don't know the exact path you could use find2perl. It generates File::Find::find() code for you. To execute the command immediately:
$ find2perl /path/to/dir -name filename.txt -exec echo exists {} | perl

Code generated by find2perl
#! /usr/bin/perl -w
    eval 'exec /usr/bin/perl -S $0 ${1+""$@""}'
        if 0; #$running_under_some_shell

use strict;
use File::Find ();

# Set the variable $File::Find::dont_use_nlink if you're using AFS,
# since AFS cheats.

# for the convenience of &wanted calls, including -eval statements:
use vars qw/*name *dir *prune/;
*name   = *File::Find::name;
*dir    = *File::Find::dir;
*prune  = *File::Find::prune;

sub wanted;
sub doexec ($@);

use Cwd ();
my $cwd = Cwd::cwd();

# Traverse desired filesystems
File::Find::find({wanted => \&wanted}, '/path/to/dir');
exit;

sub wanted {
    /^filename\.txt\z/s &&
    doexec(0, 'echo','exists','{}');
}

sub doexec ($@) {
    my $ok = shift;
    my @command = @_; # copy so we don't try to s/// aliases to constants
    for my $word (@command)
        { $word =~ s#{}#$name#g }
    if ($ok) {
        my $old = select(STDOUT);
        $| = 1;
        print ""@command"";
        select($old);
        return 0 unless <STDIN> =~ /^y/;
    }
    chdir $cwd; #sigh
    system @command;
    chdir $File::Find::dir;
    return !$?;
}
",A,1
50049447,2018-04-26 18:06:59.843000+00:00,"The error isn't with the way you're assigning things, but with the way you're accessing them.
Just as you have to do self.bSayisi to set an attribute, you have to do self.isim to access one. So:
print(self.isim, ""başarılı"")

(and the same for the other line…)

If you're wondering why you were able to access other values like bSayisi without self.bSayisi—that's just because you happen to have a parameter named bSayisi that happens to have the same value as self.bSayisi (because you just made that true a few lines earlier). If you changed it to, say, self.bSayisi = bSayisi*2, or you renamed the parameter to myBSayisi and did self.bSayisi = myBSayisi, you'd see that just using bSayisi instead of self.bSayisi was no longer correct.

However, while this eliminates the error, I'm not sure it actually does what you want. At the time you're doing this print, you haven't assigned an isim value to the object yet, so it's going to get the class value as a default, so it's always just going to be "" "". Is that really what you wanted? 
If not, you need to move the print calls to some other method that you can call later, after having assigned isim. For example:
class sayfa():
    isim="" ""
    def __init__(self,bSayisi,ySayisi,pSayisi,iSayisi,tSayisi):
        self.bSayisi=bSayisi
        self.ySayisi=ySayisi
        self.pSayisi=pSayisi
        self.iSayisi=iSayisi
        self.tSayisi=tSayisi
    def displaystuff(self):
        if ((((self.bSayisi+self.ySayisi+self.pSayisi)/self.iSayisi)/self.tSayisi)*100)>0.2:
             print(self.isim,""başarılı"")
        else:
             print(self.isim,""başarısız"")

ybs1=sayfa(365000,65000,870,500,1125000)
ybs1.isim=""YBS-1""
ybs1.displaystuff()


Of course moving the isim into the constructor works, by avoiding the problem you were running into. It's not an answer to how to add data after the __init__ method, of course, because you're instead adding the data in the __init__ method. When that's appropriate, it's the simplest answer.
But if it looks weird in this case (I'll take your word for it; I don't know exactly what this code is trying to do), it's probably the wrong answer for this particular class. 
In which case, you do need to know how to add data after the __init__ method, as you asked. Or, rather, you need to know how to access that data—because you were already adding it correctly.",A,1
18291246,2013-08-17 16:59:23.613000+00:00,"You gave your ProductID the type intiger; that is not a type SQLite recognizes. Correct that to be integer and the column will auto-increment.
See SQLite Autoincrement for more details.",A,2
12603283,2012-09-26 13:50:16.280000+00:00,"You can control the Unix terminal more fully with the curses library. The library essentially lets you build a simple terminal GUI.
If you need more, take a look at Urwid as well. Urwid offers more complex GUI widgets for the discerning terminal GUI developer. :-)",A,2
28116599,2015-01-23 18:42:15.170000+00:00,"get_text() (as the name suggests) already returns a text -- Unicode string. You should not call str(); you can print Unicode text directly:
>>> str(u'\N{SNOWMAN}')                                                                                   
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
UnicodeEncodeError: 'ascii' codec can't encode character u'\u2603' in position 0: ordinal not in range(128)
>>> print u'\N{SNOWMAN}'
☃

If you need to convert Unicode string to bytes; call .encode() method (don't use str()):
bytestring = unicode_text.encode(character_encoding)
",A,0
49748326,2018-04-10 07:58:13.627000+00:00,"Computer language is not English. Humans are great at guessing what subject is repeated in a compound sentence, computers are not. You always explicitly state what you are testing against on both sides of a logical operator.
That's because in the expression structure <left> and <right>, both left and right are always independent expressions. Expressions can build on expressions on expressions, but a computer programming language will not just re-use (a part of) the left expression in the right expression.
So yes, you have to explicitly name grade again.
Or you could use a different expression form. You could use a chained comparison expression; Python lets you collapse any expression of the form <foo> <comparison1> <shared> and <shared> <comparison2> <bar> into <foo> <comparison1> <shared> <comparison2> <bar>, and the shared expression will be executed just once. 
So if you turned
grade >= 80 and grade <= 89

into
80 <= grade and grade <= 89

you can replace that with
80 <= grade <= 89

However, note that the preceding test already handled the grade > 89 case, you can safely drop the upper bound tests:
def grade_converter(grade):
    if grade >= 90:
        return ""A""
    elif grade >= 80:
        return ""B""
    elif grade >= 70:
        return ""C""
    elif grade >= 65:
        return ""D""
    else:
        return ""F""

Last but not least, you can use a Computer Science trick. Rather than test each grade band separately, one by one, you could use bisection; this always works when your options are sorted.
Instead of starting at the highest value, start in the middle; that divides the possibilities in 2. From there, you keep halving the possibilities until you have the right grade band. This means you only have to do, at most, Log(N) tests for N possibilities, while starting at the top grade will require up to N tests. For 5 tests that's not much of a difference (1.6 steps on average, vs 5), but when N becomes really large, then you'll quickly notice a difference; if you had 1 million options, you could find the matching option in less than 14 steps, guaranteed.
The Python library includes an optimised implementation in the bisect module:
import bisect

def grade_converter(grade):
    grades = ['F', 'D', 'C', 'B', 'A']
    limits = [65, 70, 80, 90]
    return grades[bisect.bisect(limits, grade)]
",A,5
14856988,2013-02-13 15:28:22.300000+00:00,"input only accepts one argument, you are passing it 3. You need to use string formatting or concatenation to make it one argument:
answer = input(""Is it {} ?"".format(guess))

You were confusing this with the print() function, which does indeed take more than one argument and will concatenate the values into one string for you.",A,6
28565040,2015-02-17 15:31:24.313000+00:00,"
to give the user input when asked in java while executing the script through python

#!/usr/bin/env python
import os
from glob import glob
from subprocess import Popen, PIPE, call

wdir = ""codewar/media""

# 1) list all .java files in directory
for path in glob(os.path.join(wdir, ""*.java"")):
    # 2) compile the java file
    if call(['javac', path]) != 0: # error
        continue
    # 3) if there is no error it then executes the file using its 
    # class name which is same as file name
    classname = os.path.splitext(os.path.basename(path))[0]
    p = Popen(['java', '-cp', wdir, classname], 
              stdin=PIPE, stdout=PIPE, stderr=PIPE,
              universal_newlines=True) # convert to text (on Python 3)
    out, err = p.communicate(input='12345')
    if p.returncode == 0:
        print('Got {result}'.format(result=out.strip().rpartition(' ')[2]))
    else: # error
        print('Error: exit code: {}, stderr: {}'.format(p.returncode, err))

The key as @user2016436 suggested is to use .communicate() method here.


But I want in such a way that while its running and displays enter a
  number : the screen should wait for my input and when i enter the
  number it resumes its execution

If you don't need to capture the output and you want to provide the input manually from the keyboard then you don't need to use Popen(.., PIPE) and .communicate(), just use call() instead:
#!/usr/bin/env python
import os
from glob import glob
from subprocess import call

wdir = ""codewar/media""

# 1) list all .java files in directory
for path in glob(os.path.join(wdir, ""*.java"")):
    # 2) compile the java file
    if call(['javac', path]) != 0: # error
        continue
    # 3) if there is no error it then executes the file using its
    # class name which is same as file name
    classname = os.path.splitext(os.path.basename(path))[0]
    rc = call(['java', '-cp', wdir, classname])
    if rc != 0:
        print('Error: classname: {} exit code: {}'.format(classname, rc))
",A,1
17399637,2013-07-01 07:48:12.037000+00:00,"Use a dictionary, and store only the highest value found so far. Because the scores are given as letters, that means you need to find the 'lowest' letter, lexographically:
import csv

students = {}

with open(inputcsvfile, 'rb') as scoressource:
    reader = csv.reader(scoressource)
    for name, score in reader:
        if score < students.get(name, 'Z'):
            students[name] = score

with open(outputcsvfile, 'wb') as scoresdest:
    writer = csv.writer(scoresdest)
    for name, score in students.iteritems():
        writer.writerow([name, score])
",A,1
38664249,2016-07-29 17:18:02.477000+00:00,"In the days of yore, built-in types such as int and dict and list were very different from types built with class. You could not subclass built-in types for example.
Gradually, in successive Python 2.x releases, the differences between class types and builtin types have eroded; the introduction of new-style classes (inheriting from object) in Python 2.2 was one such (major) step. See Unifying types and classes in Python 2.2.
Removing the use of type in built-in type representations is just the last step in that process. There was no reason anymore to use the name type anymore.
In other words, between Python 2.7 and 3.x this is a cosmetic change, nothing more.",A,7
21843093,2014-02-18 02:14:37.690000+00:00,"Trying to access data files as relative paths off dirname(__file__) is a really bad idea, unless you only want to run the program in the build tree. If you want to be able to install and run the program—whether using py2exe or any other mechanism—you're just asking for trouble.
If you're trying to package files with the executable, the right way to do this is with either the data_files mechanism from py2exe, or, if you can install and use setuptools, the more powerful and flexible Package Resources mechanism. 
If, on the other hand, you want the user to supply the CSV files after installing your app, you almost certainly don't want to make him put them into your app's directory. The usual thing to do is access them as relative paths off the current working directory. In other words, just use ., not dirname(__file__).
But, if you really want to do this… the problem is that __file__ is the filename of your main script—which of course isn't useful when you're running a bundled executable. You can use sys.argv[0] or sys.executable, depending on your use case.",A,1
8012356,2011-11-04 16:13:50.060000+00:00,"As the parameter name self suggests the C foo() function might make more sense as a method in Python:
cdef extern void c_foo ""foo"" (char* n, MyClass_ptr)

cdef class MyClass:
   cdef MyClass_ptr this

   def foo(self, unicode n not None):
       b = n.encode('ascii')
       cdef char* p = b
       c_foo(p, self.this)

See also Wrapping C library.
Cython translates your code to C (or C++). At this point you can forget it exists. Generated C code is an ordinary C extension for Python. This code contains ordinary Python C API calls such as PyBytes_AsString() (convert Python bytes to char*) or PyInt_FromLong() (convert C int to Python integer).
MyClass_ptr is used as is:
struct __pyx_obj_3foo_MyClass {
  PyObject_HEAD
  MyClass_ptr this; /* <-- using MyClass_ptr as is */
};

i.e., pure Python code never sees it so there is no conversion from/to Python objects.",A,0
20080698,2013-11-19 19:48:09.450000+00:00,"In Python 3, you need to use io.BytesIO() (with a bytes value) to simulate an uploaded file:
rv = self.app.post('/add', data=dict(
                               file=(io.BytesIO(b""this is a test""), 'test.pdf'),
                           ), follow_redirects=True)

Note the b'...' string defining a bytes literal.
In the Python 2 test examples, the StringIO() object holds a byte string, not a unicode value, and in Python 3, io.BytesIO() is the equivalent.",A,11
15701643,2013-03-29 10:39:47.500000+00:00,"As Matthias says in the comment, it doesn't look as if you're actually invoking Django at all - those are normal Apache directory indexes (so naturally Django won't be parsing the templates, since it's not even being called).
Since you're just starting out, you should not be using Apache at all, but start up the local development server (./manage.py runserver) as explained in the tutorial.",A,1
17390757,2013-06-30 13:19:04.730000+00:00,"The function object has a code object associated with it; you can exec that code object:
>>> a = 3
>>> b = 4
>>> c = 5
>>> def add():
...     d = a+b+c
... 
>>> exec add.func_code

This will not however set d in the local namespace, because it is compiled to create a local name only, using the STORE_FAST opcode. You'd have to transform this bytecode to use STORE_DEREF opcodes instead, requiring you to learn AST transformations.
If you did want to go down this route, then read Exploring Dynamic Scoping in Python, which teaches you the ins and outs of Python scoping and byte code.",A,5
51061767,2018-06-27 11:34:33.620000+00:00,"Python is not Java. If you want to access a class, you need to either import it directly, or reference it via the module you did import. So:
import LicensePlateDetector
...
lpd = LicensePlateDetector.LicensePlateDetector(image)

or
from LicensePlateDetector import LicensePlateDetector
...
lpd = LicensePlateDetector(image)

This is one reason why PEP8 recommends naming files as lower_case_with_underscore.py.",A,1
36886384,2016-04-27 09:55:31.883000+00:00,I don't understand why you think you need to define current_jobs on Account. That is automatically provided for you via the reverse relationship as company_set; if you need it to be current_jobs you can set the related_name attribute.,A,1
11736964,2012-07-31 09:25:05.307000+00:00,"You need to create a ModelForm subclass with your customizations, and pass that as the form argument to modelformset_factory.
class DealForm(forms.ModelForm):    
    deal_type = forms.CharField(max_length=30,
                                widget=forms.TextInput(attrs={'readonly': 'True'}))

    def clean_deal_type(self):
        return self.instance.deal_type

...

deal_formset = modelformset_factory(Deal, form=DealForm,
                                    fields=['status', 'deal_type'], extra=0)

Note there's a fair amount of flexibility in how you do this - the fields could go in an inner Meta class on the form, you could keep your modelformset subclass and put the form declaration there, and so on.",A,1
26947518,2014-11-15 15:41:31.150000+00:00,"Pass a list with the default value; there is no need for a list comprehension here either:
A.extend(B or ['no value'])

I used or here because an empty list is considered false in a boolean context; it is more compact here and avoids repeating yourself.
The conditional expression version would be:
A.extend(B if B else ['no value'])
",A,4
20094001,2013-11-20 10:57:14.800000+00:00,"No, you cannot change the representation of a function object; if you wanted to add documentation, you'd add a docstring:
def foo():
    """"""Frob the bar baz""""""

and access that with help(foo) or print foo.__doc__.
You can create a callable object with a custom __repr__, which acts just like a function:
class MyCallable(object):
    def __call__(self):
        return ""spam""
    def __repr__(self):
        return 'hehe function created by awesome programmer'

Demo:
>>> class MyCallable(object):
...     def __call__(self):
...         return ""spam""
...     def __repr__(self):
...         return 'hehe function created by awesome programmer'
... 
>>> hehe = MyCallable()
>>> hehe
hehe function created by awesome programmer
>>> hehe()
'spam'
",A,10
37623501,2016-06-03 21:12:52.243000+00:00,You have syntax errors; you can't have a line starting with . immediately after a line ending with ;. Remove those semicolons after each when clause.,A,1
30154817,2015-05-10 18:04:03.857000+00:00,"It's very hard to figure out what you're trying to do or what your problem is from an invalid JSON fragment, a vague description, and no code, but…
I think your elements is supposed to be a list with a bunch of dicts in it, each of which has a type. You want all of the elements whose type is the string 'node'. And then, for each of them, you want the lat and lon values. So, let's write that:
coords = []
for element in elements:
    if element['type'] == 'node':
        coords.append((element['lat'], element['lon']))

Or, more briefly:
[(e['lat'], e['lon']) for e in elements if e['type'] == 'node']

That won't run on your broken example, but if I remove the two stray } before each of the nodes keys in the first two elements so it'll parse (of course I'm just guessing at whether that's what your actual structure is supposed to be…), I get this:
[(55.6933076, 21.1517616),
 (55.6931543, 21.1514953),
 (55.6991153, 21.1647621),
 (55.7001553, 21.1671538)]


If you'd rather have that as, say, a dict mapping each node's ID to its lat/lon tuple, same idea:
coords = {}
for element in elements:
    if element['type'] == 'node':
        coords[element['id']] = (element['lat'], element['lon'])

Or:
{e['id']: (e['lat'], e['lon']) for e in elements if e['type'] == 'node'}

Which gives me:
{279385160: (55.7001553, 21.1671538),
 470874618: (55.6933076, 21.1517616),
 1142007444: (55.6991153, 21.1647621),
 2362142222: (55.6931543, 21.1514953)}
",A,0
8669342,2011-12-29 14:54:23.907000+00:00,"You have two choices for working in two-dimensional space like this.

A list of lists.  [ [0, 0, ..., 0], [0, 0, ..., 0], ... [0, 0, ..., 0] ] The outer list is the 'X' access, the inner list is the 'Y' access.  Each point is space[x][y].  You build it with space = list( list( EMPTY for j in range(Y_size) ) for i in range(X_size) ) or something similar.
You mask off rectangles with some filler algorithm that sets values into a rectangular patch of space.  
for x in range( low, high ):
    for y in range ( low, high ):
        space[x][y]= FILLED # or whatever object you're putting there.

A mapping.  { (0,0): 0, (0,1): 0, ... (X,Y): 0 }.  Each point is space[x,y].  You build it with space = dict( ( (x,y), EMPTY ) for x in range(X_size) for y in range(Y_size) ).
You mask off rectangles with almost the same filler algorithm.  Just change the syntax slightly.
",A,0
2782095,2010-05-06 15:05:53.163000+00:00,"You should use the ForeignKey to content_type, which stores this information.
{% if news_item.content_type.model == 'Artwork' %}
",A,3
288643,2008-11-13 22:52:07.917000+00:00,"Since floating-point values can't be compared for equality, an integer may have been used to make the SQL simpler.
For example
(0.3==3*.1)

is usually False.
However
abs( 0.3 - 3*.1 )

Is a tiny number (5.55e-17).  But it's  pain to have to do everything with (column-SomeValue) BETWEEN -0.0001 AND 0.0001 or ABS(column-SomeValue) < 0.0001.  You'd rather do column = SomeValue in your WHERE clause.",A,4
20094561,2013-11-20 11:22:51.357000+00:00,"list.extend() extends a list in place. Python standard library methods that alter objects in-place always return None (the default); your list comprehension executed a.extend() twice and thus the resulting list consists of two None return values.
Your a.extend() calls otherwise worked just fine; if you were to print a it would show:
[1, 2, 3, 4, 3, 4]

You don't see the None return value in the Python interpreter, because the interpreter never echoes None results. You could test for that explicitly:
>>> a = []
>>> a.extend(['foo', 'bar']) is None
True
>>> a
['foo', 'bar']
",A,2
20445861,2013-12-07 19:53:57.837000+00:00,"You can use the ast module to analyse the code.
import ast

class ImportNodeVisitor(ast.NodeVisitor):
    def visit_Import(self, node):
        print(ast.dump(node))
        self.generic_visit(node)

    def visit_ImportFrom(self, node):
        print(ast.dump(node))
        self.generic_visit(node)


with open(sourcefilename) as sf:
    tree = ast.parse(sf.read(), sourcefilename)
    ImportNodeVisitor().visit(tree)

produces
Import(names=[alias(name='foo', asname=None)])
ImportFrom(module='bar', names=[alias(name='*', asname=None)], level=0)
",A,5
27673045,2014-12-28 01:38:42.667000+00:00,"The proper way is to address the outer dictionary, then index the list, then get the 'Last' key:
last = data['result'][0]['Last']

Demo:
>>> from pprint import pprint
>>> data = {""success"":""true"",""message"":"""",""result"":   
...  [{""Last"":""0.00000000"",""Bid"":""42258.06451613"",""Ask"":""100000000.0""}]}
>>> pprint(data)
{'message': '',
 'result': [{'Ask': '100000000.0',
             'Bid': '42258.06451613',
             'Last': '0.00000000'}],
 'success': 'true'}
>>> data['result']
[{'Ask': '100000000.0', 'Bid': '42258.06451613', 'Last': '0.00000000'}]
>>> data['result'][0]
{'Ask': '100000000.0', 'Bid': '42258.06451613', 'Last': '0.00000000'}
>>> data['result'][0]['Last']
'0.00000000'

Your first attempt tried to address all the keys in the top-level dictionary, which would have failed because data['success'] is a string, and strings cannot be indexed by ['message'].
Your second attempt failed because the top-level object is a dictionary, not a sequence, and there is no 0 key in it.",A,3
23385814,2014-04-30 10:34:59.770000+00:00,"You only need to add __init__.py to directories that must be treated as packages. Any folder outside the PYTHONPATH search path is never a package and doesn't need to have an __init__.py.
You don't need __init__ in the top-level directory listed on PYTHONPATH either, because that's not a package. C is not a package here, it is a folder on module search path. You cannot import from C; only D is a package.
For your setup, if C:A\B\C is on the module search path, then only items in C can be imported. C:A\B\C\module.py is importable as module, and C:A\B\C\D\ is importable if there is an __init__.py file in that directory, as import D.
Python uses the search path to find top-level names you import; if you try to import foo, the search path is tried one by one, each a location on your harddisk. C:\A\B\C is one such location, it'll look inside that location, but the directory name of the location is never part of the search space.
See the Python tutorial on packages for more information.",A,3
40769841,2016-11-23 16:38:38.060000+00:00,"It should be: 
def __init__(self, period=None, *args, **kwargs):
",A,4
30344874,2015-05-20 08:57:35.590000+00:00,"Python indexes are zero based. The last element is at index 3, not 4:
>>> a = [1,2,3,4]
>>> a[3:]
[4]
",A,1
48546649,2018-01-31 16:13:50.010000+00:00,"The code you posted shows that your AnalyticsView is in its own file; you need to import that file.
from survey.views.analytics_view import AnalyticsView
",A,1
19046955,2013-09-27 09:10:47.013000+00:00,"When you import calcoo, all code at the top level is executed. That includes parsing sys.argv values.
Move whatever should only be executed when running the module as a script to a block guarded by a test for the module name; if the name is __main__ then your code is run as a script, otherwise it was imported as a module:
class Calculator():
    def sumatory(self):                                   
        return float(operator1) + float(operator2)
    def substract(self):
        return float(operator1) - float(operator2)

if __name__ == '__main__':
    import sys
    operator1= sys.argv[1]
    operation= sys.argv[2]
    operator2= sys.argv[3]
    try:
        operator1 = float(sys.argv[1]) 
        operator2 = float(sys.argv[3])
    except ValueError:
        sys.exit(""Error: Non numerical Parameters"")

    if operation == ""sum"":
        print Calculator().sumatory()
    elif operation == ""substract"":
         print Calculator().substract()
    else:
        print ""Error, operation not supported.""

Now, when you import calcoo, only the Calculator class will be defined; the rest of the code will not be run.",A,2
15117793,2013-02-27 16:56:54.207000+00:00,"The uncertainties.Variable() class uses a __slots__ attribute to save memory. As a result, to be pickleable, it must define a __getstate__ method as well (see Why am I getting an error about my class defining __slots__ when trying to pickle an object?).
If you need to add your own additional attributes, you'll have to override that __getstate__ method. Declaring your additional attributes in a __slot__ attribute of your own is probably a good idea too:
from uncertainties import Variable

class value(Variable):
    __slots__ = ('show',)  # only list *additional* slots

    def __init__(self, args, showing=False):
        self.show = showing
        super(value, self).__init__(args[0], args[1])

    def __getstate__(self):
        obj_slot_values = {}
        for cls in type(self).mro():
            obj_slot_values.update((k, getattr(self, k)) for k in getattr(cls, '__slots__', ()))
        # Conversion to a usual dictionary:
        return obj_slot_values

This new __getstate__ is required because the Variable.__getstate__ method assumes that there will only be one __slots__ attribute, while each class in the MRO may have one instead.
This is really a limitation of the uncertainties library; I've submitted a pull request that addresses this and would remove the need to override the __getstate__ method in a subclass.",A,6
26784333,2014-11-06 16:23:12.510000+00:00,"You are trying to pass floating point values to random.randint(). That function only takes integers.
You need to use the random.uniform() function instead; it'll produce a uniformly random value between the lower and upper bound (inclusive):

Return a random floating point number N such that a <= N <= b for a <= b and b <= N <= a for b < a.

Demo:
>>> import random
>>> random.uniform(0.1, 0.9)
0.6793304134926453
",A,10
11684685,2012-07-27 09:02:37.017000+00:00,"You seem to have missed the whole point of AppEngine, which is that Google takes care of scaling your app for you automatically. You seem to be confusing 'instance' with 'version' - you have control over which version of your app is serving, but Google dynamically creates and kills instances of that app depending on load. That's the main benefit of using AppEngine in the first place.",A,2
17125900,2013-06-15 16:55:23.730000+00:00,"It is slow because you are re-reading a file for each loop iteration, and create a new function object. Neither of these two things are dependent on the loop variable; move these out of the loop to only run once.
Furthermore, the simple function can be inlined; calling a function is relatively expensive. And don't call ''.join() twice, either. And you are only using lowercase letters to generate the words, so .lower() is redundant:
with open('/Users/kyle/Documents/english words.txt') as word_file:
    english_words = set(word.strip().lower() for word in word_file)

for p1 in itertools.combinations('abcdefghijklmnopqrstuvwxyz', 4):
    word = ''.join(p1)
    print '{} is {}'.format(word, word in english_words)

Since you are generating words of length 4, you could save yourself some memory by only loading words of length 4 from your english words file:
with open('/Users/kyle/Documents/english words.txt') as word_file:
    english_words = set(word.strip().lower() for word in word_file if len(word.strip()) == 4)
",A,13
49507575,2018-03-27 08:09:21.033000+00:00,"The proper translation of your PHP code would be:
def get_all_categories(cls, parent_id=None, indent=''):
    output = []
    categories = cls.db.query(Category).filter(~Category.is_deleted, Category.parent_id==parent_id)
    for cat in categories.order_by(Category.sort_order):
        output.append(
            '<option value={cat.id}>{indent}{cat.name}</option>\n'.format(
                cat=cat, indent=indent))
        if cat.id != parent_id:
            output.append(cls.get_all_categories(cat.id, indent + '&nbsp;&nbsp;'))
    return ''.join(output)

There is no need to pass output in to the recursive call, just collect the output and add it to the current output being built. I used a list to avoid repeated string concatenation (which would give you quadratic performance, rather than linear).
You were also missing the ORDER BY clause; there is no need to order by parent_id when you are at the same time filtering on the parent id (they are all going to be the same), so you only need to sort on the sort_order column.
I'm assuming this is SQLAlchemy; you can use ~ to state a WHERE NOT <column> clause.
You should also consider separating the presentation from retrieving the categories as a tree. You can then move to a generator approach and avoid the memory footprint of having to hold the whole tree in memory (although SQLAlchemy will cache most anyway):
def get_all_categories(cls, parent_id=None):
    categories = cls.db.query(Category).filter(~Category.is_deleted, Category.parent_id==parent_id)
    for cat in categories.order_by(Category.sort_order):
        yield {category: cat, children: cls.get_all_categories(cat.id)}

and then use the templating engine to render the children elements recursively.
For a demo, lets first build some categories in an in-memory database; this uses a simplified version of your model; the tree is as follows:

Foo


Spam
Ham

Bar


Gone (marked as deleted)


The setup code:
>>> from sqlalchemy import create_engine
>>> engine = create_engine('sqlite:///:memory:')
>>> from sqlalchemy.ext.declarative import declarative_base
>>> Base = declarative_base()
>>> from sqlalchemy import Column, Integer, String, Boolean, ForeignKey
>>> class Category(Base):
...     __tablename__ = 'category'
...     id = Column(Integer, primary_key=True, autoincrement=True)
...     name = Column(String(50))
...     parent_id = Column(Integer, ForeignKey('category.id'))
...     sort_order = Column(Integer)
...     is_deleted = Column(Boolean, default=False)
...
>>> Base.metadata.create_all(engine)
>>> from sqlalchemy.orm import sessionmaker
>>> session = sessionmaker(bind=engine)()
>>> foo = Category(name='Foo', sort_order=1)
>>> bar = Category(name='Bar', sort_order=2)
>>> session.add_all([foo, bar])
>>> session.flush()
>>> spam = Category(name='Spam', parent_id=foo.id, sort_order=1)
>>> ham = Category(name='Ham', parent_id=foo.id, sort_order=2)
>>> gone = Category(name='Gone', parent_id=bar.id, sort_order=1, is_deleted=True)
>>> session.add_all([spam, ham, gone])
>>> session.commit()

and then using a dummy class to bind the class method to, the output can be produced:
>>> class Demo:
...     db = session
...     @classmethod
...     def get_all_categories(cls, parent_id=None, indent=''):
...         output = []
...         categories = cls.db.query(Category).filter(~Category.is_deleted, Category.parent_id==parent_id)
...         for cat in categories.order_by(Category.sort_order):
...             output.append(
...                 '<option value={cat.id}>{indent}{cat.name}</option>\n'.format(
...                     cat=cat, indent=indent))
...             if cat.id != parent_id:
...                 output.append(cls.get_all_categories(cat.id, indent + '&nbsp;&nbsp;'))
...         return ''.join(output)
...
>>> print(Demo.get_all_categories())
<option value=1>Foo</option>
<option value=3>&nbsp;&nbsp;Spam</option>
<option value=4>&nbsp;&nbsp;Ham</option>
<option value=2>Bar</option>
",A,4
5022355,2011-02-16 21:30:25.973000+00:00,"
In this scenario, how would you implement a way to count the number of comments written by an user ? 

Responsibility belongs with review.  It's an aggregate of comments.  Count is a first-class feature of any aggregate.

How do i make others entities access comments if they need to ? 

Comments are accessible via a Review.  A Review is an aggregate of comments.

What if comment have a composition relationship with some other entity ?

""What if"" questions are hard to answer without a concrete and specific example.  After all, the design is driven by the problem domain, not random thoughts.
If some ""other"" entity also appears to be a composition of Comments, you have to go back to the domain experts and try to determine where the real responsibility lies.  
One pair of question is ""if the review is removed, what happens to the comments?""  and ""If the mysterious 'other' is removed, what happens to the comments?""  This can help find the responsibilities.",A,3
21579612,2014-02-05 14:10:52.993000+00:00,"No, it is not necessary, but you need to be certain you are running your script with the right working directory. Your script working directory is evidently not D:/.
In practice, it is better to only use relative paths if you are in full control of the working directory. You can get the current working directory with os.getcwd() and set it with os.chdir() but using absolute paths is usually better.
For paths relative to the current module or script, use the __file__ global to produce a directory name:
import os.path

here = os.path.dirname(os.path.absolute(__file__))

then use os.path.join() to make relative paths absolute in reference to here.",A,6
33841988,2015-11-21 09:56:38.237000+00:00,"There are three steps:

Convert the date string into a naive datetime object:
from datetime import datetime 

dt = datetime(*map(int ,'2015-01-01'.split('-')))

Get a timezone-aware datetime object:
import pytz # $ pip install pytz

aware = pytz.timezone(""US/Mountain"").localize(dt, is_dst=None)

is_dst=None raises an exception for ambiguous or non-existing times.   Here're more details about what is is_dst flag and why do you need it, see ""Can I just always set is_dst=True?"" section
Get POSIX timestamp:
timestamp = aware.timestamp()

.timestamp() is available since Python 3.3+. See multiple solutions for older Python versions.
",A,3
14552042,2013-01-27 20:43:24.933000+00:00,"You should use has_key() rather than just accessing the key:
{% if group_inst.has_key %}
",A,2
16057600,2013-04-17 10:25:17.567000+00:00,"To process one line at a time from a big file with many lines:
def main():
    with open('input_file.txt') as file:
        for line in file:
            process_line(line)

If the file contains numbers (e.g., one short integer per line) and you need all of them then you could use numpy arrays:
from functools import partial
from itertools import imap
import numpy as np

def count_lines(file):
    """"""Return number of lines in the file.""""""
    return sum(chunk.count('\n') for chunk in iter(partial(file.read, 1<<15),''))

with open('input_file.txt', 'rb') as file:
    nlines = count_lines(file) # count lines to avoid overallocation in fromiter
    file.seek(0) # rewind
    a = np.fromiter(imap(int, file), dtype=np.int16, count=nlines)
",A,0
32873827,2015-09-30 19:07:49.497000+00:00,"A dictionary comprehension is executed in a new scope, a lot like a nested function call. You cannot expect to access the locals of the parent scope in a list comprehension.
I strongly recommend you do not use locals like this. Create a separate dictionary to act as a namespace and look up your columns in that:
namespace = {
    'zeta': value_for_zeta,
    # ... etc.
}

then use {col: [namespace[col]] for col in columns}.
Failing that, you can store the locals() dictionary in a new variable and reference that; either directly, or through passing it in as the namespace for eval():
namespace = locals()
dic = {col: [eval(col, namespace)] for col in columns}

or simply:
namespace = locals()
dic = {col: [namespace[col]] for col in columns}

This works now because namespace is a closure; a name taken from the parent scope.
Note that the same limits apply to generator expressions, set comprehensions, and in Python 3, list comprehensions. Python 2 list comprehensions were implemented before all the other types and followed a different implementation strategy that did not involve a new scope, but this approach did not allow for generator expressions to work and the new approach with a separate scope was generally found to work better.",A,6
25126678,2014-08-04 20:06:43.980000+00:00,"If I understand you correctly, you have a User, and you want to get the CarList. Since you have a ForeignKey from CarList to User, that is a simple backwards relation:
carlists = user.carlist_set.all()
",A,3
4007468,2010-10-24 07:54:29.703000+00:00,"As ars says, your methods return a queryset. So what you need to do is to create a custom subclass of QuerySet, which contains the by_this and by_that methods, and then in MyModelManager.get_query_set return your subclassed queryset.",A,0
28369952,2015-02-06 16:08:57.983000+00:00,"time.sleep() may return earlier if the sleep is interrupted by a signal or later (depends on the scheduling of other processes/threads by OS/the interpreter).
To improve accuracy over multiple iterations, to avoid drift for large number of iterations, the countdown may be locked with the clock:
#!/usr/bin/env python
import sys
import time

for i in reversed(range(1, 1001)):
    time.sleep(1 - time.time() % 1) # sleep until a whole second boundary
    sys.stderr.write('\r%4d' % i)
",A,1
4984201,2011-02-13 12:47:27.077000+00:00,"There's no need to guess. You should be able to find out from the code of the models themselves. 
If for some odd reason you haven't got access to the code - and I can't imagine why - you can always use the interactive shell - import your models there, do a query, then do dir(my_obj) to find a list of all the attributes of my_obj.",A,0
51884155,2018-08-16 19:29:54.897000+00:00,"An image is represented as a 3D array:

Dimension 0 is the row.
Dimension 1 is the column.
Dimension 2 is the plane (red, green, or blue).

So, for example, photo_data[10, 20, 1) is the green value for pixel (10,20).
So, photo_data[0] is all three planes of all pixels in the first row.
But photo_data[:, :, 0] is the red values of all pixels in all rows and all columns.",A,1
20959001,2014-01-06 20:49:21.193000+00:00,"The easiest way to check if a module is available is to import it. And this is almost always what you want, as well. If the module isn't available, this will fail. If the module is available… well, the reason you're checking is because you need to verify that you will be able to import it, so you're not wasting any time importing it now.
More generally, in Python, it's easier to ask forgiveness than permission. If you want to know whether you can do something, just try to do it.
So, this is almost an answer on its own:
import mymodule

The code will ""early return, break the execution if it's not available."" The only problem is that you don't want to see the error message.
There are two ways around this.

First, you could wrap the code up like this:
try:
    import mymodule
except ImportError:
    pass
else:
    # everything else you were going to do.

Whether ""the code"" is the whole function body, or the line of code that calls the function, or a chunk of top-level code in your script or module, or whatever, it makes no difference.

If you want to avoid the extra block indent, you can ""return early"" in various different ways. (Although really, it's almost always trivial to just extract all the indented code out into a new function, which means you end up with just a one-line function call being in the else block.) 
How you do that depends on what level you're trying to return from, and that isn't clear from your question. If you want to return early from a function, just return (possibly with an appropriate value). If you want to return early from the whole program, just sys.exit(0). So:
try:
    import mymodule
except ImportError:
    sys.exit(0)
# the rest of your code

You could also exit with a positive value, which is a way to tell the shell or whoever else called your program that it ""failed"", without printing out an error traceback.",A,1
14006878,2012-12-22 21:53:43.813000+00:00,"Try adding headers to your request based on what your browser sends; start with adding an Accept header (406 normally means the server didn't like what you want to accept).
See ""Adding headers"" in the documentation:
req = mechanize.Request(url)
req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
page = mechanize.urlopen(req)

The Accept header value there is based on the header sent by Chrome.",A,2
25943733,2014-09-19 23:32:18.477000+00:00,"
It seems like there are multiple virtual shells for the server script being started, and the server just randomly switches between them or starts new ones.

Exactly.
Apache has a variety of different things that it does to maximize concurrency, one of which is to spawn a bunch of child processes.
With mod_python, each child process gets its own independent Python interpreter, which means they each have their own copies of any global variables.

How can I prevent this?

Well, you could disable forking in Apache. But this is a bad idea. Unless you really know what you're doing, this will kill your scalability. And it still won't help if, say, you restart the server (which Apache will sometimes do to itself to clear up memory leaks, unless you configure it not to).
Web services are supposed to be stateless—or, rather, if they have any state, they're supposed to package it up and persist it between requests. For per-user state, you can store it in a cookie, or a hidden form field, and get their browser to pass it back. But for system-wide state, that doesn't work; you need to save it to something external. 
The usual solution is a database. But for something this simple, you could use anything—even a plain text file and a flock. Which is verbose, but easy to understand without having to learn SQL or some other database interface, and it makes you think about the concurrency issues (and understanding them is critical), so I'll show that:
import contextlib
import fcntl
from mod_python import apache

@contextlib.contextmanager
def flocking(f, flag=fcntl.LOCK_EX):
    fcntl.flock(f, flag)
    try:
        yield f
    finally:
        fcntl.flock(f, fcntl.LOCK_UN)

def bump_counter():
    while True:
        try:
            with flocking(open('storage.lock', 'r+')) as f:
                val = int(f.read())
                f.seek(0)
                f.write(str(val))
                return val
        except OSError:
            pass
        try:
            with flocking(open('storage.lock', 'x')) as f:
                val = 0
                f.write(str(val))
                return val
            except OSError:
                pass

def handler(req):
    req.content_type = 'text/plain'
    req.write('counter: '+str(bump_counter()))

    return apache.OK

Most of the code is about error handling, mainly to deal with the case that this is the first-ever request, so the file doesn't exist yet. In that case, we try to open it in x mode, which will only succeed if the file doesn't exist, just in case two requests come in right as you start up and they both think they're first. This way, one of them will fail, and will go back and try the whole loop again.
In real life, you'll want better error handling, because ""file not found"" isn't the only reason this could fail, and you don't want the server to just block forever when that happens. But in real life, you're probably going to use a database that takes care of this for you.",A,2
51218056,2018-07-06 21:53:13.537000+00:00,"First, there is almost never a good reason to call readlines(). A file is already an iterable of lines, so you can just loop over the file; reading all of those lines into memory and building a giant list of them is just wasting time and memory.
Calling read(), on the other hand, can sometimes be useful. It does have to read the entire thing into memory as one giant string, but doing a regex search over a giant string can speed things up enough, compared to searching line by line, that the wasted time and space is more than compensated for.
But if you want to reduce this to a single pass over the file, since you already have to iterate line by line, there really is no other option but to also do the regex searches line by line. This should work (you haven't shown your patterns, but based on the names, I'm guessing that they aren't expected to cross line boundaries, and aren't multiline or dotall patterns), but whether it's actually faster or slower will depend on all kinds of factors.
At any rate, it's certainly worth trying, to see if it helps. (And, while we're at it, I'm going to use a with statement to make sure you close the file, instead of leaking it as you do in your second part.)
CurBeginA = BeginSearchDVar
CurEndinA = EndinSearchDVar
BeginSearchDVar = BeginTimeFirstEpoch
EndinSearchDVar = EndinTimeFirstEpoch    
matchesBegin, matchesEnd = None, None
TheTimeStamps = []
with open(logfile) as f:
    for line in f:
        if not matchesBegin:
            matchesBegin = re.search(str(BeginTimeFirstEpoch), line)
        if not matchesEnd:
            matchesEnd = re.search(str(EndinTimeFirstEpoch), line)
        TheTimeStamps.append(line.split(' ')[0][1:-1])


There are a few other minor changes you can make here that might help.

I don't know what BeginTimeFirstEpoch is, but the fact that you're using str(BeginTimeFirstEpoch) implies that it's not a regex pattern at all, but something like a datetime object or an int? And that you don't really need the match objects, you just need to know whether there is a match? If so, you can drop the regex and do a plain substring search, which is a bit faster:
matchesBegin, matchesEnd = None, None
beginStr, endStr = str(BeginTimeFirstEpoch), str(EndinTimeFirstEpoch)
with …
    # …
    if not matchesBegin:
        matchesBegin = beginStr in line
    if not matchesEnd:
        matchesEnd = endStr in line


If your search strings and timestamps and so on are all pure ASCII, it might be faster to work on the file in binary mode, only decoding the bits you need to store, instead of everything:
matchesBegin, matchesEnd = None, None
beginStr, endStr = str(BeginTimeFirstEpoch).encode(), str(EndinTimeFirstEpoch).encode()
with open(logFile, 'rb') as f:
    # …
    if not matchesBegin:
        matchesBegin = beginStr in line
    if not matchesEnd:
        matchesEnd = endStr in line
    TheTimeStamps.append(line.split(b' ')[0][1:-1].decode())


Finally, I doubt str.split is anywhere near a bottleneck in your code, but, just in case… why bother splitting on all spaces when we only want the first split?
TheTimeStamps.append(line.split(b' ', 1)[0][1:-1].decode())
",A,1
22248438,2014-03-07 11:08:24.840000+00:00,"No, you must use either the * bare parameter, or use a single *args parameter, called a var-positional parameter (see the next item in that glossary entry). By adding it to your function signature you force any parameters that follow it to be keyword-only parameters.
So the function signature could be:
def func(positional_arg1, *variable_args, kw_only1, kw_only2):

and variable_args will capture any extra positional arguments passed to the function, or you could use:
def func(positional_arg1, *, kw_only1, kw_only2):

and the function will not support extra positional arguments beyond the first one.
In both cases, you can set kw_only1 and kw_only2 only by using them as keyword arguments when calling func(). Without default values (no =<expression> in their definition) they are still required arguments.",A,5
44175422,2017-05-25 08:05:19.970000+00:00,"No, the whole point of processes is that there's no way an action on an object in memory in one process can affect any of the others. ",A,0
7348956,2011-09-08 13:38:14.613000+00:00,"[ str(round(355/113.0, i)) for i in range(1,12) if prime(i) ]
",A,1
22473696,2014-03-18 08:15:11.613000+00:00,"You're assuming the current working directory is the Django project, but that's unlikely to be the case unless you  specifically changed it in the wsgi file.
However, you should not be logging to a separate file. The default logging setup will log either to Apache's own log in /var/log or in a site-specific one in a subdirectory of that. Remove your extra configuration and let Django log there.",A,0
1057656,2009-06-29 10:41:40.373000+00:00,"Life is simpler if you use delegation instead of inheritance.  This is Python.  You aren't obligated to inherit from Base.
class LooksLikeDerived( object ):
    def __init__( self ):
        self.base= Base()

    @property
    def foo(self):
        return 1 + self.base.foo # always works

    @foo.setter
    def foo(self, f):
        self.base.foo = f

But what about other methods of Base?  You duplicate the names in LooksLikeDerived and simply.
def someMethodOfBase( self, *args, **kw ):
    return self.base.someMethodOfBase( *args **kw )

Yes, it doesn't feel ""DRY"".  However, it prevents a lot of problems when ""wrapping"" some class in new functionality like you're trying to do.",A,9
27418771,2014-12-11 08:53:35.393000+00:00,"No, Python does not remember function calls or other calculations automatically. In general, it would be very bad if it did—imagine if every call to, say, random.randrange(6) returned the same value as the first call.

However, it's not hard to explicitly make it remember calls for specific functions where it's useful. This is usually called ""memoization"".
See the lru_cache decorator in the docs, for a nice example built into the stdlib.* All you have to do to make it remember every call to get_single(foo, bar) is change the definition of get_single like this;
@functools.lru_cache(maxsize=None)
def get_single(foo, bar):
    # etc.

Or, if get_single is someone else's code that you're importing and can't touch, you can just wrap it:
get_single = functools.lru_cache(maxsize=None)(othermod.get_single)

… and then call your wrapper instead of the module's version.
* Note that lru_cache was added in Python 3.2. If you're using 2.7 (or, for some reason, 3.0-3.1), you can install the backport from PyPI, or find any of dozens of other memoizing caches on PyPI or ActiveState—or even, noticing that the functools docs link to the source, like many other stdlib modules meant to also serve as example code, copy the source to your own project. Although, IIRC, the 3.2 code needs a small change to work with 2.7 because it relies on nonlocal to hide its internals.

That being said, even if you know get_single is memoized, it's still not very good style to call it twice. If you only need to do this once, just write the three lines of code. If you need to do it repeatedly, write a wrapper function that wraps up those three lines or code, and then calling that function will be shorter than even the two-line version.",A,5
24845466,2014-07-19 22:11:00.607000+00:00,"You are trying to connect to http://wwww.lolnexus.com/TR/search?name=Fred&region=TR, with four w characters in the domain name. That name does not exist.
Correct the hostname:
core = 'http://www.lolnexus.com'
",A,3
44701308,2017-06-22 13:47:59.323000+00:00,"The ** operator has specific binding behaviour; from the power operator documentation:

The power operator binds more tightly than unary operators on its left; it binds less tightly than unary operators on its right.
[...]
Thus, in an unparenthesized sequence of power and unary operators, the operators are evaluated from right to left (this does not constrain the evaluation order for the operands): -1**2 results in -1.

So your second example is executed as:
-(1.0 ** (1.0/3.0))

That is to say, the - unary operator applies to the result of ** as that operator binds more tightly. As a result you have positive number is raised to the power 1/3rd, and only then made negative.
In your first example, the expression is parsed as
(2.0 - (5.0**(0.5))) ** (1.0/3.0)

There is no unary operator here, but the ** power operator does have a higher precedence than the binary - subtraction operator.
This then resolves as
(2.0 - 2.23606797749979) ** (1.0/3.0)

which is
(-0.2360679774997898) ** (1.0/3.0)

so is trying to raise a negative number to a fraction.
Python 2 ** (and the pow() function) don't support producing a complex number support when the inputs are at most float objects. Convert your negative float value to a complex() number first:
>>> (-1.0) ** 0.5
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: negative number cannot be raised to a fractional power
>>> (-1+0j) ** 0.5
(6.123233995736766e-17+1j)
>>> (2+0j - 5.0**(0.5)) ** (1.0/3.0)
(0.30901699437494756+0.535233134659635j)

This changed in Python 3, where a complex result is returned for a negative number raised to a fractional power.",A,1
47260907,2017-11-13 09:27:30.340000+00:00,Don't do json.dumps when passing the data to Response. ,A,1
44285365,2017-05-31 12:52:14.567000+00:00,"The staticmethod decorator returns a staticmethod object. This object implements the descriptor protocol, same as functions do.
What this does is not that it gets rid of the instance, instead the staticmethod.__get__ ignores binding altogether and just returns the unaltered function object. For regular functions, function.__get__ would instead bind by returning a method object (which then tracks the instance and the function to combine them when called).
You can reproduce this by manually invoking the descriptor protocol:
>>> class Demo:
...     def regular(self):
...         pass
...     @staticmethod
...     def static():
...         pass
...
>>> Demo.__dict__['regular']  # bypass __getattribute__
<function Demo.regular at 0x108515268>
>>> Demo.__dict__['static']   # bypass __getattribute__
<staticmethod object at 0x1084d4f60>
>>> Demo.__dict__['regular'].__get__(Demo())  # descriptor protocol, pass in an instance
<bound method Demo.regular of <__main__.Demo object at 0x1084e2668>>
>>> Demo.__dict__['static'].__get__(Demo())  # descriptor protocol, pass in an instance
<function Demo.static at 0x1085152f0>

By accessing the attributes of the Demo class through Demo.__dict__, we bypass the descriptor protocol normally enforced by the __getattribute__ method. As you can see, for a regular method a function object is returned, but for static a staticmethod object is found instead.
Calling .__get__(Demo()) on either to invoke the descriptor protocol then produces a method object, and the un-altered function object, respectively. This is exactly what direct access to the same names on an instance produces:
>>> Demo().regular
<bound method Demo.regular of <__main__.Demo object at 0x1084dde10>>
>>> Demo().static
<function Demo.static at 0x1085152f0>

Note that the same protocol is also the reason that classmethod objects are passed type(instance) instead of the instance as a first argument, and also why property objects call the underlying function on access.",A,4
5934451,2011-05-09 08:34:33.163000+00:00,"ac is a Contact instance, not a ContractPartyInvolved one. So ac.agreed won't work (unless you also have an agreed field on Contact, which you don't show).
Instead of following the ManyToMany relationship and getting contract.agreed_contacts.all(), you want to follow the reverse ForeignKey from Contact to CPI. From there you can access all the fields on CPI, and also continue following the relationship to Contact and get the contact name:
cpis = contract.contractpartyinvolved_set.select_related().all()


{% for cpi in cpis %}
    {{ cpi.agreed }} || {{ cpi.contact.name }}
{% endfor %}

Note that I've used select_related on the initial query, to save further db hits as you know you're going to be following the relationship. If you don't want to access Role as well, you can limit the select_related by specifying select_related('Contact').",A,2
14241093,2013-01-09 15:53:34.590000+00:00,"If you added __getinitargs__() then it is up to you to make sure your new class can handle the arguments passed to __init__(). Old data that doesn't have the __getinitargs__ data will still lead to __init__ to be called but with no arguments.
Make the arguments to __init__ optional via keyword arguments:
def __init__(self, otherarg=None):
    if otherarg is None:
        # created from an old-revision pickle. Handle separately.
        # The pickle will be loaded *normally* and data will still be set normally
        return
    self.otherarg = otherarg

When loading the old-style pickle, the data for these classes will still be restored. You can use __setstate__() to transform the internal state as needed.
Alternatively, temporarily remove the __getinitargs__ method from the class:
initargs = foo.__getinitargs__.__func__
del foo.__getinitargs__
obj = pickle.load(f)
foo.__getinitargs__ = initargs

and re-dump your pickles from the now-loaded objects with __getinitargs__ reinstated.
I've tested both methods and in both cases the old data is loaded correctly and you can then dump your objects again to a new pickle file with __getinitargs__ just fine.",A,2
50842038,2018-06-13 16:24:39.197000+00:00,"No guarantees are made by Python. The os.path.getsize() function returns the  st_size field of a os.stat() call. This is a direct call to the stat system call.
All the documentation for stat simply names st_size as the file size, in bytes.
On my Debian test system stat gives true filesizes:
$ stat -fc %s .   # fs block size
4096
$ head -c 2048 < /dev/urandom > 2kb
$ head -c 6168 < /dev/urandom > 6kb
$ head -c 12345 < /dev/urandom > 12andabitkb
$ ls --block-size=1 -s *kb     # block use in bytes
16384 12andabitkb   4096 2kb   8192 6kb
$ ls --block-size=4K -s *kb    # block count per file
4 12andabitkb  1 2kb  2 6kb
$ python3 -c 'import os, glob; print(*(""{:<11} {}"".format(f, os.path.getsize(f)) for f in glob.glob(""*kb"")), sep=""\n"")'
2kb         2048
12andabitkb 12345
6kb         6168
",A,2
17644417,2013-07-14 22:27:22.450000+00:00,"You are not splitting the text into words; instead you are looping over the characters one by one.
Split your text using the str.split() method leaving the arguments to the default (splitting on variable-width whitespace):
print(min(textInput.split(), key = len))
",A,1
15409922,2013-03-14 12:56:41.273000+00:00,"Does it really matter?
The effect is the same. Neither 10 nor 11 is included in the output of xrange(), and xrange(1, 11, 2) is equivalent to xrange(1, 10, 2).
The Python 2 range type (the result of xrange()) stores the range length, not the end value, so to create the repr output it calculates that end value for you. And because you used a step value, the calculation shows the result of the formula start + length * step. For the implementation, the length is the more important value, the end value can safely be discarded and recalculated as needed.
So, when you create xrange(1, 10, 2), it calculates the range length and stores that instead of the end value:
if (step > 0 && lo < hi)
return 1UL + (hi - 1UL - lo) / step;
else if (step < 0 && lo > hi)
return 1UL + (lo - 1UL - hi) / (0UL - step);
else
return 0UL;

The Python 3 Range object stores the end value in addition to the length, so you can query the object for it and display it in the repr output.",A,2
871482,2009-05-16 01:52:52.717000+00:00,In Komodo the Tab and Shift Tab both work as expected to indent and unindent large blocks of code.,A,0
16334019,2013-05-02 08:58:26.077000+00:00,"One way of doing this would be to put the chunk of HTML in a separate template file, and use {% include %} to include it within each loop. Building on that, you could define the whole loop as an inclusion template tag that takes the object list as a parameter.
Alternatively, you could concatenate the three lists in your view, and just loop through the result.",A,4
23197711,2014-04-21 12:44:15.480000+00:00,"
I want inherit parent share variables except shared parent locks

You must not do this. It makes access to ""share variables"" not thread-safe.

sharedLock protects shared variable. If the same shared variable can be modified in a recursive call then you need RLock(). Here shared means shared among all subclasses.
It looks like you want a standalone function (or a static method) instead of the classmethod:
def getIncremented(_lock=Lock(), _shared=[0]):
    with _lock:
      _shared[0] += 1
      return _shared[0]

Thus all classes use the same shared variable (and the corresponding lock).
If you want each class to have its own shared variable (here shared means shared among instances of this particular class) then don't use cls.shared that may traverse ancestors to get it.
To hint that subclasses shouldn't use a variable directly, you could use the syntax for a private variable:
class A:
   __shared = 0
   __lock = Lock()

If a subclass overrides a method that uses __shared then it won't use A.__shared by accident in the code directly.",A,3
30742820,2015-06-09 21:05:31.337000+00:00,"You are making it more complicated than it needs to be:
def front_back(a, b):
    mid_a, mid_b = (len(a) + 1) // 2, (len(b) + 1) // 2
    front_a, back_a = a[:mid_a], a[mid_a:]
    front_b, back_b = b[:mid_b], b[mid_b:]
    print front_a + front_b + back_a + back_b

By adding 1 before dividing by 2 (floor division), you round up.
Demo:
>>> def front_back(a, b):
...     mid_a, mid_b = (len(a) + 1) // 2, (len(b) + 1) // 2
...     front_a, back_a = a[:mid_a], a[mid_a:]
...     front_b, back_b = b[:mid_b], b[mid_b:]
...     print front_a + front_b + back_a + back_b
... 
>>> front_back('Kitten', 'Donut')
KitDontenut

You could inline the slicing even:
def front_back(a, b):
    mid_a, mid_b = (len(a) + 1) // 2, (len(b) + 1) // 2
    print a[:mid_a] + b[:mid_b] + a[mid_a:] + b[mid_b:]
",A,12
8102833,2011-11-12 05:51:13.863000+00:00,"In general you need an xml parser to process xml documents. Regular expressions are not powerful enough to handle all cases.
Using perl syntax for regexes:
m{
  [ ][ ]  # double spaces
  |
  \t    # tab
  |    
  $\s*$  # extra enter separated only by whitespace. Note: requires `m` flag
  |  
  # XXX: it works only on simple xml
  <[^<>]*$[^<>]*> # line break inside tag
  |
  # XXX: it works only on simple xml
  [ ]</[^<>]+> |
  </[^<>]+>[ ] # space after/before closing tag
}mxg;

demo",A,0
12070240,2012-08-22 09:35:10.003000+00:00,"You are importing the module datetime, which doesn't have a strptime function.
That module does have a datetime object with that method though:
import datetime
dtDate = datetime.datetime.strptime(sDate, ""%m/%d/%Y"")

Alternatively you can import the datetime object from the module:
from datetime import datetime
dtDate = datetime.strptime(sDate, ""%m/%d/%Y"")

Note that the strptime method was added in python 2.5; if you are using an older version use the following code instead:
import datetime, time
dtDate = datetime.datetime(*time.strptime(sDate, ""%m/%d/%Y"")[:6])
",A,21
19803772,2013-11-06 03:52:50.320000+00:00,"You should escape backslashes inside the string literal. Compare:
>>> print(""\U00000023"")  # single character
#
>>> print(r""\U00000023"") # raw-string literal with 
\U00000023
>>> print(""\\U00000023"") # 10 characters
\U00000023

>>> print(""a\nb"")  # three characters (literal newline)
a
b
>>> print(r""a\nb"") # four characters (note: `r""""` prefix)
a\nb
",A,1
8056522,2011-11-08 20:13:55.433000+00:00,"In this script it takes five times more time to setup SMTP connection (5 seconds) than to send a e-mail (1 second) so it could make sense to setup a single connection and send several e-mails instead of creating the connection each time:
#!/usr/bin/env python3
import smtplib    
from contextlib import contextmanager
from datetime   import datetime
from email.mime.text import MIMEText
from netrc      import netrc
from timeit     import default_timer as timer

@contextmanager
def logined(sender, password, smtp_host='smtp.gmail.com', smtp_port=587):
    start = timer(); smtp_serv = smtplib.SMTP(smtp_host, smtp_port, timeout=10)
    try: # make smtp server and login
        smtp_serv.ehlo_or_helo_if_needed()
        smtp_serv.starttls()
        smtp_serv.ehlo()
        print('smtp setup took (%.2f seconds passed)' % (timer()-start,))
        start = timer(); smtp_serv.login(sender, password)
        print('login took %.2f seconds' % (timer()-start,))
        start = timer(); yield smtp_serv
    finally:
        print('Operations with smtp_serv took %.2f seconds' % (timer()-start,))
        start = timer(); smtp_serv.quit()
        print('Quiting took %.2f seconds' % (timer()-start,))

smtp_host = 'smtp.gmail.com'
login, _, password = netrc().authenticators(smtp_host)
with logined(login, password, smtp_host) as smtp_serv:
    for i in range(10):
        msg = MIMEText('#%d timestamp %s' % (i, datetime.utcnow()))
        msg['Subject'] = 'test #%d' % i
        msg['From'] = login
        msg['To'] = login
        smtp_serv.send_message(msg) 

Output
smtp setup took (5.43 seconds passed)
login took 0.40 seconds
Operations with smtp_serv took 9.84 seconds
Quiting took 0.05 seconds

If your Python version doesn't have .send_message() then you could use:
smtp_serv.sendmail(from, to, msg.as_string())
",A,3
18882784,2013-09-18 21:36:48.473000+00:00,"Import x from __main__ as well:
timeit.timeit(""function(x)"", setup=""from __main__ import function, x"")

Just like function, x is a name in the __main__ module, and can be imported into the timeit setup.",A,20
8887165,2012-01-16 22:26:50.773000+00:00,"I can't at all see how this could be any better explained. As miku points out in the comment, the documentation clearly says ""the full name of a template to use"". Plus, the example given directly underneath shows the use of a string.
Python is not Java, nor any statically typed language. The documentation doesn't say it wants a string, because it doesn't: it wants something string-like that contains the name. 
And don't forget, you've got immediate access to the code. Don't understand the function? Well, look at https://code.djangoproject.com/browser/django/trunk/django/shortcuts/__init__.py and there it is.
Even better, in Python you have an option that you simply don't have in Java: look in the shell. Do from django.shortcuts import render, then do help(render) to see what it says, and if that doesn't help try calling it with a few different arguments right there to see what it does.
If you insist on programming Python like it is Java, you will find it difficult. If you program Python like it's Python, you will find it much simpler.",A,8
50326155,2018-05-14 08:38:56.010000+00:00,"Most likely, pip is referring to Python 2; you probably need to use pip3.",A,2
47777804,2017-12-12 16:56:20.353000+00:00,"Your curl command just posts a username and password as form data, not as a Authorization header.
Do the same with requests, and use a session object to handle the capture and sending of cookies:
with requests.Session() as session:
    formdata = {
        'username': raw_input(""username: ""),
        'password': raw_input(""Password: ""),
    }
    myResponse = session.post(url, data=formdata, verify=True)

    second_response = session.get('http://IP_ADDRESS:9080/nbapiemswsweb/rest/v1/Search/Element')
",A,0
1804600,2009-11-26 16:22:53.650000+00:00,"You have several choices.

Get the OpenVMS source, and continue to maintain Open VMS as if it were a Linux distribution.  Some folks don't mind keeping up with Linux distributions and OpenVMS distributions.  It can be done.
Try to recompile the VMS C into Linux.  This can be trivial if the C used only standard libraries.  This can be very, very difficult if the C used a lot of VMS libraries.
Once you have facts at your fingertips, you can reevaluate this course of action.  Since you didn't list a bunch of VMS library methods this program uses, it's impossible to tell how entangled it is with the OS.
This may be trivial or impossible.  It's difficult to tell without analysis of the source.
Write bridge libraries from VMS to Linux.  If your program only does a few VMS things, this isn't very difficult.  If your program does extensive VMS things, this is craziness. 
The bridge -- in the long run -- is a terrible idea.  Managers love it, however. 
An alternative is to replace the VMS library calls with proper, portable Linux calls rather than write bridges.  This is better in the long run, because it excises the non-portable features of the program.
Rewrite it from scratch in Python.  That is usually simpler than trying to port the C code.  It will be shorter, cleaner, simpler, and portable.
",A,3
8589313,2011-12-21 11:48:44.190000+00:00,"You're passing the value from request.POST, where it's always a string, but your stored procedure is expecting an integer.
Use form.cleaned_data['inparam1'] instead - the form will have converted it to the proper type, ie an integer.",A,0
46203045,2017-09-13 16:54:37.187000+00:00,"You have microseconds and a timezone offset. You can parse the first with %f, and the latter with %z, provided you use Python 3.2 or newer:
>>> datetime.strptime(my_datetime, ""%Y-%m-%dT%H:%M:%S.%f%z"")
datetime.datetime(2017, 5, 16, 19, 0, 0, 451000, tzinfo=datetime.timezone(datetime.timedelta(-1, 72000)))
",A,1
14167826,2013-01-05 02:09:15.717000+00:00,"If you're trying to delegate a whole slew of properties from any A object to its b member, it's probably easier to do that inside __getattr__, __setattr__, and __delattr__, e.g.:
class A(object):
    delegated = ['c', 'd', 'e', 'f']
    def __getattr__(self, attr):
        if attr in A.delegated:
            return getattr(self.b, attr)
        raise AttributeError()

I haven't shown the __setattr__ and __delattr__ definitions here, for brevity, and to avoid having to explain the difference between __getattr__ and __getattribute__. See the docs if you need more information.
This is readily extensible to classes that want to proxy different attributes to different members:
class A(object):
    b_delegated = ['c', 'd', 'e', 'f']
    x_delegated = ['y', 'z']
    def __getattr__(self, attr):
        if attr in A.b_delegated:
            return getattr(self.b, attr)
        elif attr in A.x_delegated:
            return getattr(self.x, attr)
        else:
            raise AttributeError()

If you need to delegate all attributes, dynamically, that's almost as easy. You just get a list of self.b's attributes (or self.b.__class__'s) at init time or at call time (which of the four possibilities depends on exactly what you want to do), and use that in place of the static list b_delegated. 
You can of course filter this by name (e.g., to remove _private methods), or by type, or any arbitrary predicate (e.g., to remove any callable attributes).
Or combine any of the above.
At any rate, this is the idiomatic way to do (especially dynamic) proxying in Python. It's not perfect, but trying to invent a different mechanism is probably not a good idea.
And in fact, it's not really meant to be perfect. This is something you shouldn't be doing too often, and shouldn't be trying to disguise when you do it. It's obvious that a ctypes.cdll or a pyobjc module is actually delegating to something else, because it's actually useful for the user to know that. If you really need to delegate most of the public interface of one class to another, and don't want the user to know about the delegation… maybe you don't need it. Maybe it's better to just expose the private object directly, or reorganize your object model so the user is interacting with the right things in the first place.",A,2
6109453,2011-05-24 11:06:42.017000+00:00,"Firstly, neither of those ORM calls you give will work: fur_set is an attribute of a cat instance, not of the Cat.objects Manager.
Secondly, Django has no specific support for database views at all, so your question about using this particular bit of functionality with a view is a bit strange. You could define CatView as a separate (unmanaged) model, although you'd need to be careful about updating and saving. Then you can get the same effect as the reverse relation by querying the Fur object directly:
Fur.objects.filter(cat=my_cat_view.id)
",A,1
34655705,2016-01-07 12:55:15.613000+00:00,"There is no digest method any more. You have overridden it with the digest attribute. There is only one namespace.
A short example:
class test():
    def __init__(self):
        self.attr = 1
    def attr(self):
        pass

t = test()
t.attr        # 1
t.attr()      # TypeError: 'int' object is not callable
test.attr     # <unbound method test.attr>
test.attr(t)  # works
",A,5
1586115,2009-10-18 21:43:28.700000+00:00,"You have two separate questions here - it would have been better to split them out.
To run a separate script, you're best off creating a ./manage.py command. See the documentation on how to do this.
For your second question, the code you give is not valid Python, since there is no 'null' value - you mean None. However even then the code will not work, as that isn't how you write that query using the Django ORM. You want something like this:
if not MyModel.objects.filter(content=mytext).count():

which asks the database how many items there are with content=mytext, and is True if there are none.",A,5
53796810,2018-12-15 20:05:19.197000+00:00,"No that is not the reason. This code doesn't make sense; you can't define fields dynamically like that. Fields depend on columns in the database, a model must have a fixed number of fields. And goly_domaci is itself a field, it doesn't have a value at this point; it only has a value when accessed from an actual instance, at which point it's far too late to be defining other fields.
But this isn't what you want to do anyway. There is no point defining all those separate foreign keys to the same target model. What you want to do is define a separate model for Goals, which points to this model (I assume Zapas means Game).
class Goal(models.Model):
    game = models.ForeignKey(Zapas)
    team = models.ForeignKey(Tym, on_delete=models.CASCADE)
    strelec = models.ForeignKey(Hrac, on_delete=models.CASCADE)
    nahraval = models.ForeignKey(Hrac, on_delete=models.SET_NULL, blank=True, null=True)

Then you can drop your goly_domaci and goly_hoste fields altogether, as you can calculate them when you need to display them:
goly_hoste = my_zpas.goal_set.filter(team=my_zpas.tym_hoste).count()
goly_domaci = my_zpas.goal_set.filter(team=my_zpas.tym_domaci).count()
",A,1
1017673,2009-06-19 12:22:40.973000+00:00,"Python is almost a functional language.  It's ""functional lite"".  
It has extra features, so it isn't pure enough for some.  
It also lacks some features, so it isn't complete enough for some.  
The missing features are relatively easy to write.  Check out posts like this on FP in Python.",A,11
25858723,2014-09-16 00:00:05.810000+00:00,"For Python 3.x, the right thing to do would probably be to try to call encode. (For 3.5+, this will hopefully be as simple as try s.encode('utf-8') except AttributeError: s.)
But for 2.x, the encode will succeed even on str1—by first decoding the UTF-8 string as ASCII (or whatever sys.getdefaultencoding() returns) so it can re-encode it, so you definitely don't want that.
When there is no way to safely EAFP, you have no choice but to LBYL. So:
if isinstance(s, unicode):
    return s.encode('utf-8')
else:
    return s


Note that I'm using isinstance here, not calling type and comparing. As PEP 8 says:

Object type comparisons should always use isinstance() instead of comparing types directly.

Why? Because instances of subtypes (subclasses, classes registered with ABCs, etc.) are, by definition, supposed to always count as instances of their supertypes. There are some rare cases where you explicitly need to break that rule, in which case type comparisons are what you want. But otherwise, don't use them.",A,1
52011883,2018-08-24 21:21:10.270000+00:00,"As other answers have explained, this probably isn't a great design. 
First, a ""number"" could be an int, or some user-defined subclass of int, or a float, or some user-defined Quaternion type. Normally, you just use duck-typing: it x ** 2 works, then x is quacking like a number, and that's good enough.
But a list of ints doesn't quack like an int. So, what can you do?
Well, usually, you'll want to explicitly loop over them:
>>> xs = [1, 2, 3]
>>> sqs = [square(x) for x in xs]

… or write a function that does that:
>>> def squares(xs): return [square(x) for x in xs]
>>> sqs = squares(xs)

… or use a type that knows how to vectorize mathematical operators:
>>> xs = np.array([1, 2, 3])
>>> sqs = square(xs)


In fact, even when you do want to handle two different types, you can often rely on duck typing:
def square(x):
    try:
        return x**2
    except ValueError:
        return [i**2 for i in x]

This will square anything that's number-like enough to be squarable, and iterate over squaring all of the elements of anything that isn't, and raise a reasonable exception for anything that fails (either because it's not iterable, or because its elements aren't squarable).

Occasionally, you really do need to type-switch. But you still want to keep as close to duck-typing as possible, and that means using isinstance (so that, e.g., a user subtype of int still counts as a number) and, usually, using abstract base classes (so that, e.g., a user Quaternion type still counts as a number).
In this case, that means either treating numbers specially and assuming anything else is an iterable:
def square(x):
    if isinstance(x, numbers.Number):
        return x**2
    else:
        return [i**2 for i in x]

… or treating iterables specially and assuming everything else is a number:
def square(x):
    if isinstance(x, collections.abc.Iterable):
        return [i**2 for i in x]
    else:
        return x**2

Or maybe treating both specially and calling everything else an error:
def square(x):
    if isinstance(x, numbers.Number):
        return x**2
    elif isinstance(x, collections.abc.Iterable):
        return [i**2 for i in x]
    raise ValueError(f""'{type(x).__name__}' instance is not a number or numbers"")
",A,1
4165457,2010-11-12 14:09:15.747000+00:00,Have you actually installed the middleware in settings.py?,A,0
22167908,2014-03-04 09:32:24.803000+00:00,IDLE is not a proper shell. Do this from a Python session in a normal terminal.,A,1
18969407,2013-09-23 21:42:48.807000+00:00,"The most pythonic way to do this is to just try the lookup, and handle failure if it happens:
try:
    print('Hi '+employees[entered] +' you are an employee of companyx.com')
except KeyError:
    print('You dont belong here')


There's no reason for the for loop; the whole point of dictionaries is that you can look things up in one step, d[key], instead of having to loop over the keys and check whether each one == key.
You could use in to check whether the key exists, and then look it up. But that's a little silly—you're looking up the key to see whether you can look up the key. Why not just look up the key?
You can do this by using the get method, which returns None (or you can pass a different default value) if the key is missing:
name = employees.get(entered)
if name:
    print('Hi '+name +' you are an employee of companyx.com')
else:
    print('You dont belong here')

But it's Easier to Ask Forgiveness than Permission. Besides being slightly more concise, using try and except makes it clear that finding the name is the normal case that should be true, and not finding it is the exceptional case.",A,2
435712,2009-01-12 15:13:41.037000+00:00,"Since nobody's mentioned it yet here's a zip() solution:
>>> def chunker(iterable, chunksize):
...     return zip(*[iter(iterable)]*chunksize)

It works only if your sequence's length is always divisible by the chunk size or you don't care about a trailing chunk if it isn't.
Example:
>>> s = '1234567890'
>>> chunker(s, 3)
[('1', '2', '3'), ('4', '5', '6'), ('7', '8', '9')]
>>> chunker(s, 4)
[('1', '2', '3', '4'), ('5', '6', '7', '8')]
>>> chunker(s, 5)
[('1', '2', '3', '4', '5'), ('6', '7', '8', '9', '0')]

Or using itertools.izip to return an iterator instead of a list:
>>> from itertools import izip
>>> def chunker(iterable, chunksize):
...     return izip(*[iter(iterable)]*chunksize)

Padding can be fixed using @ΤΖΩΤΖΙΟΥ's answer:
>>> from itertools import chain, izip, repeat
>>> def chunker(iterable, chunksize, fillvalue=None):
...     it   = chain(iterable, repeat(fillvalue, chunksize-1))
...     args = [it] * chunksize
...     return izip(*args)
",A,4
13652765,2012-11-30 20:35:01.753000+00:00,"Assuming you're referring to the consumer key and consumer secret, you're not supposed to be able to create those programmatically. That's why you have to sign in to a web page with a CAPTCHA in order to create one.",A,3
11518277,2012-07-17 07:58:55.253000+00:00,"Supervisor only supports expansions with environment variables in a limited number of locations, each of which is documented in the configuration documentation.
Unfortunately, the [supervisord] directory option is not one of those; it only supports the %(here) variable, nothing else.
You could file a feature request for this in the supervisord issue tracker if this an important issue for you.
In my projects, we generally use zc.buildout to make deployment and development environment setup predictable and repeatable, and generate the supervisor configuration from a template. There is a specialized buildout recipe to make this task easier.",A,9
12014828,2012-08-18 00:39:29.190000+00:00,"You get it because you are decoding from a bytestring without specifying a codec:
unicode(input_str)

Add a codec there (here I assume your data is encoded in utf-8, 0xe1 would be the first of a 3-byte character):
unicode(input_str, 'utf8')
",A,2
40443850,2016-11-05 22:21:48.777000+00:00,"The statements in the class body are all executed when the class statement is executed, as if the they are all inside a function. This produces a set of names, that then are used to set the class attributes. See the class statement documentation:

The class’s suite is then executed in a new execution frame (see Naming and binding), using a newly created local namespace and the original global namespace. (Usually, the suite contains mostly function definitions.) When the class’s suite finishes execution, its execution frame is discarded but its local namespace is saved. [4] A class object is then created using the inheritance list for the base classes and the saved local namespace for the attribute dictionary.

Each of the def ... statements produce function objects, assigned to the function name, so get_height and set_height are both just functions, that then end up as attributes on the class. The same applies for the height = property(get_height, set_height) line; it accesses the names get_height and set_height, calls the property() callable with these as parameters and the result is assigned to the name height.
You may be confused how methods and the property object later on get access to the instance (the self argument in methods). Both functions and property objects are descriptor objects; any descriptor object that is accessed on an instance and is found on the class is automatically bound to the instance through this protocol. Also see the Descriptor HOWTO.
You can execute all those steps manually in the interpreter:
>>> def get_height(self):
...     print(""Inside the getter method"")
...     return self._height
...
>>> def set_height(self, value):
...     if value < 0:
...         raise ValueError(""Height cannot be negative"")
...     print(""Inside the setter method"")
...     self._height = value
...
>>> height = property(get_height, set_height)
>>> height
<property object at 0x105758188>
>>> height.fget
<function get_height at 0x10560ce18>
>>> height.fset
<function set_height at 0x10567e730>
>>> class FakeHeight:
...     _height = 42
...
>>> instance = FakeHeight()
>>> height.__get__(instance)  # descriptor access
Inside the getter method
42
>>> height.__set__(instance, 82)  # descriptor setting
Inside the setter method
>>> instance._height
82
",A,2
23219844,2014-04-22 12:38:27.947000+00:00,"The problem isn't anything to do with unmanaged models - it's just a question of syntax.
The ForeignKey declaration is correct, but accessing the related information is done via the dot syntax: obj.model1.name.",A,1
49497232,2018-03-26 17:27:07.637000+00:00,"As you already know, you can't pass something like the string '[""depends""][0][""image""]' to dict.get and expect that to work. But there are a couple of options if you really need to specify a ""path"" to the object within a nested data structure like this.

The first is to do it explicitly, just passing a sequence of keys instead of a single key:
def get_entry_from_loaded_yml(self, repo, commit, entry_keys, failflag=True):
    # ...
    try:
        entry = loaded
        for key in entry_keys:
            entry = entry[key]
    except LookupError:
        logger.error(""Could not find value for {} in {}"".format(entry, yml_file))
        if failflag:
            sys.exit(1)
        return None
    else:
        return entry

And now, you can do this:
gho.get_entry_from_ivpbuild_yml(repo, commit, ('depends', 0, 'image'))


Alternatively, you can use a library that handles ""key paths"", in a format like dpath (which is essentially a simplified version of XPath) or ObjectiveC's KVC. There are multiple libraries on PyPI for doing this (although some work on undecoded JSON strings rather than decoded nested objects, to allow searching huge JSON texts efficiently; those obviously won't work for you… and I don't know of any that work on YAML instead of JSON, but they might exist). Then your code would look something like this:
def get_entry_from_loaded_yml(self, repo, commit, entry, failflag=True):
    # ...
    result = dpath_search(loaded, entry, default=None):
    if result is None:
        logger.error(""Could not find value for {} in {}"".format(entry, yml_file))
        if failflag:
            sys.exit(1)
        return None
    else:
        return result

# ...

gho.get_entry_from_ivpbuild_yml(repo, commit, 'depends/0/image')

This has the advantage that if you ever need to look up a (possibly nested) sequence of multiple values, it can be as simple as this:
for result in dpath_search(loaded, 'depends/*/image'):
",A,0
525099,2009-02-08 03:25:30.440000+00:00,"""I don't find the error at compile but at run time""
Correct.  True for all non-compiled interpreted languages.
""I need to change and run the script again""
Also correct.  True for all non-compiled interpreted languages.
""Is there a way to have it break and let me modify and run?""
What?
If it's a run-time error, the script breaks, you fix it and run again.
If it's not a proper error, but a logic problem of some kind, then the program finishes, but doesn't work correctly.  No language can anticipate what you hoped for and break for you.
Or perhaps you mean something else.
""...code that needs a lot of enums""
You'll need to provide examples of code that needs a lot of enums.  I've been writing Python for years, and have no use for enums.  Indeed, I've been writing C++ with no use for enums either.
You'll have to provide code that needs a lot of enums as a specific example.  Perhaps in another question along the lines of ""What's a Pythonic replacement for all these enums.""
It's usually polymorphic class definitions, but without an example, it's hard to be sure.",A,9
25390535,2014-08-19 18:30:51.013000+00:00,"No, the parent process handle does not go to /dev/null. You did not change the process handle, you only rebound the Python sys.stdout to point to an new file object.
And you can still find the original sys.stdout object under sys.__stdout__, by the way, and that object is really just a wrapper to translate Python string objects to characters to write to the actual file handle in C.
The actual process handle is a file descriptor number, and the OS directs that file descriptor for the process. To change that handle from Python, you'll have to do some file descriptor juggling with os.close() and os.dup2() calls:
import os

def freopen(filename, mode, fobj):
    new = open(filename, mode)
    newfd = new.fileno()
    targetfd = fobj.fileno()
    os.close(targetfd)
    os.dup2(newfd, targetfd)

then use:
freopen('/dev/null', 'w', sys.stdout)

It'll be much easier to just pass the file object directly to the subprocess however:
with open(""/dev/null"", ""w"") as devnull:
    subprocess.Popen([""echo"", ""foobar""], stdout=devnull)
",A,3
12019630,2012-08-18 15:06:42.997000+00:00,"You need to start a new transaction before the .drop_all() call; MySQL sees you reading from the table in this transaction, and locks the table against being dropped:
session.commit()
Base.metadata.drop_all()

Committing a transaction implicitly begins a new transaction.
MySQL makes guarantees about transaction isolation; your transaction will read consistent data and won't see changes committed by other transactions until you start a new transaction. A DROP TABLE statement however makes it impossible for MySQL to keep these guarantees so the table is being locked.
Alternatively, you could alter the transaction isolation level, telling MySQL you don't care about the isolation guarantees. Because session connections are pooled, this can only be done for all connections or none at all; use the isolation_level argument to create_engine():
engine = create_engine(
    'mysql://username:passwd@localhost/databasename',
    isolation_level='READ UNCOMMITTED')

See the SET TRANSACTION documentation for details about each isolation level.",A,2
30549307,2015-05-30 17:37:23.043000+00:00,"Python functions are first order objects; just put them in the sequence:
arr = (one, two, three)

for fnc in arr:
    fnc()

You could store strings too, but then you need to turn those back into the function object first. That'd just be extra busywork you don't really need to do.
You can still turn strings into objects; the globals() function gives you the current global namespace as a dictionary, so globals()['one'] gives you the object referenced by the name one, but this would also give you access to every global in your module; if you then made a mistake it could lead to hard to track bugs or even security holes (as end-users could potentially abuse functions you didn't intent to be called).
If you really need to map names to functions, because, say, you need to take input from something else that only produces strings, use a predefined dictionary:
functions = {
    'one': one,
    'two': two,
    'three': three,
}

and map your string to the function:
function_to_call = 'one'
functions[function_to_call]()

Your function names do not need to match the string values here. By using a dedicated dictionary you limit what can be called.",A,7
30459850,2015-05-26 13:22:20.643000+00:00,"You are trying to access the wrong server:
url = os.getenv(""URL_DATASTORE"", ""http://localhost:8081/jsonrpc/""),
#                                        ^^^^^^^^^

You are running it on a different VM, not localhost.",A,1
19459790,2013-10-18 22:07:23.257000+00:00,"You are (ab)using conditional expressions, and they must be of the form true_expression if test else false_expression. These expressions always produce a value, unlike an if compound statement.
Note that you should not test for == True; boolean expressions are already true or false without that test. Don't use <> either, that operator has been deprecated and has removed from Python 3 altogether. When testing for None, a singleton, you'd use is not None however.
You are testing against type() results; that looks like you want to use isinstance() tests instead.
You are also using int() on values, then calling .lower() on the result. There is no int.lower() method, so those calls will fail with an AttributeError.
The following is closer to working just fine, unless there are more types than int, float, str or unicode:
[int(a[0].internal_value) if isinstance(a[0].internal_value, (float, int)) or a[0].internal_value.isdigit() 
 else str(a[0].internal_value).lower()
 for a in ws.iter_rows() if a[0].internal_value is not None]

However, I'd farm out the conversion to filter function instead:
def conversion(value):
    if isinstance(value, (float, int)):
        return int(value)
    return str(value).lower()

then use that in a list comprehension:
[conversion(a[0].internal_value) for a in ws.iter_rows() if a[0].internal_value is not None]
",A,9
19629434,2013-10-28 07:44:36.747000+00:00,"You'll have to loop over fileInfo manually, sets are the wrong tool here. You can find 'missing' items with dictionary views, which do use set operations:
# all items in fileInfo that are not in old_fileInfo
to_backup = fileInfo.viewkeys() - old_fileInfo

# find changed, all items in fileInfo that *are* in old_fileInfo
for f in fileInfo.viewkeys() & old_fileInfo:
    ctime, mtime = fileInfo[f]
    old_ctime, old_mtime = old_fileInfo[f]
    if ctime != old_ctime or mtime != old_mtime:
        to_backup.add(f)

This produces a set of filenames to back up.
In Python 3, use fileInfo.keys() instead of fileInfo.dictkeys(). In Python 2.6 and older, use set(fileInfo) instead of fileInfo.dictkeys().
Demo:
>>> fileInfo = {'/root/py/backup_tar/backup.log': [1382945505.0, 1382945505.0], '/root/py/backup_tar/fileinfo.pk': [1382943020.0, 1382943020.0], '/root/py/backup_tar/test.tar.gz': [1382945505.0, 1382945505.0], '/root/py/backup_tar/file_backup.py': [1382944678.0, 1382944678.0], '/root/py/backup_tar/tar.py': [1382693438.0, 1382693438.0], '/root/py/backup_tar/test.pk': [1382945505.0, 1382945505.0]}
>>> old_fileInfo = {'/root/py/backup_tar/tar.py': [1382693438.0, 1382693438.0], '/root/py/backup_tar/file_backup.py': [1382944678.0, 1382944678.0], '/root/py/backup_tar/test.pk': [1382945487.0, 1382945487.0], '/root/py/backup_tar/fileinfo.pk': [1382943020.0, 1382943020.0], '/root/py/backup_tar/backup.log': [1382945377.0, 1382945377.0]}
>>> to_backup = fileInfo.viewkeys() - old_fileInfo
>>> for f in fileInfo.viewkeys() & old_fileInfo:
...     ctime, mtime = fileInfo[f]
...     old_ctime, old_mtime = old_fileInfo[f]
...     if ctime != old_ctime or mtime != old_mtime:
...         to_backup.add(f)
... 
>>> to_backup
set(['/root/py/backup_tar/backup.log', '/root/py/backup_tar/test.tar.gz', '/root/py/backup_tar/test.pk'])
",A,1
346330,2008-12-06 14:41:43.233000+00:00,"""imports ... require your module to be on your PYTHONPATH""
Right.  
So, what's wrong with setting PYTHONPATH?",A,2
50111321,2018-05-01 04:24:43.867000+00:00,"If you want to iterate over the lines one by one, you do this:
for line in file:
    do_something_with(line)

Your second version isn't working because all you do is reassign the local variable user_input to each line over and over again, so at the end, you just have the last one.
You need to actually do something with the line, right there in the loop. Whether that means process it and write some output, or add it to a list or a dict or some other collection for later, depends on what you're trying to do.
For example, if you were building a set of all the words in the file, you could do this:
words = set()
for line in file:
    words |= set(line.split())


If you want to read the whole file all at once to get a list of all of the lines, on the other hand, you do this:
lines = file.readline()

… or this:
lines = list(file)

If you need to loop over the lines multiple times, or access them in arbitrary order, this can be very useful.
If not, whatever you wanted to do with lines, you could probably just have done directly with file instead, so you're wasting time and memory building that list.

Your code is mixing the two together:
for line in file:
    user_input = file.readline()

In other words, for each line in the file, you're trying to read the whole file. This doesn't make any sense. Either do something with each line, or read the whole file just one time.",A,1
14571031,2013-01-28 20:53:23.437000+00:00,"No, you have to test all path prefixes; routing allows for many, arbitrary URLs to be possible. Moreover, with path predicates in the mix, multiple routes could match the same URL and choosing between them depends on other information from the request.
To prepare your breadcrumbs, instead loop over the sub-paths once and determine for each if there is a matching view; the easiest way to do this is to reuse the code underlying the pviews command; this code needs the current request:
from pyramid.scripts.pviews import PViewsCommand

pvcomm = PViewsCommand()

urlpath = request.environ['PATH_INFO']
parts = urlpath.split('/')
existing_views = {}

for i in range(1, len(parts)):
    path = '/'.join(parts[:i])
    view = pvcomm._find_view(path, request.registry)
    if view is not None:
        existing_views[path] = view

You can now look up path prefixes in the existing_views dictionary.",A,1
760118,2009-04-17 11:54:36.170000+00:00,"First, can you prove the ORM is stopping your performance?  Sometimes performance problems are simply poor database design, or improper indexes.  Usually this comes from trying to force-fit Django's ORM onto a legacy database design.  Stored procedures and triggers can have adverse impact on performance -- especially when working with Django where the trigger code is expected to be in the Python model code.
Sometimes poor performance is an application issue.  This includes needless order-by operations being done in the database.
The most common performance problem is an application that ""over-fetches"" data.  Casually using the .all() method and creating large in-memory collections.  This will crush performance.  The Django query sets have to be touched as little as possible so that the query set iterator is given to the template for display.
Once you choose to bypass the ORM, you have to fight out the Object-Relational Impedance Mismatch problem.  Again.  Specifically, relational ""navigation"" has no concept of ""related"": it has to be a first-class fetch of a relational set using foreign keys.  To assemble a complex in-memory object model via SQL is simply hard.  Circular references make this very hard; resolving FK's into collections is hard.
If you're going to use raw SQL, you have two choices.

Eschew ""select related"" -- it doesn't exist -- and it's painful to implement.
Invent your own ORM-like ""select related"" features.  A common approach is to add stateful getters that (a) check a private cache to see if they've fetched the related object and if the object doesn't exist, (b) fetch the related object from the database and update the cache.

In the process of inventing your own stateful getters, you'll be reinventing Django's, and you'll probably discover that it isn't the ORM layer, but a database design or an application design issue.",A,1
50163755,2018-05-03 20:59:08.830000+00:00,"
Is there a way to do this using dict.setdefault() ?

Of course! There's nothing magic about setdefault; it just returns my_dict[name] if it exists, or sets my_dict[name] = default and returns default if not.
Either way, what it returns is the (potentially new) dict in my_dict[name], so all you have to do is update it:
my_dict.setdefault(name, {})[id] = pin

Or, if you really like using update instead of just [] =:
my_dict.setdefault(name, {}).update({id: pin})

For example:
>>> my_dict.setdefault('name1', {})['id10']= 'pin10'
>>> my_dict.setdefault('name3', {})['id20']= 'pin20'
{'name1': {'id1': 'pin1', 'id10': 'pin10', 'id2': 'pin2'},
 'name2': {'id3': 'pin3', 'id4': 'pin4'},
 'name3': {'id20': 'pin20'}}


More generally, you could obviously rewrite your existing code to this:
if name not in my_dict:
    my_dict[name] = {}
my_dict[name].update({ id : pin }

Whenever you can do that, you can use setdefault. (But notice that we're calling a mutating method or assigning to a key/index/member of my_dict[name] here, not just assigning a new value to my_dict[name]. That's the key to it being useful.)

Of course almost any time you can use setdefault, you can also use defaultdict (as demonstrated in CoryKramer's answer) (and vice-versa). If you want defaulting behavior all the time, use defaultdict. Only use setdefault if you want defaulting behavior just this once (e.g., you want defaulting while building the dict up, but you want KeyErrors later when using it).",A,5
35611009,2016-02-24 19:13:53.867000+00:00,"Your foreign key is the wrong way round. If you want multiple mediafiles in one category, the fk needs to live on the MediaFile model. That way the inline will work.",A,2
45630784,2017-08-11 08:47:03.763000+00:00,"Yes. This will hit the database because you are running a specific query - exists on a filter - that was not present in the original prefetch.
In your case, since you already have the workers, it would be better to filter them in Python:
[workplace for workplace in workplaces 
 if any(w.salary == 0 for w in workplace.worker_set.all())]

Alternatively, if this is the only reason you need the prefetch, you could use a Prefetch object in the original query to only fetch the workers with 0 salary.
(Just noticed you are using 1.4, which doesn't have Prefetch objects. You should definitely upgrade; not only for that reason, but mainly because 1.4 is very old and unsupported, and is therefore certainly a security risk.)",A,0
29566095,2015-04-10 16:14:04.107000+00:00,"You can loop over the values (so over dict.itervalues() or dict.values(), depending on your Python version), and use isinstance() to test each. Combine that with the any() function and a generator expression to make it efficient:
return any(isinstance(v, dict) for v in dictObj.itervalues())

Demo:
>>> dictObj = {'foo': 'bar'}
>>> any(isinstance(v, dict) for v in dictObj.itervalues())
False
>>> dictObj = {'foo': {'spam': 'eggs'}}
>>> any(isinstance(v, dict) for v in dictObj.itervalues())
True

If you want to detect any mapping type (not just dictionaries), you could test for collections.Mapping instead of dict; this would let you handle any alternative mapping implementations too.",A,6
32495601,2015-09-10 07:23:50.273000+00:00,"This is surely a very bad idea. Your remote database will surely be updated all the time, with live data created by the users of your website. Why would you want to override that with stale data from dev?
And this:

I'm assuming the remote database does not have my local migrations

is not only false, it's quite literally missing the whole point of migrations, which is that you run the same code in dev and prod to keep the tables in sync.
Finally, sqlite isn't really suitable for production use in anything but a toy site. Use a proper db like Postgres.",A,-1
13608455,2012-11-28 15:22:58.623000+00:00,"You'll have to decode the JSON structure and loop through all the dictionaries until you find a match:
for person in json.loads(user_stream):
    if person['name'] == user:
        id = person['id']
        break
else:
    # The else branch is only ever reached if no match was found
    raise ValueError('No such person')

If you need to make multiple lookups, you probably want to transform this structure to a dict to ease lookups:
name_to_id = {p['name']: p['id'] for p in json.loads(user_stream)}

then look up the id directly:
id = name_to_id.get(name)  # if name is not found, id will be None

The above example assumes that names are unique, if they are not, use:
from collections import defaultdict

name_to_id = defaultdict(list)
for person in json.loads(user_stream):
    name_to_id[person['name']).append(person['id'])

# lookup
ids = name_to_id.get(name, [])  # list of ids, defaults to empty

This is as always a trade-off, you trade memory for speed. ",A,2
8663739,2011-12-29 03:34:52.203000+00:00,"If there is barely enough memory you could try to encode the object to json incrementally:
import json
import sys

d = dict.fromkeys(range(10))
for chunk in json.JSONEncoder().iterencode(d):
    print(chunk) # print each chunk on a newline for demonstration

Don't accumulate the output in a string use file/socket and write/send chunks immediately.
Output
{
""0""
: 
null
, 
""1""
: 
null
, 
""2""
: 
null
, 
""3""
: 
null
, 
""4""
: 
null
, 
""5""
: 
null
, 
""6""
: 
null
, 
""7""
: 
null
, 
""8""
: 
null
, 
""9""
: 
null
}
",A,5
18671403,2013-09-07 08:28:46.657000+00:00,"Your data structure should really be moved to a dictionary, keyed on the cat value. Use collections.defaultdict() and collections.Counter() to keep track of the values and make summing easier:
from collections import defaultdict, Counter

totals = defaultdict(Counter)

for item in data:
    cat = item.pop('cat')
    totals[cat] += Counter(item)

Demo:
>>> from collections import defaultdict, Counter
>>> data = {'cat':'red', 'a':1, 'b':2}, {'cat':'red', 'a':3, 'b':3}, {'cat':'blue', 'a':1, 'b':3}
>>> totals = defaultdict(Counter)
>>> for item in data:
...     cat = item.pop('cat')
...     totals[cat] += Counter(item)
... 
>>> totals
defaultdict(<class 'collections.Counter'>, {'blue': Counter({'b': 3, 'a': 1}), 'red': Counter({'b': 5, 'a': 4})})
>>> totals['blue']
Counter({'b': 3, 'a': 1})
>>> totals['red']
Counter({'b': 5, 'a': 4})

If you still require a sequence of dictionaries in the same format, you can then turn the above dictionary of counters back into 'plain' dictionaries again:
output = []
for cat, counts in totals.iteritems():
    item = {'cat': cat}
    item.update(counts)
    output.append(item)

resulting in:
>>> output
[{'a': 1, 'b': 3, 'cat': 'blue'}, {'a': 4, 'b': 5, 'cat': 'red'}]
",A,2
26723646,2014-11-03 21:34:45.163000+00:00,"I'm willing to bet that you installed 1.9.1 or earlier of PyGame, since most of the pre-built binaries are 1.9.1 or earlier.
And if you look at the docs for get_raw, it clearly says:

New in pygame 1.9.2.

If you want to use 1.9.2-only features, you obviously need to install 1.9.2. Which means you can't use the official pre-built binaries.
There are some platforms that have their own binaries (maybe in RPM or DEB packages) or unofficial binaries (like Christoph Gohlke's archive for Windows).
Or you can always build and install PyGame yourself. The install docs appear to be broken at the moment, but the compilation docs cover all the tricky parts. And really, it's not that tricky; once you install all the prereqs, the usual [sudo] python setup.py install—or, better, [sudo] pip install .—should just work. (If you're lucky, pip install pygame may even work, but only if 1.9.2 has actually been released.)
Using a version called 1.9.2a0, which sounds like a pre-alpha for 1.9.2. doesn't guarantee that you're going to get all 1.9.2 features. If there is no later version released yet… well then, you can't use new features before they're released, unless you want to use an as-yet-unreleased development build. (Note that they have automated nightly development builds for common Windows and Mac targets, so if you happen to match one of those targets, you may be in luck. Then again, those build are often broken for months at a time, so you may have to build it yourself.)
So, those are your choices: wait for 1.9.2 to get an official binary release, use a dev build, or stop trying to use a feature that isn't in the latest release yet.",A,1
116621,2008-09-22 18:28:28.067000+00:00,"Are you trying to deduce  the structure of a document in plain test?  Are you doing what docutils does?
You might be able to simply use the Docutils parser rather than roll your own.",A,2
937602,2009-06-02 01:40:02.393000+00:00,"""edit: PS - I thought about using the post_init signal... for the code that accesses the db, is that a good idea?""
Never.
If you have code that's accessing the model before the tables are created, you have big, big problems.  You're probably doing something seriously wrong.
Normally, you run syncdb approximately once.  The database is created.  And your web application uses the database.
Sometimes, you made a design change, drop and recreate the database.  And then your web application uses that database for a long time.
You (generally) don't need code in an __init__.py module.  You should (almost) never have executable code that does real work in an __init__.py module.  It's very, very rare, and inappropriate for Django.
I'm not sure why you're messing with __init__.py when Django Cron says that you make your scheduling arrangements in urls.py.

Edit
Clearing records is one thing.
Messing around with __init__.py and Django-cron's base.py are clearly completely wrong ways to do this.  If it's that complicated, you're doing it wrong.
It's impossible to tell what you're trying to do, but it should be trivial.
Your urls.py can only run after syncdb and after all of the ORM material has been configured and bound correctly.
Your urls.py could, for example, delete some rows and then add some rows to a table.  At this point, all syncdb issues are out of the way.
Why don't you have your logic in urls.py?",A,4
23868669,2014-05-26 11:05:08.893000+00:00,"You want an else clause, and drop the return:
lambda obj: None if isinstance(obj, DataLoader) else obj

A lambda expression has to return something, but they don't need return, their one expression is always the return value.
A conditional expression likewise has to produce something; what should the expression produce when isinstance(obj, DataLoader) is false?",A,6
31266030,2015-07-07 10:27:46.097000+00:00,"Django model instances are normal Python objects. Like almost any other Python object, you can freely add attributes to them at any time.
object_instance = Object.objects.get(pk=pk)
object_instance.special_value = SomeBusinessLogic(data_to_aggregate)
",A,3
49352106,2018-03-18 19:38:03.900000+00:00,"There are a couple of errors here.
You can't have a div as a child of a table element. You should remove the duplicate table-responsive-sm div inside the table. 
Also, the table should only have a single tbody element. The tbody should be outside the for loop. 
So:
<div class=""container"">
  <div class=""table-responsive-sm"">
    <table class=""table"">
      <tbody>
        {% for laptop in laptops %}
          <tr>
            <td>{{laptop.laptop_id}}</td>
            <td>{{laptop.make}}</td>
            <td>{{laptop.model}}</td>
            <td>{{laptop.specification}}</td>
            <td>{{laptop.warranty}}</td>
            <td>{{laptop.condition}}</td>
            <td>{{laptop.price}}</td>
          </tr>
        {% endfor %}
      </tbody>
    </table>
  </div>
</div>
",A,3
51575528,2018-07-28 21:49:18.993000+00:00,"It's much easier to reason about month-arithmetic if you convert the year and month to a single number, with years * 12 + (month - 1); this can be converted back to a year and month pair by a floor division and modulus operation. For example, 2017-10 (October) is 24213 months since the year zero:
>>> 2017 * 12 + (10 - 1)
24213

You can trivially add to or remove a number of months from that figure. You can get the year out again by floor division, and the month is found with % modulus and adding back 1:
>>> 24213 // 12  # year
2017
>>> (24213 % 12) + 1  # month
10

With this in mind, you can then use a range() to generate any number of months:
from datetime import date

def months(start_date, end_date, day=1):
    """"""Produce a date for every month from start until end""""""
    start = start_date.year * 12 + (start_date.month - 1)
    if start_date.day > day:
        # already in this month, so start counting at the next
        start += 1
    end = end_date.year * 12 + (end_date.month - 1)
    if end_date.day > day:
        # end date is past the reference day, include the reference
        # date in the output
        end += 1
    # generate the months, just a range from start to end
    for ordinal in range(start, end):
        yield date(ordinal // 12, (ordinal % 12) + 1, day)

The above is a generator function that yields consecutive months; call list() on it if you need to have the complete sequence:
>>> start_date = date(2017, 10, 25)
>>> end_date = date(2018, 3, 28)
>>> list(months(start_date, end_date))
[datetime.date(2017, 11, 1), datetime.date(2017, 12, 1), datetime.date(2018, 1, 1), datetime.date(2018, 2, 1), datetime.date(2018, 3, 1)]

Note that at no point do you need to convert dates to strings! You can trivially get the month value from the instance by using the .month attribute.
To make a comparison, I've converted the other two solutions to be generators too:
from calendar import monthrange
from datetime import timedelta
from dateutil import rrule

def andray_timedelta_one(start_date, end_date):
    delta = end_date - start_date
    first_days_of_month = []
    for i in range(delta.days + 1):
        d = start_date + timedelta(i)
        if d.day == 1:
            yield d

def matthew_timedelta_monthrange(start_date, end_date):
    if start_date.day == 1:
        yield start_date

    start_date = start_date.replace(day=1)

    while start_date <= end_date:
        # add the number of days in the month for this month/year
        try:
            start_date += timedelta(monthrange(start_date.year, start_date.month)[1])
            yield start_date
        except OverflowError:
            # trying to add to close-to-date.max would raise this exception
            return

def sunitha_rrule(start_date, end_date):
    # already an iterable
    return rrule.rrule(rrule.MONTHLY, bymonthday=1, dtstart=start_date, until=end_date)

# for completion's sake, I renamed mine to martijn_months

That way their performance can be compared fairly, and we can use a deque(..., maxlen=0) trick to quickly consume their output without requiring huge amounts of memory. We can then run each of the functions over the range date.min through to date.max, the maximum possible range of dates; that's nearly 120 thousand date objects to produce:
>>> sum(1 for _ in months(datetime.date.min, datetime.date.max))
119988

These are the results:
>>> from timeit import Timer
>>> from collections import deque
>>> bootstrap = 'from __main__ import date, deque, {} as test'
>>> test = 'deque(test(date.min, date.max), maxlen=0)'
>>> for f in (
...         andray_timedelta_one,
...         sunitha_rrule,
...         matthew_timedelta_monthrange,
...         martijn_months):
...     loop_count, total_time = Timer(test, bootstrap.format(f.__name__)).autorange()
...     print(f'{f.__name__:<30}: {total_time/loop_count*1000:.5f}ms')
...
andray_timedelta_one          : 2001.27048ms
sunitha_rrule                 : 1517.70081ms
matthew_timedelta_monthrange  : 154.68727ms
martijn_months                : 38.86803ms

As you can see, my approach is orders of magnitude faster.

Andray's approach wastes a huge amount of time creating every date in the calendar, adding one day at a time. 
The rrule approach Sunitha picked is nice and concise, but the function has to account for much more complex date arithmetic, and so this very simple case is not optimised. This makes rrule() slow!
Matthew is a lot more efficient, but the calculations that calendar.monthrange() executes are still overkill for a simple plus-one operation on a year-month combination. We don't need to know if the current month has 31, 30, 29 or 28 days to make this calculation!
",A,0
36828697,2016-04-24 20:44:07.793000+00:00,"A class has a __weakref__ descriptor object; this acts just like a property or a method; only when you access the attribute on an object is it automatically bound. The actual data for a weak reference is stored in a C structure, part of the data structure Python uses to represent classes and instances in memory. 
As such, instances don't need their own __weakref__ attribute. The class descriptor is bound to the instance data structure, and the C code then just looks in the right C struct to retrieve the information needed.
Accessing the attribute on the class, produces the descriptor object itself. This is not None; it is the descriptor object. On an attribute, the bound attribute produces the weak references. No weak references, means None is returned.
You can re-create the descriptor behaviour by accessing the object via A.__dict__['__weakref__'] (to bypass the normal type.__getattribute__() binding behaviour), then directly calling __get__ on that:
>>> import weakref
>>> class A(object): pass
...
>>> a = A()
>>> A.__weakref__
<attribute '__weakref__' of 'A' objects>
>>> descriptor = A.__dict__['__weakref__']
>>> descriptor.__get__(None, A)
<attribute '__weakref__' of 'A' objects>
>>> a = A()
>>> a.__weakref__ is None
True
>>> descriptor.__get__(a) is None
True
>>> wr = weakref.ref(a)  # add a weak reference
>>> wr
<weakref at 0x10bd86d68; to 'A' at 0x10bad3588>
>>> a.__weakref__
<weakref at 0x10bd86d68; to 'A' at 0x10bad3588>
>>> descriptor.__get__(a)
<weakref at 0x10bd86d68; to 'A' at 0x10bad3588>
",A,3
44691968,2017-06-22 06:41:49.853000+00:00,"No, the :: has no special meaning other that that's the picked delimiter for Trover classifier levels.
The classifiers are just metadata, helping people find specific packages and learn what the package is about. The package maintainer picks these when writing the setup.py script; the setup.cfg file is an alternative way of specifying the same.",A,1
17770399,2013-07-21 08:20:09.017000+00:00,"You misunderstand the way scope and classes work in Python. actsToValidate is a class-level property, and will be evaluated when the class is defined. __init__, on the other hand, is evaluated when the class is instantiated.
And even if they were evaluated in the right order, it still wouldn't work, because you haven't put modelToUse into the class scope, where it can be seen by the field definition.
You need to define the field at the class level without the queryset, then add it in the __init__ method. Like this:
class ActsAddForm(forms.Form):
    actsToValidate=forms.ModelChoiceField(empty_label=""Select an act to validate"", widget=forms.Select(attrs={'onchange': 'this.form.submit();'}))

    def __init__(self,*args,**kwargs):
        modelToUse = kwargs.pop('modelToUse')
        super(ActsAddForm,self).__init__(*args,**kwargs)
        self.fields['actsToValidate'].queryset=modelToUse.objects.filter(validated=0)
",A,1
28187716,2015-01-28 08:32:01.037000+00:00,"You can return an empty dictionary object from dict.get() to ease chaining calls:
b.get('x', {}).get('y', {}).get('z')

but perhaps you'd be better off catching the KeyError exception:
try:
    value = b['x']['y']['z']
except KeyError:
    value = None
",A,8
12374235,2012-09-11 16:39:11.003000+00:00,"Neither global does what you think it does. In both cases chosen and chosen2 are class variables, not globals.
But when you declared chosen as a global and assigned to it, python happily created a global variable (separate from Listener1.chosen) and stored your value.
But for chosen2 you are not assigning anything to it; you are trying to treat it as a dict instead, but you didn't create the variable by assignment so that fails.
You want to use self.chosen2 instead; as a class variable, it'll be available to all instances of Listener2. You can also use Listener2.chosen2. For chosen, you can use Listener1.chosen to refer to it. The global keywords can be dropped altogether.
In any case, declaring a variable as global does not mean outside of my current scope. It means at module scope instead, so always outside of your functions, classes, and method definitions.",A,2
14086590,2012-12-29 22:46:50.580000+00:00,"You'll have to use a list comprehension to apply slicing to contained elements:
[v[-2:] for v in test[::10]]

This gives:
>>> [v[-2:] for v in test[::10]]
['11', '21', '31', '41', '51', '61', '71', '81', '91']

test[::10] returns a list itself, and [-2::] just takes the last two elements from that list, and does not apply that slice to the contained elements of the list.",A,6
11176337,2012-06-24 09:27:26.230000+00:00,"The events system used by Pyramid fulfils the exact same use-cases as the Signals system. Your application can define arbitrary events and attach subscribers to them.
To create a new event, define an interface for it:
from zope.interface import (
    Attribute,
    Interface,
    )

class IMyOwnEvent(Interface):
    foo = Attribute('The foo value')
    bar = Attribute('The bar value')

You then define an actual implementation of the event:
from zope.interface import implementer

@implementer(IMyOwnEvent)
class MyOwnEvent(object):
    def __init__(self, foo, bar):
        self.foo = foo
        self.bar = bar

The interface is actually optional, but helps documentation and makes it easier to provide multiple implementations. So you could get away with omitting the interface definition and @implementer parts altogether.
Wherever you want to signal this event, use the registry.notify method; here I assume you have a request available to reach the registry:
request.registry.notify(MyOwnEvent(foo, bar))

This'll send the request to any subscribers you've registered; either with config.add_subscriper or with pyramid.events.subscriber:
from pyramid.events import subscriber
from mymodule.events import MyOwnEvent

@subscriber(MyOwnEvent)
def owneventsubscriber(event):
    event.foo.spam = 'eggs'

You can also use the IMyOwnEvent interface instead of the MyOwnEvent class and your subscriber will be notified of all events that implement the interface, not just your specific implementation of that event.
Note that notifying subscribers never catches exceptions (like send_robust in Django would do).",A,9
18108316,2013-08-07 16:08:18.087000+00:00,"Sure, by using multiple assignment targets:
a, f = f, f + 1

or by just plain incrementing f on a separate line:
a = f
f += 1

because readable trumps overly clever.
There is no ++ operator because integers in Python are immutable; you rebind the name to a new integer value instead.",A,13
14120601,2013-01-02 10:41:39.280000+00:00,"The content of the file in question is included in the returned data. You are getting the full GitHub view of that file, not just the contents.
If you want to download just the file, you need to use the Raw link at the top of the page, which will be (for your example):
https://raw.github.com/someguy/brilliant/master/somefile.txt

Note the change in domain name, and the blob/ part of the path is gone.
To demonstrate this with the requests GitHub repository itself:
>>> import requests
>>> r = requests.get('https://github.com/kennethreitz/requests/blob/master/README.rst')
>>> 'Requests:' in r.text
True
>>> r.headers['Content-Type']
'text/html; charset=utf-8'
>>> r = requests.get('https://raw.github.com/kennethreitz/requests/master/README.rst')
>>> 'Requests:' in r.text
True
>>> r.headers['Content-Type']
'text/plain; charset=utf-8'
>>> print r.text
Requests: HTTP for Humans
=========================


.. image:: https://travis-ci.org/kennethreitz/requests.png?branch=master
[... etc. ...]
",A,18
21054384,2014-01-10 21:05:37.043000+00:00,"The problem is that each of those functions in tests is referring to the variable i.
More commonly, you do this inside a function, in which case you have a local-to-the-defining-scope variable i, which gets stored in a closure, as nicely explained in These Nasty Closures.
But here, it's even simpler: i is a global variable, so there is no closure. The functions are compiled to look up i as a global variable when run. Since i has changed, the functions will see the changed value when they run. Simple as that.

The traditional way around this (which works for both closures and globals) is fondly known as ""the default-value hack"", even though it's not really a hack. (See the explanation in the FAQ.) Ryan Haining's answer explains how to do this:
lambda x, i=i: x%i==0

This creates a parameter named i, with a default value equal to the value of i at the time the function is created. Then, inside the function, when you access parameter i, and you get that value.

A different way around this, which may seem more familiar if you're using to languages like JavaScript, is to create a function-creating function, and pass the value of i as an argument to that function-creating function, as in user2864740's answer:
(lambda i: lambda x: x%i)(i)

This avoids ""polluting"" the signature of the function with an extra parameter (that someone could accidentally pass an argument to), but at the cost of creating and calling a function for no good reason.

A third way around this is to use partial. In cases where all you're trying to do is partially apply a function, using partial instead of defining a wrapper function as a lambda can be cleaner.
Unfortunately, in this case, the function is hidden inside an operator, and the function operator.mod that exposes it doesn't take keyword arguments, so you can't usefully partial its second operand. So, this is a bad solution in this case. If you really wanted to, you could just write a wrapper that behaves better and partial that:
def opmod(a, b):
    return a % b

partial(operator.mod, b=i)

In this case, I think you're better off with the other solutions; just keep this one in your head for cases where it is appropriate.",A,6
18938286,2013-09-21 22:23:33.847000+00:00,"Just use a list comprehension:
nested_lst_of_tuples = [tuple(l) for l in nested_lst]

Demo:
>>> nested_lst = [['tom', 'cat'], ['jerry', 'mouse'], ['spark', 'dog']]
>>> [tuple(l) for l in nested_lst]
[('tom', 'cat'), ('jerry', 'mouse'), ('spark', 'dog')]
",A,29
6875767,2011-07-29 15:54:11.313000+00:00,"Because you haven't actually called the view function if your test succeeds.
if 'isPrivateAlfaUser' not in request.session or request.session['isPrivateAlfaUser'] != True:
    return render_to_response('homepage.html') 
else:
    return func(request, *args, **kwargs)

As a secondary note, you don't need the outer level of wrapper here because your decorator doesn't take any arguments. If you drop that, you also need to drop the () on the decorator itself.",A,4
25884718,2014-09-17 07:26:50.357000+00:00,"It is a Python literal; it uses Python syntax for everything. Python syntax for dictionaries and lists and strings is quite close to the equivalent JSON syntax. The Python strings are byte strings however, not Unicode values like they would be in JSON.
Any null JSON values will be represented as None in Python, and any booleans will be rendered as True and False rather than the JSON true and false.
You can parse it with the ast.literal_eval() function, but this is going to be slower than parsing JSON. Don't ever be tempted to use eval() on such data; all it takes is for the API DNS entry to be hijacked for an attacker to feed you arbitrary Python objects and compromise your process.
Frankly, offering a Python syntax is rather pointless, in my view. They offer PHP and Ruby variants too; all that varies are the delimiters around the types. This is, at best, a curiosity; presumably parsing JSON will be faster in those other languages too (as parsing Python, Ruby or PHP requires support for a lot more types), or opening those processes up for eval()-style attacks.",A,3
8616989,2011-12-23 14:08:36.020000+00:00,"Because you're only outputting one link per div, whereas it's clear from that site that there are multiple links, each inside its own li, and multiple lis per ul. You'll need to loop through all the lis.",A,0
23922512,2014-05-28 21:53:13.600000+00:00,"You misnamed the method initializer:
def _ini_(self):

will not be called automatically when creating your instance. As a result, the song attribute is never created, trying to access it later leads to the AttributeError exception.
Name it __init__:
class MyStuff(object):
    def __init__(self):
        self.song = ""Hey Brother""
    def apple(self):
        print ""I got a iphone""

Note the double underscores before and after the word init.
Demo:
>>> class MyStuff(object):
...     def __init__(self):
...         self.song = ""Hey Brother""
...     def apple(self):
...         print ""I got a iphone""
... 
>>> music = MyStuff()
>>> music.apple()
I got a iphone
>>> print music.song
Hey Brother
",A,3
47113501,2017-11-04 17:30:09.877000+00:00,"The server responds with a URL encoded as Latin-1 which is not URL encoded; non-ASCII bytes are shown as 0x?? hex escapes:
Location: /etf/ishares_core_dax0xAE_ucits_etf_de

The 0xAE byte there is not a valid URL character; the server is violating standards here. What they should be sending is
Location: /etf/ishares_core_dax%AE_ucits_etf_de

or
Location: /etf/ishares_core_dax%C2%AE_ucits_etf_de

Using escaped data for the Latin-1 or UTF-8 encoding of the URL.
We can patch requests to be more robust in the face of this error, by returning the Location header unchanged:
from requests.sessions import SessionRedirectMixin

def get_redirect_target(
        self, resp, _orig=SessionRedirectMixin.get_redirect_target):
    try:
        return _orig(self, resp)
    except UnicodeDecodeError:
        return resp.headers['location']

SessionRedirectMixin.get_redirect_target = get_redirect_target

With this patch applied the redirects work as expected.
I created a pull request to improve Location handling.",A,3
53004815,2018-10-26 08:41:43.607000+00:00,"There are several problems with this code. The initial one is that you try to subclass the HTMLParser module, not the class that that module contains. You should do:
from HTMLParser import HTMLParser

Once you've fixed that, you will find problems in your __init__ method; you can't just reference super like that without calling it, and in Python 2 you need to pass the current class and the self instance there. But in fact there is no point in defining a method just to call the superclass version; you should delete that __init__ definition altogether.",A,0
16499906,2013-05-11 17:31:57.323000+00:00,"I suspect you are using a version of Python earlier than 2.6.5, with the latest Django version. Django 1.5 is only compatible with Python 2.6.5+. You should upgrade Python, or use Django 1.4.",A,6
36470920,2016-04-07 08:40:26.423000+00:00,"You are creating slice points; are you slicing after the current element or not. You can generate these with booleans:
from itertools import product

def sublists(lst):
    for doslice in product([True, False], repeat=len(lst) - 1):
        slices = []
        start = 0
        for i, slicehere in enumerate(doslice, 1):
            if slicehere:
                slices.append(lst[start:i])
                start = i
        slices.append(lst[start:])
        yield slices

Demo:
>>> from pprint import pprint
>>> mylist = [101, 102, 103, 104, 105, 106]
>>> pprint(list(sublists(mylist)))
[[[101], [102], [103], [104], [105], [106]],
 [[101], [102], [103], [104], [105, 106]],
 [[101], [102], [103], [104, 105], [106]],
 [[101], [102], [103], [104, 105, 106]],
 [[101], [102], [103, 104], [105], [106]],
 [[101], [102], [103, 104], [105, 106]],
 [[101], [102], [103, 104, 105], [106]],
 [[101], [102], [103, 104, 105, 106]],
 [[101], [102, 103], [104], [105], [106]],
 [[101], [102, 103], [104], [105, 106]],
 [[101], [102, 103], [104, 105], [106]],
 [[101], [102, 103], [104, 105, 106]],
 [[101], [102, 103, 104], [105], [106]],
 [[101], [102, 103, 104], [105, 106]],
 [[101], [102, 103, 104, 105], [106]],
 [[101], [102, 103, 104, 105, 106]],
 [[101, 102], [103], [104], [105], [106]],
 [[101, 102], [103], [104], [105, 106]],
 [[101, 102], [103], [104, 105], [106]],
 [[101, 102], [103], [104, 105, 106]],
 [[101, 102], [103, 104], [105], [106]],
 [[101, 102], [103, 104], [105, 106]],
 [[101, 102], [103, 104, 105], [106]],
 [[101, 102], [103, 104, 105, 106]],
 [[101, 102, 103], [104], [105], [106]],
 [[101, 102, 103], [104], [105, 106]],
 [[101, 102, 103], [104, 105], [106]],
 [[101, 102, 103], [104, 105, 106]],
 [[101, 102, 103, 104], [105], [106]],
 [[101, 102, 103, 104], [105, 106]],
 [[101, 102, 103, 104, 105], [106]],
 [[101, 102, 103, 104, 105, 106]]]

If you want to drop the last entry (containing a list with only one list in it, in turn containing all elements), replace the last 2 lines with:
if start:
    slices.append(lst[start:])
    yield slices
",A,2
35875485,2016-03-08 18:46:02.347000+00:00,"You are assigning to new attributes:
self.x0 = f * self.x
self.y0 = f * self.y

x0 and y0 are different attributes from x and y. Thus, x and y remain unchanged.",A,3
1398673,2009-09-09 09:57:34.140000+00:00,"The easy way to do this is to save all instances of the class in a list.
a = Example()
b = Example()
all_examples = [ a, b ]

Objects don't spring into existence spontaneously.  Some part of your program created them for a reason.  The creation is done for a reason.  Collecting them in a list can also be done for a reason.
If you use a factory, you can do this.
class ExampleFactory( object ):
    def __init__( self ):
        self.all_examples= []
    def __call__( self, *args, **kw ):
        e = Example( *args, **kw )
        self.all_examples.append( e )
        return e
    def all( self ):
        return all_examples

makeExample= ExampleFactory()
a = makeExample()
b = makeExample()
for i in makeExample.all():
    print i
",A,-1
15679105,2013-03-28 10:13:10.887000+00:00,"You generally wouldn't do this.
You normally would create a base class in baseclass_module instead:
# baseclass_module.py

class BaseClass(object):
    def method1(self):
        # ...

    def method2(self):
        # ...

Then use that in other modules:
from baseclass_module import BaseClass

class Bla(BaseClass):
    # ...
",A,5
22993136,2014-04-10 15:59:39.333000+00:00,"With a simple key function:
sorted(d.iteritems(), key=lambda i: i[1]['created_at'])

This produces a sorted list of (key, nested_dict) tuples, sorted on the 'created_at' key of the nested dictionaries.
In Python 3, replace iteritems() with items(). You cannot avoid creating a list; sorting requires an orderable, mutable sequence to move the items around in.",A,3
44875491,2017-07-02 21:44:50.073000+00:00,"PyCharm is taking them from the Mapping declaration in the typing module (or their own internal version of the file):
class Mapping(Collection[KT], Generic[KT, VT_co],
              extra=collections_abc.Mapping):
    __slots__ = ()

(with the T_co covariant inherited from further up the base classes, from Iterable).
I'd replace those suggestions with your more concrete versions:
class MyList(Mapping[str, T]):
    def __getitem__(self, k: str) -> T:
        pass

    def __iter__(self) -> Iterator[str]:
        pass

    def __len__(self) -> int:
        pass
",A,1
28832448,2015-03-03 12:58:55.787000+00:00,"Using the reduce() function to access and set elements:
try:
    # Python 3 moved reduce to the functools module
    from functools import reduce
except ImportError:
    # Python 2 reduce is a built-in
    pass

def get_target(d, keys):
    return reduce(lambda d, k: d.setdefault(k, {}), keys, d)

def set_target(d, keys, value):
    parent = get_target(d, keys[:-1])
    parent[keys[-1]] = value

result = {}
set_target(result, yourlist[:-1], yourlist[-1])

The get_target() and set_target() functions are re-usable on already-built nested structures, they are not limited to building a dictionary from scratch. I adapted get_target() from an earlier, related post.
Demo:
>>> def get_target(d, keys):
...     return reduce(lambda d, k: d.setdefault(k, {}), keys, d)
... 
>>> def set_target(d, keys, value):
...     parent = get_target(d, keys[:-1])
...     parent[keys[-1]] = value
... 
>>> result = {}
>>> yourlist = ['item1', 'item2', 'item3', 'item4']
>>> set_target(result, yourlist[:-1], yourlist[-1])
>>> result
{'item1': {'item2': {'item3': 'item4'}}}
",A,3
38150584,2016-07-01 17:00:45.067000+00:00,"Use zip() to pair up the elements of the input lists:
lista, listb = zip(*inputlist)

The * applies the elements in inputlist as separate arguments, as if you called zip() as zip([1, 2], [2, 3], [5, 4]). zip() takes the first element of each argument and returns those together, and then the second element, etc.
This produces tuples, not lists, really, but that's easy to remedy:
lista, listb = map(list, zip(*inputlist))

Demo:
>>> inputlist = [[1, 2], [2, 3], [5, 4]]
>>> zip(*inputlist)
[(1, 2, 5), (2, 3, 4)]
>>> lista, listb = map(list, zip(*inputlist))
>>> lista
[1, 2, 5]
>>> listb
[2, 3, 4]
",A,2
16201347,2013-04-24 20:15:12.317000+00:00,"You are looking for a sliding window generator instead:
from itertools import islice

def window(seq, n=2):
    ""Returns a sliding window (of width n) over data from the iterable""
    ""   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   ""
    it = iter(seq)
    result = tuple(islice(it, n))
    if len(result) == n:
        yield result    
    for elem in it:
        result = result[1:] + (elem,)
        yield result

list(window(a_list, 6))    

which gives:
>>> list(window(a_list, 6)) 
[('chicken', 'pizza', 'burger', 'beer', 'vodka', 'potato'), ('pizza', 'burger', 'beer', 'vodka', 'potato', 'fries'), ('burger', 'beer', 'vodka', 'potato', 'fries', 'mustache')]

You are not creating python sets here, you need to be careful with your terminology.
Specifically, you are not testing when the second index reaches the end of the list:
a_windows = []
index1 = 0
index2 = 6

while index2 <= len(a_list):
    a_windows.append(a_list[index1:index2])
    index1 += 1
    index2 += 1

which works:
>>> a_windows
[['chicken', 'pizza', 'burger', 'beer', 'vodka', 'potato'], ['pizza', 'burger', 'beer', 'vodka', 'potato', 'fries'], ['burger', 'beer', 'vodka', 'potato', 'fries', 'mustache']]
",A,2
27797634,2015-01-06 11:24:56.197000+00:00,"There's no need to ""intercept"" anything. Rendered Django templates are simply strings, and there is a handy shortcut to do exactly what you want:
from django.template.loader import render_to_string
rendered = render_to_string('my_template.html', {'foo': 'bar'})
",A,4
29301028,2015-03-27 12:45:21.933000+00:00,"As Alex hints, you need to add the request context processor; it's not activated by default.
'django.core.context_processors.request',
",A,2
22488397,2014-03-18 18:48:28.300000+00:00,"metronome is still as string, not an integer. Store the result of int() back into the variable:
metronome = int(mentronome)
",A,0
3387698,2010-08-02 12:24:22.607000+00:00,"How are you expecting this to work? Redirection happens by getting the browser to request another URL. Anything you want to pass as a parameter to the redirection must therefore go into the URL you're redirecting to. It simply doesn't make sense to put a queryset into a URL parameter.
Presumably you could pass whatever arguments you used to get the queryset in the first place, but that's a lot of extra work.
Do you really need to redirect at all? What about simply calling the new view from your original one, and returning its response?",A,1
1873371,2009-12-09 11:44:43.770000+00:00,"The point of ORM is to Map Objects to Relations.
The point of ORM is -- explicitly -- not to sweat the details of a specific SQL join.
One fundamental guideline to understanding ORM is this.
SQL joins are a hack because SQL doesn't have proper navigation.
To do ORM design, we to intentionally set the SQL join considerations aside as (largely) irrelevant.  Give up the old ways.  It's okay, really.  The SQL crutches aren't supporting us very well.
Step 1.  Define the domain of discourse.  The real-world objects.  
Step 2.  Define implementation classes that are a high-fidelity model of real-world things.
Step 3.  Map the objects to relations.  Here's where the hack-arounds start.  SQL doesn't have a variety of collections -- it only has tables.  SQL doesn't have subclasses, it only has tables.  So you have to design a ""good-enough"" mapping between object classes and tables.  Ideally, this is one-to-one.  But in reality, it doesn't work out that way.  Often you will have some denormalization to handle class hierarchies.  Other than that, it should work out reasonably well.  
Yes you have to add many-to-many association tables that have no object mapping.
Step 4.  You're done.  Write your application.
""But what about my query that joins 5 (or 3) tables but only takes one attribute from each table?""
What about it?  One of those tables is the real object you're dealing with.  The other of those 5 (or 3) tables are either part of 1-m nested collections, m-1 containers or m-m associations.  That's just navigation among objects.
A 1-m nested collection is the kind of thing that SQL treats as a ""result set"".  In ORM it will become a proper object collection.  
A m-1 contain is the typical FK relationship.  In ORM it's just a fetch of a related object through ordinary object navigation.
A m-m association is also an object collection.  It's a strange collection because two objects are members of each other's collections, but it's just an object collection.
At no time do you design an object that matches a query.  You design an object that matches the real world, map that to the database.
""What about performance?""  It's only a problem when you subvert the ORM's simple mapping rules.  Once in a blue moon you have to create a special-purpose view to handle really big batch-oriented joins among objects.  But this is really rare.  Often, rethinking your Java program's navigation patterns will improve performance.
Remember, ORM's cache your results.  Each ""navigation"" may not be a complete ""round-trip"" to the database query.  Some queries may be batched by the ORM for you.",A,1
21908271,2014-02-20 12:47:36.493000+00:00,"The 3rd argument to open() is buffering:
open(file, mode='r', buffering=-1, encoding=None,
     errors=None, newline=None, closefd=True, opener=None) -> file object

Pass the character encoding as a keyword parameter instead:
with open(the_file, encoding=""utf-8"") as file:
    text = file.read()
",A,2
34384459,2015-12-20 18:42:34.057000+00:00,"You are running into floating point limitations; math.pow() returns a floating point number, so both operands are coerced to floats. For x = 23, math.factorial(x) returns an integer larger than what a float can model:
>>> math.factorial(23)
25852016738884976640000
>>> float(math.factorial(23))
2.585201673888498e+22

The right-hand-side operator is a much smaller floating point number (only 7 digits), it is that difference in exponents that causes the modulus operator error out.
Use ** to stick to integers:
for x in range(20, 50):
    print(x, math.factorial(x), 2 ** x, math.factorial(x) % (2 ** x))

Integer operations are only limited to how much memory is available, and for x = 23 the correct value is calculated, continuing to work correctly all the way to x = 49:
>>> x = 23
>>> print(x, math.factorial(x), 2 ** x, math.factorial(x) % (2 ** x))
23 25852016738884976640000 8388608 6815744
>>> x = 49
>>> print(x, math.factorial(x), 2 ** x, math.factorial(x) % (2 ** x))
49 608281864034267560872252163321295376887552831379210240000000000 562949953421312 492581209243648

Note that for even for smaller floating point modulus calculations, you really should be using the math.fmod() function, for reasons explained in the documentation. It too fails for this case however, again because you are reaching beyond the limits of floating point math:
>>> print(x, math.factorial(x), math.pow(2, x), math.fmod(math.factorial(x), math.pow(2, x)))
23 25852016738884976640000 8388608.0 0.0
",A,3
50383047,2018-05-17 03:49:18.777000+00:00,"You're reading a blog post from 2013, which contains a link to a urllib3 Github issue, which was fixed about a month later, and support was added to requests shortly afterward, and is there in all 2.x versions.
So, you're looking for a workaround for a problem that was solved almost 5 years ago.
To use an HTTPS proxy, you just configure it the same way as an HTTP proxy:
proxies = {
    'https': 'http://10.10.1.10:12345',
}
page = requests.get('https://example.org', proxies=proxies)

If you, e.g., run nc -kl 1080 on 10.10.1.10, you'll see this:
CONNECT example.org:443 HTTP/1.0

And if you run an actual HTTPS proxy there, it will just work.

You also claim that urllib doesn't handle HTTPS proxies, but it always has. It's slightly more painful to set up, but still not that hard:
ph = urllib.request.ProxyHandler({'https': '192.168.42.100:1080'})
op = urllib.request.build_opener(p)
page = op.open('https://example.com')

… or, if you want to use it for everything rather than a single request:
ph = urllib.request.ProxyHandler({'https': '192.168.42.100:1080'})
urllib.request.install_opener(urllib.request.build_opener(p))
page = urllib.request.open('https://example.com')


And of course if you have your default proxy settings configured in the appropriate way for your platform, you don't even need to do this much; both requests and urllib will just use them.",A,1
17122588,2013-06-15 10:23:16.180000+00:00,"It is not true that ""multiple tag fields is not possible"". You can have multiple fields with the same name.",A,3
20754111,2013-12-24 02:34:33.707000+00:00,"You don't really need a JavaScript parser or anything fancy here; you could do this with very simple regular expressions. For example:
r = re.compile(r'user_pref\(""network.proxy.socks"", .*?\);')
with open('prefs.js') as f:
    contents = f.read()
contents = r.sub(r'user_pref(""network.proxy.socks"", ""the.value.i.want"");', 
                 contents)
with open('newprefs.js', 'w') as f:
    f.write(contents)

If you don't understand even simple regular expressions, this isn't too hard to do with trivial string manipulation either. For example:
with open('prefs.js') as fin, open('newprefs.js', 'w') as fout:
    for line in fin:
        if 'user_pref(""network.proxy.socks"",' in line:
            line = 'user_pref(""network.proxy.socks"", ""the.value.i.want"");\n'
        fout.write(line)

And to edit more lines, just do the same thing more times.",A,1
13423911,2012-11-16 20:26:02.713000+00:00,"Here's a less broken datetime-based solution to convert from datetime object to posix timestamp:
future = datetime.datetime.utcnow() + datetime.timedelta(minutes=5)
return (future - datetime.datetime(1970, 1, 1)).total_seconds()

See more details at Converting datetime.date to UTC timestamp in Python.",A,11
41332139,2016-12-26 13:39:26.050000+00:00,"You can always invert a test and use continue to skip the iteration:
for fileinfo in response['Contents']:
    if key not in fileinfo['Key']:
        continue
    if '/' not in fileinfo['Key']:
        self._s3.download_file(bucket, fileinfo['Key'], file)
        continue

    filekeysplit = fileinfo['Key'].rsplit('/', 1)
    if filekeysplit[1] == '':
        continue
    if not os.path.exists(file):
        os.makedirs(file)
    fileout = os.path.join(file, filekeysplit[1])
    self._s3.download_file(bucket, fileinfo['Key'], fileout)

We can pull out the double download_file() call; skip keys that end in / early. You only need to create directories once, outside the loop (I'd rename file to directory here too). I'd use str.rpartition() here instead of str.rsplit():
# file has been renamed to directory, no need to test,
# as `os.makedirs()` does this for us
os.makedirs(directory)

for fileinfo in response['Contents']:
    if key not in fileinfo['Key']:
        continue
    __, slash, basename = fileinfo['Key'].rpartition('/')
    if not basename and slash:  # ended in ""/""
        continue

    target = directory
    if slash:  # there was a partition
        target = os.path.join(target, basename)
    self._s3.download_file(bucket, fileinfo['Key'], target)
",A,1
49471839,2018-03-25 02:05:53.620000+00:00,"On the server, you're sending this:
send = zlib.compress(img1.tobytes())
size = img1.size

send = send + ""@"" + str(size[0]) + ""@""+ str(size[1]) + ""@0@0""

On the client, you're parsing it like this:
to_pic = img.split('@')[0]
print to_pic
scrn = open(""monitor_serv.png"", ""wb"")
scrn.write(zlib.decompress(to_pic))

There's going to be a @ byte in almost all arbitrary compressed files. So your to_pic is going to be truncated at the first one. Which means zlib will almost always give you an error saying you've given it a truncated stream.
You need to come up with some other way to frame the data. Some options:

Instead of sending data@width@height@0@0 prefixed by the byte length of that string, you could send just data prefixed by its byte length, width, and height.
If you want to use @ as a delimiter, you could escape any @ bytes inside the actual image data, and then unescape on the other side. For example, you could replace('@', '@@'), and then re.split on the first single @ sign, and then replace('@@', '@').
If you rsplit to pull off the last four @s instead of split to pull off the first one… it's a bit hacky, but it would work here, because none of the other fields could ever have an @ in them, just the compressed image data field.


There are other issues with the protocol framing that you need to rethink, but they're all things that, when you're sending smallish files over localhost sockets, will only occasionally come up; this is the only one that's almost bound to come up almost every time. Unfortunately, that doesn't mean you don't need to fix the other ones; it just means they'll be harder to debug.

Meanwhile, there's another flaw in your design: 
What you get back from ImageGrab.grab() (even before cropping) isn't a PNG image, it's raw PIL/Pillow Image data. You compress that on the server, uncompress it on the client, and save those bytes as a PNG file. You can't do that.
One option is to use Pillow on the client as well: create an Image object from the decompressed bytes, then tell it to save itself to a PNG file.
Another option is to have the server export to bytes in PNG format instead of giving you the raw bytes. There are two big advantages to this version: No need for PIL installed on the client side, and PNG data is already compressed so you can scrap all your zlib stuff and write much simpler code.",A,1
42063755,2017-02-06 08:57:02.980000+00:00,"The problem is not with your test, but has to do with an incompatibility between your base formset class and the formset factory.
Your AFormset class inherits from BaseInlineFormSet. That class expects an fk property to exist, which should determine the foreign key of the form model to the object to which it is ""inline"". That property is created by the inlineformset_factory function. However, you are using modelformset_factory to construct your concrete formset class; this does not set that fk property.
You should either use inlineformset_factory (and pass in the parent model), or change your formset class to inherit from BaseModelFormSet if it is not actually inline.",A,0
14559632,2013-01-28 10:06:03.843000+00:00,"The right way to do it is to raise an exception. By default, the exception will propagate out from the __init__ to whoever called it, and eventually all the way to the top, halting the script.
But if you want to handle that exception and continue, use a try/catch block at whatever level you want to continue from, as described in Handling Exceptions.
For example:
class ThingyAlreadyExistsError(RuntimeError):
    pass

class Thingy(object):
    def __init__(self, pathname):
        if os.path.exists(pathname):
            yn = raw_input('{} already exists. Overwrite (y/N)?'.format(pathname))
            if yn.lower != 'y':
                raise ThingyAlreadyExistsError(pathname)
        # finish initialization

thingies = []
for pathname in pathnames:
    try:
        thingy = Thingy(pathname)
    except ThingyAlreadyExistsError:
        continue
    thingies.append(thingy)

If you want to catch this before even getting to the __init__, you could always do the check in the __new__ method, or in a @classmethod factory function, or in the for loop, in which case you don't even need an exception; just don't initialize. But there's nothing stopping you from raising an exception inside __init__.",A,0
2358076,2010-03-01 18:30:01.643000+00:00,"""Use a markup language that produces safe HTML.""
Clearly, the only sensible approach.
""The problem with this is that most markup languages are not very powerful layout-wise.""
False.
""no way to center elements in ReST.""
False.
Centering is a style -- a CSS feature -- not a markup feature.  

The want to center is to assign an CSS Class to a piece of text.   The .. class:: directive does this.
You can also define your own interpreted text role, if that's necessary for specifying an inline class on a piece of <span> markup.
",A,1
3398855,2010-08-03 17:03:05.427000+00:00,"
Am I on the right path?

Are you using Tests to Drive Development?  i.e., TDD?
If so, you're on the right path.
""Integration Test"" and ""Unit Test"" are murky definitions.  Scholars can split hairs in trying to find ways to distinguish them.
Don't waste time on hair splitting.
Write tests first.",A,4
32423815,2015-09-06 12:56:38.883000+00:00,"The opencc module is not compatible with Python 3. It can currently only be used on Python 2.
Specifically, the version module is part of the opencc package, but in Python 3 you'd need to use absolute imports, from opencc.version import __version__ or from .version import __version__. There will be other issues with the code too.",A,6
14106824,2012-12-31 23:39:10.103000+00:00,"The easy answer is: don't print within the while loop, but concatenate up a string to print at the end:
result = ''
while z < y+1:
    a = x[z-1]
    result += str(test[a])
    z = z + 1
print result

If there are going to be a whole bunch of things, instead of concatenating strings on the fly, use a list and join at the end:
result = []
while z < y+1:
    a = x[z-1]
    result.append(str(test[a]))
    z = z + 1
print ''.join(result)

But you say you want to get an integer. Do you need that integer, for any reason other than to print it out? If so, you could build up a string and then convert it to an integer:
result = []
while z < y+1:
    a = x[z-1]
    result.append(str(test[a]))
    z = z + 1
val = int(''.join(result))

But this is silly. Why not just build up the value as an integer in the first place?
val = 0
while z < y+1:
    a = x[z-1]
    val = val * 10 + test[a]
    z = z + 1

Since you seem to be using Python 2, the only way to print without adding a newline, instead adds a space, which doesn't help. But Python 3—and Python 2 with from __future__ import print_function—has a more flexible alternative:
while z < y+1:
    a = x[z-1]
    print(test[a], end='')
    z = z + 1
print()

Or you can write directly to stdout:
while z < y+1:
    a = x[z-1]
    sys.stdout.write(str(test[a]))
    z = z + 1
sys.stdout.write('\n')
",A,3
49424257,2018-03-22 08:55:45.513000+00:00,"For the code posted, there is no difference.
If your subclasses implement __bool__ or __len__ however, the first example will fail, as not self._instance could return True even when an instance has been set. You really want to use if self._instance is None: instead:
>>> class AlwaysFalse(object):
...     def __bool__(self): return False
...
>>> if not AlwaysFalse():
...     print(""It doesn't exist? Should we create a new one?"")
...
It doesn't exist? Should we create a new one?
>>> AlwaysFalse() is None
False

Other than that, the differences are cosmetic.
You also want to use identity testing to check if a singleton implementation work correctly; a subclass could implement the __eq__ method and return True even if the two objects are distinct (so not singletons):
>>> class EqualNotSingleton(object):
...     def __eq__(self, other): return True
...
>>> EqualNotSingleton() == EqualNotSingleton()
True
>>> EqualNotSingleton() is EqualNotSingleton()
False
",A,6
15967897,2013-04-12 09:38:18.840000+00:00,"The expression broken down:

^: match at the start of the string
(?P<pk>\d+): Match 1 or more digits (0-9) and capture that as the named group pk
/results/: Match the literal text /results/
$: Match at the end of the string.

So a URL path that starts with digits, followed by the text /results/ matches:
1234/results/
42/results/
3/results/

but anything else does not. 
If used in a Django url configuration, the digits are captured and passed into the attached view as the pk keyword parameter.",A,4
28515421,2015-02-14 12:00:54.817000+00:00,"The window is created in any case, add a pause after btn.pack() to see it even without .mainloop():
#!/usr/bin/env python3
from tkinter import Tk, Button

root = Tk()
Button(root, text =""click me"", command=root.destroy).pack()

input('Press Enter to exit..') # normally, you should use `root.mainloop()` here

IDLE itself is implemented using tkinter and therefore it calls .mainloop().
turtle is also implemented using tkinter; turtle.done() calls .mainloop() internally.",A,1
20863600,2013-12-31 21:56:10.057000+00:00,"Mako is a normal Python package. Unless you're using a framework that already integrates it, you install it in the same ways you install any Python package—using pip or easy_install, or python setup.py installer, or double-clicking the Windows .msi or .exe, etc.
If you haven't read Installing Python Modules, you should. You will need to learn how to install modules that you download, rather than just bang at this one without understanding what you're doing, cross your fingers, and repeat as soon as you need another package.
However, there are two easy ways.

The Download Mako page explicitly suggests that you install it via pip. This is always easy, once you've got pip working. You don't even have to download the package or know where to find it, just how to type its name. If pip is on your %PATH% and set up properly, it's just:
C:\> pip install mako

If you have pip, but it's not on your %PATH%, put it there. If you don't know how to do that, search SuperUser for how to modify the environment for your version of Windows.
If you don't have pip, get it. (If you don't have a C compiler set up, you will probably want to do that at some point as well, but you shouldn't need it for Mako.)

Or, you can look at Christoph Gohkle's site to see if he has pre-built Windows binary installer packages for Mako, as he does for many popular packages. And he does.

Meanwhile, your questions are very confused:

after I transferred my website to the host server, can the python source code function properly in the html ?

If the host server has some way of running Python scripts, and it has Mako installed into the site-packages of the appropriate Python interpreter, and you're using something that can execute Mako templates, then yes.
Otherwise, no. Mako is not a web development framework or a full web server, much less something that magically transforms a web server on a complete different computer magically into a server. You will need some way of running those Mako templates on the server.

What about PHP installation?

Um… No thanks?
What's your question here? There may be a PHP framework that can drive Mako templates, but I wouldn't expect there to be one, and I wouldn't recommend searching for one. What you want is a Python framework. Many Python frameworks can use Mako, but you have to pick one, set it up, and learn how it works.",A,2
48084535,2018-01-03 20:17:47.230000+00:00,"You are not handling the line separator. All but perhaps the last line string will end in a \n newline character. This makes b equal to '\n', not the empty string.
You should strip whitespace from b first:
for line in file_object:
    a, b = line.split(',')
    if not b.strip():
        dummy.append(a)

The str.strip() method returns a new string with all whitespace (newlines included) from the start and end removed. You could also apply it to the line first, before splitting:
a, b = line.strip().split(',')

but then you would leave whitespace after the comma in place, as well as remove whitespace at the start of the value assigned to a.
You should really use the csv module to read comma-separated data:
def pathway(file):
    dummy = []

    with open(file) as file_object:
        reader = csv.reader(file_object)
        for row in reader:
            if len(row) == 1:
                dummy.append(row[0])

    print(dummy)
",A,0
12857440,2012-10-12 10:51:23.717000+00:00,You can't use Jython on Android. ,A,0
24512938,2014-07-01 14:34:58.203000+00:00,"They are perfectly normal; you have nested dictionaries. Use as many indexes as needed:
weapons['swordType']['daggers']['cardboard']['brokenCardboardDagger']['damage']

If you want to have that 'path' stored in a separate variable, use a loop to extract each subsequent nested value, or use reduce(). See Any Functional Programming method of traversing a nested dictionary?",A,2
48724217,2018-02-10 18:33:32.250000+00:00,"Mock the requests.post function, and on the mock set the side_effect attribute to the desired exception:
@patch('requests.post')
def test_request_post_exception(self, post_mock):
    post_mock.side_effect = requests.exceptions.ConnectionError()
    # run your test, code calling `requests.post()` will trigger the exception.

From the linked documentation:

This can either be a function to be called when the mock is called, an iterable or an exception (class or instance) to be raised.
[...]
An example of a mock that raises an exception (to test exception handling of an API):
>>> mock = Mock()
>>> mock.side_effect = Exception('Boom!')
>>> mock()
Traceback (most recent call last):
  ...
Exception: Boom!


(Bold emphasis mine).
This is also covered in the Quick Guide section:

side_effect allows you to perform side effects, including raising an exception when a mock is called:
>>> mock = Mock(side_effect=KeyError('foo'))
>>> mock()
Traceback (most recent call last):
 ...
KeyError: 'foo'

",A,3
23816320,2014-05-22 20:18:34.533000+00:00,"The Session.cookies object implements the full mutable mapping interface, so you can call:
s.cookies.clear()

to clear all the cookies.
Demo:
>>> import requests
>>> s = requests.session()
>>> s.get('http://httpbin.org/cookies/set', params={'foo': 'bar'})
<Response [200]>
>>> s.cookies.keys()
['foo']
>>> s.get('http://httpbin.org/cookies').json()
{u'cookies': {u'foo': u'bar'}}
>>> s.cookies.clear()
>>> s.cookies.keys()
[]
>>> s.get('http://httpbin.org/cookies').json()
{u'cookies': {}}

Easiest however, is just to create a new session:
s = requests.session()
",A,28
1834754,2009-12-02 18:08:39.590000+00:00,"Using eval is weak, not a clearly bad practice.

It violates the ""Fundamental Principle of Software"".  Your source is not the sum total of what's executable.  In addition to your source, there are the arguments to eval, which must be clearly understood.  For this reason, it's the tool of last resort.
It's usually a sign of thoughtless design.  There's rarely a good reason for dynamic source code, built on-the-fly.  Almost anything can be done with delegation and other OO design techniques.
It leads to relatively slow on-the-fly compilation of small pieces of code.  An overhead which can be avoided by using better design patterns.

As a footnote, in the hands of deranged sociopaths, it may not work out well.  However, when confronted with deranged sociopathic users or administrators, it's best to not give them interpreted Python in the first place.  In the hands of the truly evil, Python can a liability; eval doesn't increase the risk at all.  ",A,27
14523173,2013-01-25 13:54:53.637000+00:00,"You were almost there, but it isn't your getattr or setattr that's the problem. You end up setting the class back on self, not the instance you created:
def __init__(self, colorList):
    for color in colorList:
        blobSpec       = getattr(blobLib, color)
        blob           = blobSpec()    # create an instance of the blob
        blob.Obj       = blob.returnObj(self.page) 
        setattr(self, color, blob)

It's the same thing as calling the class directly (BlueBlob()), but now through a variable.",A,2
38761356,2016-08-04 07:44:02.873000+00:00,"Don't call the .next() method directly. Use the next() function on an iterator instead:
for u in up:
    head = next(u)[1:].strip()
    q = """".join(s.strip() for s in next(u))

The next() function will invoke the correct hook on both Python 2 and 3.
However, the error message you see from futurize indicates that you may also have bound the name next elsewhere in your code.
If you have something like:
next = some_expression

or
def next(...):
    # some function

as a global, then you are shadowing the built-in next() function. Rename any other use of next as a global in that module to avoid issues.
For example, the following demo code throws the message you see:
$ cat demo.py
def next(): pass
n = g.next()
$ bin/futurize demo.py
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored demo.py
--- demo.py (original)
+++ demo.py (refactored)
@@ -1,2 +1,2 @@
 def next(): pass
-n = g.next()
+n = g.__next__()
RefactoringTool: Files that need to be modified:
RefactoringTool: demo.py
RefactoringTool: Warnings/messages while refactoring:
RefactoringTool: ### In file demo.py ###
RefactoringTool: Line 1: Calls to builtin next() possibly shadowed by global binding
RefactoringTool: ### In file demo.py ###
RefactoringTool: Line 1: Calls to builtin next() possibly shadowed by global binding

Note how the tool then used g.__next__() instead of g.next(), to avoid using next() as a function.
Removing the next function from that code results in:
$ cat demo.py
# def next(): pass
n = g.next()
$ bin/futurize demo.py
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored demo.py
--- demo.py (original)
+++ demo.py (refactored)
@@ -1,2 +1,2 @@
 # def next(): pass
-n = g.next()
+n = next(g)
RefactoringTool: Files that need to be modified:
RefactoringTool: demo.py
",A,2
34679767,2016-01-08 14:49:09.327000+00:00,"If you want to specify a string that has a literal \ and n in it you either need to double the backslash, or use a raw string literal:
>>> '""hello\\n""'
'""hello\\n""'
>>> r'""hello\n""'
'""hello\\n""'

Such a string can then be evaluated as a Python expression containing a string literal:
>>> eval(r'""hello\n""')
'hello\n'

If your output is produced by a child process outputting the value with pprint.pprint(), you are doing more than just read that stream, as that produces perfectly valid Python syntax. Don't copy and paste that output into a Python interpreter, for example, because that'll just interpret the escape sequences directly (so before you pass it to eval()). If you are developing in an interpreter, you could use pprint.pformat() to produce a variable with the output, rather than write to stdout.
If you are trying to use Python repr() or pprint.pprint() output to pass data between systems, however, stop right there. Use a proper serialisation format instead, such as JSON. If that's not an option, at the very least use ast.literal_eval() to limit what your code accepts to only Python literals, and not arbitrary code (such as '__import__(""os"").system(""rm -rf /"")).",A,2
12643842,2012-09-28 16:26:31.283000+00:00,"Simply cast to a string:
db.session.query(Vehicle).filter(str(Car.id) == Vehicle.value)

if Car.id is a local variable that is an int.
If you need to use this in a join, have the database cast it to a string:
from sqlalchemy.sql.expression import cast

db.session.query(Vehicle).filter(cast(Car.id, sqlalchemy.String) == Vehicle.value)

If the string value in the other column contains digits and possibly whitespace you may have to consider trimming, or instead casting the string value to an integer (and leave the integer column an integer). ",A,20
4558456,2010-12-29 22:32:14.740000+00:00,"Celery is often used for this.
Start with this related questions: https://stackoverflow.com/questions/tagged/celery.",A,4
30109173,2015-05-07 18:40:33.037000+00:00,"You appear to have the expectation that creating a string from a list is round-trippable. It is not meant to be; lists are not end-user presentable objects and you get the same output as repr(listobject); debug information for developer consumption only.
The list() callable creates a new list object from any arbitrary iterable object; Python strings are iterable producing individual characters when you do so, list(stringobject) always produces a list with individual characters.
As such, list() will never attempt to interpret a string argument as Python syntax; and doing so would not even work if the original list contained objects without a Python literal notation. Take for example:
>>> def foo(): return 'bar'
... 
>>> alist = [foo]
>>> alist
[<function foo at 0x106c748c0>]

You cannot take that debug string output and turn that back into the original list, especially if you run this in a Python interpreter where there is not even such a function defined.",A,6
1206155,2009-07-30 12:02:48.157000+00:00,"Let's think about this.
Each Base 40 character takes up 8 bits.  It encodes a number from 0 to 39, which is just a hair more than of 5 bits of actual information.
A single byte can represent 256 different values.  A base 40 encoding only represents 40 different values in that same byte.
This seems to be a net loss of 2-3 bits per byte encoded.
Having said that, a base 40 value IS a string and nothing more.  No fancy database declaration is required -- it's just a string.
You write two functions -- toBase40( someBytes ) and toBytes( someBase40string ) to convert your base40 strings to ordinary strings.
If you want a well-done existing solution, research base64.",A,2
20052978,2013-11-18 16:39:09.677000+00:00,"Don't use call(); it'll wait for the process to complete.
Use the subprocess.Popen() class directly, without waiting:
from subprocess import Popen

Popen(['google-chrome'])

You can redirect its output to the bit bin, if desired:
from subprocess import Popen, STDOUT
import os

Popen(['google-chrome'], stdout=os.open(os.devnull, os.O_RDWR), stderr=STDOUT)

If all you want do do is spawn a browser with a given URL, take a look at the webbrowser module; it uses the same call under the hood to spawn a browser process in the background (including the redirection to /dev/null). Although Google Chrome is not listed on the documentation page, the 2.7.5 source code of the module does list google-chrome and chrome as recognized browser strings.",A,3
3841742,2010-10-01 17:45:32.750000+00:00,"For a queryset of A objects, you can do an 'in' startup query:
B.objects.filter(a__in=MyQueryset) 

If you want to find all C objects that are related through B to A, you need to follow the relationships via the double-underscore syntax. Something like:
C.objects.filter(b__a__in=MyAQueryset)
",A,4
46687550,2017-10-11 11:52:29.597000+00:00,"get_or_create returns a tuple of (instance, created) where the second element is a boolean showing whether or not the operation resulted in a new item being created. You should capture these separately, rather than passing the whole thing to the form. You can use _ to indicate that you don't care about the created value.
survey, _ = Survey.objects.get_or_create(organisation=organisation)
form = SurveyForm(instance=survey)
",A,4
20011077,2013-11-15 21:39:42.797000+00:00,"There are two big problems with your code:
First, line.replace won't do anything to line itself. As the docs say, it will:

Return a copy of the string with all occurrences of substring old replaced by new…

But you're not storing that new string, or doing anything else with it.
Second, you never write anything to converted.
To fix both at once:
for line in file:
    if '0' in line:
        converted.write(line.replace('0', 'ZERO'))
    elif '1' in line:
        converted.write(line.replace('1', 'ZERO'))
    else:
        return

However, you also have a number of small problems. You return the first time you find a line with no 0s or 1s. If a line has both 0s and 1s, you will only replace the 0s. You never close the file, which means the file may never get flushed to disk, and could end up empty or incomplete. So, let's fix all of those problems as well:
with open(fileName, ""r"") as file, open('converted.txt', 'w') as converted:
    for line in file:
        line = line.replace(""0"", ""ZERO"")
        line = line.replace(""1"", ""ONE"")
        converted.write(line)

It's perfectly safe to replace all the 0s even if there aren't any—it just won't do anything. (If you were trying to optimize things by skipping the expensive work if there was no work to do, the ""0"" in line takes just as long as the replace when there's nothing to do, so you've actually pessimized things… which is a good lesson to learn early in your programming career.) This means you don't need the if statements at all, you don't have to fix the way you've chained them up, and you don't have the problem with the return in the else.
And the with statement automatically calls close on both file and converted for you as soon as you leave it (even if you leave early because of, say, an unexpected exception).",A,1
40772922,2016-11-23 19:44:03.757000+00:00,"No, not at all. Python must evaluate expressions that form part of the arguments to a function fully before calling the function itself. In your second case this means that self.get_google_events() will always be called, before get_or_set can determine whether or not not retrieve the value from the cache.
Note also that your first case can be made slightly more efficient: the way you have it now, you're making two calls to get unnecessarily. Instead, just make one:
events = cache.get(cache_name)
if not events:
    events = self.get_google_events()
    cache.set(cache_name, events, 60 * 10)
",A,6
37407817,2016-05-24 08:11:12.957000+00:00,"Personally I would use Celery for this; executing delayed function calls is its job. And I don't know why knowing what messages belong where would be more of a problem there than doing it in a thread. 
But you might also want to investigate the new Django-Channels work that Andrew Godwin is doing, since that is intended to support async background tasks.",A,1
35100119,2016-01-30 09:31:37.913000+00:00,".replace(""µ"", ""micro"") is not practical. It doesn't handle all other Unicode characters. It is unmanageable to assume that no code will print unprintable Unicode characters ever.
You don't need to change your code if it prints Unicode already (the default): don't hardcode the character encoding of your environment inside your script. There are multiple ways to support Unicode-deficient environments e.g., set PYTHONIOENCODING=:backslashreplace envvar and/or you could set sys.displayhook to format the output like IPython does (note: it might create issues with doctest and other similar modules).
Replacing sys.stdout makes sense if you extend the functionality in a way that is independent from the rest of your interpreter (e.g., you shouldn't put the logic that knows about your interpreter's prompt in there). win-unicode-console package is the example where replacing standard streams may be justified (it can print any Unicode character. Though it doesn't fix displaying non-BMP characters in the default Windows console and naturally the corresponding font has to support the desirable characters too).
The actual solution may use a combination of several approaches depending on what object is best to be responsible for managing the information at a given abstraction level e.g., look how IPython implements color-printing (pyreadline), see Which character encoding is the IPython terminal using?

The question is about cleaning up my own mess if someone's terminal doesn't render what I force feed it.

Even if you need to support only the text that you generate; you shouldn't put .replace(""µ"", ""micro"") inside sys.stdout object. Instead, put .replace(""µ"", ""micro"") where you generate µ i.e., generate micro instead.",A,1
6760742,2011-07-20 10:53:23.093000+00:00,"Don't do that in the form. Override the save_model method on your admin subclass - it has access to the request.
class MessageAdmin(admin.ModelAdmin):

    def save_model(self, request, obj, form, change):
        obj.user = request.user
        super(MessageAdmin, self).save(request, obj, form, change)
",A,7
12110949,2012-08-24 14:03:24.177000+00:00,"You are setting the system time from the localtime timestamp. The latter is adjusted for the local timezone, while SetSystemTime requires you to use the UTC timezone.
Use time.gmtime() instead:
tt = time.gmttime()
win32api.SetSystemTime(year, month, 0, day, 
    tt.tm_hour, tt.tt_min, tt.tt_sec, 0)

You then also avoid having to deal with whether or not you are in summer time (DST) now, vs. March when you would be in winter time.
Alternatively you can use a datetime.datetime.utcnow() call and get the millisecond parameter as a bonus:
import datetime
tt = datetime.datetime.utcnow().time()
win32api.SetSystemTime(year, month, 0, day,
    tt.hour, tt.minute, tt.second, tt.microsecond//1000)

Note that I left the weekday item set to 0 in both examples; it is ignored when calling SetSystemTime. If it was not ignored, then your code example had the value wrong; the Python value ranges from 0 to 6 for Monday through to Sunday, while the Win32 API wants 1 through to 7 for Sunday through to Saturday. You'd have to add 2 and use modulo 7:
win32_systemtime_weekday = (python_weekday + 2) % 7)
",A,6
27193456,2014-11-28 17:22:57.270000+00:00,The attribute is slug_field (not slug_filed).,A,1
12247791,2012-09-03 12:21:39.313000+00:00,"You can limit the split to the first ';' only:
static, dynamic = line.split(';', 1)

Your static part splitting might take a little more doing, but if you know the number of spaces is going to be static in the first part, perhaps the same trick could work there:
static = static.split(' ', 4)[-1]

If the first part of the line is more complex (spaces in the TYPE part) I fear that removing everything before that is going to be a more difficult affair. Your best bet is to figure out the limited set of values TYPE could assume and to use a regular expression with that information to split the static part.",A,1
18261153,2013-08-15 20:24:32.893000+00:00,"You are running this in xterm, which does not support UTF-8 by default. Run it as xterm -u8 or use uxterm to fix that.
The other way to work around that, is to use a different locale; set your locale to Latin-1 for example:
export LANG=sv_SE.ISO-8859-1

but then you are limited to 256 codepoints, versus the full range (several million) of the Unicode standard.
Note that Python 2 never decoded the input; writing out what you read from the terminal will look fine because the raw bytes you read are interpreted by the terminal in the same locale; reading and writing Latin-1 bytes works just fine. That's not quite the same as processing Unicode data, however.",A,1
51219779,2018-07-07 03:57:39.747000+00:00,"Unlike many other languages you might be used to (e.g., C++), Python doesn't have any notion of ""type casts"" or ""conversion operators"" or anything like that.
Instead, Python types' constructors are generally written to some more generic (duck-typed) protocol.

The first thing to do is to go to the documentation for whichever constructor you care about and see what it wants. Start in Builtin Functions, even if most of them will link you to an entry in Builtin Types.
Many of them will link to an entry for the relevant special method in the Data Model chapter.

For example, int says:

… If x defines __int__(), int(x) returns x.__int__(). If x defines __trunc__(), it returns x.__trunc__() …

You can then follow the link to __int__, although in this case there's not much extra information:

Called to implement the built-in functions complex(), int() and float(). Should return a value of the appropriate type.

So, you want to define an __int__ method, and it should return an int:
class MySpecialZero:
    def __int__(self):
        return 0 


The sequence and set types (like list, tuple, set, frozenset) are a bit more complicated. They all want an iterable:

An object capable of returning its members one at a time. Examples of iterables include all sequence types (such as list, str, and tuple) and some non-sequence types like dict, file objects, and objects of any classes you define with an __iter__() method or with a __getitem__() method that implements Sequence semantics.

This is explained a bit better under the iter function, which may not be the most obvious place to look:

… object must be a collection object which supports the iteration protocol (the __iter__() method), or it must support the sequence protocol (the __getitem__() method with integer arguments starting at 0) …

And under __iter__ in the Data Model:

This method is called when an iterator is required for a container. This method should return a new iterator object that can iterate over all the objects in the container. For mappings, it should iterate over the keys of the container.
Iterator objects also need to implement this method; they are required to return themselves. For more information on iterator objects, see Iterator Types.

So, for your example, you want to be an object that iterates over the elements of self.data, which means you want an __iter__ method that returns an iterator over those elements. The easiest way to do that is to just call iter on self.data—or, if you want that aslist method for other reasons, maybe call iter on what that method returns:
class Test():
    def __init__(self):
        self.data = [1,2,3]
    def aslist(self):
        return self.data
    def __iter__(self):
        return iter(self.aslist())

Notice that, as Edward Minnix explained, Iterator and Iterable are separate things. An Iterable is something that can produce an Iterator when you call its __iter__ method. All Iterators are Iterables (they produce themselves), but many Iterables are not Iterators (Sequences like list, for example).

dict (and OrderedDict, etc.) is also a bit complicated. Check the docs, and you'll see that it wants either a mapping (that is, something like a dict) or an iterable of key-value pairs (those pairs themselves being iterables). In this case, unless you're implementing a full mapping, you probably want the fallback:
class Dictable:
    def __init__(self):
        self.names, self.values = ['a', 'b', 'c'], [1, 2, 3]
    def __iter__(self):
        return zip(self.names, self.values)


Almost everything else is easy, like int—but notice that str, bytes, and bytearray are sequences.

Meanwhile, if you want your object to be convertible to an int or to a list or to a set, you might want it to also act a lot like one in other ways. If that's the case, look at collections.abc and numbers, which not provide helpers that are not only abstract base classes (used if you need to check whether some type meets some protocol), but also mixins (used to help you implement the protocol).
For example, a full Sequence is expected to provide most of the same methods as a tuple—about 7 of them—but if you use the mixin, you only need to define 2 yourself:
class MySeq(collections.abc.Sequence):
    def __init__(self, iterable):
        self.data = tuple(iterable)
    def __getitem__(self, idx):
        return self.data[idx]
    def __len__(self):
        return len(self.data)

Now you can use a MySeq almost anywhere you could use a tuple—including constructing a list from it, of course.
For some types, like MutableSequence, the shortcuts help even more—you get 17 methods for the price of 5.

If you want the same object to be list-able and dict-able… well, then you run into a limitation of the design. list wants an iterable. dict wants an iterable of pairs, or a mapping—which is a kind of iterable. So, rather than infinite choices, you only really have two:

Iterate keys and implement __getitem__ with those keys for dict, so list gives a list of those keys.
Iterate key-value pairs for dict, so list gives a list of those key-value pairs.

Obviously if you want to actually act like a Mapping, you only have one choice, the first one.
The fact that the sequence and mapping protocols overlap has been part of Python from the beginning, inherent in the fact that you can use the [] operator on both of them, and has been retained with every major change since, even though it's made other features (like the whole ABC model) more complicated. I don't know if anyone's ever given a reason, but presumably it's similar to the reason for the extended-slicing design. In other words, making dicts and other mappings a lot easier and more readable to use is worth the cost of making them a little more complicated and less flexible to implement.",A,6
28388261,2015-02-07 22:34:27.230000+00:00,"Your problem is that you reset data_json['datos_TEU'][i]['values'] to an empty list at the beginning of every iteration of the j loop, so it will only ever have one element. Move that line to before the nested loop.
Note that your code could be written much more Pythonically:
for tarifa in tarifas:
    tot = {'tarifa': tarifa}
    data_json['datos_TOT'].append(tot)

    teu = {'tarifa': tarifa}
    values = []
    for j, termino in enumerate(terminos):
        value = {'termino': termino, 'periodo': ""{0}-{1}"".format(j,j+1)}
        values.append(value)
    teu['values'] = values
    data_json['datos_TEU'].append(teu)
",A,0
16039375,2013-04-16 14:10:32.383000+00:00,"The with syntax was only fully enabled in Python 2.6.
You must be using Python 2.5 or earlier:
Python 2.5.5 (r255:77872, Nov 28 2010, 19:00:19) 
[GCC 4.4.5] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> with open(""travellerList.txt"", ""a"") as myfile:
<stdin>:1: Warning: 'with' will become a reserved keyword in Python 2.6
  File ""<stdin>"", line 1
    with open(""travellerList.txt"", ""a"") as myfile:
            ^
SyntaxError: invalid syntax

Use from __future__ import with_statement in Python 2.5 to enable the syntax there:
>>> from __future__ import with_statement
>>> with open(""travellerList.txt"", ""a"") as myfile:
...     pass
... 

From the with statement specification:

New in version 2.5.
[...]
Note: In Python 2.5, the with statement is only allowed when the with_statement feature has been enabled. It is always enabled in Python 2.6.

The point of using a file as a context manager is that it'll be closed automatically, so your myfile.close() call is redundant.
For Python 2.4 or earlier you are out of luck, I'm afraid. You'd have to use a try- finally statements instead:
myfile = None
try:
    myfile = open(""travellerList.txt"", ""a"")
    # Work with `myfile`
finally:
    if myfile is not None:
        myfile.close()
",A,4
42653605,2017-03-07 16:39:37.140000+00:00,"So your fundamental issue is simply one of syntax. Defining tableListQuery as ""my query string"", params defines a tuple; you can't pass that tuple directly to execute, and the documentation does not say you can.[1]
You need to do as shown in the original snippet you quoted: 
tableListQuery = ""SELECT name FROM table WHERE type=:dbFile ORDER BY Name""
params = {'dbFile': dbFile}
userCursor.execute(tableListQuery, params) 

Now the second issue, as pointed out in the comments, is that you can't use SQL parameters for things like field or table names; only for WHERE values. You would need to use normal Python string formatting for that.
[1] You could do it with the * operator, but that's probably confusing at this point.",A,2
27632628,2014-12-24 06:36:35.197000+00:00,"If you see the output even if subprocess' stdout is redirected to a file then either the child process writes to stderr or directly to terminal.
To check redirect stderr temporarily to STDOUT and see whether the output is still visible.
If you don't want to redirect stderr of the whole pipeline; you could try to redirect only those commands that produce undesired output on stderr e.g., curl has --stderr option. Or other options that control the progress report.
To redirect the output that is written directly to terminal; you could use pty (pseudoterminal):
#!/usr/bin/env python
import os
import pty
import sys

with open(dump_file_path, 'wb') as file:
    def read(fd):
        data = os.read(fd, 1024)
        file.write(data)
        file.flush()
        return data

    pty.spawn(['/bin/sh', '-c', cmd], read)
",A,0
17784466,2013-07-22 09:50:21.363000+00:00,"Classes already register what subclasses are defined; call the class.__subclasses__() method to get a list:
>>> class Monster(object):
...     pass
... 
>>> class Lochness(Monster):
...     pass
... 
>>> class Yeti(Monster):
...     pass
... 
>>> Monster.__subclasses__()
[<class '__main__.Lochness'>, <class '__main__.Yeti'>]

.__subclasses__() returns a list of currently still alive subclasses. If you ever would clear all references to Yeti (del Yeti in the module, delete all instances, subclasses, imports, etc.) then it'd no longer be listed when you call .__subclasses__(). Note that in essence, .__subclasses__() is a CPython implementation detail, but the method is present in all Python versions that support new-style classes (2.2 and up, all the way to 3.x).
Otherwise, the canonical way to hook into class creation is to define a metaclass:
class MonstersMeta(type):
    def __new__(metaclass, name, bases, namespace):
        cls = super(MonstersMeta, metaclass).__new__(metaclass, name, bases, namespace)
        if issubclass(cls, Monster) and not cls is Monster:
            Monster.monsters.append(cls)
        return cls

class Monster(object):
    __metaclass__ = MonstersMeta
    monsters = []

class Lochness(Monster):
    pass

class Yeti(Monster):
    pass

Demo:
>>> class Monster(object):
...     __metaclass__ = MonstersMeta
...     monsters = []
... 
>>> class Lochness(Monster):
...     pass
... 
>>> class Yeti(Monster):
...     pass
... 
>>> Monster.monsters
[<class '__main__.Lochness'>, <class '__main__.Yeti'>]

or you can use a class decorator:
def registered_monster(cls):
    Monster.monsters.append(cls)
    return cls

class Monster(object):
    monsters = []

@registered_monster
class Lochness(Monster):
    pass

@registered_monster
class Yeti(Monster):
    pass

Demo:
>>> class Monster(object):
...     monsters = []
... 
>>> @registered_monster
... class Lochness(Monster):
...     pass
... 
>>> @registered_monster
... class Yeti(Monster):
...     pass
... 
>>> Monster.monsters
[<class '__main__.Lochness'>, <class '__main__.Yeti'>]

The difference being where you put the responsibility of registering monsters; with the base MonstersMeta type, or with explicit decorators.
Either way, the metaclass or the class decorator registers a permanent reference. You can use the weakref module if you really, really want to emulate the .__subclasses__() behaviour.",A,11
438696,2009-01-13 11:30:50.457000+00:00,"You can use map to transform a list, then join them up.
"","".join( map( str, list_of_things ) )

BTW, this works for any objects (not just longs).",A,14
25488211,2014-08-25 14:29:25.630000+00:00,"Generally speaking, you don't. You instead assert that your mocked method has been called with the expected parameters as part of your test:
gms_mock = mock_client.return_value.service.GetMemberStatus
gms_mock.return_value = 'E'

member_status = service.get_member_status(11)

self.assertEqual('E', member_status)
gms_mock.assert_called_with(11, '', '')

All your attempt did was record a mock call, as if the code-under-test had called GetMemberStatus, then assigned 'E' on the return_value attribute of the Mock instance returned by that call.
You could set the side_effect attribute to a function instead; it'll be passed the arguments and you could test there:
def gms_mock_call(*args):
    assert args == (11, '', '')
    return 'E'

mock_client.return_value.service.GetMemberStatus.side_effect = gms_mock_call

but that really puts testing what arguments are passed in in the wrong location.",A,2
44911821,2017-07-04 17:56:56.577000+00:00,"Python has no block scoping, only functions and classes introduce a new scope.
Because you have no function here, there is no need to use a global statement, cows and bulls are already globals.
You have other issues too:

input() returns a string, always.
Indexing works on strings (you get individual characters), are you sure you wanted that? 
user_input[index] == num is always going to be false; '1' == 1 tests if two different types of objects are equal; they are not.
user_input[index] in random_no is also always going to be false, your random_no list contains only integers, no strings.

If the user is to enter one random number, convert the input() to an integer, and don't bother with enumerate():
user_input = int(input(""Guess the no: ""))
for num in random_no:
    if user_input == num:
        cows += 1
    elif user_input in random_no:
        bulls += 1
",A,7
19600066,2013-10-25 21:56:29.720000+00:00,"The err ""Connection reset by peer"", aka ECONNRESET, means that the server—or some router or proxy between you and the server—closed the connection forcibly.
So, specifying a longer timeout on your end isn't going to make any difference. You need to figure out who's closing the connection and configure it to wait longer.
Plausible places to look are the server application itself, whatever server program drives that application (e.g., if you're using Apache with mod_wsgi, Apache), a load-balancing router or front-end server or reverse proxy in front of that server, or a web proxy in front of your client.
Once you figure out where the problem is, if it's something you can't fix yourself, you may be able to fix it by trickling from the server to the client—have it send something useless but harmless (an HTTP 100, an extra header, some body text that your client knows how to skip over, whatever) every 120 seconds. This may or may not work, depending on what component is hanging up.",A,1
31504549,2015-07-19 18:32:43.020000+00:00,"You are using the name node for both the class and the for loop target:
class node:
    # ...

for node in trie:
    # ...
trie.append(node(0, [], 0))

If trie is empty node will remain unbound in the function as there are no values to assign to it.
You'll have to rename one of those. I suggest you follow the Python style guide and use CamelCase for class names:
class Node:

and lower_case_with_underscores for your function (and a name better reflecting the responsibility):
def construct_trie(patterns):
",A,4
49346899,2018-03-18 10:34:45.083000+00:00,"Well, part of your confusion is that the blog post you're reading is just wrong. About multiple things. Try to forget that you ever read it (except to remember the site and the author's name so you'll know to avoid them in the future).
It is true that tuples are hashable and lists are not, but that isn't relevant to their equality-testing functions. And it's certainly not true that ""it simply compares the hash values and it knows if they are equal!"" Hash collisions happen, and ignoring them would lead to horrible bugs, and fortunately Python's developers are not that stupid. In fact, it's not even true that Python computes the hash value at initialization time.*
There actually is one significant difference between tuples and lists (in CPython, as of 3.6), but it usually doesn't make much difference: Lists do an extra check for unequal length at the beginning as an optimization, but the same check turned out to be a pessimization for tuples,** so it was removed from there.
Another, often much more important, difference is that tuple literals in your source get compiled into constant values, and separate copies of the same tuple literal get folded into the same constant object; that doesn't happen with lists, for obvious reasons.
In fact, that's what you're really testing with your timeit. On my laptop, comparing the tuples takes 95ns, while comparing the lists takes 169ns—but breaking it down, that's actually 93ns for the comparison, plus an extra 38ns to create each list. To make it a fair comparison, you have to move the creation to a setup step, and then compare already-existing values inside the loop. (Or, of course, you may not want to be fair—you're discovering the useful fact that every time you use a tuple constant instead of creating a new list, you're saving a significant fraction of a microsecond.)
Other than that, they basically do the same thing. Translating the C source into Python-like pseudocode (and removing all the error handling, and the stuff that makes the same function work for <, and so on):
for i in range(min(len(v), len(w))):
    if v[i] != w[i]:
        break
else:
    return len(v) == len(w)
return False

The list equivalent is like this:
if len(v) != len(w):
    return False
for i in range(min(len(v), len(w))):
    if v[i] != w[i]:
        break
else:
    return True
return False


* In fact, unlike strings, tuples don't even cache their hashes; if you call hash over and over, it'll keep re-computing it. See issue 9685, where a patch to change that was rejected because it slowed down some benchmarks and didn't speed up anything anyone could find.
** Not because of anything inherent about the implementation, but because people often compare lists of different lengths, but rarely do with tuples.",A,7
26223452,2014-10-06 19:29:47.747000+00:00,"Your first problem is that you're not actually creating attributes in the first place. If you want attributes, create attributes. If you want a key-value mapping, create a mapping. What you've created is a special-purpose, limited-functionality mapping with a non-standard API. There's no good reason for that.
If you want to ensure that something is an attribute of something, it has to be an attribute in the first place, which you normally do with self.spam = eggs assignments (although in some cases you do it with setattr or even self.__dict__.update).


How do I ensure that data1 and data2 are attributes of the Parent superclass, and that data3 and data4 are attributes of the Child subclass?

If ensuring that is part of the class's invariants, you want to ensure it explicitly in each class, and pass any remaining arguments up with super. For example:
class Parent:
    def __init__(self, **kwargs):
        self.data1 = kwargs.pop('data1', None)
        self.data2 = kwargs.pop('data2', None)
        super().__init__(**kwargs)

class Child(Parent):
    def __init__(self, **kwargs):
        self.data3 = kwargs.pop('data3', None)
        self.data4 = kwargs.pop('data4', None)
        super().__init__(**kwargs)

But you may notice that you're not really getting any benefit out of the extra generality here; you can—and probably should— write the same thing like this:
class Parent:
    def __init__(self, *, data1=None, data2=None, **kwargs):
        super().__init__(**kwargs)
        self.data1 = data1
        self.data2 = data2

class Child(Parent):
    def __init__(self, *, data3=None, data4=None, **kwargs):
        super().__init__(**kwargs)
        self.data3 = data3
        self.data4 = data4


All that being said, I think you're operating from a misapprehension in the first place:

My concern is that, because kwargs are so flexible (and nebulous), I can't maintain the parent-child relationship of my classes -- data1, data2, data3, and data4 will all get applied to the Child class.

No, none of these attributes are being applied to either class. They're being applied to every instance of Child—but there's no question as to whether they got those attributes as a Parent instance or as a Child instance; they got them just by being an object with a __dict__ and having a self.spam = eggs assignment. Whether you look at the instance as a Parent or as a Child, those attributes are there—as well as random extra attributes that you add long after construction, or standard attributes inherited from object, or anything else.
If you really, really want to enforce that Parent instances that aren't Child instances only have data and data2, you can do that with __slots__, @property, explicit descriptors… let's do the first, since it's simplest and clearest here:
class Parent:
    __slots__ = ('data1', 'data2')
    def __init__(self, **kwargs):
        self.data1 = kwargs.pop('data1', None)
        self.data2 = kwargs.pop('data2', None)
        super().__init__(**kwargs)

class Child(Parent):
    __slots__ = ('data3', 'data4')
    def __init__(self, **kwargs):
        self.data3 = kwargs.pop('data3', None)
        self.data4 = kwargs.pop('data4', None)
        super().__init__(**kwargs)

Now, any attempt to assign a data3 attribute on a Parent instance will raise an AttributeError, unless it's actually an instance of some subclass that adds a data3 slot, like Child.",A,4
39190674,2016-08-28 11:13:02.420000+00:00,You are using Django version 1.9; the MIDDLEWARE setting was only introduced in version 1.10. For earlier versions like yours you should be using MIDDLEWARE_CLASSES - or upgrade to 1.10.,A,0
13505484,2012-11-22 03:26:35.663000+00:00,"If your intent is to get the top 1000 most frequent words in the words list you could try:
import collections

# get top words and their frequencies
most_common = collections.Counter(words).most_common(1000)
",A,1
26722501,2014-11-03 20:22:06.690000+00:00,"To ""automatically detect new extensions"", just go through all the files and look at their extensions:
for f in os.listdir():
    name, ext = os.path.splitext(f)
    ext = ext.lstrip('.')
    if ext:
        os.mkdirs(ext)
        move_file(filename, ext)

Note that this won't work if some of your ""extensions"" aren't actually extensions. I notice that your existing code searches for things like *txt and *iso, which will of course match stuff like this_is_not_txt and spam.aniso and so forth. If that's a required feature rather than a bug, then you're going to have to come up with some rule for what you mean by ""extension"" before you can implement it in code…
Also notice that files without an extension at all will be left where they are (because of that if ext:), and files with an empty extension (just .) will be as well (because the if check happens after the lstrip). You can of course change either of those if desired (although you'll need to come up with a subdirectory name that can't possibly be ambiguous with any possible actual extension…).
Finally, this won't handle ""double extensions""—for example, .tar.gz files will go into the same directory as .gz. Again, this is easy to change if desired.",A,3
18015338,2013-08-02 11:05:22.480000+00:00,"Use raw_input() instead, input() interprets the input values as Python code turning your 2.5 input into a float number.",A,2
30726445,2015-06-09 08:12:19.100000+00:00,"I don't really see how your SQL does what you describe in your comment. However, this code does something similar to that description.
from django.db.models import Max
Candidate.objects.filter(
    languagelevel__language__name__in=['English', 'Spanish'],
    languagelevel__level__gte=2
).annotate(max_level=Max('languagelevel__level')).order_by('max_level')
",A,0
49403260,2018-03-21 10:01:51.597000+00:00,"If you want renters, you should query User, not Agreement.
renters = User.objects.exclude(renter01=None)

(Note, having renter01 as the reverse relation makes no sense; unless you have a good reason, you should keep it as the default, which is agreement_set.)",A,2
54914462,2019-02-27 21:04:22.487000+00:00,"You should always use keyword arguments when instantiating models.
new_item=Facture(content=request.POST['content'])

Note also, you should really be using a form here.",A,0
21650975,2014-02-08 19:48:10.330000+00:00,"Just use a Python expression to split at the first dot:
tal:define=""subdomain python:domain.partition('.')[0]""

or, if using Python 2.4 or earlier:
tal:define=""subdomain python:domain.split('.', 1)[0]""

This uses str.partition() or str.split() to return a list of strings; the local name is the first part; [0] selects the first element of that list.
Demo using a Python prompt:
>>> 'sub1.mydomain.com'.partition('.')[0]
'sub1'
>>> 'sub1.mydomain.com'.split('.', 1)[0]
'sub1'
",A,5
21142447,2014-01-15 16:10:59.163000+00:00,"You need to escape any regular expression metacharacters with re.escape():
pattern = re.compile(r'\b(' + '|'.join(map(re.escape, d)) + r')\b')
",A,7
35893879,2016-03-09 14:08:49.450000+00:00,"Your nChar() function always returns None:
def nChar(nc):
    grid = len(nc)
    for char in nc:
        if len(set(char)) != grid: 
            return

None is considered false in boolean contexts.
Have your function return True or False instead:
def nChar(nc):
    grid = len(nc)
    for line in nc:
        if len(set(line)) != grid: 
            return False
    return True
",A,1
27158650,2014-11-26 20:51:18.270000+00:00,"Because listboxes allow for single vs. continuous vs. distinct selection, and also allow for an active element, this question is ambiguous. The docs explain all the different things you can do.
The selection_set method adds an item to the current selection. This may or may not unselect other items, depending on your selection mode.
If you want to guarantee that you always get just that one item selected no matter what, you can clear the selection with selection_clear(0, END), then selection_set that one item.
If you want to also make the selected item active, also call activate on the item after setting it.
To understand about different selection modes, and how active and selected interact, read the docs.",A,6
20843567,2013-12-30 16:33:32.877000+00:00,".lstrip() removes the set of characters, not a word. All characters p, r, a, d and e, in any order, are removed from the start of pradeep. The last two characters e and d are still part of that set and are removed as well. You would get the same result if you used .lstrip('drape') or .lstrip('adepr').
If you want to remove a word from the start, use slicing:
example = 'pradeep'
example[5:] if example.startswith('prade') else example

or, as a function:
def remove_start(inputstring, word_to_remove):
    return inputstring[len(word_to_remove):] if inputstring.startswith(word_to_remove) else inputstring

Demo:
>>> def remove_start(inputstring, word_to_remove):
...     return inputstring[len(word_to_remove):] if inputstring.startswith(word_to_remove) else inputstring
... 
>>> remove_start('pradeep', 'prade')
'ep'
",A,5
15841930,2013-04-05 19:24:28.327000+00:00,"You are masking your method balance with a instance attribute balance. Rename one or the other. You could rename the instance attribute by pre-pending it with an underscore for example:
def set(self, bal):
    self._balance = bal

def balance(self):
    return self._balance

Attributes on the instance trump those on the class (except for data descriptors; think propertys). From the Class definitions documentation:

Variables defined in the class definition are class attributes; they are shared by instances. Instance attributes can be set in a method with self.name = value. Both class and instance attributes are accessible through the notation “self.name”, and an instance attribute hides a class attribute with the same name when accessed in this way.
",A,13
20740589,2013-12-23 09:30:40.883000+00:00,"You are using the exit() function the Python site module sets for use in the interactive interpreter.
This is Python code you are calling, not C code. This triggers the recursion depth exception handler a few more times until you are far enough away from the stack limit.
When you are right up against the limit, trying to call exit() fails because you hit the stack limit. So a RuntimeError is raised, falling back a call to 998. Here you try to call exit() again, which tries to do some more work raising the RuntimeError again, falling back another level, etc. until there is enough stack space left to finally call raise SystemExit() in the python function.
Use sys.exit() instead to avoid adding more strain to the stack.
Or, for future reference, use the sys.getrecursionlimit() function to just ask Python directly what the limit is.",A,7
374532,2008-12-17 13:33:06.723000+00:00,"First, read about unittest.  The documentation contains examples.
Second, since you want packages (not modules) the list is shorter.  There are 15 packages in Python 2.5 distribution.  Pick One At Random.  Here's a subset that might meet some of your criteria.  

bsddb: 7 modules - many test - test_all.py
ctypes: 4 modules - tests - runtests.py
distutils: many modules - tests - test_dist.py
email: many modules - tests - test_email.py
sqlite3 - 2 modules - tests - (not clear if there's an overall test, I got bored of looking)
",A,0
19801692,2013-11-06 00:15:53.830000+00:00,"Generally, SQL engines only support parameters on values, not on the names of tables, columns, etc. And this is true of sqlite itself, and Python's sqlite module.
The rationale behind this is partly historical (traditional clumsy database APIs had explicit bind calls where you had to say which column number you were binding with which value of which type, etc.), but mainly because there isn't much good reason to parameterize values.
On the one hand, you don't need to worry about quoting or type conversion for table and column names. On the other hand, once you start letting end-user-sourced text specify a table or column, it's hard to see what other harm they could do.
Also, from a performance point of view (and if you read the sqlite docs—see section 3.0—you'll notice they focus on parameter binding as a performance issue, not a safety issue), the database engine can reuse a prepared optimized query plan when given different values, but not when given different columns.

So, what can you do about this?
Well, generating SQL strings dynamically is one option, but not the only one.
First, this kind of thing is often a sign of a broken data model that needs to be normalized one step further. Maybe you should have a BookMetadata table, where you have many rows—each with a field name and a value—for each Book?
Second, if you want something that's conceptually normalized as far as this code is concerned, but actually denormalized (either for efficiency, or because to some other code it shouldn't be normalized)… functions are great for that. create_function a wrapper, and you can pass parameters to that function when you execute it.",A,1
11690401,2012-07-27 14:58:39.947000+00:00,"You are missing something. In the narrative documentation on static assets it states:

Instead of representing a URL prefix, the name argument of a call to add_static_view() can alternately be a URL. Each of examples we’ve seen so far have shown usage of the name argument as a URL prefix. However, when name is a URL, static assets can be served from an external webserver. In this mode, the name is used as the URL prefix when generating a URL using pyramid.request.Request.static_url().

In the API documentation similar wording is used:

When add_static_view is called with a name argument that represents a URL prefix, as it is above, subsequent calls to pyramid.request.Request.static_url() with paths that start with the path argument passed to add_static_view will generate a URL something like http://<Pyramid app URL>/images/logo.png, which will cause the logo.png file in the images subdirectory of the mypackage package to be served.

Using a URL switches the behaviour of add_static_view altogether and the path argument is interpreted as a symbolic path only for the .static_url() method. That latter detail is perhaps not described as explicitly in the documentation, you could file an issue in the pyramid issue tracker if you feel strongly about that.",A,2
22505966,2014-03-19 12:32:30.693000+00:00,"You could put answers in a list and call random.shuffle() on it:
import random

answers = [
    ""Open Systematic Information"",
    ""Open Systems Interconnect"",
    ""Organised Stairway Interweb"",
    ""Open Safe Internet"",
]
random.shuffle(answers)

for letter, answer in zip(""ABCD"", answers):
    print(""{}- {}"".format(letter, answer))

Each time you run it, it may produce different output e.g.:
A- Organised Stairway Interweb
B- Open Systematic Information
C- Open Safe Internet
D- Open Systems Interconnect
",A,2
5448314,2011-03-27 09:24:46.323000+00:00,"Josh has the explanation, but you are doing this wrong. If you want to overrride the post_save_redirect, then pass that in as a parameter when you call the view:
def change_password(request,template_name=""password_change_form.html""):
    return password_change(request, template_name=template_name, 
                           post_save_redirect=reverse('my_done_page'))

See the documentation.",A,2
13209638,2012-11-03 13:28:21.820000+00:00,"You should look at Urwid.
Urwid provides a higher-level toolkit on top of curses and includes an event loop to handle keyboard and mouse input. It either uses it's own select-based event loop, or can hook into gevent or Twisted.
In addition to handling keyboard input efficiently you'll also have a host of options to handle user input with edit boxes, list controls and more.",A,5
15437147,2013-03-15 16:12:18.880000+00:00,".items() returns an iterator, so you can simply call next() to get the first item:
status = next(tweepy.Cursor(api.user_timeline).items())

This could raise a StopIteration exception if there are no items at all. You could add a default to next() to prevent that:
status = next(tweepy.Cursor(api.user_timeline).items(), None)
",A,5
376799,2008-12-18 03:03:27.680000+00:00,"Yes.  
Inside a transaction, your application sees everything.  
No other transaction, however, sees any part of the change.
The point of a transaction is to make a sequence of statements appear to be one atomic change to the database. 
If you commit, all statements in the transaction are finalized, and everyone else can see the effects.
If you rollback, no statement in the transaction is finalized, and no change occurs to the database.
Not all statements can be part of a transaction, BTW.  DDL (Create and Drop, for example) will end any previous transaction.",A,14
27661521,2014-12-26 20:14:56.570000+00:00,"Email recipients must be comma separated, not using a semi-colon; that is just a Microsoft convention:
recepient = ""user1@hotmailcom,user2@hotmail.com""
msg['To'] = recepient

You don't need to use str.join() when you already have one string.
If you have a list of recipients, then using str.join() would make sense:
recepients = [""user1@hotmailcom"", ""user2@hotmail.com""]
msg['To'] = ', '.join(recepients)
",A,2
24653658,2014-07-09 12:24:51.563000+00:00,"You mean this?
def head(it):
    val = []
    for elem in it:
        val.append(elem)
        yield val

This takes any iterable, not just lists.
Demo:
>>> for first_elements in head('abcd'):
...     print first_elements
... 
['a']
['a', 'b']
['a', 'b', 'c']
['a', 'b', 'c', 'd']
",A,7
22176896,2014-03-04 16:02:52.843000+00:00,"super() doesn't look at __bases__; it looks at the Method Resolution Order (MRO), through type(self).mro():
>>> E.mro()
[<class '__main__.E'>, <class '__main__.C'>, <class '__main__.D'>, <class '__main__.A'>, <type 'object'>]

As you can see, D is in there, because it is a base class of E; when you call super(C, E()).name(), D comes next in the MRO.
The MRO will always include all base classes in a hierarchy; you cannot build a class hierarchy where the MRO could not be established. This to prevent classes being skipped in a diamond inheritance pattern.
How the MRO works is explained in detail in The Python 2.3 Method Resolution Order.
You may also want to read Guido van Rossum's explanation; he uses a diamond pattern with:
class A:
  def save(self): pass

class B(A): pass

class C(A):
  def save(self): pass

class D(B, C): pass

to illustrate why an MRO is important; when calling D().save() you'd want C.save() to be invoked (more specialized), not A.save().
If you really wanted to skip D from C.name, you'd have to explicitly find C.__bases__[0] in the MRO, then tell super() to start search for the next .name() method from there:
mro = type(self).mro()
preceding = mro[0]
for i, cls in enumerate(mro[1:], 1):
    if cls in self.__bases__:
        preceding = mro[i - 1]
name = super(preceding, self).name()

For your E.mro() and class C, this'll find D, as it precedes the first base class of C, A. Calling super(D, self).name() then tells super() to find the first class past D with a name() method, which is A here.",A,3
7892330,2011-10-25 16:14:06.700000+00:00,"DO NOT USE THIS.
You have created your own user model, and appear to be storing passwords in plain text. This is very dangerous. Do not do it.
Django has a built-in user authentication framework, fully documented here, which properly hashes passwords for security, and includes login forms. Use that instead. Even if you need more fields, use the built-in user model anyway and create a UserProfile model that includes your extra fields.",A,7
12232033,2012-09-01 22:45:41.447000+00:00,"Python class accepts Python arguments. To pass a C++ argument you need to wrap it:
# distutils: language = c++

cdef extern from ""Foo.h"" namespace ""baz"":
    cdef cppclass Bar:
         Bar(double d)
         double get()

cdef class PyBar: # wrap Bar class
    cdef Bar *thisptr
    def __cinit__(self, double d):
        self.thisptr = new Bar(d)
    def __dealloc__(self):
        del self.thisptr
    property d:
        def __get__(self):
            return self.thisptr.get()

PyBar instances can be used as any other Python objects both from Cython and pure Python:
class PyClass:
    def __init__(self, PyBar bar):
        self.bar = bar

print(PyClass(PyBar(1)).bar.d)
",A,2
32072702,2015-08-18 12:40:09.047000+00:00,"self.rfile is a simple file-like wrapper around the socket object (see the socket.makefile() function which produces this file object). The wrapper doesn't support seeking, as there only is a stream of data feeding this object, not a random-access region on a disk.
PIL on the other hand, requires random access to the whole file (through seeking) as most image formats use different sections in the file to store different information that the PIL objects need access to at different times.
Your only choice is to copy the data from self.rfile to a file object that does support seeking. I recommend you use tempfile.SpooledTemporaryFile() for this, as it'll store data in memory until a threshold is reached before moving the data to disk.
You'll need to be careful you only copy only up to the Content-Length header bytes into a local file; it is an error to send fewer or more bytes than that. If you don't your server could easily be brought to its knees by sending way bigger POST requests than your disk space can handle.
Perhaps use a while loop to copy a buffer across, until Content-Length bytes have been reached, or the socket no longer returns data:
from tempfile import SpooledTemporaryFile

def do_POST(self):
    try:
        length = int(self.headers.get('content-length'))
    except (TypeError, ValueError):
        # no Content-Length or not a number
        # return error
    if length > SOME_MAXIMUM_LENGTH:
        # return error

    with SpooledTemporaryFile() as imgfile:
        read = 0
        while read < length:
            buffer = self.rfile.read(1024)
            if not buffer:
                # too short, return error
            imgfile.write(buffer)
            read += len(buffer)
        if read > length or self.rfile.read(1):
            # too long, return error

        img_file.seek(0)
        img = Image.open(img_file)

If you are accepting a multipart/form-data request with this handler, you'll actually have to parse out that specific request type differently. Use the cgi.FieldStorage() class to handle the parsing, it'll put files into TemporaryFile objects for you, directly to disk:
from cgi import FieldStorage

def do_POST(self):
    if self.headers.get('content-type', '').lower() == 'multipart/form-data':
        fields = FieldStorage(self.rfile, self.headers, environ={'METHOD': 'POST'})
        imgfile = fields['image_file']  # or whatever exact field name you expect
",A,2
5463419,2011-03-28 18:31:27.013000+00:00,"
Is it possible to parse a file line by line, and edit a line in-place while going through the lines?

It can be simulated using a backup file as stdlib's fileinput module does.
Here's an example script that removes lines that do not satisfy some_condition from files given on the command line or stdin:
#!/usr/bin/env python
# grep_some_condition.py
import fileinput

for line in fileinput.input(inplace=True, backup='.bak'):
    if some_condition(line):
        print line, # this goes to the current file

Example: 
$ python grep_some_condition.py first_file.txt second_file.txt

On completion first_file.txt and second_file.txt files will contain only lines that satisfy some_condition() predicate.",A,41
14437927,2013-01-21 11:46:06.530000+00:00,"You need to decode the page from UTF-8 to Unicode; there are UTF-8 sequences in there (next to non-breaking-space HTML entities):
>>> print h.unescape(content.decode('utf8'))
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">
<html>
<head>
<BASE href=""http://kermit.columbia.edu"">
<META http-equiv=""Content-Type"" content=""text/html; charset=utf-8"">
<title>UTF-8 Sampler</title>
</head>
<body bgcolor=""#ffffff"" text=""#000000"">
<h1><tt>UTF-8 SAMPLER</tt></h1>

<big><big>  ¥ · £ · € · $ · ¢ · ₡ · ₢ · ₣ · ₤ · ₥ · ₦ · ₧ · ₨ · ₩ · ₪ · ₫ · ₭ · ₮ · ₯ · &#8377</big></big>



<p>
<blockquote>
Frank da Cruz<br>
<a hre

You got encoding and decoding confused; the content is already UTF-8 encoded.
Note that the &#8377 is an error in the page itself, the ; was omitted. A HTML5 parser or browser would probably assume that the ; can be added and decode it anyway:
>>> print h.unescape('&#8377;')
₹

You'd have to fix those entities with a regular expression first:
>>> import re
>>> brokenrefs = re.compile(r'(&#x?[a-e0-9]+)\b', re.I)
>>> print h.unescape(brokenrefs.sub(r'\1;', content.decode('utf8')))
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">
<html>
<head>
<BASE href=""http://kermit.columbia.edu"">
<META http-equiv=""Content-Type"" content=""text/html; charset=utf-8"">
<title>UTF-8 Sampler</title>
</head>
<body bgcolor=""#ffffff"" text=""#000000"">
<h1><tt>UTF-8 SAMPLER</tt></h1>

<big><big>  ¥ · £ · € · $ · ¢ · ₡ · ₢ · ₣ · ₤ · ₥ · ₦ · ₧ · ₨ · ₩ · ₪ · ₫ · ₭ · ₮ · ₯ · ₹</big></big>



<p>
<blockquote>
Frank da Cruz<br>
<a hre
",A,3
1979714,2009-12-30 11:32:44.857000+00:00,"Use the WSGI Reference Implementation wsgiref already provided with Python
Use REST protocols with JSON (not XML-RPC).  It's simpler and faster than XML.
Background jobs are started with subprocess.",A,1
48222395,2018-01-12 08:38:03.847000+00:00,"This isn't a question about Django. The browser simply can't do this with an HTML form. The action attribute of the form is set when it is loaded.
You could possibly write some JavaScript to make it do this. But that would be the wrong thing to do. Queries like search should be part of the querystring, not the URL.",A,1
46462821,2017-09-28 07:00:11.360000+00:00,"You are iterating over the dictionary itself, which yields the keys for that dictionary. In your flagSet['emojiFlagSet'] dictionary, those keys are strings (such as 'Andorra' and 'Afghanistan').
You wanted the values instead, it is the values that are dictionaries too. Loop over dict.values():
for country in flagSet['emojiFlagSet'].values():
    print(country['unicode'])

Note that the fact that you imported the datastructure from another module has no bearing on all of this.",A,1
20356276,2013-12-03 16:18:58.417000+00:00,"Move your try within the loop, and ignore the exception by using pass as the exception handler:
digitsum = 0
for i in s:
    try:
        digitsum += int(i)
    except ValueError:
        pass  # ignore non-digit characters

You won't hit a TypeError here unless any i is a type of object that int() cannot handle; e.g. anything not a number or a string:
>>> int({})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: int() argument must be a string or a number, not 'dict'
",A,5
25099259,2014-08-02 20:46:45.283000+00:00,"You don't show your view or form, but presumably you are just using the default ModelForm which represents a ForeignKey as a ModelChoiceField. So, the value to post for event is not the name, but the ID, which you should capture when you create the Event at the start of the test.
I should add, however, that I don't think you should be testing this at all. Unit tests are for your code, not Django's. unique_together is a part of Django itself and as such is very well-covered by the Django's own unit tests. There is no need for you to replicate that functionality explicitly. 
Edit
I don't understand your second comment. Your test should look like this:
def test_duplicate_job_in_same_event(self):
    event = Event.objects.create(...)
    job = Job.objects.create(event=event, ...)
    response = self.client.post('/AdminUnit/job/',{'event' : event.id, ...})
",A,3
16211223,2013-04-25 09:33:19.940000+00:00,"You don't need groupby; you want to use collections.defaultdict to collect series of [timestamp, value] pairs keyed by label:
from collections import defaultdict
import csv

per_label = defaultdict(list)

with open(inputfilename, 'rb') as inputfile:
    reader = csv.reader(inputfile)
    next(reader, None)  # skip the header row

    for timestamp, value, label in reader:
        per_label[label.strip()].append([timestamp.strip(), float(value)])

Now per_label is a dictionary with labels as keys, and a list of [timestamp, value] pairs as values; I've stripped off whitespace (your input sample has a lot of extra whitespace) and turned the value column into floats.
For your (limited) input sample that results in:
{'CPU pid=26298:percent': [['15:22:57', 849.0], ['15:22:58', 0.0]],
 'Disks I/O': [['15:22:58', 28683.0]],
 'JMX 31690:gc-time': [['15:22:58', 0.0]],
 'JMX MB': [['15:22:57', 461000.0], ['15:22:58', 503000.0]],
 'Memory pid=26298:unit=mb:resident': [['15:22:58', 3369078.0]]}
",A,2
13182301,2012-11-01 17:22:41.830000+00:00,"You have a literal % character in your string; double that to prevent Python from trying to interpret it as a format string:
""... very long string ... who seek the %% who lives beyong ...""

or perhaps you missed out on a s there instead:
""... very long string ... who seek the %s who lives beyong ...""

You really want to look into tripple-quoting your string though. Python let's you use """""" as string delimiters, and in such a literal string newlines are preserved:
MADLIBS = """"""\
HEAD KNIGHT OF NI: %s 
 KNIGHTS OF NI : %s 
 ARTHUR : Who are you ? 
 HEAD KNIGHT : We are the %s who say. %s 
 RANDOM : %s 
 ARTHUR : No ! Not the %s who say %s 
 Head Knight: The Same! 
 Bedevere: Who are they? 
 Head Knight: We are the keepers of the %s words : %%s %%s and %%s 
 Random: %%s 
 Arthur: Those who %s them seldom live to tell the tale. 
 Head Knight: The %%s who say %%s demand a sacrifice 
 Arthur: %s of %s we are but %s travellers who seek the %% who lives beyong these woods 
 Head Knight: %s 
 Knights of Ni: %s 
 Arthur: Ow Agh 
 Head Knight: We shall say %s again to you if you don't appease us 
 Arthur: Well what is it you want? 
 Head Knight: We want a %%s 
 Arthur: A what? 
 Knights of Ni: %%s 
 Arthur and Party: OWWW, AHHH 
 Arthur: Please, No More! We will find you a %%s 
 Head Knight: You must return here with a %%s or else, you will never pass through these woods alive. 
 Arthur: O %%s of %%s you are %%s and %%s and we will return with a %%s. 
 Head Knight: One that looks %%s 
 Arthur: Of Course 
 Head Knight: and not too %%s 
 Arthur: Yes
""""""

That way you can more clearly that you have several doubled % percent marks in your string that probably need fixing.",A,3
14245987,2013-01-09 20:35:55.477000+00:00,"You can concatenate lists quite easily:
c.execute(statement, list_of_vars + [0, 100])
",A,4
29123089,2015-03-18 13:14:23.617000+00:00,"You shouldn't do it like that. You should always redirect after a POST anyway; so here you just redirect back to 'home'.
from django.shortcuts import redirect
...
return redirect('home')

The register branch should be a redirect as well, of course.",A,2
35337254,2016-02-11 10:49:20.103000+00:00,"You are confusing the b'...' syntax for a function. It is not, it is a notation to create bytes objects, just like using 123 or 0xdeadbeaf or 0o creates integer objects, and[...]` creates a list.
The error message when you pass in a string tells you what to do:
>>> import hashlib
>>> hashlib.sha1('')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Unicode-objects must be encoded before hashing

Encode your string, using a suitable codec. If you want to support the full Unicode range for password characters (always a good idea), use a codec like UTF-8 or UTF-16 or UTF-32:
 encrypted_password = hashlib.sha1((email + passw).encode('utf8'))
",A,2
1093574,2009-07-07 17:11:33.183000+00:00,"""What's the simplest solution to 'remember' the state of the checkboxes?""
Web transactions are stateless.
There are several common ways to implement state.

Sessions.  If your user is logged in, they have a session.  Store stuff in their session.  The form they POST goes into the session.  This can then be retrieved in order to post a new form back to them.
Cookies.  Sessions use cookies.  You can use cookies manually.  You have two choices here.

Put the form checkbox information into the cookie.  Send it down with the response.  Check for the cookie in each request so you know what to seed the form with.
Put a session key in the cookie.  This is what Django does for you.  Don't implement this yourself.

Query strings in the URL.  You have the same two choices as with cookies.  Either make the settings a big-old base64 string or pass an ID that stands in for the session. 
",A,2
30732046,2015-06-09 12:27:25.737000+00:00,"os.path.getctime() returns a float that represents ""seconds since epoch"" (values returned by time.time()) -- it is not a datetime object naive or otherwise.
datetime.fromtimestamp() returns a naive datetime object representing local time unless you pass the explicit tzinfo object as a second parameter, code example.",A,0
27770640,2015-01-04 21:39:31.973000+00:00,"You are not using a regular expression method. Use the re module; the re.split() function does what you want just fine:
>>> import re
>>> re.split(r'[^0-9]+', '(15,20) (17,30 )   ( 40 ,  25)')
['', '15', '20', '17', '30', '40', '25', '']

The str.split() method doesn't take a regular expression.",A,4
13655200,2012-12-01 00:41:33.017000+00:00,"First, as kuyan suggested, see http://wiki.vg/Main_Page, which has links to various programs that may be useful, either directly or for source to look at.
For example, under Utilities, the first thing that comes up is a logging proxy.
And a bit down, there's mc3p, the program suggested by Joran Beasley—a Python proxy, with plugin support. It only works up to 1.2.5, but sadimusi/mc3p claims to be a 1.4.2-compatible fork. As J.F. Sebastian says, mc3p has an interface for log plugins, so you can just write one that logs to postgres.
If you want to read the packets yourself, that's not hard. You can write a generic TCP proxy in a few dozen lines of Python—or write one in 2 lines of shellscript around netcat that tees the data to your Python script. 
The hard part isn't intercepting the data; it's parsing the protocol. Minecraft probably isn't sending ""Nightbane: 1 tnt for 100.000 Dollars each"", but something like ""offer:Nightbane:1:tnt:100"" or ""\x13\x09Nightbane\x00\x01\x72\x00\x64"". From what the wiki says, the protocol is documented, but poorly, and sometimes inaccurately, and the wiki is sometimes incorrect too, and the official code is very ugly and hard to read. Which means that the best way to figure out the protocol is probably by reading sadimusi/mc3p or one of the other projects like McPacketSniffer or ProtoProxy, at which point you have to ask whether it would be easier to just use that project instead of reimplementing it.
At any rate, scraping the screen should be your last resort.",A,2
14683271,2013-02-04 08:55:08.263000+00:00,"Please do read the Python Unicode HOWTO; it explains how to process and include non-ASCII text in your Python code.
If you want to include Japanese text literals in your code, you have several options:

Use unicode literals (create unicode objects instead of byte strings), but any non-ascii codepoint is represented by a unicode escape character. They take the form of \uabcd, so a backslash, a u and 4 hexadecimal digits:
ru = u'\u30EB'

would be one character, the katakana 'ru' codepoint ('ル').
Use unicode literals, but include the characters in some form of encoding. Your text editor will save files in a given encoding (say, UTF-16); you need to declare that encoding at the top of the source file:
# encoding: utf-16

ru = u'ル'

where 'ル' is included without using an escape. The default encoding for Python 2 files is ASCII, so by declaring an encoding you make it possible to use Japanese directly.
Use byte string literals, ready encoded. Encode the codepoints by some other means and include them in your byte string literals. If all you are going to do with them is use them in encoded form anyway, this should be fine:
ru = '\xeb\x30'  # ru encoded to UTF16 little-endian

I encoded 'ル' to UTF-16 little-endian because that's the default Windows NTFS filename encoding.

Next problem will be your terminal, the Windows console is notorious for not supporting many character sets out of the box. You probably want to configure it to handle UTF-8 instead. See this question for some details, but you need to run the following command in the console:
chcp 65001

to switch to UTF-8, and you may need to switch to a console font that can handle your codepoints (Lucida perhaps?).",A,11
19822991,2013-11-06 21:16:23.303000+00:00,"today.readline() returns a single line. The for-loop iterates over characters in that line. And as @gnibbler pointed out the today file is at the end of file at the time today.readline() is called (so it returns an emtpy string).
In general, to delete something in the middle of a file you need to replace the file completely. fileinput module can help:
import fileinput

for line in fileinput.input([date], inplace=1):
    if delete not in line:
       print line, # stdout is redirected to the `date` file

Here's almost the same but without fileinput:
import os
from tempfile import NamedTemporaryFile

filename = date
dirpath = os.path.dirname(filename)
with open(filename) as file, NamedTemporaryFile(""w"", dir=dirpath) as outfile:
    for line in file:
        if delete not in line:
           print >>outfile, line, # copy old content
    outfile.delete = False # don't delete outfile
os.remove(filename) # rename() doesn't overwrite on Windows
os.rename(outfile.name, filename) # or just `os.replace` in Python 3.3+
",A,1
23040729,2014-04-13 08:50:26.303000+00:00,"You sorted all data, including the header, which means it is still there but perhaps in the middle of your resulting output somewhere.
This is how you'd sort a CSV on one column, preserving the header:
import csv
import operator

with open('rawforsankey.csv', ""rb"") as sankey:
    raw_reader = csv.reader(sankey)
    header = next(raw_reader, None)
    sorted_data = sorted(raw_reader, operator.itemgetter(0))

with open('newfiletest.csv', 'wb') as myfiletest:
    wr = csv.writer(myfiletest, quoting=csv.QUOTE_ALL)
    if header:
        wr.writerow(header)
    wr.writerows(sorted_data)

Just remember that sorting is done lexicographically as all columns are strings. So 10 sorts before 9, for example. Use a more specific sorting key if your data is numeric, for example.",A,2
26998882,2014-11-18 16:15:54.030000+00:00,"No, you can't mix and match like this. You'll need to install a version of mod_wsgi that is compiled against Python 2.7.",A,4
9926167,2012-03-29 13:19:16.957000+00:00,"You've excluded book from the AuthorForm, so Django doesn't care what the value of cleaned_data['book_id'] is - it'll just ignore it.
Instead, you need to set the book property directly on the unsaved author instance:
    book = book_form.save()
    author = author.form.save(commit=False)
    author.book = book
    author.save()
",A,6
27850242,2015-01-08 21:57:41.780000+00:00,"Raising an exception in one case is a pretty meaningful difference. If trying to remove an element from a set that is not there would be an error, you better use set.remove() rather than set.discard().
The two methods are identical in implementation, except that compared to set_discard() the set_remove() function adds the lines:
if (rv == DISCARD_NOTFOUND) {
    set_key_error(key);
    return NULL;
}

This raises the KeyError. As this is slightly more work, set.remove() is a teeniest fraction slower; your CPU has to do one extra test before returning. But if your algorithm depends on the exception then the extra branching test is hardly going to matter.",A,20
19575497,2013-10-24 20:02:07.817000+00:00,"This is trivial with the csv module:
with open('output.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(data)

You already have the header in data as the first row; you can write all rows in one go with the writer.writerows() method. That's all there is to it, really.",A,13
50852386,2018-06-14 07:56:21.583000+00:00,"You don't access model instance attributes via dictionary notation. You need to use dot notation, as you have done everywhere else.
image.design_id = string
",A,1
17682694,2013-07-16 17:01:12.513000+00:00,"That's not something you do as a field. Even if that syntax worked, it would only give the value when the class was defined, not at the time you access it. You should do this as a method, and you can use the @property decorator to make it look like a normal attribute.
@property
def name(self):
    return ''.join(
        [self.lastname,' ,', self.firstname, ' ', self.middlename])

self.lastname etc appear as just their values, so no need to call any other method to convert them.",A,61
2954340,2010-06-02 00:51:36.090000+00:00,"Stored procedures are -- often -- a complete waste of effort.
When in doubt, actually measure the performance.  You'll often find that stored procedures add complexity for no recognizable benefit.  You may have no performance enhancement from your SP's.
Some folks think they're very ""Important"".  It's essential to actually measure performance rather than quibble or debate.",A,0
14470399,2013-01-23 00:16:48.187000+00:00,"If you can open all 1445 output files at once, this is pretty easy:
paths = ['abc{}.dat'.format(i) for i in range(1445)]
files = [open(path, 'w') for path in paths]
for inpath in ('input{}.dat'.format(i) for i in range(40000)):
    with infile as open(inpath, 'r') as infile:
        for linenum, line in enumerate(infile):
            files[linenum].write(line)
for f in files:
    f.close()

If you can fit everything into memory (it sounds like this should be about 0.5-5.0 GB of data, which may be fine for a 64-bit machine with 8GB of RAM…), you can do it this way:
data = [[] for _ in range(1445)]
for inpath in ('input{}.dat'.format(i) for i in range(40000)):
    with infile as open(inpath, 'r') as infile:
        for linenum, line in enumerate(infile):
            data[linenum].append(line)
for i, contents in enumerate(data):
    with open('abc{}.dat'.format(i), 'w') as outfile:
        outfile.write(''.join(contents)

If neither of these is appropriate, you may want some kind of hybrid. For example, if you can do 250 files at once, do 6 batches, and skip over batchnum*250 lines in each infile.
If the batch solution is too slow, at the end of each batch in each file, stash infile.tell(), and when you come back to the file again, use infile.seek() to get back there. Something like this:
seekpoints = [0 for _ in range(40000)]
for batch in range(6):
    start = batch * 250
    stop = min(start + 250, 1445)
    paths = ['abc{}.dat'.format(i) for i in range(start, stop)]
    files = [open(path, 'w') for path in paths]
    for infilenum, inpath in enumerate('input{}.dat'.format(i) for i in range(40000)):
        with infile as open(inpath, 'r') as infile:
            infile.seek(seekpoints[infilenum])
            for linenum, line in enumerate(infile):
                files[linenum].write(line)
            seekpoints[infilenum] = infile.tell()
    for f in files:
        f.close()
",A,3
25343965,2014-08-16 21:03:06.377000+00:00,"The documentation is very explicit about this:

filter() will always give you a QuerySet, even if only a single object matches the query - in this case, it will be a QuerySet containing a single element.
If you know there is only one object that matches your query, you can use the get() method on a Manager which returns the object directly.

Your first snippet returns a QuerySet, which has an update method. The second snippet returns a model instance, which doesn't.
Note that you have not shown the exact code you are using: thing[0].update would give exactly the same error as the second snippet.",A,3
5515042,2011-04-01 14:52:16.253000+00:00,"
When I try to picture how the Strategy Pattern fits this scenario it looks very similar.

Similar?  It should look identical.
The distinction is one of how the context and delegation works.  In principle a Command is the ""active"" agent.  A Strategy is injected into some active agent. That distinction is pretty subtle.
It barely changes the design.  What does change is the expectation.  
Command objects (more-or-less) stand alone.  They're built to do their work, and then they can vanish.  No one cares about them any more.  Perhaps they also use the Memento pattern, and have some future life, but perhaps not.
Strategy objects (more-or-less) live with the object into which they're injected.  A Strategy would be part of some larger object, and could be replaced by a different implementation without breaking or changing anything else.
But the essential interface is largely the same.

In most examples the strategy is a simple object with a single method, 

Those are poor examples.

however in my case the strategy will need a reference to the transaction as well as validation parameters.

Not unusual.  Nothing wrong with it.  ",A,1
4924237,2011-02-07 17:14:58.803000+00:00,"If I understand you correctly, you're not understanding that ForeignKeys and ManyToManyFields return different things.
A ForeignKey is a one-to-many relationship, with the 'one' on the side that it's pointing to. That means that if you defined default_project as a ForeignKey, self.default_project returns a single Project instance which you can use and assign as any other instance.
However, a ManyToManyField - as the name implies - has ""many"" relationships on both sides. So self.project_assignments doesn't return a single instance, it returns a QuerySet, which is the way Django handles lists of instances retrieved from the database. So you can use add and remove to manipulate that list, or slice it to get a single instance.
For example, if you wanted to set the default_project FK to the first project in the project assignments list, you would do:
self.default_project = self.project_assignments.all()[0]

(although in real code you would have to guard against the probability that there are no assignments, so that would raise an IndexError).",A,4
25449388,2014-08-22 14:28:22.410000+00:00,"There is no better way; you can name the variable _ to indicate it is ignored:
''.join(random.choice(y) for _ in xrange(x))

_ is just a convention; experienced programmers reading your code will understand that it signifies 'not used' here, Python doesn't care either way.
From a performance perspective, using a list comprehension here happens to be faster:
''.join([random.choice(y) for _ in xrange(x)])

because the implementation requires two scans to first determine the output length first; this double scan means any generator expression is turned into a list anyway. Using a list comprehension here short-cuts that conversion and is faster.",A,4
30502635,2015-05-28 09:34:06.993000+00:00,"You shouldn't be using the generic views for this, that's not what they are for. If you want to render a template, you should use the render shortcut: it takes exactly the same arguments.
return render(request, 'bprofile/init.html', targs)
",A,2
25959082,2014-09-21 12:17:26.763000+00:00,"The reason attempt 1 fails is basic Django query syntax: you must use __lt for comparisons:
Session.objects.filter(expire_date__lt=datetime.now()).delete()

Attempt 2 can never work, as the session model has no relationship with the user one.",A,2
7827338,2011-10-19 19:57:04.363000+00:00,"jython can't import *.java file you need to compile it to *.class.
Makefile:
.PHONY: test_beach

test_beach: test_beach.py beach.jar
    jython -J-classpath beach.jar $<

beach.jar: Beach.class
    jar -cf $@ $<

%.class: %.java
    javac $<

Example
$ make -k 
javac Beach.java
jar -cf beach.jar Beach.class
jython test_beach.py
*sys-package-mgr*: processing modified jar, '/path/to/beach.jar'
Cocoa Beach

Other files
test_beach.py:
#!/usr/bin/env jython
import Beach

beach = Beach(""Cocoa Beach"",""Cocoa Beach"")
print beach.getName()

Beach.java:
//NOTE: if you declare `package a.b;` here then you should put it in a/b directory
public class Beach {

    private String name;
    private String city;


    public Beach(String name, String city){
        this.name = name;
        this.city = city;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getCity() {
        return city;
    }

    public void setCity(String city) {
        this.city = city;
    }

}
",A,7
13955197,2012-12-19 15:04:13.057000+00:00,"The \ character is used to form character escapes; \f has special meaning.
Use / or use raw string r'' instead.  Alternatively, you could ensure that Python reads the backslash as a backslash by escaping it with an additional \.
r'D:\ful_automate\dl'
'D:\\ful_automate\\dl'
'D:/ful_automate/dl'

Demo to show the difference:
>>> 'D:\ful_automate\dl'
'D:\x0cul_automate\\dl'
>>> r'D:\ful_automate\dl'
'D:\\ful_automate\\dl'
",A,16
33844700,2015-11-21 14:50:59.273000+00:00,"You replaced the time module you imported at the top with a floating point value here:
time = end - start

In the next iteration of your while loop, time is now a float object, and not the module object, so your time.time() call fails.
Rename that variable to something that doesn't clash:
elapsed_time = end - start
print(""You took"", elapsed_time, ""seconds to guess."")

As a side note, you don't need to use a sentinel variable (Exit); just use while True: and exit the loop with a break statement:
while True:
    # ...

    if Guess == 'quit':
        break

    # ...

In other circumstances, you'd use while not Exit: rather than test for == False.",A,3
23410921,2014-05-01 15:30:59.583000+00:00,You should remove the ?nav=2 from the URL you're posting to.,A,0
14224395,2013-01-08 21:33:06.423000+00:00,"No, but it's pretty easy to make one:
def myrange(start, step, count):
    return range(start, start + (step * count), step) 

short demonstration:
>>> myrange(10, 2, 5)
[10, 12, 14, 16, 18]
>>> myrange(10, -2, 5)
[10, 8, 6, 4, 2]

In python 3 it'll return a range object, just like the regular range() function would:
>>> myrange(10, 2, 5)
range(10, 20, 2)
>>> list(myrange(10, 2, 5))
[10, 12, 14, 16, 18]
",A,6
36105902,2016-03-19 19:04:15.120000+00:00,"Your function addToInventory() returns None, which you assign to inv:
inv = addToInventory(inv, dragonLoot)

Now inv is set to None and you pass that to displayInventory().
Add return inv at the end of addToInventory().
Remember, printing is not the same thing as returning; printing writes data to your terminal, the caller of a function doesn't receive that data.",A,2
52895197,2018-10-19 15:08:37.343000+00:00,"No, html.unescape() is not configurable. I'd just re-escape using html.escape() plus a manual transformation of U+00A0 NO-BREAK SPACE codepoints:
import html

def reescape(s):
    return html.escape(html.unescape(s)).replace('\xa0', '&nbsp;')

Note that this also escapes & ampersands, because otherwise any input with '&amp;lt;' becomes '&lt;', which is indistinguishable from input that used '&lt;' (where the lt; part can be any valid HTML entity minus the ampersand).
Demo:
>>> sample = '''\
... The sensitivity of&nbsp;different micro-organisms to&nbsp;heat varies,
... but if&nbsp;water is&nbsp;held at&nbsp;70&nbsp;&deg;C (158&nbsp;&deg;F)
... for ten minutes, many organisms are killed &lt;...&gt;.
... However, &laquo;Test&raquo; allows...
... '''
>>> print(reescape(sample))
The sensitivity of&nbsp;different micro-organisms to&nbsp;heat varies,
but if&nbsp;water is&nbsp;held at&nbsp;70&nbsp;°C (158&nbsp;°F)
for ten minutes, many organisms are killed &lt;...&gt;.
However, «Test» allows...
",A,0
20576150,2013-12-13 21:16:49.587000+00:00,"As @Warren Weckesser's comment says, your problem is unrelated to buffering issues.
.readline() in the parent process won't return until it reads a newline or reaches EOF. Your child process doesn't print any newlines at all so your parent process doesn't print anything until the child process ends.
The minimal fix is just to remove comma at the end of print i, in the child script.
This also works:
#!/usr/bin/env python
import sys
from subprocess import Popen, PIPE

p = Popen([sys.executable or 'python',
           '-u', # unbuffer stdout (or make it line-buffered on Python 3)
           '-c',
           """"""
import time

for i in range(5):
    print(i) # <-- no comma i.e., each number is on its own line
    time.sleep(1)
""""""], stdout=PIPE, bufsize=1)
for line in iter(p.stdout.readline, b''):
    print(int(line)**2)

Example:
 $ python parent.py
 0
 1
 4
 9
 16

The numbers are printed every seconds without waiting for the child process to end.
If you don't want to change the child script then you should use readline() that stops at whitespace instead of a newline character e.g.:
#!/usr/bin/env python
import sys
from subprocess import Popen, PIPE

p = Popen(['python2', 'child.py'], stdout=PIPE, bufsize=0)
for token in generate_tokens(p.stdout):
    print(int(token))

where generate_tokens() yields whitespace-separated tokens:
def generate_tokens(pipe):
    buf = []
    while True:
        b = pipe.read(1) # read one byte
        if not b: # EOF
            pipe.close()
            if buf:
                yield b''.join(buf)
            return
        elif not b.isspace(): # grow token
            buf.append(b)
        elif buf: # full token read
            yield b''.join(buf)
            buf = []

It also prints integers as soon as they are printed by the child.",A,8
9997913,2012-04-03 16:58:34.193000+00:00,"You simply can't do that on a non-custom field, because there is no actual ""field"": it's a simple attribute, with no class behind it.
You'll need to define a custom field class and override to_python - don't forget to set the metaclass to SubfieldBase.",A,1
27825218,2015-01-07 17:36:02.967000+00:00,"I don't understand your second option, but iterating a list is O(n) and you must iterate the list to convert it to a set. Any operation on each element - eg hashing - is a constant factor which is dominated by the linear time of iteration.",A,1
33641235,2015-11-10 22:55:40.593000+00:00,"
I want line to be the output of perl file.pl < some_file.txt

#!/usr/bin/env python
from subprocess import check_output

with open('some_file.txt', 'rb', 0) as file:
    line = check_output(['perl', 'file.pl'], stdin=file)

To fix Can't open perl script ""file.pl"": No such file or directory, pass the full path to file.pl e.g., if it is in the same directory as your Python script then you could use get_script_dir() function, to get the path.",A,0
283684,2008-11-12 11:38:53.710000+00:00,"First, 4 hex digits aren't 4 bytes.  Since all your examples show 4 hex digits -- 2 bytes -- I'll assume you mean 2 bytes.
There are only 65,536 distinct hash values, here's what you do.
Execute the hash function for all 65,536 values from 0000 to FFFF.  Tabulate the results.  That table is the function.  It maps input value to output value.
While lame, it's always correct, it's not terribly big (65K bytes), and it's really fast after you've done the calculation.
You can't reverse engineer hash functions very easily.  The good ones are sophisticated state machines that use all of the input bits in some ""fair"" way so that the output values are dramatically different for input values that differ by only a few bits.
If you compare 0000 with 0001, 0002, 0004, 0008, 0010, 0020, 0040, 0080, 0100, 0200, 0400, 0800, 1000, 2000, 4000 and 8000, you might be able to figure out what each bit contributes to the hash.  But I doubt it.",A,4
36261885,2016-03-28 12:01:05.573000+00:00,"Both the key and msg arguments to hmac.new() must be bytes objects. You are feeding it a str object, as that is what the urllib.parse.urlencode() function produces:

Convert [...] to a percent-encoded ASCII text string. If the resultant string is to be used as a data for POST operation with the urlopen() function, then it should be encoded to bytes, otherwise it would result in a TypeError.

So not only should your self.Secret be bytes, you need to encode the post_data element too, even more so since you'll be passing it to urlopen() for a POST operation:
post_data = urlencode(args).encode('ASCII')
sign = hmac.new(self.Secret, post_data, hashlib.sha512).hexdigest()
headers = {'Sign': sign, 'Key': self.APIKey}
ret = urlopen(Request(url, post_data, headers))
",A,2
27252939,2014-12-02 15:25:25.553000+00:00,"If the if LOGGING_ACTIVATED: test happens at the module level, you need to make sure that that module is not yet imported first. Module-level code runs just once (the first time the module is imported anywhere), you cannot test code that won't run again.
If the test is in a function, note that the global name used is LOGGING_ACTIVATED, not config.LOGGING_ACTIVATED. As such you need to patch out main.LOGGING_ACTIVATED here:
@patch(""main.LOGGING_ACTIVATED"", True)

as that's the actual reference you wanted to replace.
Also see the Where to patch section of the mock documentation.
You should consider refactoring module-level code to something more testable. Although you can force a reload of module code by deleting the module object from the sys.modules mapping, it is plain cleaner to move code you want to be testable into a function.
So if your code now looks something like this:
if LOGGING_ACTIVATED:
    amqp_connector = Connector()

consider using a function instead:
def main():
    global amqp_connector
    if LOGGING_ACTIVATED:
        amqp_connector = Connector()

main()

or produce an object with attributes even.",A,29
50991287,2018-06-22 15:32:03.577000+00:00,"You registered two different events with apscheduler, which in your configuration stores your event callback information in a database in a serialized form:

Job stores house the scheduled jobs. The default job store simply keeps the jobs in memory, but others store them in various kinds of databases. A job’s data is serialized when it is saved to a persistent job store, and deserialized when it’s loaded back from it.

Bold emphasis mine.
The two jobs you have are executed independently, so they were deserialized independently leading to two new instances being created from the serialized data.
You can't count on separate events to act on the same instance. You'll need to externalize your state, by adding enough identifying information to recreate the state when the job fires.",A,1
44977235,2017-07-07 18:11:57.577000+00:00,"If all you care about is the check for abstract methods, then yes. Just move the abstract method set computation to the __init_subclass__ method:
def __init_subclass__(cls, **kwargs):
    super().__init_subclass__(**kwargs)
    # Compute set of abstract method names
    abstracts = {name
                 for name, value in vars(cls).items()
                 if getattr(value, ""__isabstractmethod__"", False)}
    for base in cls.__bases__:
        for name in getattr(base, ""__abstractmethods__"", set()):
            value = getattr(cls, name, None)
            if getattr(value, ""__isabstractmethod__"", False):
                abstracts.add(name)
    cls.__abstractmethods__ = frozenset(abstracts)

The base object.__new__ implementation then uses an non-empty __abstractmethods__ set to prevent instantiation. 
But ABCs also support virtual subclass registration; the two hook methods this requires have to be implemented on the metaclass.",A,3
36202208,2016-03-24 14:10:08.247000+00:00,"The double underscore is the way to go.
books = Product.objects.filter(category__collection='books')
",A,3
9140543,2012-02-04 11:34:53.850000+00:00,"So many things wrong with that snippet... 
Anyway:
memarr = [a['fields'] for a in data]
",A,0
3442041,2010-08-09 16:19:10.653000+00:00,"Add the field to the readonly_fields tuple as well.
Note this only works in Django 1.2+.",A,32
23116288,2014-04-16 17:32:34.413000+00:00,"You never did anything with the selected data; print the rows by iterating over the cursor after executing a query:
query = 'show tables'
cursor.execute(query)
for row in cursor:
    print row
",A,1
26388741,2014-10-15 17:41:59.140000+00:00,"By calling max, you're asking it to compare all the values.
In Python 2.x, most values can be compared to each other, even if they're of different types; the comparison will be meaningful in some arbitrary and implementation-specific way (in CPython, it mostly comes down to comparing the names of type objects themselves), but that's rarely if ever useful to you.
In Python 3.x, most values of unrelated types can't be compared to each other, so you'd just get a TypeError instead of a useless answer. But the solution is the same.
If you want to compare the numbers and ignore the names, you can filter out all non-numbers, skip every even element, use a key function that converts all non-numbers to something smaller than any number, or almost anything else that avoids trying to compare the names and the numbers. For example:
if max(self.thislist[1::2]) > 150:


As a side note, using data structures like this is going to make a lot of things more complicated. It seems like what you really want here is not a list of alternating names and numbers, but a dict mapping names to numbers, or a list of name-number pairs, or something similar. Then you could write things more readably. For example, after this:
self.thisdict = dict(zip(self.thislist[::2], self.thislist[1::2]))

… you can do things like:
if max(self.thisdict.itervalues()) > 150:
",A,5
34943247,2016-01-22 09:31:42.463000+00:00,"cl isn't a general thing; it's the name of a variable, so it wouldn't be in the documentation. In this particular case, it's a variable holding a Changelist object, but only because that is what is passed in to the template context.",A,2
29107241,2015-03-17 18:44:14.623000+00:00,"The following is much more concise, using datetime.date() objects to find the first day of the next month each time, until you reach the end date:
from datetime import datetime, timedelta

def genDatePeriods(startDate, endDate, format='%Y-%m-%d'):
    curr = datetime.strptime(startDate, format).date()
    end = datetime.strptime(endDate, format).date()

    while curr <= end:
        # first day of the next month, using modular arithmetic
        next_month = curr.replace(
            month=curr.month % 12 + 1, year=curr.year + curr.month // 12,
            day=1)
        curr_formatted = curr.strftime(format)
        # end date is next month's first day, minus one day,
        # or the given endDate, whichever comes first
        end_formatted = min(next_month - timedelta(days=1), end).strftime(format)
        yield [curr_formatted, end_formatted]
        curr = next_month

Demo:
>>> for res in genDatePeriods('2014-01-15', '2015-02-03'):
...     print res
... 
['2014-01-15', '2014-01-31']
['2014-02-01', '2014-02-28']
['2014-03-01', '2014-03-31']
['2014-04-01', '2014-04-30']
['2014-05-01', '2014-05-31']
['2014-06-01', '2014-06-30']
['2014-07-01', '2014-07-31']
['2014-08-01', '2014-08-31']
['2014-09-01', '2014-09-30']
['2014-10-01', '2014-10-31']
['2014-11-01', '2014-11-30']
['2014-12-01', '2014-12-31']
['2015-01-01', '2015-01-31']
['2015-02-01', '2015-02-03']
",A,1
16879994,2013-06-02 05:14:23.410000+00:00,"You're not running an event loop anywhere. Instead, you're just initializing everything, and then going to sleep for 10 seconds. During that 10 seconds, your code is doing nothing, because that's what you told it to do. That means no updating the screen, responding to mouse clicks, or anything else.
There are a few different ways to drive pygame, but the simplest is something like this:
while True:
    for event in pygame.event.get():
        if event.type == pygame.QUIT: sys.exit()
        # any other event handling you need
    # all the idle-time stuff you want to do each frame
    # usually ending with pygame.display.update() or .flip()

See the tutorial for more information.

As a side note, your initialization code has a bunch of problems. You iterate through three drivers, but you only set SDL_VIDEODRIVER once, so you're just trying 'fbcon' three times in a row. Also, you've got code to detect the X display, but you don't allow pygame/SDL to use X, so… whatever you were trying to do there, you're not doing it. Finally, you don't need a found flag in Python for loops; just use an else clause.",A,2
50633578,2018-05-31 22:18:28.743000+00:00,"It looks like you printed out a dict, not a string. If so, you definitely don't want to convert it that dict to its string representation, and then try to convert that string representation to a JSON string. You just want to encode the dict to JSON directly.
The only obvious tricky bit is converting Decimal values to something JSON can handle. You have to decide what you want—maybe just convert to float, maybe convert to float but also verify that you're not losing any precision in doing so, maybe convert to string, or maybe even convert to some special pickle-ish thing like {'type': 'decimal.Decimal', 'args': ['0E-8']}. Which one you want depends entirely on how you're planning to use the JSON later.
Once you've decided what you want to do, it's pretty easy. The docs for json.Encoder have an example showing how to support arbitrary iterators by converting them to lists, which is pretty easy to adapt to, say, support Decimals by converting them to floats:
class DecimalEncoder(json.Encoder):
    def default(self, o):
        if isinstance(o, decimal.Decimal):
            return float(o)
        return super().default(o)

enc = DecimalEncoder()
json_string = enc.encode(big_dict_with_decimals)

I haven't tested against your dict, or laboriously scanned through to see if there are any other weird types. But if there are, you handle them the same way as Decimal.",A,0
15958312,2013-04-11 20:30:55.960000+00:00,"As Joel Cornett says, you probably should be using dicts rather than lists in the first place.
But if you need the lists for some reason… well, if you're going to search through a list multiple times, you probably want to build a dict to search through:
d1 = {elem[0]: elem for elem in f1}

Then, instead of this:
for z in f3:
    if word == z[0]:

… you can just do this:
z = d3.get(word)
if z is not None:

You may also want to follow EAFTP and just try the whole thing. Your whole loop then looks  like this:
for word, score in d.iteritems():
    try:
        x, y, z = d1[word], d2[word], d3[word]
    except KeyError:
        continue
    A = x[2] * x[3]
    # etc.

This is assuming you specifically need three lists, as opposed to an arbitrary number. If you needed to be able to work with any number of lists, you'd do this:
list_of_dicts = [{elem[0]: elem for elem in lst} for lst in list_of_lists]
for word, score in d.iteritems():
    try:
        values = [d[word] for d in list_of_dicts]
    except KeyError:
        continue
    A = values[0][2] * values[0][3]
    # etc.


There are a few alternatives to this, but this is probably the one you want. 
You can sort each list and use bisect instead of an iterative linear search, or use something like SortedCollection to wrap that up for you, or blist.sortedlist for a similar type. This makes the search O(log N) instead of O(N), and makes the code simpler. But a dict makes the search O(1) instead of O(N), and makes the code exactly as simple as using a sorted list, so, unless you're dealing with keys that are not hashable (and you're not), why bother?
You can also wrap up the for/if by writing a find_in_list function, which gives you the same simplicity as a dict or sortedlist, but without the performance gains. This could be useful if the keys are neither hashable nor orderable, or if you had a huge number of tiny lists (so tiny that a linear search is actually faster than a dict or tree lookup—maybe around size 2-3?). But otherwise, you're just doing extra work (to write the find_in_list wrapper) to slow yourself down, so again, why bother?",A,4
25093021,2014-08-02 08:07:21.003000+00:00,"You have both a column and a property named area:
area = Column(Geometry('Polygon'))

# ...

def getAreaPoints(self):
    return self.getPoly().area
area = property(getAreaPoints)          

The latter replaced the former. As such, self.getPoly() uses the property when accessing self.area, and the property calls self.getPoly() again.
Rename either the column or the property if you wanted to use the column in the getPoly() method.",A,1
40951302,2016-12-03 18:49:26.020000+00:00,"It entirely depends on where first.js exists in relation to second.js in the HTML.
The browser has no knowledge of Django, or blocks, or templates. All it sees is the rendered HTML. If the content block is included in the parent template at a point that is before the one where first.js is included, then it will be loaded first.",A,3
14224943,2013-01-08 22:13:22.943000+00:00,"Python dictionaries have O(1) search complexity.
See the Time complexity wiki page.
Python dictionaries are implemented as a hash table, and keys are hashed; you can influence the hashing by implementing a __hash__ method.",A,2
15343947,2013-03-11 16:44:39.527000+00:00,"You are testing for the wrong thing, use:
if option.lower() == 'a':

or perhaps:
if option in ('A', 'a'):

The expression option == 'A' or 'a' is interpreted as:
(option == 'A') or 'a'

and a non-empty string is always considered True in python.",A,2
23201158,2014-04-21 16:12:41.730000+00:00,"By default, Python runs using C locale:
>>> from datetime import datetime
>>> datetime.strptime(""Tue Feb 11, 2014 5:38 PM"", ""%a %b %d, %Y %I:%M %p"")
datetime.datetime(2014, 2, 11, 17, 38)
>>> import locale
>>> locale.nl_langinfo(locale.T_FMT_AMPM)
'%I:%M:%S %p'

Changing locale partially helps on my system:
>>> locale.setlocale(locale.LC_TIME, 'es_ES.UTF-8')
'es_ES.UTF-8'
>>> datetime.strptime(""Lun Ene 27, 2014 9:52 am""[:-2], ""%a %b %d, %Y %I:%M %p"")
datetime.datetime(2014, 1, 27, 9, 52)
>>> locale.nl_langinfo(locale.T_FMT_AMPM)
''

T_FMT_AMPM is not set for some reason on my system for es_ES.UTF-8. To fix it, you could manually add 12 hours if the time string ends with 'pm'.
The strftime() and time behaviour  is the same.
Note: the locale name may be different on other systems.",A,0
3555378,2010-08-24 10:11:47.157000+00:00,"If you want to avoid the hideous drop-down selection widget that gets generated for choice fields, you can do this. 
Model:
variable = model.CharField(max_length=1)

Form:
def clean_variable(self):
    data = self.cleaned_data['variable']
    if data not in string.digits:
        raise forms.ValidationError(""Not a digit"")
    return data
",A,0
46609979,2017-10-06 16:19:54.210000+00:00,You have a stray comma at the end of CELERY_TASK_SERIALIZER. Remove it.,A,0
49455238,2018-03-23 17:19:42.497000+00:00,"The problem here is that your XPath starts off looking for an a child node under the root—but the root is the a node, and the b and d nodes are its children.
You can see this if you just print things out:
>>> print(tree)
<Element 'a' at 0x10bbe4bd8>
>>> print(tree.getchildren())
[<Element 'b' at 0x10bf8e318>,
 <Element 'd' at 0x10bf8ef98>,
 <Element 'd' at 0x10be1e818>]

What you probably want is this:
for elem in tree.iterfind('d/e/f'):

Or, as pointed out by Tim in the comments, if your tree is the result of parsing a file rather than the root of the tree, you’ll want to do this:
root = tree.getroot()
for elem in root.iterfind(‘d/e’f’):
",A,0
35058854,2016-01-28 10:29:19.857000+00:00,"You should not do this. Settings are for things that don't change; if your email is dynamic, you should set that in the view when you send the mail.",A,0
28747137,2015-02-26 16:04:21.683000+00:00,"Run it with sudo:
#!/usr/bin/env python
from subprocess import check_call

check_call(['sudo', '/usr/sbin/pm-hibernate'])

and configure the user that runs this script to be allowed to run pm-hibernate without a password e.g., create /etc/sudoer.d/pm-hibernate file and write %admin ALL = NOPASSWD: /usr/sbin/pm-hibernate in it (sudo visudo -f /etc/sudoers.d/pm-hibernate).
Make sure /etc/sudoers imports /etc/sudoers.d directory (sudo visudo) and /etc/sudoer.d/pm-hibernate file has 0440 permissions:
$ sudo chmod 0440 /etc/sudoer.d/pm-hibernate

see also sudo less /etc/sudoers.d/README.
The command allows all members of admin group to run sudo pm-hibernate without a password. Run groups, to see the groups you belong.",A,1
25631844,2014-09-02 20:29:42.023000+00:00,"You can simplify this by using the appropriate functions from os.path.
First, f you call normpath you no longer have to worry about both kinds of path separators, just os.sep (note that this is a bad thing if you're trying to, e.g., process Windows paths on Linux… but if you're trying to process native paths on any given platform, it's exactly what you want). And it also removes any trailing slashes.
Next, if you call basename instead of split, you no longer have to throw in those trailing [1]s.
Unfortunately, there's no equivalent of basename vs. split for splitext… but you can write one easily, which will make your code more readable in the exact same way as using basename.
As for the rest of it… regexp is the obvious way to strip out any digits (although you really don't need the + there). And, since you've already got a regexp, it might be simpler to toss the _ in there instead of doing it separately.
So:
def stripext(p):
    return os.path.splitext(p)[0]

for f in files:
    a = stripext(stripext(os.path.basename(os.path.normpath(f))))
    b = re.sub(r'[\d_]', '', a)

Of course the whole thing is probably more readable if you wrap if up as a function:
def process_path(p):
    a = stripext(stripext(os.path.basename(os.path.normpath(f))))
    return re.sub(r'[\d_]', '', a)

for f in files:
    b = process_path(f)

Especially since you can now turn your loop into a list comprehension or generator expression or map call:
processed_files = map(process_path, files)



I'm simply curious about the speed, as I'm under the impression compiled regex functions are very fast.

Well, yes, in general. However, uncompiled string patterns are also very fast.
When you use a string pattern instead of a compiled regexp object, what happens is this:

The re module looks up the pattern in a cache of compiled regular expressions.
If not found, the string is compiled and the result added to the cache.

So, assuming you don't use many dozens of regular expressions in your app, either way, your pattern gets compiled exactly once, and run as a compiled expression repeatedly. The only additional cost to using the uncompiled expressions is looking it up in that cache dictionary, which is incredibly cheap—especially when it's a string literal, so it's guaranteed to be the exact same string object every time, so its hash will be cached as well, so after the first time the dict lookup turns into just a mod and an array lookup.
For most apps, you can just assume the re cache is good enough, so the main reason for deciding whether to pre-compile regular expressions or not is readability. For example, when you've got, e.g., a function that runs a slew of complicated regular expressions whose purpose is hard to understand, it can definitely help to give each one of them a name, so you can write for r in (r_phone_numbers, r_addresses, r_names): …, in which case it would be almost silly not to compile them.",A,2
28762586,2015-02-27 10:15:00.350000+00:00,"If you want to ""inherit"" environment then you could use execfile() to run the module:
import os
import sys

# mangle sys.argv to pass arg1, arg2
# ...
# run
execfile('script.py') 
",A,0
6009124,2011-05-15 14:51:11.120000+00:00,"If you're sure you just want it in development, you can simply use print - the message appears in the console.
However, you will need to delete the print statement before you deploy to production, as this will cause an error with Apache.
A better long-term solution is to use the logging module, configured to output to the console.",A,4
20905486,2014-01-03 14:07:16.330000+00:00,"Use a separate list to track your numbers:
results = []
result = []
for num in inputlist:
    if sum(result) + num < 100:
        result.append(num)
    else:
        results.append([tuple(result), sum(result)])
        result = [num]
if result:
    results.append([tuple(result), sum(result)])

For your sample input, this produces:
[[(25, 50), 75], [(70,), 70], [(32, 10, 20), 62], [(50, 40), 90], [(30,), 30]]
",A,1
54465409,2019-01-31 16:47:35.977000+00:00,"This is new behaviour in Pandas 0.24, where subtracting Period() objects give you a DateOffset subclass.
You can get the numeric value from the DateOffset.n attribute:
from operator import attrgetter

df['duration_dataset'] = (
    df['date_1'].dt.to_period('M') -
    df['date_2'].dt.to_period('M')).apply(attrgetter('n'))

This produces
      date_1     date_2  duration_dataset
0 2018-03-31 2017-12-31                 3
1 2018-09-30 2017-12-31                 9

for your sample dataframe.
Rather than convert your dates to periods, you could instead convert them to a month count since the year 0, then subtract those numbers:
df['duration_dataset'] = (
    df['date_1'].dt.year * 12 + df['date_1'].dt.month - 1 -
    (df['date_2'].dt.year * 12 + df['date_2'].dt.month - 1)
)
",A,2
24092157,2014-06-06 23:46:19.120000+00:00,"The exception message actually offers you a hint. Compare the non-unpacking option:
>>> import sys
>>> sys.setrecursionlimit(4)  # to get there faster
>>> def f(): f()
... 
>>> f()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
RuntimeError: maximum recursion depth exceeded

with:
>>> def f(): f(*())
... 
>>> f()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
RuntimeError: maximum recursion depth exceeded while calling a Python object

Note the addition of the while calling a Python object. This exception is specific to the PyObject_CallObject() function. You won't see this exception when you set an odd recursion limit:
>>> sys.setrecursionlimit(5)
>>> f()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 1, in f
  File ""<stdin>"", line 1, in f
RuntimeError: maximum recursion depth exceeded

because that is the specific exception raised in the ceval.c frame evaluation code inside PyEval_EvalFrameEx():
/* push frame */
if (Py_EnterRecursiveCall(""""))
    return NULL;

Note the empty message there. This is a crucial difference.
For your 'regular' function (no variable arguments), what happens is that an optimized path is picked; a Python function that doesn't need tuple or keyword argument unpacking support is handled directly in the fast_function() function of the evaluation loop. A new frameobject with the Python bytecode object for the function is created, and run. This is one recursion check.
But for a function call with variable arguments (tuple or dictionary or both), the fast_function() call cannot be used. Instead, ext_do_call() (extended call) is used, which handles the argument unpacking, then uses PyObject_Call() to invoke the function. PyObject_Call() does a recursion limit check, and 'calls' the function object. The function object is invoked via the function_call() function, which calls PyEval_EvalCodeEx(), which calls PyEval_EvalFrameEx(), which makes the second recursion limit check.
TL;DR version
Python functions calling Python functions are optimised and bypass the PyObject_Call() C-API function, unless argument unpacking takes place. Both Python frame execution and PyObject_Call() make recursion limit tests, so bypassing PyObject_Call() avoids incrementing the recursion limit check per call.
More places with 'extra' recursion depth checks
You can grep the Python source code for Py_EnterRecursiveCall for other locations where recursion depth checks are made; various libraries, such as json and pickle use it to avoid parsing structures that are too deeply nested or recursive, for example. Other checks are placed in the list and tuple __repr__ implementations, rich comparisons (__gt__, __lt__, __eq__, etc.), handling the __call__ callable object hook and handling __str__ calls.
As such, you can hit the recursion limit much faster still:
>>> class C:
...     def __str__(self):
...         global depth
...         depth += 1
...         return self()
...     def __call__(self):
...         global depth
...         depth += 1
...         return str(self)
... 
>>> depth = 0
>>> sys.setrecursionlimit(10)
>>> C()()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 9, in __call__
  File ""<stdin>"", line 5, in __str__
RuntimeError: maximum recursion depth exceeded while calling a Python object
>>> depth
2
",A,18
40692403,2016-11-19 11:34:53.853000+00:00,"Bash caches name lookups; clear python3 from that cache:
hash -d python3

The next time you use python3 the PATH directories are searched again and the result is cached again.",A,1
45223340,2017-07-20 19:09:03.353000+00:00,"eval() only evaluates expressions. break and continue are not expressions, they are (simple) statements.
You can't run such statements from a variable here; the compiler needs to know up front where such statements are going to be used, because there are jump offsets involved in the bytecode. Just use if tests:
if decision == 'y':
    continue
elif decision == 'n':
    break
",A,4
28384434,2015-02-07 16:21:21.747000+00:00,"You can't do what you want, not if you are calling One.__init__ from Two.__init__.
If you want to alter what attributes are set, simply don't call One.__init__() here. Set your own attributes instead:
class One(object):
    def __init__(self, name):
        self.name = name


class Two(One):
    def __init__(self, other):
        self.other = other

Now self.name will never be set. This most likely will break the rest of functionality in One, something you probably don't want to do. The rest of the methods in that class are likely to rely on certain attributes having been set.
In OOP terms, if Two is not a special kind of One object, don't inherit from One. If Two is a kind of One object, don't try to make it into something else.",A,2
47968813,2017-12-25 12:49:32.723000+00:00,"The example in the documentation is outdated (it is a bug. You can help fixing it). It worked as described in Python 3.3. mock.has_data() should raise AttributeError in Python 3.4+:

Changed in version 3.4: The request methods add_data, has_data, get_data, get_type, get_host, get_selector, get_origin_req_host and is_unverifiable that were deprecated since 3.3 have been removed.

The documentation should use a method that exists on request.Request object (such as mock.has_header()). The point is to illustrate the difference between mock.nonexisting (raises AttributeError if request.Request has no such attribute) and mock.some_existing_attribute.some_nonexisting_attribute (the latter doesn't raise AttributeError even if mock.some_existing_attribute has no some_nonexisting_attribute attribute):
>>> from unittest.mock import Mock
>>> from urllib import request
>>> mock = Mock(spec=request.Request)
>>> mock.has_header()  # there is request.Request.has_header
<Mock name='mock.has_header()' id='174952746'>
>>> mock.nonexisting   # there is no request.Request.nonexisting
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/...python3.6/unittest/mock.py"", line 582, in __getattr__
    raise AttributeError(""Mock object has no attribute %r"" % name)
AttributeError: Mock object has no attribute 'nonexisting'
Mock object has no attribute 'nonexisting'
>>> mock.has_header.nonexisting   # no exception but there is no has_header.nonexisting
<Mock name='mock.has_header.nonexisting' id='249631353'>

In other words, spec is not recursive.",A,0
26368048,2014-10-14 18:44:10.247000+00:00,"list1 = ['a1', 'a2', 'a3']
list2 = ['b1', 'b2', 'b3']
list3 = ['c1', 'c2', 'c3']

for row in zip(list1, list2, list3):
    print("", "".join(row))

Output
a1, b1, c1
a2, b2, c2
a3, b3, c3

zip() returns an iterator in Python 3. You could use itertools.izip() on Python 2 as @Ashwini Chaudhary suggested.",A,1
439299,2009-01-13 15:16:12.933000+00:00,"""whether it's possible to run VMware inside VMware""  What?
I can run Windows with Sharepoint in a VMWare machine that's hosted somewhere.
Or, I can run Windows with Sharepoint in a WMWare machine that's actually a VMWare machine that's hosted somewhere.
Why on earth would I add a level of nesting?  Why not just go with Windows with Sharepoint hosted somewhere?  
You can have any number of VMWares running on a single host.  Lots of different versions doing lots of different things.
Nesting them doesn't make sense.",A,-1
18233635,2013-08-14 13:51:22.967000+00:00,"I'm not sure what you're confused about. C3 defines an  __init__ method that increases the value of self.w. So, both instances will get instance variables with a value of 4, whereas the class object itself has a class variable with the value of 3.",A,0
30265875,2015-05-15 17:55:26.630000+00:00,"You are probably getting an API error message. Use print resp.text to see what the server tells you is wrong here.
Note that you can always inspect resp.history to see if there were any redirects; if there were any you'll find a list of response objects.
Do not set the Host or Connection headers; leave those to requests to handle. I doubt the Origin or Referer headers here needed either. Since this is an API, the User-Agent header is probably also overkill.",A,4
32787533,2015-09-25 17:23:38.863000+00:00,"Because it's an AutoField. Your can't set those; they are automatically assigned by the database, hence the name. Therefore there's no point in showing then on the form.",A,1
22312503,2014-03-10 22:15:25.117000+00:00,"Don't forget to include the CSRF token in your form:
<form method=post action=""/home/location"">
    {{ form.csrf_token }}
    ....
    <button type=""submit"" class=""btn btn-primary"" value=""Submit"">
                    Directions
                </button>
</form>
",A,1
164978,2008-10-02 23:04:41.817000+00:00,"@dr_pepper
Remove literal duplicates in $PATH:
$ export PATH=$(perl -F: -ane'print join q/:/, grep { !$c{$_}++ } @F'<<<$PATH)

Print unique clean paths from %PATH% environment variable (it doesn't touch ../ and alike, replace File::Spec->rel2abs by Cwd::realpath if it is desirable) It is not a one-liner to be more portable: 
#!/usr/bin/perl -w
use File::Spec; 

$, = ""\n""; 
print grep { !$count{$_}++ } 
      map  { File::Spec->rel2abs($_) } 
      File::Spec->path;
",A,4
26396470,2014-10-16 04:56:07.543000+00:00,"Here's memory-efficient version that produces one key/value pair at a time:
def left_outer_join(keys, pairs, default=None):
    right = iter(pairs)
    right_key = float('-inf') # sentinel: any left key must be larger than it
    for left_key in keys:
        if left_key == right_key: # *keys* and *right* are in sync
            value = right_value  # from previous iteration
        elif left_key < right_key: # *keys* is behind *right*
            value = default
        else: # left_key > right_key: *keys* is ahead of *right*
            for right_key, right_value in right: # catch up with *keys*
                if left_key <= right_key: # drop while left_key > right_key
                    break
            value = right_value if left_key == right_key else default
        yield left_key, value

It is O(n+m) single pass algorithm. 
Example:
left_sorted_list = [1, 2, 3, 4, 5]
right_sorted_list = [[2, 21], [4, 45], [6, 67]]
print(list(left_outer_join(left_sorted_list, right_sorted_list)))
# -> [(1, None), (2, 21), (3, None), (4, 45), (5, None)]

keys and pairs can be unlimited sorted iterators (such as produced by heapq.merge() function) of keys and key/value pairs correspondingly.",A,4
3141822,2010-06-29 14:46:15.697000+00:00,"The findall may not create a list object.  If it is some kind of generator function, then it has a value which is ""consumed"" by traversing the results once.
After consuming the results yielded by this function, there are no more results.
tMatchList = self._testReplacePDFTag.findall(lines)

p = self._pdfPathRegex.search(lines)
print tMatchList   #tMatchList is printing just fine here if it has any elements
if p:
    print tMatchList #now it's empty, 

Try this.
tMatchList = list( self._testReplacePDFTag.findall(lines) )
",A,0
25263958,2014-08-12 12:11:37.857000+00:00,"The backslash is the other slash, \:
>>> print('This is a test \n This is a test')
This is a test 
 This is a test

You are using a forward slash, /, instead.
Also see the backslash tag wiki excerpt:

The backslash character \ (not to be confused with slash /)...
",A,4
22916930,2014-04-07 15:36:54.363000+00:00,"Don't use url_for(); that's for generating URLs, not file paths.
Instead, use send_from_directory() with the static directory as the root:
from flask import app, send_directory

@app.route('/pic/<string:address>', methods= [""GET""])
def here(address=None):
    return send_from_directory(app.static_folder, 'images/123_Octavia_St.jpg', mimetype='image/jpg')

or, for static files, just reuse the view Flask uses for serving static files itself, app.send_static_file():
from flask import app

@app.route('/pic/<string:address>', methods= [""GET""])
def here(address=None):
    return app.send_static_file('images/123_Octavia_St.jpg')
",A,4
20105624,2013-11-20 19:54:13.883000+00:00,"raw_input is a function. status == raw_input == 'y' will never be true: that is comparing status with the function, and with 'y'.
I suspect that's simply a typo, and you just meant if status == 'y': ",A,3
2366642,2010-03-02 20:51:25.280000+00:00,"You may like this better: http://docs.python.org/library/linecache.html
""In case of a crash I'm marking my position in another file with the tell() method of the file.""
Good.
""Is there a way to mark the line and be able to go back to that line position""
That's what you are doing.  You're marking the line with tell.  The tell value is the position used by seek.  
If it helps, pretend you don't know what the units are.  Pretend that tell is just a ""magic"" key that finds the proper line.",A,2
19435171,2013-10-17 19:09:28.207000+00:00,"There are two reasons you don't want a message box here.
First, the whole point of a message box is that it's a modal dialog with some standardized buttons, and you don't want those buttons.
Second, the whole point of a modal dialog is that it's modal—it runs its own event loop, and doesn't return until the dialog is dismissed. This means (unless you're using background threads) your app can't do anything while displaying it.

The first problem is easy to solve. tkMessageBox is just a simple wrapper around tkCommonDialog.Dialog. It's worth looking at the source to see just how simple it is to construct a dialog box that does what you want. But tkSimpleDialog.Dialog is even simpler than tkCommonDialog (hence the name). For example:
class WaitDialog(tkSimpleDialog.Dialog):
    def __init__(self, parent, title, message):
        self.message = message
        Dialog.__init__(self, parent, title=title, message=message)
    def body(self, master):
        Label(self, text=self.message).pack()
    def buttonbox(self):
        pass

def wait(message):
    WaitDialog(root, title='Wait', message=message)

That's all it takes to create a modal dialog with no buttons. Dialog Windows and the source to tkSimpleDialog have more details.

The second problem is even easier to solve: If you don't want a modal dialog, then all you want is a plain old Toplevel. You may want it to be transient, so it stays on top of the master, hides with it, doesn't show up on the taskbar, etc., and you may want to configure all kinds of other things. But basically, it's this simple:
def wait(message):
    win = Toplevel(root)
    win.transient()
    win.title('Wait')
    Label(win, text=message).pack()
    return win

Now you can call wait() and continue to run:
def wait_a_sec():
    win = wait('Just one second...')
    root.after(1000, win.destroy)

root = Tk()
button = Button(root, text='do something', command=wait_a_sec)
root.mainloop()
",A,3
60697,2008-09-13 17:08:00.077000+00:00,"In Emacs:

kill-line C-k 
transpose-lines C-x C-t

C-a C-k C-k -- kill whole line including newline (or kill-whole-line by C-S-backspace).
C-u <number> C-k -- kill <number> of lines (including newlines).
C-y -- yank back the most recently killed text (aka paste)",A,4
25858265,2014-09-15 23:12:14.203000+00:00,"This program is going to be horribly inefficient no matter what you do. 
But you can fix this MemoryError so it'll just take forever to run instead of failing.
First, note that a 12-letter word has 479,001,600 permutations. Storing all of those in memory is going to take more than 2GB of memory. So, how do you solve that? Just don't store them all in memory. Leave the iterator as an iterator instead of making a list, and then you'll only have to fit one at a time, instead of all of them.
There's one problem here: You're actually using that list in the if l==i: line. But clearly that's a mistake. There's no way that a list of strings can ever equal a single string. You might as well replace that line with raise TypeError, at which point you can just replace the whole loop and fail a whole lot faster. :)
I think what you wanted there is if anagram in words:. In which case you have no need for l, except for in the for loop, which means you can safely leave it as a lazy iterator:
for i in words:
    l = map(''.join, itertools.permutations(i))
    l = (x for x in l if x != i)
    for anagram in l:
        if anagram in words:
            f2.write(i + ""\n"")

I'm assuming Python 3.x here, since otherwise the list call was completely unnecessary. If you're using 2.x, replace that map with itertools.imap.

As a side note, f.read(1000) is usually going to get part of an extra word at the end, and the leftover part in the next loop. Try readlines. While it's useless with no argument, with an argument it's very useful:

Read and return a list of lines from the stream. hint can be specified to control the number of lines read: no more lines will be read if the total size (in bytes/characters) of all lines so far exceeds hint.

So, f.readlines(1000) will let you read buffers of about 1K at a time, without getting partial lines. Of course now, instead of having to split on newlines, you have to rstrip them:
words = [line.rstrip('\n') for line in f.readlines(1000)]


However, you've got another problem. If you're only reading about 100 words at a time, the chances of finding an anagram are pretty slim. For example, orchestra is not going to be anywhere near carthorse in the dictionary, so there's no way to find unless you remember the entire file. But that should be fine; a typical Unix dictionary like web2 has around 200K lines; you an easily read that into memory and keep it around as a set without making even a dent on your 2GB. So:
words = set(line.rstrip('\n') for line in f)


Also, note that you're trying to print out every word in the dictionary that has an anagram (multiple times, if it has multiple anagrams). Even with an efficient algorithm, that's going to take a long time—and spew out more data than you could possibly want. A more useful program might be one that takes an input word (e.g., via input or sys.argv[1]) and outputs just the anagrams of that word.

Finally:

Even after using l as a generator it taking up too much off time though no failing with memory error. Can you explain the importance of words as a set rather than a list. [Finished in 137.4s] just for 200 bytes, you have mentioned it before, but how to overcome it using words as set?

As I said at the top, ""This program is going to be horribly inefficient no matter what you do.""
In order to find the anagrams of a 12-letter word, you're going through 479 million permutations, and checking each one against a dictionary of about 200 thousand words, so that's 479M * 200K = 95 trillion checks, for each word. There are two ways to improve this, the first involving using the right data structures for the job, and the second involving the right algorithms for the job.
Changing the collection of things to iterate over from a list into a generator (a lazy iterable) turns something that took linear space (479M strings) into something that takes constant space (some fixed-size iterator state, plus one string at a time). Similarly, changing the collection of words to check against from a list into a set turns something that takes linear time (comparing a string against every element in the list) into something that takes constant time (hashing a string, then seeing if there's anything in the set with that hash value). So, this gets rid of the * 200K part of your problem.
But you've still got the 479M part of the problem. And you can't make that go away with a better data structure. Instead, you have to rethink the problem. How can you check whether any permutation of a word matches any other words, without trying all the permutations?
Well, some permutation of the word X matches the word Y if and only if X and Y have the same letters. It doesn't matter what order the letters in X were in; if the set is the same, there is at least one matching permutation (or exactly one, depending on how you count duplicate letters), and if not, there are exactly 0. So, instead of iterating through all the permutations in the word to look up, just look up its set. But it does matter if there are duplicates, so you can't just use set here. You could use some kind of multi-set (collections.Counter) works… or, with very little loss in efficiency and a big gain in simplicity, you could just sort the letters. After all, if two words have the same letters in some arbitrary order, they have the same letters in the same order when they're both sorted.
Of course you need to know which words are anagrams, not just that there is an anagram, so you can't just look it up in a set of letter sets, you have to look it up in a dictionary that maps letter sets to words. For example, something like this:
lettersets = collections.defaultdict(set)
for word in words:
    lettersets[''.join(sorted(word))].add(word)

So now, to look up the anagrams for a word, all you have to do is:
anagrams = lettersets[''.join(sorted(word))]

Not only is that simple and readable, it's also constant-time.
And if you really want to print out the massive list of all anagrams of all words… well, that's easy too:
for _, words in lettersets.items():
    for word in words:
        print('{} is an anagram of {}'.format(word, ', '.join(words - {word})))

Now, instead of taking 479M*200K time to find anagrams for one word, or 479M*200K*200K time to find all anagrams for all words, it takes constant time to find anagrams for one word, or 200K time to find all anagrams for all words. (Of course there is 200K setup time added to the start to create the mapping, but spending 200K time up-front to save 200K, much less 479M*200K, time for each lookup is an obvious win.)
Things get a little trickier when you want to, e.g., find partial anagrams, or sentence anagarms, but you want to follow the same basic principles: find data structures that let you do things in constant or logarithmic time instead of linear or worse, and find algorithms that don't require you to brute-force your way through an exponential or factorial number of candidates.",A,5
26724805,2014-11-03 22:56:20.613000+00:00,"If you want to use shape, you either need to wrap it in try/except, or type-check first. (The former is usually better.)
Also, note that (1) is not a 1-element tuple, it's just the number 1. Tuples are made by commas, not by parentheses.
But there's a bigger problem here: shape doesn't exist on lists, tuples, or other collections, only on NumPy arrays. So, you'll need a second fallback to deal with them. You could use the built-in len function, which works on all sized containers, or you could use np.shape, which works on all containers that NumPy knows how to convert to arrays (while everything else will end up as a single-element scalar of dtype object, so its shape will be ()). I'm not sure which is appropriate in this case. (One question to help decide the answer: does a 10-element dict count as having length 10, or 0?)
Maybe something like this:
def f(x):
    try:
        if x.shape != (1,):
            # Error or exception
    except AttributeError:
        try:
            if len(x) != 1:
                # Error or exception
        except TypeError:
            pass

However, note that strings have a size—a string is an iterable over its individual characters—and yet, often, you want to treat them as singular values.
More generally, an API that can take either a single value or a collection of values is usually a sign of something wrong with the design elsewhere in the program. Why aren't you generating single-value collections in the first place? Sometimes there's no way around this, because you're accepting values from someone else's code, or directly from the end-user—but make sure that's really true before going out of your way to overcomplicate things.",A,1
19269161,2013-10-09 10:14:15.647000+00:00,"The package is trying to download a public suffix list from a URL that currently does not work:
http://mxr.mozilla.org/mozilla-central/source/netwerk/dns/effective_tld_names.dat?raw=1

This is due to a DDOS attack on that URL, Mozilla has blocked the URL for now.
This has already been reported to the project, and a fix has been proposed albeit that the latter only works if you already have a cached copy of the public suffix list.
In the meantime, use the publicsuffix package instead; it bundles the data in the package itself and does not require a URL request.
Update: Mozilla now host the file at https://publicsuffix.org/list/effective_tld_names.dat and any access to the MXR source repository without a mxr.mozilla.org Referer header redirects you to that new location.",A,2
49685126,2018-04-06 04:05:18.373000+00:00,"seek is always in terms of bytes,* not characters, even for files opened in text mode.
There's no way it could work remotely efficiently otherwise—the millionth character in a UTF-8 text file could be at byte 1,000,000 or at byte 2,739,184, and the only way to find out is to go back to the start and encode 999,999 characters.**
But read is only reading bytes if you're in binary mode; in text mode, those bytes are decoded to Unicode strings on the fly. (Since you're reading the file sequentially, this isn't usually a performance issue—but when it is, you've always got binary mode.)
If you have a known position you want to be able to return to, you can ""mark"" it by calling tell and then seek back to it later, but otherwise, seeking isn't very useful in text files, except to the start or end of the file of course.

* In fact, it's not even documented to be bytes for text files; anything other than 0 or ""an opaque number"" as returned by tell produces ""undefined behavior"". I believe it always will seek to the exact specified byte position—but because of the way the decoder pipeline works, this can cause mojibake even if you don't seek into the middle of a character, particularly with encodings that use shift codes. To handle these cases, tell makes special snapshots that can be restored on a later seek, but of course there is no snapshot for some random point in the file.
** That's not quite true—you could build up a table of offsets as you read along, or whenever you try to seek, and maybe even by reading ahead. But this definitely isn't something you'd want Python doing on every file just for the rare cases where you want to seek by character index; it's something you'd want to tune specifically to the uncommon case you care about. The linecache module—which is in the standard library because the debugger needs it—does roughly equivalent work, and comes with pretty readable source as long as you ignore the bits about the tokenizer, so if you want to build a character indexer yourself, it may be good sample code to get started.",A,4
39707686,2016-09-26 16:17:35.870000+00:00,"Like the error says, you can't assign to a function call. variable_name.lower() is a function call.
You've put the call in the wrong place. You want to call it on the result of input:
variable_name = input().lower()
",A,0
600195,2009-03-01 18:03:24.403000+00:00,"Remove prefix from a string
# ...
if line.startswith(prefix):
   return line[len(prefix):]

Split on the first occurrence of the separator via str.partition()
def findvar(filename, varname=""Path"", sep=""="") :
    for line in open(filename):
        if line.startswith(varname + sep):
           head, sep_, tail = line.partition(sep) # instead of `str.split()`
           assert head == varname
           assert sep_ == sep
           return tail

Parse INI-like file with ConfigParser
from ConfigParser import SafeConfigParser
config = SafeConfigParser()
config.read(filename) # requires section headers to be present

path = config.get(section, 'path', raw=1) # case-insensitive, no interpolation

Other options

str.split()
re.match()
",A,109
30718251,2015-06-08 20:13:43.450000+00:00,"If you can't change the output filenames e.g., by passing them as a parameter to the subprocess or by specifying the output directory as a parameter then try to run the subprocesses in a different directory:
from subprocess import check_call

check_call(args, cwd=""subdir"")

Make sure that args use absolute paths so that the input files would be found.
A better alternative is to import the modules and call specific functions  with necessary parameters instead.",A,0
21173225,2014-01-16 21:26:05.873000+00:00,"There is no WHERE part in an INSERT statement; INSERT only adds a new row, so there is nothing to limit what existing rows the statement applies to here.
You want to use an UPDATE instead:
cursor.execute(""UPDATE players SET wins=? WHERE username = ?"", (wins[3], players))
",A,2
30545727,2015-05-30 11:41:26.663000+00:00,"You never set self.root; you only rebound p. p is a separate variable, setting it will not set self.root.
Without self.root set, your tree remains empty.
Note that because None is a singleton, in Python you normally use is to test for the object. You made several other mistakes in your put() method that you want to correct, like creating circular references with p.right = p rather than insert the new tree node.
I've picked some different variable names to make it clearer what happens; newnode instead of tree and current instead of p:
def put(self, indata):
    newnode = BinTree(indata)

    if self.root is None:
        self.root = newnode
        return self.root

    current = self.root
    while True:
        if indata > current.item:
            # if bigger, go right
            if current.right is None:
                # if right slot is empty, make one
                current.right = newnode
                return newnode
            else:
                current = current.right
        elif indata < current.item:
            # if smaller, go left
            if current.left is None:
                # if left slot is empty, make one
                current.left = newnode
                return newnode
            else:
                # if left slot is full, go deeper
                current = current.left
        else:
            # if equal
            return False

I'd probably return None if the node already existed in the tree, rather than False; that way you can better test for the condition:
newnode = tree.put(datapoint)
if newnode is None:
    # already present in the tree, do something with that info

Your isempty method is otherwise correct; it could be simplified though, as the == and is operators already produce True or False; just return that outcome:
def isempty(self):
    return self.root is None

With these changes, the isempty() method works for me:
>>> class BinTree():
...     def __init__(self, item = None):
...         self.item = item
...         self.left = None
...         self.right = None
... 
>>> class Tree():
...     def __init__(self):
...         self.root = None
...     def put(self, indata):
...         newnode = BinTree(indata)
...         if self.root is None:
...             self.root = newnode
...             return self.root
...         current = self.root
...         while True:
...             if indata > current.item:
...                 # if bigger, go right
...                 if current.right is None:
...                     # if right slot is empty, make one
...                     current.right = newnode
...                     return newnode
...                 else:
...                     current = current.right
...             elif indata < current.item:
...                 # if smaller, go left
...                 if current.left is None:
...                     # if left slot is empty, make one
...                     current.left = newnode
...                     return newnode
...                 else:
...                     # if left slot is full, go deeper
...                     current = current.left
...             else:
...                 # if equal
...                 return False
...     def isempty(self):
...         return self.root is None
... 
>>> tree = Tree()
>>> tree.isempty()
True
>>> tree.put(5)
<__main__.BinTree object at 0x10a520cf8>
>>> tree.isempty()
False
",A,0
18064936,2013-08-05 18:17:14.167000+00:00,"Unless you need set attributes dynamically and bypass descriptors or the .__setattr__() hook, do not assign attributes directly to .__dict__.
Not all instances have a .__dict__ attribute even, not if the class defined a .__slots__ attribute to save memory.
If you do need to set attributes dynamically but don't need to bypass a descriptor or a .__setattr__() hook, you'd use the setattr() function normally.",A,10
36040155,2016-03-16 15:30:27.417000+00:00,".re.match() matches only at the start of a string. Use re.search() instead, see match() vs. search().
Note that you appear to misunderstand what a raw string literal is; r'%s' % string does not produce a special, different object. r'..' is just notation, it still produces a regular string object. Put the r on the original string literal instead (but if you use double quotes you do not need to quote the single quote contained):
regexp = r""'([\w\d\.\$\:\-\[\]\<\>]+).*\s([\w\d\.\$\:\-\[\]\<\>]+)""

For this specific regex it doesn't otherwise matter to the pattern produced.
Note that the pattern doesn't actually capture what you want to capture. Apart from the escaped ' at the start (which doesn't appear in your text at all, it won't work as it doesn't require dots and dollars to be part of the name. As such, you capture Could and b.a instead, the first and last words in the regular expression.
I'd anchor on the words class and method instead, and perhaps require there to be dots in the class name:
regexp = r'class\s+((?:[\w\d\$\:\-\[\]\<\>]+\.)+[\w\d\$\:\-\[\]\<\>]+).*method ([\w\d.\$\:\-\[\]\<\>]+)'

Demo:
>>> import re
>>> regexp = r'class\s+((?:[\w\d\$\:\-\[\]\<\>]+\.)+[\w\d\$\:\-\[\]\<\>]+).*method ([\w\d.\$\:\-\[\]\<\>]+)'
>>> msg = ""Could not find class android.app.Notification$Action$Builder, referenced from method b.a""
>>> re.search(regexp, msg, re.M | re.I)
<_sre.SRE_Match object at 0x1023072d8>
>>> re.search(regexp, msg, re.M | re.I).groups()
('android.app.Notification$Action$Builder', 'b.a')
",A,1
245295,2008-10-28 23:49:30.580000+00:00,"""When one says ""everything is an object"" (like in Python), does he indeed mean that ""everything is first-class""?""
Yes.
Everything in Python is a proper object.  Even things that are ""primitive types"" in other languages.
You find that an object like 2 actually has a fairly rich and sophisticated interface.
>>> dir(2)
['__abs__', '__add__', '__and__', '__class__', '__cmp__', '__coerce__', '__delattr__', '__div__', '__divmod__', '__doc__', '__float__', '__floordiv__', '__getattribute__', '__getnewargs__', '__hash__', '__hex__', '__index__', '__init__', '__int__', '__invert__', '__long__', '__lshift__', '__mod__', '__mul__', '__neg__', '__new__', '__nonzero__', '__oct__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__str__', '__sub__', '__truediv__', '__xor__']

Because everything's a first-class object in Python, there are relatively few obscure special cases.  
In Java, for example, there are primitive types (int, bool, double, char) that aren't proper objects.  That's why Java has to introduce Integer, Boolean, Double and Character as first-class types.  This can be hard to teach to beginners -- it isn't obvious why both a primitive type and an class have to exist side-by-side.
It also means that an object's class is -- itself -- an object.  This is different from C++, where the classes don't always have a distinct existence at run-time.
The type of 2 is the type 'int' object, which has methods, attributes and a type.
>>> type(2)
<type 'int'>

The type of a built-in type like int is the type 'type' object.  This has methods and attributes, also.
>>> type(type(2))
<type 'type'>
",A,16
17948641,2013-07-30 13:37:58.707000+00:00,"Quoting from the reference documentation:

Python evaluates expressions from left to right.

So yes, you can count on that (with one exception, see below).
A call (the (...) part after a primary, such as a function name) is just another expression primary, and the arguments for the call are just more expressions.
Note: There is one exception to this rule. When using *expression in a call (to expand an iterable to form additional positional arguments), then this expression is evaluated before any keyword argument expressions:
>>> from itertools import count
>>> def bar(n, r=(), c=count()): print(f'{next(c)}: bar({n!r})'); return r
...
>>> def foo(*args, **kwargs): pass
...
>>> foo(bar('a1'), spam=bar('a2'), *bar('varargs'), **bar('kwargs', {}))
0: bar('a1')
1: bar('varargs')
2: bar('a2')
3: bar('kwargs')

The linked documentation states:

A consequence of this is that although the *expression syntax may appear after explicit keyword arguments, it is processed before the keyword arguments[.]
",A,33
8652092,2011-12-28 05:09:55.280000+00:00,"General layout (not tested):
#!/usr/bin/env python
from contextlib import closing
from selenium.webdriver import Firefox # pip install selenium

url = ""http://example.com""

# use firefox to get page with javascript generated content
with closing(Firefox()) as browser:
    n = 1
    while n < 10:
        browser.get(url) # load page
        link = browser.find_element_by_link_text(str(n))
        while link:
           browser.get(link.get_attribute(""href"")) # get individual 1,2,3,4 pages
           #### save(browser.page_source)
           browser.back() # return to page that has 1,2,3,next -like links
           n += 1
           link = browser.find_element_by_link_text(str(n))

        link = browser.find_element_by_link_text(""next"")
        if not link: break
        url = link.get_attribute(""href"")
",A,10
30470252,2015-05-26 22:46:27.930000+00:00,"I think the simplest answer, especially if you don't care about Windows, is to just store the bytes in the environment, as suggested in my other answer.
But if you want something clean and debuggable, you might be happier using something designed as a text-based format.
pickle does have a ""plain text"" protocol 0, as explained in kindall's answer. It's certainly more readable than protocol 3 or 4, but it's still not something I'd actually want to read.
JSON is much nicer, but it can't handle datetime out of the box. You can come up with your own encoding (the stdlib's json module is extensible) for the handful of types you need to encode, or use something like jsonpickle. It's generally safer, more efficient, and more readable to come up with custom encodings for each type you care about than a general ""pack arbitrary types in a turing-complete protocol"" scheme like pickle or jsonpickle, but of course it's also more work, especially if you have a lot of extra types.
JSON Schema lets you define languages in JSON, similar to what you'd do in XML. It comes with a built-in date-time String format, and the jsonschema library for Python knows how to use it. 
YAML has a standard extension repository that includes many types JSON doesn't, including a timestamp. Most of the zillion 'yaml' modules for Python already know how to encode datetime objects to and from this type. If you need additional types beyond what YAML includes, it was designed to be extensible declaratively. And there are libraries that do the equivalent of jsonpickle, defining new types on the fly, if you really need that.
And finally, you can always write an XML language.",A,0
33805403,2015-11-19 13:33:40.803000+00:00,"document['rel'] appears to be a string: ""r/instanceOf"" in all the examples you posted. The find methods on strings returns the position of an item in the string, or -1 if it is not found; -1 is True in a boolean context.
You don't care about the position of the string anyway, so you shouldn't be using find. Use plain in:
if 'Synonym' in document['rel']:
    ...

if 'Related' in document['rel'] or 'Derived' in document['rel']:
",A,0
16322529,2013-05-01 16:46:23.360000+00:00,"It doesn't sound like you really wanted fileinput in the first place, since you're not trying to concatenate multiple files, handle the name - as ""put stdin here"", etc.
But if you do want fileinput, instead of trying to reproduce all of its behavior, just wrap it up.
You want to read all of the input into one string, but all it provides is functions that give you one line or one file at a time. So, what can you do? Join them together:
soup = BeautifulSoup(''.join(f))

That's it.",A,0
21805636,2014-02-16 00:25:48.373000+00:00,"You are using a relative path in your code, so the file datafromdb.json gets written to the current working directory. But a cronjob doesn't run your Python script in the same working directory as you would in the shell. See What is the 'working directory' when cron executes a job over on the Unix & Linux Stack Exchange website.
Use an absolute filepath instead; one that start with a / and spells out the full path to the file.",A,1
26372002,2014-10-14 23:19:17.310000+00:00,"If ""you want to pick with replacement (that is, each time there's a 1/3 chance of each)"":
#!/usr/bin/env python
import random

questions = [""question1"", ""question2"", ""question3""]
while True: # infinite loop, press Ctrl + C to break
    print(random.choice(questions)) 

""without replacement (that is, each one only shows up once, so after 3 choices there's nothing left)"":
#!/usr/bin/env python
import random

questions = [""question1"", ""question2"", ""question3""]
random.shuffle(questions)
while questions: # only `len(questions)` iterations
    print(questions.pop()) 

""or some hybrid (e.g., pick all 3 in random order, then repeat all 3 in random order again, etc."":
#!/usr/bin/env python
import random

questions = [""question1"", ""question2"", ""question3""]
while True: # infinite loop, press Ctrl + C to break
    random.shuffle(questions)
    for q in questions: # only `len(questions)` iterations
        print(q) 
",A,1
14339895,2013-01-15 14:29:29.563000+00:00,"You cannot do this with a property; you either have to use a classmethod, or create your own descriptor for that:
class classproperty(property):
    def __get__(self, inst, cls):
        return self.fget(cls)

I'm abusing the property decorator here; it implements __set__ and __del__ as well, but we can just ignore those here for convenience sake.
Then use that instead of property:
class Category(object):
    def __init__(self, name):
        self.name = name

    @classproperty
    def GOOD(cls):
        return cls(name='good')

    @classproperty
    def BAD(cls):
        return cls(name='bad')

Now accessing Category.GOOD works:
>>> Category.GOOD
<__main__.Category object at 0x10f49df50>
>>> Category.GOOD.name
'good'
",A,2
19910096,2013-11-11 15:35:47.920000+00:00,"They indeed do the same thing, but you cannot use a variable with the attribute setting syntax, while you can do that with setattr().
In other words, this works:
foo = 'count'
setattr(x, foo, 2)

but this doesn't:
foo = 'count'
x.foo = 2

Use setattr() for dynamic attribute setting, where the name of the attribute is taken from a variable. Use object.attributename = value for static attributes, where you know the name beforehand.
In addition, the attribute access syntax is limited to proper Python identifiers, while setattr() can use any string. This means that you can use:
setattr(x, '3 little piggies', 42)

where attempting that with static attributes will not work because Python identifiers do not allow for spaces, and cannot start with digits.",A,4
26818289,2014-11-08 14:47:55.380000+00:00,"This is exactly the sort of problem that generic relations were created to solve. The documentation on that link describes a tagging system, but it can just as easily be applied to likes.",A,3
18609639,2013-09-04 09:06:48.760000+00:00,"You'll have to name each parameter explicitly:
data = {'q.field':'project_id', 'q.value': 's56464dsf6343466'}

There is no standard for marshalling more complex structures into GET query parameters; the only standard that exists gives you key-value pairs. As such, the requests library doesn't support anything else.
You can of course write your own marshaller, one that takes nested dictionaries or lists and generates a flat dictionary or tuple of key-value pairs for you.",A,4
37769452,2016-06-11 23:42:42.983000+00:00,"You first joined all strings together into one big string, and then converted that string to a list, which always results in all the individual characters being pulled out as elements:
>>> list(map(lambda x: str(x * 5), range(1, 4)))
['5', '10', '15']
>>> ''.join(map(lambda x: str(x * 5), range(1, 4)))
'51015'
>>> list(''.join(map(lambda x: str(x * 5), range(1, 4))))
['5', '1', '0', '1', '5']

As you can see above, all you need to do is remove the str.join() call, just use list() directly on map():
list(map(lambda x: str(x * 5), range(1, 4)))
",A,10
36814255,2016-04-23 17:42:44.967000+00:00,"You are incrementing x each time you add x - 1 to it. So for 10 times 10 you start with 10, then add 10 - 1 = 9, making 19, and store that back in x. The next iteration you add 19 - 1 = 18 to x = 19, so now you have x = 37. Then you add 37 - 1 = 36 to 37, so now you have x = 73. Next you'd add 72, and you are already way past the real answer of 100, in just 4 steps; there are another 6 to go!
Keep your sum in a separate variable, and start that at zero. Add y to that each time:
total = 0
for i in range(x):
    total = total + y

print(total)

There is no need to subtract 1 from x, the for loop takes care of counting for you.
None of this is actually using recursion. This is a recursive version:
def mult(x, y):
    if x == 0:
        return 0
    return y + mult(x - 1, y)

So if x is 0, there is no multiplication and you return 0, otherwise you sum y to the multiplication of y and x - 1. Yes, here you do subtract one.",A,1
12911429,2012-10-16 09:21:42.027000+00:00,"Use the IIntIds utility:
from zope.component import getUtility
from zope.intid.interfaces import IIntIds

intidutil = getUtility(IIntIds)
object = intidutil.getObject(id)

Alternatively, you could use the .queryObject() method, it takes a default keyword parameter (defaults to None), returned if the id is not found.",A,3
17474065,2013-07-04 15:37:58.013000+00:00,"Simply return the negative number:
res_list.sort(key=lambda x: (float(x[1]), -float(x[2])))

This reverses the B sort order, but do note that the data is sorted on column A first.
Note that you can combine the sorting with the previous line in your code by using the sorted() function:
res_list = sorted(res, key=lambda x: (float(x[1]), -float(x[2]))
",A,3
27167695,2014-11-27 09:57:01.830000+00:00,"You haven't called the method. 
if not request.user.profile.is_valid_member():

Method objects themselves are always True.",A,1
698005,2009-03-30 16:41:01.613000+00:00,"This has already been done.  Several times over.
Here is a parser for the C language written in Python.  Start with this.
http://wiki.python.org/moin/SeeGramWrap
Other parsers.
http://wiki.python.org/moin/LanguageParsing
http://nedbatchelder.com/text/python-parsers.html
You could probably download any ANSI C Yacc grammar and rework it into PLY format without too much trouble and use that as a jumping-off point.",A,4
39045286,2016-08-19 18:05:33.220000+00:00,"You have a circular import:
import runAnalytics
# ..
from main import input1

By the time main is being imported again, runAnalytics has not yet had a chance to execute the def run():.. section.
Resolve this by removing the from main import input1 line, and pass that object in as an argument instead:
def run(input1):

passing this in from the main.py module when you call the function:
loadAnalytics = tkinter.Button(loadApplication, text = ""Load Analytics"", command = lambda: runAnalytics.run(input1))

Apart from the circular import, there is also the issue that whatever file you run as the main script in Python will be stored as the __main__ module. Importing that same script again will lead to a second module being created, now under the name main, and any objects created in that module are distinct from those in __main__.
Next, you'll want to remove the loadAnalytics.mainloop() call from run as you should not start a new mainloop from an already running loop. You probably also want to create a new TopLevel window instead of creating another Tk() root. You'd have to pass in loadApplication to run too if you go this way.",A,3
25038942,2014-07-30 14:05:37.207000+00:00,"Why not use a single field with CHOICES?
TYPE_CHOICES = (
  ('equation', 'equation'),
  ('text', 'text'),
  ('image', 'image'),
  ('table', 'table')
)

type = models.CharField(choices=TYPE_CHOICES)
",A,0
22631232,2014-03-25 10:07:11.303000+00:00,"I don't really understand why you're doing this as CGI at all. You're not doing any server-side processing, so a static HTML file would be more appropriate. (And if you are going to do server-side processing, you should be using a proper templating system.)
However I would be amazed if serving your JS files on a file path - d:\whatever - rather than a web URL was actually working.",A,0
45532393,2017-08-06 13:15:39.977000+00:00,"You need to test if the highest count in the counter is greater than 2; you can use Counter.most_common() to extract the highest count:
if Counter(a).most_common(1)[0][1] > 2:

Counter.most_common() returns a list of (value, count) pairs, even when you ask for just a single pair; [0] gets that one (value, count) pair from the list, and [1] extracts the count.",A,0
18542821,2013-08-30 23:23:20.983000+00:00,"The problem has nothing to do with n being a dictionary; it's that n is a class attribute instead of an instance attribute. That means all instances of the class share a single value.
The solution is simply to turn it into an instance variable—assign it inside a method (usually the __init__ method) instead of inside the class definition.
class A(object):
    def __init__(self):
        self.n = {}

You may ask why the same thing doesn't happen with a. Well, it does. You're creating a single shared a, just as with n.
But you're not calling any methods on that shared a that change it (in fact, you can't, because int is immutable; it doesn't have any methods that change it). With n, that self.n[key] = object may not look like an method call (it's actually self.n.__setitem__(key, object)), but it's obvious that it's changing the value of n in-place, which is the key here.
You're just assigning a new value to self.a. That creates a new instance attribute that shadows the class attribute of the same name—which is confusing, but it works as you want it to. You could get the same behavior with n if you wanted just by building a new value and assigning it to self.n:
def add(self, key, obj):
    new_n = copy.copy(n)
    new_n[key] = obj
    self.n = new_n
",A,5
52210857,2018-09-06 19:21:07.030000+00:00,"The only way you can access an instance variable is as an attribute of self. 
When you just refer to var, that's never an instance variable; it's always a local, enclosing, global, or builtin variable.

In your method definition:
def lookup(self, var=var):
    print(var)

… you have a parameter named var. Parameters are local variables. The print(var) inside the body prints that local variable.
What about this?
def lookup(self, var=var):
    var = var

Again, var is a local variable—a parameter. So, you're just assigning the current value of that local variable to the same variable. Which has no useful effect, but of course it's perfectly legal.

Where does the parameter's value come from? At function call time, if you pass an argument, that argument gets bound to the parameter; if you don't, it gets filled in with the default value.
OK, so where does the default value come from?
At function definition time (when the def statement is executed), var is looked up in the current scope—that is, the body of the class definition—and its value is stored as a default in the function object (it should be visible as foo.lookup.__defaults__[0]).
So, the default value is ""value goes here"". 
Notice that it's not a closure capture or other reference to the class attribute. When the class statement is executed, it uses that same class body namespace to build the class's attributes, so you end up with foo.var as another name for the same value that's in foo.lookup.__defaults__[0]. But they're completely independent names for that value; you can reassign foo.var = 3, and the default value for lookup's parameter will still be ""value goes here"".

So, to answer your specific questions:

Will it store it in the class variable, the instance variable, or a new variable with the same name?

None of the above. It stores it in a local variable that already exists, because it's a parameter.

Will it get the class variable, instance variable, or the method's variable?

If by ""the method's variable"" you mean the parameter, it's the last one.

How do I explicitly reference these variables?

The same way you explicitly reference anything else:

var is a local-enclosing-global-or-builtin variable.
self.var is an instance attribute, or a class attribute if there is no instance attribute.
type(self).var is a class attribute, even if there is an instance attribute.
",A,3
25724619,2014-09-08 12:46:12.197000+00:00,"You are not asking about the version of django-admin, which is simply a script that comes with Django, but about the version of Django itself.
You don't say how you installed Django in the first place. The best way is to use pip: to install a specific version, you would do:
pip install django==1.6.5
",A,0
22739924,2014-03-30 03:29:32.583000+00:00,"It's not safe to share a connection between threads; at the very least you need to use a lock to serialize access. Do also read http://docs.python.org/2/library/sqlite3.html#multithreading as older SQLite versions have more issues still.
The check_same_thread option appears deliberately under-documented in that respect, see http://bugs.python.org/issue16509.
You could use a connection per thread instead, or look to SQLAlchemy for a connection pool (and a very efficient statement-of-work and queuing system to boot).",A,13
52835784,2018-10-16 12:45:41.843000+00:00,"You have a stray comma after the definition of ordering in BookInstance, which turns it into a tuple containing a list. Remove the comma.",A,2
31360367,2015-07-11 18:22:13.173000+00:00,"If you really must, you can use an operator.itemgetter() object to extract values for multiple keys as a tuple:
from operator import itemgetter

a, b = itemgetter('a', 'b')(myfunc())

This is still not pretty; I'd prefer the explicit and readable separate lines where you first assign the return value, then extract those values.
Demo:
>>> from operator import itemgetter
>>> def myfunc():
...     return {'a': 1, 'b': 2, 'c': 3}
... 
>>> itemgetter('a', 'b')(myfunc())
(1, 2)
>>> a, b = itemgetter('a', 'b')(myfunc())
>>> a
1
>>> b
2
",A,2
33055546,2015-10-10 15:18:57.343000+00:00,"Split on the comma (using b'..' byte string literals), then use int() to convert to integers, using list comprehensions to process all strings and values in each string:
[[int(num) for num in value.split(b',')] for value in yourlist if value]

The if value filter skips empty strings.
This produces nested lists; one per bytestring:
>>> yourlist = [b'688284,332,2830336', b'661114,40,37229', b'978148,1,81', b'262250,69,736665', b'269715,68,605568', b'171278,73,1026179', b'1249503,1,15', b'246783,64,424574', b'-1,1,0', b'1826857,1,25', b'1515172,1,0', b'-1,1,0', b'-1,1,0', b'1655032,1,0', b'-1,1,0', b'-1,1,0', b'1453895,1,0', b'1520874,1,0', b'1561752,1,0', b'1508907,1,0', b'1416987,1,0', b'1437689,1,0', b'1421569,1,0', b'1391397,1,0', b'-1,-1', b'-1,-1', b'-1,-1', b'']
>>> [[int(num) for num in value.split(b',')] for value in yourlist if value]
[[688284, 332, 2830336], [661114, 40, 37229], [978148, 1, 81], [262250, 69, 736665], [269715, 68, 605568], [171278, 73, 1026179], [1249503, 1, 15], [246783, 64, 424574], [-1, 1, 0], [1826857, 1, 25], [1515172, 1, 0], [-1, 1, 0], [-1, 1, 0], [1655032, 1, 0], [-1, 1, 0], [-1, 1, 0], [1453895, 1, 0], [1520874, 1, 0], [1561752, 1, 0], [1508907, 1, 0], [1416987, 1, 0], [1437689, 1, 0], [1421569, 1, 0], [1391397, 1, 0], [-1, -1], [-1, -1], [-1, -1]]

If you want a flat list, use just one list comprehension combining the loops:
[int(num) for value in yourlist if value for num in value.split(b',')]

However, it sounds like you are really parsing CSV values here, from a web URL. Decode the data to text and feed it to a csv.reader() object to handle the splitting:
import io
import csv

response = urllib.request.urlopen(url)
codec = response.info().get_param('charset', 'latin1')
reader = csv.reader(io.TextIOWrapper(response, encoding=codec))
for row in reader:
    row = [int(col) for col in row]
    # do something with each row

or read the response in one go (the urllib.request library seems to throw in a large delay when using anything but a straight-up .read() call for your sample URL):
response = urllib.request.urlopen(url)
codec = response.info().get_param('charset', 'latin1')
data = response.read().decode(codec)
reader = csv.reader(data.splitlines())
for row in reader:
    row = [int(col) for col in row]
    # do something with each row

The get_param() call queries if the server told us what codec to use to decode the response, falling back to ISO-8859-1 (Latin-1) as the default for HTTP text responses.",A,2
31916107,2015-08-10 09:31:37.580000+00:00,"A staticmethod object is a descriptor. The magic you are missing is that Python calls the __get__ method when accessing the object as an attribute on a class or instance.
So accessing the object as C.foo results in Python translating that to C.__dict__['foo'].__get__(None, C), while instance_of_C.foo becomes type(instace_of_C).__dict__['foo'].__get__(instance_of_C, type(instance_of_C)).
The staticmethod object is defined in C code, but an equivalent in Python would be:
class staticmethod(object):
    def __init__(self, callable):
        self.f = callable
    def __get__(self, obj, type=None):
        return self.f
    @property
    def __func__(self):
        return self.f

where self.f is the original wrapped function.
All this is needed because functions are themselves descriptors too; it is the descriptor protocol that gives you method objects (see python bound and unbound method object for more details). Since they too have a __get__ method, without a staticmethod object wrapping the function, a functionobj.__get__ call produces a method object instead, passing in a self argument.
There is also a classmethod, which uses the second argument to descriptor.__get__ to bind a function to the class, and then there are property objects, which translate binding into a function call directly. See How does the @property decorator work?.",A,13
23437566,2014-05-02 21:56:34.573000+00:00,"Your question unfortunately doesn't make any sense. The reason why you don't get any Google results for this is that there's simply no such thing as a status code for a request. The status code is what the server sends back to the client as a result of the request: it's unavoidably part of the response, not the request.
I suspect however that this is an XY problem. What, exactly, are you trying to achieve? If you need some data to be preserved through a redirect, then you probably want to store it in the session.",A,1
3092171,2010-06-22 10:21:15.100000+00:00,"
What's the best way to get user input  

Don't mess with a ""bundled"" app.  Just use the command line.
Tell your user to open a terminal window and run your app from the command line.  Saves everyone a lot of pain.  
Users don't ""need"" bundled apps.  They can type.  Try it.",A,1
16012995,2013-04-15 10:31:34.933000+00:00,"You could use zip() on d.values():
x, y, z = zip(*d.values())

This still creates 3 lists, but is more compact. You want to avoid creating loops for the individual column_val() parameters, you'd end up with 4 loops instead.
Demo for your d sample:
>>> d = {u'a': (1,2,4), u'b': (1, 0, 1), u'c': (8, 3,1)}
>>> x, y, z = zip(*d.values())
>>> x, y, z
((1, 8, 1), (2, 3, 0), (4, 1, 1))

Note that the order of the values in x, y and z is not a given, because dictionaries do not preserve order.",A,6
1293546,2009-08-18 12:36:18.963000+00:00,"Post-process the dump file with python.  You may be happier than perl or sed.
If you're running on Linux, you already have it installed.  If you're running on Windows, the installer is painless.
Before that, however, learn to use SQL UPDATE and SQL ALTER.  You'll be happiest doing things the right way.",A,-3
19921297,2013-11-12 05:08:04.440000+00:00,"If your only problem with doing the setattr after the class definition is that it's ugly and in the wrong place, what about using a decorator to do it?
def add_constants(names):
    def adder(cls):
        for i, name in enumerate(names):
            setattr(cls, name, i)
        return cls
    return adder

@add_constants(""state1 state2"".split())
class Node:
    pass
",A,2
16472458,2013-05-09 22:51:24.210000+00:00,"I'm going to take a guess here: You don't care about threads at all, you just want to ""run a script"" as a ""second process"".
That's easy. Running a script is just like running anything else; you use the subprocess module. Since the script is running inside an entirely separate Python interpreter instance, in an entirely separate process, ""the main window can be used, even if the second process works""—or even if it just spins or blocks forever.
For example:
class myClass( wx.Frame ):
    def __init__(self):
        print ""Executing... Do you like this app?""
        self.child = subprocess.Popen([sys.executable, './child.py', '--username', 'root'])
    def onClose( self , evt ):
        self.child.wait()
        self.exit();

The only trick here is what to pass as the first argument. If you want to make sure child.py is run by the same copy of Python as the parent, use sys.executable. If you want to make sure it's run by the default Python, even if the parent is using a different one, use python. If you want to use a specific one, use an absolute path. If you want to let the shell (or, on Windows, the pylauncher thing) figure it out based on the #! line, use shell=True and just pass ./child.py as the first argument. And so on.",A,1
25426543,2014-08-21 12:36:52.597000+00:00,"You can specify a custom format:
format_currency(100, 'EUR', format=u'#,##0\xa0¤', locale='fr_FR')

See the LDML markup spec for more details; the \xa0 byte is a U+00A0 NO-BREAK SPACE character; rather than a regular space.
For fr_FR the default is #,##0.00\xa0¤, and the .00 part signals that a decimal portion should be printed with two digits, padded if not present. My example above simply removed the decimal portion, you also could use .## to allow for a fraction if the number is not an exact integer, but note that in that case a .5 value is printed without a 2nd digit!
Demo:
>>> from babel.numbers import format_currency
>>> print format_currency(100, 'EUR', format=u'#,##0\xa0¤', locale='fr_FR')
100 €
>>> print format_currency(100.0, 'EUR', format=u'#,##0.##\xa0¤', locale='fr_FR')
100 €
",A,4
25945798,2014-09-20 06:04:43.437000+00:00,"If you're looking to evaluate any Python literal exactly the same way Python would, the function ast.literal_eval does what you want:
>>> s = r""""""u'\u2016'""""""
>>> print s
u'\u2016'
>>> print ast.literal_eval(s)
‖


If you want to make sure it's a string or Unicode literal only, so the user can't confuse you with delimiter=42, your best bet is to literal_eval and then type-check:
val = ast.literal_eval(s)
if not isinstance(val, basestring):
    raise BadUserError(""Why are you giving me {} when I wanted a string?"".format(s))


In more detail:

Safely evaluate an expression node or a Unicode or Latin-1 encoded string containing a Python expression. The string or node provided may only consist of the following Python literal structures: strings, numbers, tuples, lists, dicts, booleans, and None.
This can be used for safely evaluating strings containing Python expressions from untrusted sources without the need to parse the values oneself.

In case you're wondering what the worry is about untrusted sources, imagine that you just used eval, which works on any Python expression, and I put this in the config file:
delimiter=__import__('os').system('rm -rf /')
",A,3
49488968,2018-03-26 10:12:29.210000+00:00,"The error tells you what you need to do: encode the password to a bytestring.
md5.update(passwd.encode('utf-8'))

Note though you probably shouldn't be doing this at all; if you want to use a user's password, use the set_password() method of the User model.
(And please don't randomly add encoding declarations to your script; that only effects literal characters in the file.)",A,3
16679367,2013-05-21 20:57:25.320000+00:00,"Use str.split() or str.rsplit() with a limit, then select the first element:
filename.split('_', 1)[0]

.rsplit('_', 1) will split on the last underscore, .split() on the first. Pick the one that fits your usecase best.
This gives you everything before the first or last underscore for that filename.
Using this in a set comprehension, with sorted() returning a list from that set:
unique_prefixes = sorted({filename.split('_', 1)[0] for filename in os.listdir(somedir)})

For Python 2.6 and earlier, where you don't yet have a set comprehension syntax, the following generator expression with set() would work too:
unique_prefixes = sorted(set(filename.split('_', 1)[0] for filename in os.listdir(somedir)))
",A,5
16087009,2013-04-18 15:25:37.593000+00:00,"Regular expression MatchObject results include indices of the match. What remains is to match repeating characters:
import re

repeat = re.compile(r'(?P<start>[a-z])(?P=start)+-?')

would match only if a given letter character (a-z) is repeated at least once:
>>> for match in repeat.finditer(""aaaaabbbbbbbbbbbbbbccccccccccc""):
...     print match.group(), match.start(), match.end()
... 
aaaaa 0 5
bbbbbbbbbbbbbb 5 19
ccccccccccc 19 30

The .start() and .end() methods on the match result give you the exact positions in the input string.
Dashes are included in the matches, but not non-repeating characters:
>>> for match in repeat.finditer(""a-bb-cccccccc""):
...     print match.group(), match.start(), match.end()
... 
bb- 2 5
cccccccc 5 13

If you want the a- part to be a match, simply replace the + with a * multiplier:
repeat = re.compile(r'(?P<start>[a-z])(?P=start)*-?')
",A,11
2648666,2010-04-15 20:15:40.367000+00:00,"Well, you don't say what your problem is, but you are doing at least one thing wrong.
After confirming that the formset is valid, and then saving it, for some reason you then instantiate another formset and fall straight through to the render_to_response at the end of the function, so you end up displaying a set of blank forms again.
What you should do at that point is redirect somewhere else, eg to a confirmation page.",A,0
2769598,2010-05-04 23:17:11.103000+00:00,"It might be advantageous to use  unsigned integers with explicit sizes in this case:
#include <stdio.h>
#include <inttypes.h>

int main(void) {
  uint32_t leastSignificantWord = 0;
  uint32_t mostSignificantWord = 1;
  uint64_t i = (uint64_t) mostSignificantWord << 32 | leastSignificantWord;
  printf(""%"" PRIu64 ""\n"", i);

  return 0;
}


Output

4294967296
Break down of (uint64_t) mostSignificantWord << 32 | leastSignificantWord

(typename) does typecasting in C. It changes value data type to typename.
(uint64_t) 0x00000001 -> 0x0000000000000001
<< does left shift. In C left shift on unsigned integers performs logical shift.
0x0000000000000001 << 32 -> 0x0000000100000000



| does 'bitwise or' (logical OR on bits of the operands).
0b0101 | 0b1001 -> 0b1101
",A,58
39912310,2016-10-07 08:00:04.110000+00:00,"I don't know about cheating, but it certainly goes against good database design. More to the point, it prevents you from doing almost all kinds of useful queries on that field. What if you wanted to know all courses that had dates within two days of a specific date? Almost impossible to do that with solution 2, but simple with solution 1.",A,2
244497,2008-10-28 19:23:17.793000+00:00,"The ""running forever"" situation is sometimes part of a larger state machine.  Many embedded devices (with run-forever loops) don't really run forever.  They often have several operating modes and will sequence among those modes.
When we built heat-pump controllers, there was a power-on-self-test (POST) mode that ran for a little while.  Then there was a preliminary environmental gathering mode that ran until we figured out all the zones and thermostats and what-not.  
Some engineers claimed that what came next was the ""run-forever"" loop.  It wasn't really that simple.  It was actually several operating modes that flipped and flopped.  There was heating, and defrosting, and cooling, and idling, and other stuff.
My preference is to treat a ""forever"" loop as really just one operating mode -- there may be others at some point in the future.
someMode= True
while someMode:
    try:
        ... do stuff ...
    except SomeException, e:
        log.exception( e )
        # will keep running
    except OtherException, e:
        log.info( ""stopping now"" )
        someMode= False

Under some circumstances, nothing we've seen so far sets someMode to False.  But I like to pretend that there'll be a mode change in some future version.",A,7
17488830,2013-07-05 12:23:30.470000+00:00,"This is a problem with importing, not inheritance. Course is the module: you need to inherit from Course.Course. (In Python we usually name modules in lower case, though).",A,3
38430749,2016-07-18 07:21:18.710000+00:00,"As long as you haven't actually accessed mock.return_value yet, you can test if a non-standard return value has been set with:
m.foo._mock_return_value is unittest.mock.DEFAULT

The moment you use the mock.return_value property, if mock._mock_return_value is set to unittest.mock.DEFAULT still, a new Mock instance is created and stored in mock._mock_return_value for future re-use.
Note that this attribute is an implementation detail, which is why it starts with an underscore. It is not documented and may change in a future release. However, there currently is no other method to check if mock.return_value has been set explicitly.",A,1
34695309,2016-01-09 15:25:26.813000+00:00,"Since (for reasons you won't explain) you're not using a ModelForm, you can't rely on the model validation here: you'll need to use form validation. Move the clean method to the Form subclass, and import ValidationError from django.forms.",A,1
33451636,2015-10-31 12:36:05.493000+00:00,"child_state = list(self.state) creates a shallow copy of the list. This means any lists contained are not themselves copied, the new child_state list only contains references.
You can either use the copy.deepcopy() function to have Python recursively clone the nested structure, or use a list comprehension to copy the directly contained lists:
child_state = [list(sub) for sub in self.state]
",A,4
42437638,2017-02-24 11:35:25.620000+00:00,"You appended a single list object. You did not add the elements from the list that range produces to L. A nested list object adds just one more element:
>>> L = ['a', 'bb', 'ccc']
>>> L.append(range(2))
>>> L
['a', 'bb', 'ccc', [0, 1]]

Note the [0, 1], the output of the range() function.
You are looking for list.extend() instead:
>>> L = ['a', 'bb', 'ccc']
>>> L.extend(range(2))
>>> L
['a', 'bb', 'ccc', 0, 1]
>>> len(L)
5

As an alternative to list.extend(), in most circumstances you can use += augmented assignment too (but take into account the updated L list is assigned back to L, which can lead to surprises when L was a class attribute):
>>> L = ['a', 'bb', 'ccc']
>>> L += range(2)
>>> L
['a', 'bb', 'ccc', 0, 1]
",A,8
31391882,2015-07-13 19:32:42.560000+00:00,"The repeat() iterator makes the load rows look endless; you can iterate over it forever.
Presumably the actual code looks like this:
for idx, row in enumerate(
   islice(
       chain.from_iterable(repeat(load)),
       task_id, min(task_id + step_size, last_step)
   )):

e.g. the repeat(load) is chained, so all the data in the CSV file looks like one long sequence of rows, and when you reach the end of the CSV file you simply start at the beginning again, as if the rows in the CSV file are read from an endless loop.
The islice() then picks a subset of that endless loop. By making the load rows endless, it doesn't matter if the slice tries to take more rows from the file then are present.",A,1
21111723,2014-01-14 10:52:10.440000+00:00,"Using a collections.defaultdict() object:
from collections import defaultdict

numbered = defaultdict(list)
for i in tup:
    numbered[i[1]].append(i)

Now numbered[1] contains all ones, numbered[2] a list of all twos. This solution extends to more values of i[1] naturally without having to define any additional lists or if statements.
Demo:
>>> from collections import defaultdict
>>> tup = [('x',1),('y',2),('z',1)]
>>> numbered = defaultdict(list)
>>> for i in tup:
...     numbered[i[1]].append(i)
... 
>>> numbered
defaultdict(<type 'list'>, {1: [('x', 1), ('z', 1)], 2: [('y', 2)]})
>>> numbered[1]
[('x', 1), ('z', 1)]
>>> numbered[2]
[('y', 2)]

A defaultdict is just a dict subclass with additional behaviour; you can do without it too with a little more complexity and a slight loss in speed:
numbered = {}
for i in tup:
    numbered.setdefault(i[1], []).append(i)
",A,2
41736786,2017-01-19 08:08:56.840000+00:00,"You need to use a key that extracts the value by which to sort; here that is element[1][0]:
sorted(inputlist, key=lambda e: e[1][0], reverse=True)

The reverse=True is there to sort from largest to smallest.",A,3
12667496,2012-10-01 04:18:51.200000+00:00,"If you actually want to use that algorithm, and you want to work beyond the limits of the built-in float, then yes, you need a different type.
If all you want is to get an approximate answer instead of an exception, that's easy; you can get infinite range out of the box. But if you also want to eliminate the rounding errors, you can't have infinite precision (that would take infinite time/space), so you have to know how to work out the precision needed for your range of inputs. (I'll leave that as an exercise for the reader.)
The standard library type decimal.Decimal may be all you need. It provides arbitrary-precision fixed- or floating-point decimal arithmetic according to the IEEE-854 standard. There are many cases for which it's unusable because it doesn't provide enough mathematical functions, but you only need basic arithmetic and sqrt, which are just fine. It can also be slow for huge numbers, but if you just want to calculate fib on a few three-digit numbers it's more than sufficient.
When Decimal is insufficient, there are a number of third-party modules, usually wrapping industry-standard C libraries like gmp/mpfr, such as bigfloat.
Here's how to get infinite range, but with rounding errors on roughly the same scale as the built-in float:
>>> s5 = decimal.Decimal(5).sqrt()
>>> def fib(n):
...     return ((1+s5)**n - (1-s5)**n)/(2**n*s5)
>>> fib(800)
Decimal('6.928308186422471713629008226E+166')
>>> int(fib(800))
69283081864224717136290082260000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000L
>>> s5 = bigfloat.sqrt(5)
>>> def fib(n):
...     return ((1+s5)**n - (1-s5)**n)/(2**n*s5)
>>> fib(800)
BigFloat.exact('6.9283081864226567e+166', precision=53)
>>> int(fib(800))
69283081864226566841137772774650010139572747244991592044952506898599601083170460360533811597710072779197410943266632999194601974766803264653830633103719677469311107072L

But notice that neither of these are actually the answer you'd get if you did the math perfectly; you've lost 24 digits to rounding errors. (The reason the values are different is that bigfloat is rounding in base 2, decimal in base 10.)
To fix that, you need more precision. All libraries provide some way to change the precision; bigfloat has more convenient options than most, but none are too onerous:
>>> decimal.getcontext().prec = 300
>>> s5 = decimal.Decimal(5).sqrt()
>>> def fib(n):
...     return ((1+s5)**n - (1-s5)**n)/(2**n*s5)
>>> fib(800)
69283081864224717136290077681328518273399124385204820718966040597691435587278383112277161967532530675374170857404743017623467220361778016172106855838975759985190398725.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000048
>>> def fibp(n, p):
...     with bigfloat.precision(p):
...         s5 = bigfloat.sqrt(5)
...         return ((1+s5)**n - (1-s5)**n)/(2**n*s5)
>>> fibp(800, 125)
BigFloat.exact('6.92830818642247171362900776814484912138e+166', precision=125)
>>> int(fibp(800, 125))
69283081864224717136290077681448491213794574774712670552070914552025662674717073354503451578576268674564384721027806323979200718479461097490537109958812524476157132800L
",A,2
16221095,2013-04-25 17:30:12.017000+00:00,"You need to use a unicode value. Use a unicode literal with u'' for example:
rank.set('name', u'ą')

This results in:
<data>
    <country name=""Liechtenstein"">
    <neighbor direction=""E"" name=""&#216;"" />
    <neighbor direction=""W"" name=""&#216;"" />
</country>
</data>

You are instead passing in cp1250-encoded bytes, which have to be decoded to Unicode, and that happens automatically with the default codec, ASCII. And that doesn't work, because your bytestring contains the byte with hex value B9 (ą in the cp1250 encoding) and that's not a valid ASCII value.
You may want to read up on Python and Unicode:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
",A,4
29460764,2015-04-05 18:31:00.800000+00:00,"Simply check the type against the property object:
if isinstance(MyClass.this_is_a_property, property):

You don't really have to retrieve it from the class dictionary here; looking up a property on the class as an attribute also returns the property instance.",A,6
24252587,2014-06-16 21:32:45.027000+00:00,"Yes, classes are objects too, and are governed by the same reference counting rules.
If all you do with the return value is create instances, then the only reference to the class is the __class__ attribute on those instances. If there are no more instances of that specific class, it will no longer be referenced and deleted.",A,2
37796332,2016-06-13 18:01:38.060000+00:00,"You replaced the pjoin integer with a string:
# at the top
pjoin = 0

# inside the while True and for clientInList loops
pjoin = pickle.dumps(pjoin)

pickle.dumps() returns a string object. Next time you get a response from a client, you'll now will sum an integer to a string:
pjoin += (join)

It looks to me you can remedy this by not re-using pjoin for the pickle.dumps() result:
for clientInList in clientsList:
    join = clientInList.recv(1024)
    join = pickle.loads(join)
    print(join)
    pjoin += (join)
    response = pickle.dumps(pjoin)
    clientInList.send(response)
",A,1
21030588,2014-01-09 20:36:35.130000+00:00,"It's true that Django templates are not particularly efficient. However, that's only really a problem when you have very large templates that themselves extend or include many other templates, for example in a complex content management system. With a single template containing a small number of fields like you have, template rendering is insignificant compared to the overall overhead of serving the request.
That said I'm a bit confused about both of your alternatives. Why aren't you generating JSON via the standard json library? That's the proper way to do it, not by building up strings either in templates or in Python code.
ret = {'id': project.id,
       'name': project.name,
       'snapshot_ids': [snapshot.id for snapshot in snapshots]}
ret_json = json.dumps(ret)
",A,3
26629812,2014-10-29 12:01:06.047000+00:00,"Using a list comprehension with a nested loop:
unrolled = [c for c, count in weighted for _ in range(count)]

If you are using Python 2 you could use xrange() instead.
If you like itertools, you can use itertools.chain.from_iterable() to make this into a lazy iterable:
from itertools import chain

chain.from_iterable([c] * count for c, count in weighted)

Demo:
>>> weighted = [ (""a"", 3), (""b"", 1), (""c"", 4) ]
>>> [c for c, count in weighted for _ in range(count)]
['a', 'a', 'a', 'b', 'c', 'c', 'c', 'c']
>>> from itertools import chain
>>> list(chain.from_iterable([c] * count for c, count in weighted))
['a', 'a', 'a', 'b', 'c', 'c', 'c', 'c']

I used list() to turn the chain iterator back into a sequence.",A,2
4867830,2011-02-01 20:54:18.093000+00:00,"
Is it okay to use the builtin Django webserver for this

No.

Should I install Apache and mod_wsgi? 

Yes.

If so, what are the reasons for this? Security perhaps?

Partly.
More importantly, the little toy Django server is single-threaded and any hangup in your code hangs the server.  This means that when two users click almost at the same time, user one's query must go all the way through Django before user two's query can even starts.
And this will have to include the insanely slow download speed to the desktop.
Apache (like all the alternatives, lighttpd or nginx) is multi-threaded.  The slowest part of the transaction is the download from Apache to the desktop.  You don't want Python code (and Django) handling this in a single-threaded manner.  Even for just a few users.  
Also, you don't what Django serving static media (i.e., CSS and JS library files.)
A single slow spot in your application won't effect the overall system throughput if Apache and mod_wsgi are in place.  One request 's output page can be slowly downloading to a PC desktop in parallel with another user's output.",A,21
23103750,2014-04-16 08:19:06.203000+00:00,"You shouldn't be using a ModelForm for this: they are for creating new instances or editing existing ones. Another problem is that passwords are stored in hashed form in the database, so the password entered in the form would not match the version in the db.
There is an AuthenticationForm in django.contrib.auth that you should use which takes care of all this.",A,3
36035278,2016-03-16 12:08:43.493000+00:00,"Someone is code-golfing here, and using hacky tricks to minimise the amount of code used.

< is a regular comparison operator; it returns True or False based on the two operands. The Python bool type is a subclass of int and True is 1, False is 0 when interpreted as integers. As such C[r<1] either picks C[0] or C[1]. 
& is a bit-wise operator, not a comparison operator; & 1 is masking the number to the last bit, effectively testing if a number is odd or even (the last bit is set or not). So if r is odd, C[1] is used, otherwise C[0] is.

Breaking this down:

C is a string with the o and space characters
C[r<1] picks either o or a space based on wether it is smaller than 1. It never is (random.randint(1,6) ensures this), so that is always an o. This appears to be a bug or oversight in the code.
C[r<3] picks a space for 1 and 2, an o otherwise.
C[r<5] picks an o for 5 or 6, a space otherwise.
C[r&1] picks an o for 2, 4 and 6, a space otherwise.

In all, it prints r plus one as a die. r = 1 gives you two pips, while r = 6 results in seven pips, perhaps as a stylised one?
Fixing the code for that requires incrementing all r tests and inverting the odd/even test:
s = '-----\n|' + C[r<2] + ' ' + C[r<4] + '|\n|' + C[r<6]
print(s + C[1-r&1] + s[::-1])

Demo (wrapping the string building in a function):
>>> import random
>>> def dice(r, C='o '):
...     s = '-----\n|' + C[r<2] + ' ' + C[r<4] + '|\n|' + C[r<6]
...     print(s + C[1-r&1] + s[::-1])
...
>>> for i in range(1, 7):
...     dice(i)
...
-----
|   |
| o |
|   |
-----
-----
|o  |
|   |
|  o|
-----
-----
|o  |
| o |
|  o|
-----
-----
|o o|
|   |
|o o|
-----
-----
|o o|
| o |
|o o|
-----
-----
|o o|
|o o|
|o o|
-----
",A,7
18660960,2013-09-06 15:09:05.837000+00:00,"You use logging everywhere, regardless.
In your modules, use:
import logging

log = logging.getLogger(__name__)

and log away. Use the different levels (log.info(), log.debug(), log.error(), etc.) depending on what type of message you are logging.
Your main entry point then configures the logging module output. That can include completely disabling any output, or directing debug information to a file, but logging errors to the console.
Your modules do not need to care, the logging module handles configuration of the output handlers and formatting as global state.",A,4
30790585,2015-06-11 20:16:12.567000+00:00,"You've confused yourself with the way you've named your variables. You send the list of comments to the template as ""comment"", then iterate through that using ""content"" for each item, but then revert back to using ""comment"" instead.
You should give your things logical names that reflects what they are. In your handler:
comments_query = Comment.query(ancestor=section_key(section_name)).order(-Comment.date)
comments = comments_query.fetch(10)
...
template_values = {
    ...
    'comments': comments,
    ...
 }

and in your template:
{% for comment in comments %}
   {% if comment.content %}
       <b>{{ comment.author.email }}
       ...
",A,1
15365623,2013-03-12 15:42:11.473000+00:00,"No, you are wrong on two counts.
Firstly, the use of ""threads"" is a bit vague here. Depending on how its server is configured, Django can be served either using threads or processes or both (see the mod_wsgi documentation for a full discussion). If there is a single thread per process, then you can can guarantee that only one instance of a module will be available to each process. But that is highly dependent on that configuration.
Even so, it is still not the case that there will be ""exactly one"" call to that function per request/response cycle. This is because the lifetime of a process is entirely unrelated to that cycle. A process will last for multiple requests, so that variable will persist for all of those requests.",A,3
21005339,2014-01-08 20:00:23.060000+00:00,"In Python, you always need parentheses to call a function, even if there are no arguments. So, this line:
total = randint

… doesn't call randint. Instead, it just makes total another name for the randint function itself.
So, when you do this:
total=total + randint

… you're trying to add together two functions. And that doesn't make any sense.
What you probably want to do is something like this:
total = randint(1, 6)
# …
total = total + randint(1, 6)

Or maybe, since you've already done num1 = randint(1, 6), what you wanted was:
total = num1
# …
total = total + num

If it's not clear what the difference is between the two: The first one rolls a brand new die and assigns the result to total, then rolls another new die and adds the result to total. The second one assigns the result of the previous die roll (the value in num1) to total, and then adds that same value to total again.

You have similar problems in multiple other places, like doing print str(randint) (which will print out ""<bound method Random.randint of <random.Random object at 0x89b010>>"" where you probably wanted print num1 (which will print out something like 2).",A,1
39792393,2016-09-30 13:12:16.463000+00:00,"The co in coroutine stands for cooperative. Yielding (to other routines) makes a routine a co-routine, really, because only by yielding when waiting can other co-routines be interleaved. In the new async world of Python 3.5 and up, that usually is achieved by await-ing results from other coroutines.
By that definition, the code you found is not a coroutine. As far as Python is concerned, it is a coroutine object, because that's the type given to a function object created using async def.
So yes, the tutorial is.. unhelpful, in that they used entirely synchronous, uncooperative code inside a coroutine function.
Instead of urllib, an asynchronous HTTP library would be needed. Like aiohttp:
import aiohttp

async def download_coroutine(url):
    """"""
    A coroutine to download the specified url
    """"""
    filename = os.path.basename(url)
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            with open(filename, 'wb') as fd:
                while True:
                    chunk = await resp.content.read(1024)
                    if not chunk:
                        break
                    fd.write(chunk)
    msg = 'Finished downloading {filename}'.format(filename=filename)
    return msg

This coroutine can yield to other routines when waiting for a connection to be established, and when waiting for more network data, as well as when closing the session again.
We could further make the file writing asynchronous, but that has portability issues; the aiofiles project library uses threads to off-load the blocking calls to. Using that library, the code would need updating to:
import aiofiles

async with aiofiles.open(filename, 'wb') as fd:
    while True:
        chunk = await resp.content.read(1024)
        if not chunk:
            break
        await fd.write(chunk)


Note: the blog post has since been updated to fix these issues.",A,13
3519537,2010-08-19 07:10:07.350000+00:00,"Not really related to your question, but if you want to add variables to your template context, rather than calling a separate function in each view you might want to look into context processors which do the same thing automatically.",A,1
25891456,2014-09-17 13:07:29.950000+00:00,"You haven't shown any code at all, which would have been helpful. But I expect the problem is that you're not passing the request object to the template context in your ""home"" view: usually this happens automatically if you are using a RequestContext or the render shortcut, which presumably you are doing in the other views. ",A,0
19825027,2013-11-06 23:26:52.223000+00:00,"The problem is that lname1 is a list of (k, v) pairs, lname2 is a list of (k, w), and you're trying to remove a (k, v) pair from a copy of lname2. That doesn't make any sense.
With the values you happen to have, v and w are identical for all of the k values that aren't in sKeys, so you get away with it. But that's manifestly not true in general—as implied by the (3, 'third') in your example, and as you explicitly said in a comment.
So, if you're trying to remove all (k, w) pairs from lname2 where some (k, v) is in lname1, but k is not in sKeys, you're doing it wrong; you need an extra search to find the right w:
def deleteOperation(lname1, sKeys, lname2):
    delMaster = copy.deepcopy(lname2)
    for k, v in lname1:
        if k not in sKeys:
            for k2, w in lname2:
                if k2 == k:
                    delMaster.remove((k, w))
    return delMaster


Or, reorganize things to walk lname2 first instead.

Or, most simply, convert these things into dicts and sets in the first place so the whole operation becomes trivial:
def deleteOperation(lname1, sKeys, lname2):
    for k in lname1:
        if k not in sKeys:
            del lname2[k]
    return lname2


If you don't actually need to mutate lname2, but just want to return values from lname2 chosen by using lname1, you can make this simpler. Then it would just be:
def deleteOperation(lname1, sKeys, lname2):
    d2 = dict(lname2)
    return [(k, d2[k]) for k, v in lname1 if k in sKeys]

And again, it would be even simpler if you used the right data structures in the first place:
def deleteOperation(lname1, sKeys, lname2):
    return {k: lname2[k] for k in sKeys & lname1.keys()}
",A,0
3305148,2010-07-22 01:32:06.137000+00:00,"You don't have an unknown number of ""columns"".
You have a two-part key.
Map Route:
Name, Point, Sequence
""Jogging Loop"", (5, 10), 1
""Jogging Loop"", (6, 11), 2

Alphabetized String
Name, Word, Sequence
""abet"", ""abet"", 1
""abet"", ""beat"", 2
""abet"", ""bate"", 3
",A,0
26855173,2014-11-10 23:49:43.003000+00:00,"You already can access the variable outside the loop. That's why it's printing out 4545, instead of giving you an UnboundLocalError or other exception.
The problem is that ""outside the loop"" only happens once, with the last value from the loop. So, you set ID, then set it again, then set it again, 69 times, and then finally print the 69th value. If you want to print each of the 69 values, you have to do it each time through the loop. Instead of this:
for channels_start_end in range(69): 
    ID = channels_id + channels_start_end
print ID

Do this:
for channels_start_end in range(69): 
    ID = channels_id + channels_start_end
    print ID

Or, if you want to do something with all 69 values, you have to accumulate them all, e.g., by adding them to a list, so all 69 of them are available, instead of just the last one:
IDs = []
for channels_start_end in range(69): 
    IDs.append(channels_id + channels_start_end)
print IDs


From your comments, it seems like your problem is even simpler. You don't want to print out all the values, only every 70th value. You're already got every 70th value in channels_id. So, just print that:
for channels_id in range(4127, 4547, 70):
    for channels_start_end in range(69): 
        ID = channels_id + channels_start_end
    print channels_id

But really, you don't seem to be doing anything with that ID value, or doing anything useful with the inner loop at all; instead of generating and then ignoring 69 out of every 70 values, why not just not generate them in the first place? Like this:
for channels_id in range(4127, 4547, 70):
    print channels_id


Your last problem is that the loop doesn't include 4547.
That's because a Python range only includes values less than the stop value, not less than or equal to it. For a simpler example:
>>> for i in range(0, 3, 1):
...     print i
0
1
2

Notice that it didn't print 3.
So, you could use range(4127, 4548, 70) or range(4127, 4617, 70) (it's hard to know which one is more ""natural"" in your use case without understanding what that use case is) to get all the values you want.",A,3
37396988,2016-05-23 17:22:21.993000+00:00,"I'm not sure why you think there is any algorithm here. This is a database call; it's entirely up to the database how it's implemented. 
Naturally, databases are exceedingly efficient at looking up items by a single criterion; even more so if the column is indexed, which a pk would be automatically.",A,0
17039643,2013-06-11 08:31:07.087000+00:00,"Here's an implementation based on an id -> object map (suggested by @nmclean) and collections.MutableSet:
from collections import MutableSet

class IdentitySet(MutableSet):
    key = id  # should return a hashable object

    def __init__(self, iterable=()):
        self.map = {} # id -> object
        self |= iterable  # add elements from iterable to the set (union)

    def __len__(self):  # Sized
        return len(self.map)

    def __iter__(self):  # Iterable
        return self.map.itervalues()

    def __contains__(self, x):  # Container
        return self.key(x) in self.map

    def add(self, value):  # MutableSet
        """"""Add an element.""""""
        self.map[self.key(value)] = value

    def discard(self, value):  # MutableSet
        """"""Remove an element.  Do not raise an exception if absent.""""""
        self.map.pop(self.key(value), None)

    def __repr__(self):
        if not self:
            return '%s()' % (self.__class__.__name__,)
        return '%s(%r)' % (self.__class__.__name__, list(self))

Example:
a = (1, 2)
print IdentitySet([a, (1, 2), a])
# -> IdentitySet([(1, 2), (1, 2)]) # only one instance of `a`

print s != Set([a, (1, 2), a])  # it might be unequal because new literal
                                # tuple (1, 2) might have a different id
print s | Set([a, (1, 2), a])
# -> IdentitySet([(1, 2), (1, 2), (1, 2)]) # `a` plus two tuples from literals

MutableSet automatically provides methods: clear, pop, remove, __ior__, __iand__, __ixor__, __isub__, __le__, __lt__, __eq__, __ne__, __gt__, __ge__, __and__, __or__, __sub__, __xor__, and isdisjoint.
Here, unlike in set-based implementations, compound operations always use overridden basic methods e.g., | (self.__or__(), union) is implemented in terms of self.add() and therefore it provides correct for IdentitySet semantics.",A,7
12945688,2012-10-18 01:06:40.820000+00:00,"First, if you need it to be true that noise(x) would always return the same value for the same x, no matter what, even if it's never been called, then you can't really use randomness at all. A good hash function is the only possibility.
However, if you just need to be able to restore a previous state consisting of the values for all of the previously-explored points (never-explored points may turn out different after save and load than if you hadn't quit… but how can anyone tell without access to multiple universes?), and you don't want to store all of those points, then it might be reasonable to regenerate them.
But let's back up a step. You want something that acts like a hash function. Is there a hash function you can use?
I'd imagine the algorithms in hashlib are too slow (md5 is probably the fastest, but test them all), but I wouldn't reject them without actually testing.
It's possible that the ""random period"" of zlib.adler32 (or zlib.crc32) is too short, but I wouldn't reject it (except maybe hash) without thinking through whether it's good enough. For that matter, even hash plus a decent fixed-side blender function might be good enough (at least on a 64-bit system).
Python doesn't come with anything ""between"" md5 and `adler32' out of the box. But you can find PyPI modules or source recipes for hundreds of other hash algorithms. For that matter, if you're familiar with any particular hash algorithm that sounds good, most of them are trivial—you could probably code up, e.g., an FNV hash with xor-folding in less time than it takes you to look through the alternatives.
Also, keep in mind that you can generate a bunch of random bytes at ""new game"" time, store that in the save file, and use it as salt to your hash function.
If you've exhausted the possibilities are you really do need more randomness than a fast-enough hash function with arbitrary salt can give you alone, then:
It sounds like you'll already need to store a list of the points the user has explored (because how else do you know which points you need to restore?). And the order doesn't really matter. So, you can store them in the order of exploration. That means you can regenerate the values deterministically (just by iterating the list). Which means you can use the suggestion by @delnan on your own answer.
However, seed is not the way to do that. It isn't guaranteed to put the RNG into the same state each time across runs, Python versions, machines, etc. For that, you need setstate:

To save, call random.getstate(), and pickle and stash the result.
To load, read and unpickle the state, and call random.setstate(state).

See the docs for full details.
If you're using a random.Random instance, it's exactly the same, except of course that you have to construct a random.Random before you can call setstate on it.
This is guaranteed to work between runs of your program, across machines, etc. Even with a newer version of Python. However, it's not guaranteed to work with an older version of Python. (That is, if the user saves a game with Python 2.6, then tries to load it with 2.5, the state will not be compatible. I believe the only problems come with 2.6->older and 2.3->older, but of course there's no guarantee there won't be additional ones in the future.) I'd suggest stashing the Python version, and if they've downgraded, show a warning saying ""This save file requires Python 2.6 or later. You have Python 2.5. The load may fail. Continue anyway?""
This is only guaranteed for random.Random and for the random module itself (since the top-level module functions just use a hidden random.Random). In particular, random.SystemRandom are explicitly documented not to work.
Practically speaking, you can also just pickle a random.Random directly, because the state gets pickled in. It seems like that ought to work, or what would be the sense of pickling a Random object? And it definitely does work. But it isn't actually documented to work, so I'd stick with pickling the getstate, for safety.",A,2
18409118,2013-08-23 18:05:42.123000+00:00,"From the documentation, it looks like you're supposed to give CreatePDF an encoding, otherwise ""this is guessed by the HTML5 parser"".
So, say the HTML file's headers specify whatever legacy charset was used for Devanagari. You decode that properly to Unicode somewhere before the code you've shown us, then re-encode it as UTF-8, but the headers are specifying a different charset. In that case, html5lib will guess the wrong charset, and interpret the characters incorrectly and give you mojibake.
Of course I can't be sure that's exactly the problem you're facing without a complete example, but it's likely something like that. And the most likely solution is the same for any of them: If you encode to UTF-8, tell the converter to use UTF-8 instead of guessing:
pisaStatus = pisa.CreatePDF(
    StringIO(sourceHtml.encode('utf-8')),                 
    dest=resultFile,
    encoding='utf-8')
",A,0
35162445,2016-02-02 19:42:12.537000+00:00,"Unfortunately, the six library used by urllib3 (in turn used by requests) is not compatible with IronPython. See issue 135 in the six project tracker, which may have a work-around.
Apparently, that's the only thing standing in the way of IronPython-compatibility, see issue #2324 in the requests issue tracker.
You probably could make it work by replacing the definition for exec_() with calls directly to exec(), in the six module. urllib3 doesn't actually use any calls to six.exec_() anyway, and exec() works just fine in Python 2 code.",A,4
18409967,2013-08-23 18:56:43.733000+00:00,"Why do you want to send it as part of the form? You have it in the URL already (and therefore as the userId argument to the view), there's no need to get it from anywhere else.",A,0
21651324,2014-02-08 20:16:30.710000+00:00,"I'm not entirely certain what view is supposed to be doing what here. But just getting the folder name from the form post is easy, via request.POST['FolderName'], assuming you're submitting the form with POST which you're not but should be.
And redirecting to that name is also simple, using the redirect shortcut:
from django.shortcuts import redirect

folder_name = request.POST['FolderName']
return redirect('ShowFolder', (folder_name,))
",A,1
11651223,2012-07-25 13:48:13.100000+00:00,"Zope pagetemplates do not support a <? ?> syntax at all.
However, you can loop over your python list in the tal:repeat just fine:
<ul tal:define=""persons python: context.portal_catalog(portal_type='Person');"">
    <tal:listing repeat=""i python:[0, 2, 1]"">
        <li tal:define=""p python:persons[i]"" tal:content=""p/name"">Person name</li>
    </tal:listing>
</ul>

I suspect however, that you want to let the portal_catalog do the sorting instead, using the sort_on parameter (see the Plone KB article on the catalog):
<ul tal:define=""persons python: context.portal_catalog(portal_type='Person', sort_on='sortable_title');"">
    <tal:listing repeat=""p persons"">
        <li tal:content=""p/name"">Person name</li>
    </tal:listing>
</ul>

If you want to do anything more complex, use a browser view to do the list massaging for you.",A,4
14424098,2013-01-20 11:37:22.147000+00:00,"There are two major differences between them:

Dictionaries are unordered, a list of tuples is. So if ordering matters, use the latter.
Mapping a key to a value takes constant time in a dict, doing the same in a list of tuples takes linear time. So, the larger your amount of key-value pairs, the more time it'll take to scan the list of tuples to find a match, while in a dictionary the lookup is near-instant, always.
(If your tuples are kept in sorted order, you can reduce search time to O(log n) by using binary search; but that's still slower than constant time for dictionaries).

In most cases, you use a dict. Even if ordering is required, you can use a collections.OrderedDict instead to get the best of both worlds.",A,5
19913074,2013-11-11 18:16:56.163000+00:00,"You have a directory named pygame in your path somewhere.
$ mkdir pygame  # empty directory
$ python3.3
>>> import pygame
>>> pygame
<module 'pygame' (namespace)>
>>> pygame.__path__
_NamespacePath(['./pygame'])

Remove or rename this directory, it is masking the actual pygame package.
If you use print(pygame.__path__) it'll tell you where the directory was found; in the above example it was found relative to the current directory (./).",A,5
30233671,2015-05-14 09:23:33.230000+00:00,"You are comparing strings with integers. Your if test never matches:
if i in list:

because each i is a string. Your list variable on the other hand, contains integers:
list = range(1,50)

Convert i to an integer to test against other integers:
if int(i) in list:

Some other remarks:

list is not a good variable name; you are masking the built-in type.
You could just test if i falls in a range by using comparison operators against the start and end values: 
if 1 <= int(i) < 50:

which would be faster as you don't have to scan through the list each time.
You cannot assume that the key is already present in counter. You'd have to test first or use counter.get() to return a default. For example:
counter[i] = counter.get(i, 0) + 1

To count your values, you could use the standard library collections.Counter() class:
from collections import Counter

counter = Counter()

for lines in f:
    tab_lines = lines.split()  # note, no argument!
    formatted_tab = map(int, tab_lines[-1].split(','))
    counter.update(i for i in formatted_tab if 0 < i < 50)

print counter.most_common()

In my testing, I didn't see any numbers in that file that where outside the range of 0 to 50 (exclusive), so you can probably get away with just counter.update(formatted_tab).
",A,1
16404513,2013-05-06 18:03:33.143000+00:00,"
The problem that I'm having is the way the multilines are being stored to the file, it's stored in list format e.g ['line1', 'line2', 'line3'] which is making it difficult to read as multilines because I can't get it to be read as a list, when I tried it added the whole stored string into one list item. I don't know if I'm writing to the file correctly.

OK, so the problem is with writing the file. You're reading it in correctly, it just doesn't have the data you want. And the problem is in your raw_lines function. First it assembles a list of lines in the result variable, which is good. Then it does this:
result = str(result)
result.replace('[','').replace(']','')

There are two small problems and one big one here.
First, replace:

Return[s] a copy of the string with all occurrences of substring old replaced by new.

Python strings are immutable. None of their methods change them in-place; all of them return a new string instead. You're not doing anything with that new string, so that line has no effect.
Second, if you want to join a sequence of strings into a string, you don't do that by calling str on the sequence and then trying to parse it. That's what the join method is for. For example, if your lines already end with newlines, you want ''.join(result). If not, you want something like '\n'.join(result) + '\n'. What you're doing has all kinds of problems—you forgot to remove the extra commas, you will remove any brackets (or commas, once you fix that) within the strings themselves, etc.
Finally, you shouldn't be doing this in the first place. You want to return something that can be passed to writelines, which:

Write[s] a sequence of strings to the file. The sequence can be any iterable object producing strings, typically a list of strings.

You have a list of strings, which is exactly what writelines wants. Don't try to join them up into one string. If you do, it will run, but it won't do the right thing (because a string is, itself, a sequence of 1-character strings).
So, if you just remove those two lines entirely, your code will almost work.
But there's one last problem: raw_input:

… reads a line from input, converts it to a string (stripping a trailing newline), and returns that.

But writelines:

… does not add line separators.

So, you'll end up with all of your lines concatenated together. You need the newlines, but raw_input throws them away. So, you have to add them back on. You can fix this with a simple one-line change:
result.append(line + '\n')
",A,1
20662360,2013-12-18 15:41:13.867000+00:00,"If you wanted to produce a list, just use a list comprehension:
def square_generator(optional_parameter):
    return [x ** 2 for x in somelist if x > optional_parameter]

or turn call list() on the generator (provided you didn't use list as a local name):
list(h)

Your generator is working just fine otherwise:
>>> somelist = [1, 4, -5, 10, -7, 2, 3, -1]
>>> def square_generator(optional_parameter):
...     return (x ** 2 for x in somelist if x > optional_parameter)
... 
>>> list(square_generator(0))
[1, 16, 100, 4, 9]
",A,3
3127248,2010-06-27 11:50:53.820000+00:00,"This sort of thing is rightly left out of the Django tutorial, since it has nothing to do with Django. If you don't know basic Python, then you need to follow a Python tutorial.
The main thing wrong with the code you gave is that you haven't actually called the index method, you've just referred to it. In Python, as in many other languages, to call a method you need to use method(). So you need:
Page.objects.all()[0].index()
",A,1
14289444,2013-01-12 02:04:25.777000+00:00,"First, the memory of your machine is irrelevant. It's the size of your process's address space that's relevant. With a 32-bit Python, this will be somewhere under 4GB. With a 64-bit Python, it will be more than enough.
The reason for this is that mmap isn't about mapping a file into physical memory, but into virtual memory. An mmapped file becomes just like a special swap file for your program. Thinking about this can get a bit complicated, but the Wikipedia links above should help.
So, the first answer is ""use a 64-bit Python"". But obviously that may not be applicable in your case.
The obvious alternative is to map in the first 1GB, search that, unmap it, map in the next 1GB, etc. The way you do this is by specifying the length and offset parameters to the mmap method. For example:
m = mmap.mmap(f.fileno(), length=1024*1024*1024, offset=1536*1024*1024)

However, the regex you're searching for could be found half-way in the first 1GB, and half in the second. So, you need to use windowing—map in the first 1GB, search, unmap, then map in a partially-overlapping 1GB, etc.
The question is, how much overlap do you need? If you know the maximum possible size of a match, you don't need anything more than that. And if you don't know… well, then there is no way to actually solve the problem without breaking up your regex—if that isn't obvious, imagine how you could possibly find a 2GB match in a single 1GB window.
Answering your followup question:

Since I set the buffer to 10MB, in terms of performance, is it the same as I mmap 10MB of file?

As with any performance question, if it really matters, you need to test it, and if it doesn't, don't worry about it.
If you want me to guess: I think mmap may be faster here, but only because (as J.F. Sebastian implied) looping and calling re.match 128K times as often may cause your code to be CPU-bound instead of IO-bound. But you could optimize that away without mmap, just by using read. So, would mmap be faster than read? Given the sizes involved, I'd expect the performance of mmap to be much faster on old Unix platforms, about the same on modern Unix platforms, and a bit slower on Windows. (You can still get large performance benefits out of mmap over read or read+lseek if you're using madvise, but that's not relevant here.) But really, that's just a guess.
The most compelling reason to use mmap is usually that it's simpler than read-based code, not that it's faster. When you have to use windowing even with mmap, and when you don't need to do any seeking with read, this is less compelling, but still, if you try writing the code both ways, I'd expect your mmap code would end up a bit more readable. (Especially if you tried to optimize out the buffer copies from the obvious read solution.)",A,29
32136376,2015-08-21 09:01:05.943000+00:00,"You can have multiple keys matching, so you'd need to produce a set (since order doesn't matter and keys are unique):
{key for key, words in d.iteritems() if search_word in words}

If you are only interested in the first match (because you keep your words unique, say), you can use next() and a generator expression:
next((key for key, words in d.iteritems() if search_word in words), None)

You probably want to create an inverse index if you need to test for multiple words:
reverse_index = {}
for key, words in d.iteritems():
    for word in words:
        reverse_index.setdefault(word, set()).add(key)

after which you can just use:
reverse_index.get(search_word, set())

to get the same result.
For unique words in your lists, the reverse index would simply be:
reverse_index = {word: key for key, words in d.iteritems() for word in words}
reverse_index.get(search_word)
",A,4
18290428,2013-08-17 15:32:37.197000+00:00,"You're trying to prepopulate the select box by passing the title, which is presumably what is displaying in the control. But you should pass the underlying value, which in the case of a ForeignKey is the id of the related object.",A,1
25296850,2014-08-13 21:54:44.943000+00:00,"No, your browser doesn't send any time information when making a request. The server would not be able to adjust the timezone in the HTML sent.
The common way to do what you observed is using JavaScript; see get client time zone from browser for example. Your browser executed code that translated the datetime in the page to your local timezone, not the server.
This is far more efficient as well; the server just has to send one version of the HTML page to every client; this is far more cachable.",A,0
5550178,2011-04-05 10:03:39.287000+00:00,"The locale is rarely part of the database.
The locale is part of the presentation of data in the database.
So each user provides their locale.  
Your application uses ordinary localization to translate numbers and dates.
Your application uses ordinary i18n libraries to translate the database text and application messages into localized text.",A,3
13928029,2012-12-18 07:10:16.147000+00:00,"You'll have to make the translation manually; go through the string with a regular expression for example, and replace each occurrence with the hex equivalent.
import re

replchars = re.compile(r'[\n\r]')
def replchars_to_hex(match):
    return r'\x{0:02x}'.format(ord(match.group()))

replchars.sub(replchars_to_hex, inputtext)

The above example only matches newlines and carriage returns, but you can expand what characters are matched, including using \x escape codes and ranges.
>>> inputtext = 'Some example containing a newline.\nRight there.\n'
>>> replchars.sub(replchars_to_hex, inputtext)
'Some example containing a newline.\\x0aRight there.\\x0a'
>>> print(replchars.sub(replchars_to_hex, inputtext))
Some example containing a newline.\x0aRight there.\x0a
",A,4
325825,2008-11-28 13:59:16.553000+00:00,"Stored procedures will not make things faster.  
However, rearranging your logic will have a huge impact.  The tidy, focused transactions that you design when thinking of stored procedures are hugely beneficial.
Also, stored procedures tend to use bind variables, where other programming languages sometimes rely on building SQL statements on-the-fly.  A small, fixed set of SQL statements and bind variables is fast.  Dynamic SQL statements are slow.
An application which is ""running slow lately"" does not need coding changes.

Measure.  Measure.  Measure.  ""slow"" doesn't mean much when it comes to performance tuning.  What is slow? Which exact transaction is slow?  Which table is slow?  Focus.
Control all change.  All.  What changed?  OS patch?  RDBMS change?  Application change?  Something changed to slow things down.  
Check for constraints in scale.  Is a table slowing down because 80% of the data is history that you use for reporting once a year?

Stored procedures are never the solution to performance problems until you can absolutely point to a specific block of code which is provably faster as a stored procedure.",A,5
12863734,2012-10-12 16:54:15.887000+00:00,"It is a good question. It shows three issues with a text representation at once:

'\a' Python string literal is a single BELL character.
To input backslash followed by letter 'a' in Python source code you need either use raw-literals: r'\a' or escape the slash '\\a'.
r'\d' (two characters) has special meaning when interpreted as a regular expression (r'\d' means match a digit in a regex).
In addition to rules for Python string literals you also need to escape possible regex metacharacters. You could use re.escape(your_string) in general case or just r'\\d' or '\\\\d'. '\a' in the repl part should also be escaped (twice in your case: r'\\a' or '\\\\a'):
>>> old, new = r'%hello%\dir', r'C:\art'
>>> print re.sub(re.escape(old), new.encode('string-escape'), xml)
<Tag name=""low""     Value=""C:\art""/>

btw, you don't need regular expressions at all in this case:
>>> print xml.replace(old, new)
<Tag name=""low""     Value=""C:\art""/>

at last XML attribute value can't contain certain characters that are also should be escaped e.g., '&', '""', ""<"", etc.

In general you should not use regex to manipulate XML. Python's stdlib has XML parsers.
>>> import xml.etree.cElementTree as etree
>>> xml = r'<Tag name=""low""     Value=""%hello%\dir""/>'
>>> tag = etree.fromstring(xml)
>>> tag.set('Value', r""C:\art & design"")
>>> etree.dump(tag)
<Tag Value=""C:\art &amp; design"" name=""low"" />
",A,0
17613172,2013-07-12 10:45:40.173000+00:00,"You are building a string with literal quotes. You don't need to, omit the ' characters.
path = ""//"" + ns + header
p = tree.xpath(path, namespaces=nsmap)

or use string formatting:
path = ""//{}{}"".format(ns, header)
p = tree.xpath(path, namespaces=nsmap)

Your original version was the equivalent of:
path = ""'//xis:Jv-Ins-Reinsurance'""

(note the extra single quote characters).",A,0
25230908,2014-08-10 16:52:07.910000+00:00,"You're on the right track: If you want to use fnmatch-style patterns, you should use fnmatch.filter with them.
But there are three problems that make this not quite trivial.
First, you want to apply multiple filters. How do you do that? Call filter multiple times:
for ignore in ignore_files:
    filenames = fnmatch.filter(filenames, ignore)

Second, you actually want to do the reverse of filter: return the subset of names that don't match. As the documentation explains:

It is the same as [n for n in names if fnmatch(n, pattern)], but implemented more efficiently.

So, to do the opposite, you just throw in a not:
for ignore in ignore_files:
    filenames = [n for n in filenames if not fnmatch(n, ignore)]

Finally, you're attempting to filter on partial pathnames, not just filenames, but you're not doing the join until after the filtering. So switch the order:
filenames = [os.path.join(root, filename) for filename in filenames]
for ignore in ignore_files:
    filenames = [n for n in filenames if not fnmatch(n, ignore)]
matches.extend(filenames)


There are few ways you could improve this.
You may want to use a generator expression instead of a list comprehension (parentheses instead of square brackets), so if you have huge lists of filenames you're using a lazy pipeline instead of wasting time and space repeatedly building huge lists.
Also, it may or may not be easier to understand if you invert the order of the loops, like this:
filenames = (n for n in filenames 
             if not any(fnmatch(n, ignore) for ignore in ignore_files))

Finally, if you're worried about performance, you can use fnmatch.translate on each expression to turn them into equivalent regexps, then merge them into one big regexp and compile it, and use that instead of a loop around fnmatch. This can get tricky if your patterns are allowed to be more complicated than just *.jpg, and I wouldn't recommend it unless you really do identify a performance bottleneck here. But if you need to do it, I've seen at least one question on SO where someone put a lot of effort into hammering out all the edge cases, so search instead of trying to write it yourself.",A,8
6626734,2011-07-08 15:28:58.587000+00:00,"This is the wrong way to do it.
Django has a method for splitting urls.py into separate files, and that is to use include(). Keep the urls for each application in its own urls.py, and include them into the main one.",A,2
628622,2009-03-10 01:30:00.703000+00:00,"Also, the knapsack problem (which is NP-hard) shows up fairly frequently.  It's a seductive trap for attempting to optimize things.",A,1
39052023,2016-08-20 08:17:30.357000+00:00,"You need to pass the current ordering value into the template and use it in the next/previous page links:
class CarList(LoginRequiredMixin, ListView):
    def get_context_data(self, *args, **kwargs):
        context = super(CarList, self).get_context_data(*args, **kwargs)
        context['current_order'] = self.get_ordering()
        return context

...
<a href=""/feriehus?ordering={{ current_order }}&page={{ page_obj.previous_page_number }}"">

and so on.",A,2
13281460,2012-11-08 02:11:28.363000+00:00,"If you want to loop over all of the URLs, you have to put the code that processes each URL into the loop. But you haven't done that. All you have is:
for url in urls:
    line = url

This will reassign the variables url and line over and over, finally leaving them both pointing to the last URL. And then, when you call size = urlopen(url) outside the loop, it will work on the last URL.
Try this:
with open('urls.txt') as inf:
    urls = (line.strip() for line in inf)
    for url in urls:
        site = urlopen(url)   
        soup = BeautifulSoup(site)
        for td in soup.find_all('td', {'class': 'subjectCell'}):
            print td.find('a').text
",A,0
11509461,2012-07-16 17:31:35.417000+00:00,"I don't see why you need any data migration at all.
Just move the models to the new app, and add a db_table setting in the inner Meta classes to point to the old table names.",A,14
3934895,2010-10-14 15:33:15.153000+00:00,"
Normally, I would have thought that the logic will be implemented in the try branch

""Normally""?  What does normally mean?
What is the program supposed to do?  What exceptions make sense?  What does the program do in response to the exceptions.
There's no ""normally"".  Any more than there's a normal assignment statement or a normal function definition.
Your program does what makes sense to achieve the required end-state.  There's no ""normally"".",A,1
27903667,2015-01-12 13:58:44.923000+00:00,"You went wrong here:

I went to this site https://pypi.python.org/pypi/setuptools and copied the code, opened my python 3.3 IDLE, pasted it into it and saved a file named ez_setup.py.

You saved a Python shell session, not the script itself. The shell session is the output produced by code you run in the shell, and the first lines of the shell include the Python version.
Use the New File option (CMD-N) and paste the code in there, then save that.",A,1
53659962,2018-12-06 21:30:57.233000+00:00,"This isn't the right approach. You shouldn't override __init__ for an admin class.
Rather, define get_list_display and access self.model:
def get_list_display(request):
    return [field.name for field in self.model._meta.get_fields()]
",A,0
15790785,2013-04-03 14:54:35.393000+00:00,"Nesting loops in list comprehensions work the same way as nesting regular for loops, one inside the other:
y = [x for i in range(3) for x in board[1][i]]

but in this case, just selecting board[1][:] would be easier and give you the same result; a copy of the middle row.
If you need to apply an expression to each column in that row, then just loop over board[1] directly:
y = [foobar(c) for c in board[1]]
",A,2
50756370,2018-06-08 08:20:31.247000+00:00,You've indented create_question so that it's within the QuestionModelTests class. I don't think you meant to do that - it's not used in that class and does not reference any class attributes. You should unindent it so that it is outside all classes.,A,0
28201425,2015-01-28 20:08:35.090000+00:00,"You are converting a number into bytes, not hex. Use the struct module to do this:
struct.pack('>H', values)

This produces an unsigned short, 2 bytes, in big endian order; for the length 4 that'd produce \x00\x04:
>>> import struct
>>> struct.pack('>H', 4)
b'\x00\x04'
",A,2
950531,2009-06-04 13:19:48.800000+00:00,"One possibility would be to subclass the transaction middleware so that it sends a custom signal on commit. Your code could listen for that signal, rather than post_save.",A,1
25004025,2014-07-28 20:45:08.797000+00:00,"You need a more complex key function:
sorted(dic1.iteritems(), key=lambda i: i[1][0].lower())

This sorts on the first element of the value, lowercased.
Demo:
>>> from pprint import pprint
>>> dic1={""720155"": [""CAT"", ""Software""], ""356d05"": [""ESF"", ""Software""], ""3b3758"": [""DBA"", ""Software""], ""9649db"": [""Fun"", ""Software""], ""96493f"": [""Eagle"", ""Software""], ""99701d"": [""Pas"", ""Software""], ""964971"": [""Debug"", ""Software""], ""b6f315"": [""Bap"", ""Software""], ""a0a824"": [""Server"", ""Software""], ""1e00sa"": [""Adobe"", ""Software""], ""8c8dd2"": [""EXIT"", ""Software""], ""cc1dfg"": [""email"", ""Software""]}
>>> pprint(sorted(dic1.iteritems(), key=lambda i: i[1][0].lower()))
[('1e00sa', ['Adobe', 'Software']),
 ('b6f315', ['Bap', 'Software']),
 ('720155', ['CAT', 'Software']),
 ('3b3758', ['DBA', 'Software']),
 ('964971', ['Debug', 'Software']),
 ('96493f', ['Eagle', 'Software']),
 ('cc1dfg', ['email', 'Software']),
 ('356d05', ['ESF', 'Software']),
 ('8c8dd2', ['EXIT', 'Software']),
 ('9649db', ['Fun', 'Software']),
 ('99701d', ['Pas', 'Software']),
 ('a0a824', ['Server', 'Software'])]
",A,3
42009105,2017-02-02 17:54:13.003000+00:00,"You're slightly confused about how to create a model instance from a form. The instance is returned from the call to form.save(); it's that instance to which you need to assign the address and user.
def forms_valid(self, organization_form, address_form):
    address = address_form.save(commit=False)
    address.country = ""DE""
    address.save()

    organization = organization_form.save(commit=False)
    organization.owner = self.request.user
    organization.address = address
    organization.save()

    return redirect(""list"")
",A,2
17882939,2013-07-26 13:43:28.237000+00:00,"Use a raw string literal:
re.compile(r'NSLocalizedString\(@""(?:\\.|[^""\\]*)"",\s*@""(?:\\.|[^""\\]*)""\s*\)', re.DOTALL)

because backslashes have meaning in a regular Python string too. A raw string literal (a string literal prefixed with r) ignores (most) escape sequences that Python supports.
See The Backslash Plague in the Python Regular Expression HOWTO.",A,7
35911286,2016-03-10 08:35:31.367000+00:00,"It's independent of the number of characters that match, not the number of characters in total. It is, as you note, linear in the number of characters in the string.
It's intended to avoid timing exploits that rely on comparison algorithms that bail as soon as they reach a non-matching character.",A,2
5652724,2011-04-13 16:47:02.697000+00:00,"The start and end fields are standard Archetypes DateTimeField fields, using the default CalendarWidget widgets.
The CalendarWidget reuses some fairly old and crufty calendar macros from Plone, and these read the range of selectable years from the site properties. You can change these in the ZMI, find the portal_properties tool, then the site_properties property sheet within that. The two properties to look for are:
calendar_starting_year

The starting year to show in the calendar widget. Default is 2001.

calendar_future_years_available

The number of future years, after the current year, to show in the calendar widget. Default is empty, and the widget then falls back to 5 years.

Also see the site properties documentation; you can also use a GenericSetup profile to set these.
These values apply to all usage of the calendar macros. You can also set this for just the event type, by setting the starting_year, ending_year and/or future_years properties on the CalendarWidget for the startDate and endDate fields. If you set an ending_year the future_years property is ignored.
Monkey-patch style altering of the event schema:
from Products.Archetypes.content import event

ATEventSchema['startDate'].widget.starting_year = 1999
ATEventSchema['startDate'].widget.ending_year = 2020
ATEventSchema['endDate'].widget.starting_year = 1999
ATEventSchema['endDate'].widget.ending_year = 2020
",A,3
441446,2009-01-14 00:24:41.657000+00:00,"The primary use of with statement is an exception-safe cleanup of an object used in the statement. with makes sure that files are closed, locks are released, contexts are restored, etc.
Does csv.reader have things to cleanup in case of exception?
I'd go with:
with open(""myfile.csv"") as f:
    for row in csv.reader(f):
        # process row

You don't need to submit the patch to use csv.reader and with statement together.
import contextlib

Help on function contextmanager in module contextlib:
contextmanager(func)
    @contextmanager decorator.

Typical usage:
    @contextmanager
    def some_generator(<arguments>):
        <setup>
        try:
            yield <value>
        finally:
            <cleanup>

This makes this:
    with some_generator(<arguments>) as <variable>:
        <body>

equivalent to this:
    <setup>
    try:
        <variable> = <value>
        <body>
    finally:
        <cleanup>

Here's a concrete example how I've used it: curses_screen.",A,19
7410491,2011-09-14 02:01:18.510000+00:00,"
Since I also want to learn Microsoft Access, 

Don't waste your time learning Access.

I suggest putting the data in there. Is there any easy way to do this? 

ODBC.

Or, is there some other program/method I should know for such a situation?

SQLite and MySQL are far, far better choices than MS-Access.",A,0
47672379,2017-12-06 10:45:55.593000+00:00,"You need to flush the file you write:
with NamedTemporaryFile(mode=""w"", suffix='.py', delete=False) as temp_file:
    temp_file.write(counter_code)
    temp_file.flush()

Your program is too short to ever make it out of the buffer otherwise, and you are running an empty file.
I'd also make sure the program then flushes on every write; either run python3 -u for unbuffered stdout or use flush=True on every print. That way you don't have to add so many sleeps in your parent program.
With these changes, the unittest prints
line = ""b'0\n'""

and several warnings about the subprocess still running and the stdout buffer still being open before completing.",A,1
33189530,2015-10-17 17:31:54.600000+00:00,"You've configured gunicorn to bind on a TCP port, but gunicorn is binding on a unix socket. You should use the same thing; preferably the socket, so it doesn't conflict with the port nginx is actually listening on.
In gunicorn_config.py:
bind = 'unix:/tmp/gunicorn_project.sock'
",A,1
13629066,2012-11-29 15:26:49.740000+00:00,"Use os.walk() to generate that information:
datafiles = [(root, [os.path.join(root, f) for f in files])
    for root, dirs, files in os.walk(datadir)]

That'll produce absolute paths; you can process the root variable a little more to make them relative to the setup.py directory if needed.",A,9
49212034,2018-03-10 17:36:28.490000+00:00,"You've redefined the signature of the form so that it accepts a user element. But you haven't actually passed that user when you create the form. How else are you expecting it to get into the form?
if request.method == 'POST':
    form = InvitePlayerForm(request.user, request.POST)
    ...
else:
    form = InvitePlayerForm(request.user)

(Note also, your queryset would be better as Team.objects.filter(userteam__UserId=user.) ",A,0
51451825,2018-07-21 00:30:42.420000+00:00,"The problem is that you're explicitly encoding your string into a UTF-8 bytes, and then turning that UTF-8 bytes into its string representation.
That's what this code means:
str(row[3].encode(""utf-8""))

If you don't want to do that, just don't do that:
row[3]

Here's an example that shows what you're doing:
>>> s = 'à'
>>> s
'à'
>>> s.encode('utf-8')
b'\xc3\xa0'
>>> str(s.encode('utf-8'))
""b'\\xc3\\xa0'""

What you want here is the first one.
More generally, calling str on a bytes is almost never useful. If you unavoidably have a bytes and you need a str, you get it by calling the decode method. But in this case, you don't unavoidably have a bytes. (I mean, you could write row[3].encode(""utf-8"").decode(""utf-8""), but that would obviously be pretty silly.)

As a side note—but a very important one—you should not be trying to str.format your values into the SQL string. Just use query parameters. Here's the obligatory xkcd link that explains the security/safety problem, and on top of that, you're making your code much more complicated, and even less efficient.
In other words, instead of doing this:
""VALUES ({:d}, \""{:s}\"", \""{:s}\"", \""{:s}\"", \""{:s}\"", \""{:s}\"", \""{:s}\"")"".format(row[0], urlparse(row[1]).netloc, row[1], row[2].replace(""\"""", ""'""), article_content, datetime.fromtimestamp(row[4]).strftime(""%Y-%m-%d""), updated)

… just do this:
""VALUES (%s, %s, %s, %s, %s, %s, %s)""

And then, when you later execute the query, pass the arguments—without all that complicated converting to strings and quoting and replacing embedded quotes, just the values as-is—as the arguments to execute.
db.execute(q_i, (
    row[0], urlparse(row[i]).netloc, row[1], row[2], article_content, 
    datetime.fromtimestamp(row[4]).strftime(""%Y-%m-%d""), updated))

In fact, if your next to last column is—or could be—a DATETIME column rather than a CHAR/VARCHAR/TEXT/whatever, you don't even need that strftime; just pass the datetime object.
And notice that this means that you don't need to do anything at all to article_content. The quote stuff is neither necessary nor a good idea (unless you have some other, app-specific reason that you need to avoid "" characters in articles), and the encoding stuff is not solving any problem, but only causing a new one.",A,1
38412504,2016-07-16 15:15:33.280000+00:00,"BASE_DIR is pointing to the parent directory of PROJECT_ROOT. You can re-write the two definitions as:
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(PROJECT_ROOT)

because the os.path.dirname() function simply removes the last segment of a path.
In the above, the __file__ name points to the filename of the current module, see the Python datamodel:

__file__ is the pathname of the file from which the module was loaded, if it was loaded from a file.

However, it can be a relative path, so the os.path.abspath() function is used to turn that into an absolute path before removing just the filename and storing the full path to the directory the module lives in in PROJECT_ROOT.",A,18
5623212,2011-04-11 14:56:46.020000+00:00,"Yes, you seem to be on the right track. One quibble: to check if an email address exists in the db, you should ideally use a GET rather than a POST, as no information is being updated - you are simply asking if something exists.
One alternative to returning JSON is to return pre-rendered HTML which you can inject directly into the DOM at the appropriate point. The advantage of doing it this way is that you can use Django's template mechanism to render your existing template fragments - the only difference is that you render a fragment in isolation, rather than including/extending it within an entire HTML page.",A,3
29235090,2015-03-24 14:22:17.517000+00:00,"Python functions are descriptors and Python binds these to the instance they are looked up on, or in the case of classmethod, to the class. Because you didn't use the classmethod decorator on the replacement function, it is being bound wrongly (as a regular method instead, so no cls is being passed in).
Simply wrap the target in a classmethod decorator manually:
with mock.patch.object(MyClass, 'my_method', classmethod(mocked_method)):
    MyClass.my_method()

Here I applied the @classmethod decorator manually, but you could also just use it as intended, as a decorator, directly on the target function:
@classmethod
def mocked_method(cls):
    print('I want this method to get called')

with mock.patch.object(MyClass, 'my_method', mocked_method):
    MyClass.my_method()

Demo:
>>> import mock
>>> class MyClass(object):
...     @classmethod
...     def my_method(cls):
...         print('my_method')
... 
>>> def mocked_method(cls):
...     print('I want this method to get called')
... 
>>> with mock.patch.object(MyClass, 'my_method', classmethod(mocked_method)):
...     MyClass.my_method()
... 
I want this method to get called
",A,8
12825001,2012-10-10 17:29:10.843000+00:00,"In this case, a regular expression would work just fine:
import re
ref = re.compile(u'<ref.*?ref>', re.DOTALL)

ref.sub(u'', yourtext)

Note the re.DOTALL qualilifier; you have newlines inside your <ref> sections, and we want to remove those too.
Demo:
>>> import re
>>> tst=u'\'\'\'Joseph Michael ""Joe"" Acaba\'\'\' (born May 17, 1967) is an [[Teacher|educator]], [[Hydrogeology|hydrogeologist]], and [[NASA]] [[astronaut]].<ref name=""bio"">{{Cite web|url=http://www.jsc.nasa.gov/Bios/htmlbios/acaba-jm.html|title=Astronaut Bio: Joseph Acaba|month=February | year=2006|publisher=[[NASA|National Aeronautics and Space Administration]]|author=NASA|accessdate=November 26, 2006}}</ref><ref name=""bio2"">{{Cite web|url=http://oeop.larc.nasa.gov/hep/hep-astronauts.html|title=NASA Hispanic Astronauts\n|publisher=National Aeronautics and Space Administration|author=NASA|accessdate=November 26, 2006}}</ref> In May 2004 he became the first person'
>>> ref = re.compile(u'<ref.*?ref>', re.DOTALL)
>>> ref.sub(u'', tst)
u'\'\'\'Joseph Michael ""Joe"" Acaba\'\'\' (born May 17, 1967) is an [[Teacher|educator]], [[Hydrogeology|hydrogeologist]], and [[NASA]] [[astronaut]]. In May 2004 he became the first person'
",A,4
17355535,2013-06-28 00:48:14.833000+00:00,"You haven't given us enough information to be sure, but there's a pretty good chance this is your problem:
If some_text is a unicode object, then this line:
html1, html2 = html.split(some_text) # this line spits out the error

… is calling split on a str, and passing a unicode parameter. Whenever you mix str and unicode in the same call, Python 2.x handles that by automatically calling unicode on the str. So, that's equivalent to:
html1, html2 = unicode(html).split(some_text) # this line spits out the error

… which is equivalent to:
html1, html2 = html.decode(sys.getdefaultencoding()).split(some_text) # this line spits out the error

… which will fail if there are any non-ASCII characters in html, exactly as you're seeing.

The easy workaround is to explicitly encode some_text to UTF-8:
html1, html2 = html.split(some_text.encode('utf-8'))

But personally, I wouldn't even try to work with str objects from 3 different charsets all in the same program. Why not just decode/encode at the very edges, and just deal with unicode objects everywhere in between?",A,1
19103257,2013-09-30 20:30:25.073000+00:00,"@app.route(""/"", methods = [""GET"",""PUT""]) is an executable statement: it calls the route() method of the app object. Since it's at module level, it will be executed when the script is imported.
Now, the result of calling app.route(...) is a function, and because you've used the @ to mark it as a decorator, that function will wrap index. Note that the syntax is just a shortcut for this:
index = app.route(...)(index)

in other words, Python will call the function returned by app.route() with index as a parameter, and store the result as the new index function. 
However, you're missing a level here. A normal decorator, without params, is written like this:
@foo
def bar()
   pass

and when the module is imported, foo() is run and returns a function that wraps bar. But you're calling your route() function within the decorator call! So actually your function needs to return a decorator function that itself returns a function that wraps the original function... headscratching, to be sure.
Your route method should look more like this:
def route(self, *rargs, **kargs):
    args = list(rargs)
    if kargs:
        print(kargs['methods'])
    def decorator(f):
        def wrapped(index_args):
            f(args[0])
        return wrapped
    return decorator
",A,2
8386461,2011-12-05 14:03:43.893000+00:00,"In the line above, you've called pop which removes the item from the dictionary.",A,3
30379250,2015-05-21 16:19:16.433000+00:00,"The code tries to expand variables in your Makefile, but it could not find a specific variable.
The code first parses out all VARNAME = value entries, and then in a second step tries to insert values into $(VARNAME) and ${VARNAME} references. It is that second step that failed as a variable name was not defined, or at least not found in the first step.
The pygame build system creates a makefile named Setup from a file named Setup.in, and it is the generated file that is failing here. Where Setup.in has a section marked with #--StartConfig / #--EndConfig variables should be written. You'll have to investigate what lines are missing and why.
You can rerun the config.py script to re-generate the file; it delegates to platform-specific modules for the work.",A,2
52230013,2018-09-07 21:50:07.757000+00:00,"You have multiple problems here, and I'm not sure which one you're stuck on, so I'll try to cover everything.

First, how do you ""emojify"" a string?
You need to define exactly what that means—what set of inputs do you map to what outputs, is if :8 is an input does that mean 80:80 should turn the :8 in the middle into an emoji (and, if not, what exactly is the rule that says why not), etc.
Then it's not that hard to implement—whether you're looping over s.split() or a more complicated re.finditer, whether your rules need to take context outside the current pattern into account, etc. depends on what rules you chose.
At any rate, there are a number of libraries on PyPI that do a variety of variations on this, and you already found at least one, emoji. so I'll assume this problem is solved.

Next, where do you hook that into your code?
Presumably, you have a callback that gets fired whenever the user submits a chat message. I don't know exactly how you set that up, but it doesn't really matter, so let's just make up a simple example:
def callback(widget):
    msg = widget.get()
    conn.sendall(msg)
    msglog.append(f'>>> {msg}')

This function gets the string contents out of a message input widget, and then both sends that string to the server, and displays it in a message log widget. So, all you need to do is:
def callback(widget):
    msg = widget.get()
    msg = emojify(msg)
    conn.sendall(msg)
    msglog.append(f'>>> {msg}')


Third, how do you deal with the fact that tkinter can't handle emoji (and other non-BMP characters) correctly?
This part is a dup of the question you linked. Tkinter has a known bug, which is still there as of 3.7, and the best known workaround is kind of ugly.
But, while it may be ugly, it's not that hard. The answer to that question links to another question, where Martijn Pieters provides a nice with_surrogates function that you can just call on your emojified string.
If you want something simple but hacky—which will make the server basically useless with anything but a buggy tkinter client—you can do that before sending the messages over the wire:
def callback(widget):
    msg = widget.get()
    msg = emojify(msg)
    tkmsg = with_surrogates(msg)
    conn.sendall(tkmsg)
    msglog.append(f'>>> {tkmsg}')

But a cleaner solution is to send them over the wire as proper UTF-8, and only surrogatize it for display, both here:
def callback(widget):
    msg = widget.get()
    msg = emojify(msg)
    conn.sendall(msg)
    tkmsg = with_surrogates(msg)
    msglog.append(f'>>> {tkmsg}')

… and in the code that receives messages from other users:
def on_msg(msg):
    tkmsg = with_surrogates(msg)
    msglog.append(tkmsg)
",A,2
14926243,2013-02-17 21:25:40.593000+00:00,"You can set that information on the response directly, as documented in the Vary Attributes of Rendered Responses section of the Pyramid manual:
@view_config(route_name='login', renderer='json', request_method='POST')
def post_login(request):
   ...
   ... authentication logic
   ...
   headers = remeber(request, login)
   request.response.headerlist.extend(headers)
   return { 'successful': True, 'message': 'auth OK'}
",A,11
18520738,2013-08-29 20:52:13.920000+00:00,"You are conflating thruthness with equality; they are not the same thing. Use bool() if you need to compare to actual True and False values:
>>> bool('abc')
True

You generally do not compare directly with == True or == False. Rather, you use conditional expressions or statements:
if 'abc':
    pass

while 'def':
    pass

foo if 'abc' else bar

Only empty containers, numeric zero and False and None are falsey, everything else is thruthy; you can use bool() to test for that condition explicitly. You can make custom types  look like empty containers by implementing __len__() and returning 0, or look like a number by implementing __nonzero__()* and returning False when the instance is to be the boolean equivalent of numeric zero. 
*In Python 3, use __bool__ instead.",A,6
27965109,2015-01-15 14:01:19.527000+00:00,"You are discouraged from using such names in your APIs, yes, because if and when the Python core developers pick the same name for Python itself, you'll lose out, your API can break.
So, if you are not a Python core developer or writing a PEP that may be one day become part of the Python standard library or core language definition, try to stay away from using dunder names in your API.",A,11
14871038,2013-02-14 08:57:59.817000+00:00,"The underlying setuptools code also scans for homepage and download links from the simple index and does this quite aggressively.
The couchdbkit setup.py file lists http://hg.e-engura.org/couchdbkit/ as the homepage, so all homepage links on the simple index link there.
You can prevent zc.buildout from trying to connect to that host by setting up a whitelist of hosts it can connect to:
[buildout]
# ...

allow-hosts =
   *.python.org
   *.google.com
   *.googlecode.com
   *.sourceforge.net

for example.",A,1
36826333,2016-04-24 17:10:08.420000+00:00,"Use the itertools.izip_longest() function instead, and tell it what to use for those missing columns; for a CSV file an empty string would work:
for row in izip_longest(x, y, fillvalue=''):
    # ...

Note that in Python 3, the i prefix was dropped from the function name.
If you are writing these to a CSV file, you can send the whole object to the csv.writer.writerows() function directly:
import csv
from itertools import izip_longest

with open(filename, 'wb') as outf:
    writer = csv.writer(outf)
    writer.writerows(izip_longest(x, y, fillvalue=''))
",A,5
2180031,2010-02-01 20:56:27.857000+00:00,"Read up on HTTP Digest Authentication.  If you set the highest QOP and use client nonce values, it's quite nice.
Your question is missing some really important architectural details.  If you're using REST, then you must use something like HTTP Digest.  You might also want to use SSL.",A,0
18097273,2013-08-07 07:32:13.357000+00:00,"No, your edit is incorrect. That would imply a purchase order could belong to many orders and vice versa, which makes no sense. You want a simple ForeignKey from PurchaseOrder to Order.",A,1
41682074,2017-01-16 17:41:43.117000+00:00,"Use random.shuffle() and don't assign to 26 separate variables:
import random

char = list(u""@#$%&*-¿=?~§!1234567890£º°"")
random.shuffle(char)

print(u''.join(char))

Here char is a (randomly shuffled) list, where the indices could be used instead of using separate variables. So a is found at index 0 in the list.
I made the characters a list of Unicode codepoints, since you are using non-ASCII characters here.",A,3
1988458,2010-01-01 12:25:15.853000+00:00,"You should be using formsets rather than messing around with dynamic prefixes for your PhoneNumber subform - it will make everything much easier, and this is indeed how the admin manages inline forms (see also the model formsets documentation). 
Formsets are intelligent enough that if no information is entered in one form of the formset, it does not enforce the required elements - but if one element is filled, it will enforce all the validation requirements. This sounds like it should solve your problem.",A,2
18575634,2013-09-02 14:36:32.633000+00:00,The request context processor is not active by default. You need to add it to TEMPLATE_CONTEXT_PROCESSORS in settings.py yourself.,A,1
30111001,2015-05-07 20:25:01.797000+00:00,"six.u() is meant for unicode string literals, not JSON output. You should not use it to decode the JSON to a Unicode string.
From the six.u() documenation:

A “fake” unicode literal. text should always be a normal string literal. In Python 2, u() returns unicode, and in Python 3, a string. Also, in Python 2, the string is decoded with the unicode-escape codec, which allows unicode escapes to be used in it.

Emphasis mine.
Instead, decode the string if using Python 2:
json_string = json.dumps(s)
if hasattr(json_string, 'decode'):
    # Python 2; decode to a Unicode value
    json_string = json_string.decode('ascii')

or use the unicode() function and catch the NameError in Python 3:
json_string = json.dumps(s)
try:
    # Python 2; decode to a Unicode value from ASCII
    json_string = unicode(json_string)
except NameError:
    # Python 3, already Unicode
    pass

or set ensure_ascii to False when calling json.dumps():
json_string = json.dumps(s, ensure_ascii=False)

This can still return a str type in Python 2 though, but only if the input contains nothing but ASCII-only data, so the output can safely be mixed with unicode values. 
Either way you get consistent values between Python 2 and Python 3; The six.u() decode also decodes \uhhhh JSON Unicode escape sequences to Unicode codepoints, while the Python 3 JSON result would leave those intact. With decoding you'd keep the \uhhhh sequences in both Python 2 and 3, with ensure_ascii you'd get Unicode codepoints in both.
Since this is a 3rd-party library, I filed a bug report; you cannot really recover from this mistake; you cannot insert extra backslashes up front then remove them afterward as you cannot distinguish those from normal backslashes.",A,1
13628335,2012-11-29 14:50:00.663000+00:00,"I'd simply register the new converter for a more specific interface:
 <adapter
     factory="".myconverter.MyDatetimeDataConverter""
     for=""zope.schema.interfaces.IDatetime
          z3c.form.interfaces.ITextWidget""
     />

I used ITextWidget as an example, register it for the widgets where you want your custom converter to be used.
Overrides replace adapters with the same registration. Just like the <adapter /> registration in a regular ZCML file, it'll take the interface(s) it (multi-)adapts and the interface it provides from the factory object, if need be, so if you used .adapts() on your own converter then you do not need to repeat that information in the <adapter /> registration.",A,2
1448497,2009-09-19 12:45:47.820000+00:00,"""I ask this since I read that a best practice is to test only the public methods of a class. In this case I'm not testing public methods of a class but the public API functions from the whole component.
""
I can't see the hair you're splitting.  Public API is the public API.  
Public API methods of a class or Public API functions from a component is the same thing -- a Public API -- the only thing you should test.
You have two levels of public API: class level and component level.  That says you have two levels of unit testing.  Class-level unit tests and component level unit tests.
Some folks will quibble on the definition of ""unit"" for unit testing.  Some will claim that ""unit"" is always a stand-alone class and nothing more.  I can't see how this is true, but some folks will claim it.
A unit is atomic or indivisible. It's a unit when your test does not reflect the unit's structure.",A,1
21761595,2014-02-13 17:42:17.283000+00:00,"This doesn't seem to be valid. You can only have one body element. But you can and should put all the attributes on that single element:
<body data-node_ip={{NODE_IP}} data-node_port={{NODE_PORT}}>
",A,3
36678735,2016-04-17 15:52:12.347000+00:00,"If you must use a list comprehension, then you must also rebuild each dictionary:
matchesData = [{k: str(v) if k == 'match_date' else v for k, v in d.items()} 
               for d in matchesData]

or use
matchesData = [dict(d, match_date=str(d['match_date']))
               for d in matchesData]

The first example uses a dictionary comprehension, that simply creates a new dictionary from the old by looping over each key-value pair. For the 'match_date' key the values is passed through the str() function. This means the match_date key is entirely optional.
The second version requires 'match_date' to exist, and creates a copy of the original dictionary with the dict() function, adding in an extra key (which will replace the original 'match_date' key-value pair from d).
Demo:
>>> from pprint import pprint
>>> matchesData = [
...     {'match_id': 1L, 'player_b_id': 2L, 'round_id': 1L, 'match_winner_id': 2L, 'match_date': datetime.date(2016, 3, 9), 'player_a_id': 1L, 'tournament_id': 1L},
...     {'match_id': 2L, 'player_b_id': 4L, 'round_id': 1L, 'match_winner_id': 4L, 'match_date': datetime.date(2016, 3, 10), 'player_a_id': 3L, 'tournament_id': 1L}
... ]
>>> pprint([{k: str(v) if k == 'match_date' else v for k, v in d.items()} for d in matchesData])
[{'match_date': '2016-03-09',
  'match_id': 1L,
  'match_winner_id': 2L,
  'player_a_id': 1L,
  'player_b_id': 2L,
  'round_id': 1L,
  'tournament_id': 1L},
 {'match_date': '2016-03-10',
  'match_id': 2L,
  'match_winner_id': 4L,
  'player_a_id': 3L,
  'player_b_id': 4L,
  'round_id': 1L,
  'tournament_id': 1L}]
>>> pprint([dict(d, match_date=str(d['match_date'])) for d in matchesData])
[{'match_date': '2016-03-09',
  'match_id': 1L,
  'match_winner_id': 2L,
  'player_a_id': 1L,
  'player_b_id': 2L,
  'round_id': 1L,
  'tournament_id': 1L},
 {'match_date': '2016-03-10',
  'match_id': 2L,
  'match_winner_id': 4L,
  'player_a_id': 3L,
  'player_b_id': 4L,
  'round_id': 1L,
  'tournament_id': 1L}]
",A,3
51390056,2018-07-17 20:55:08.603000+00:00,"The real issue here is that you shouldn't be writing either of these.1
self.vec.append(v1) if v1 else pass is illegal because pass is a statement, not an expression, and you can't put statements inside expressions.
self.vec.append(v1) if v1 else 0 is legal, because 0 is an expression.
But just because it's legal doesn't mean it's code you should ever write.
The point of the if expression is that it's an expression, so it (a) has a value that you can use, and (b) can be used in larger expressions.
You aren't using the value of self.vec.append(v1) (which is None), or the value of 0 (which is of course 0). You're just evaluating the first one for its side-effects and the second one as a workaround to fight against the language's syntax.
You're also not putting the expression inside another expression; you're just using it as an expression statement.
The way to write this is simple:
if v1:
    self.vec.append(v1)
else:
    pass

But of course you don't need an else clause here, so you can just write this as:
if v1:
    self.vec.append(v1)

Or, if you're desperate to save space:
if v1: self.vec.append(v1)

Sometimes it's worth sacrificing readability for performance, or occasionally even for brevity. But your attempt to abuse an if expression for its side effects is slower (especially in the false case—a jump and evaluating a literal just to discard it may be fast, but it's not nearly as fast as doing nothing at all). And it's much longer.

1. In fact, the worry that people would try to write code like this, even though there's absolutely no reason to, was the main argument against PEP 308, the proposal to add this feature. The PEP passed only because anyone who'd be silly enough to try to write code like this is going to write Perl instead of Python anyway.",A,2
39667408,2016-09-23 18:31:08.600000+00:00,"elif is not a separate statement. elif is an option part of an existing if statement.
As such, you can only use elif directly after an if block:
if sometest:
    indented lines
    forming a block
elif anothertest:
    another block

In your code, however, the elif does not directly follow a block already part of the if statement. You have lines in between that are no longer part of the block because they are not indented to the if block level anymore:
if guess_row not in range(1,6):
    print(""Out of range1."")  # part of the block
print(guess_location)        # NOT part of the block, so the block ended
print(ship_location)         
elif guess_col not in range(1,6):

This doesn't matter to separate if statements; the un-indented print() statements are executed between the if blocks.
You'll need to move those print() functions to be run after the if...elif...else statemement:
if guess_row not in range(1,6):
    print(""Out of range1."")
elif guess_col not in range(1,6):
    print(""Out of range2."")
elif ship_location == guess_location:
    print(""You sunk my battleship! You win!"")
else:
    print (""You missed!"")
    print (""You have "" + str(number_of_attempts-1) + "" attempt(s) left!"")
    print (""Try again!"")
    number_of_attempts-=1

print(guess_location)
print(ship_location)         

or fix their indentation to be part of the if and elif blocks.",A,4
47682928,2017-12-06 20:27:32.197000+00:00,"You don't write code like that for Lambda. Your handler needs to be a function which takes the event and context parameters.
See the Lambda handler docs for Python.",A,1
12060191,2012-08-21 17:42:58.337000+00:00,"You could create a dictionary that maps the column names to xpath expressions that extract corresponding values e.g.:
xpath = {
  ""ID"": ""/Total/ID/text()"",
  ""Check"": ""/Total/Response/Detail/Nix/Check/text()"", # or ""//Check/text()""
}

To populate the table row:
row = {name: tree.xpath(path) for name, path in xpath.items()}

The above assumes that you use lxml that support the full xpath syntax. ElementTree supports only a subset of XPath expressions but it might be enough in your case (you could remove ""text()"" expression and use el.text in this case) e.g.:
xpath = {
  ""ID"": "".//ID"",
  ""Check"": "".//Check"",
}
row = {name: tree.findtext(path) for name, path in xpath.items()}

To print all text with corresponding tag names:
import xml.etree.cElementTree as etree

for _, el in etree.iterparse(""xxm.xml""):
    if el.text and not el: # leaf element with text
       print el.tag, el.text

If column names differ from tag names (as in your case) then the last example is not enough to build the table.",A,2
2816913,2010-05-12 07:58:37.673000+00:00,"This is wrong on quite a few levels.
Firstly, there is no reason to catch an exception only to raise another one. If your application raises an exception, then Django's own middleware will catch it, and depending on whether or not you have DEBUG=True, either show a detailed debugging page, or mail the exception to the users mentioned in the ADMINS setting.
Secondly, you should never be getting a Webfaction error page - I can't even imagine how that is happening. If you want your users to see a nice error page, you should define 404.html and 500.html templates, or even full error-handling views if your needs are more complicated. This is fully explained in the documentation.",A,4
45897418,2017-08-26 16:19:06.587000+00:00,"This entire approach is mistaken. You don't pass data to a button. You shouldn't have that onclick function at all; instead, wrap both textarea and button in a form element. Then the form will be submitted to a view, from where you can access the data via the request and pass it to the next template.",A,1
19690513,2013-10-30 18:02:34.643000+00:00,"This feature of the Python 3 compiler has not been backported to Python 2.x.
There is no magic from __future__ import switch to enable it, your only option is to upgrade to Python 3.x.
Your second function could instead be defined as:
def (a, *b, **kwargs):
   c = kwargs.pop('c', 5)

to be Python 2 compatible.",A,18
5159361,2011-03-01 19:31:31.540000+00:00,"Don't try and parse XML manually. There are plenty of good ways of doing it, including ElementTree in the standard library.",A,5
21268417,2014-01-21 20:27:38.350000+00:00,"You can always write a source-processor that converts your 2.7 code to 2.6 code.
Or, just use one that someone else wrote. The 3to2 project has, among its fixers, one named dctsetcomp.* And it looks like it does exactly what you want. Here's a sample of its output:
RefactoringTool: Refactored dc.py
--- dc.py   (original)
+++ dc.py   (refactored)
@@ -1 +1 @@
-{x(i):y(i) for i in z}
+dict((x(i), y(i)) for i in z)
RefactoringTool: Files that need to be modified:
RefactoringTool: dc.py

There's not much documentation on how to use it, but basically it's meant to work exactly like 2to3, which is documented nicely, and used in hundreds of projects on PyPI if you need examples.
You can manually run this on your code before you're ready to distribute it, or run it as a pre-build step in setup.py (either unconditionally, or only if running on 2.6). setuptools makes it trivial to add a 2to3 step to the build/install process; subverting that to run 3to2 instead should be easy, but I haven't actually tried it.

You might want to look at the source to 3to2, or the stdlib's 2to3, if you want to write your own fixers, or if you need to debug/improve this one, or want to write a simpler parser from scratch.

A third place you can run a fixer is at runtime, with an import hook. Look at MacroPy as a great example of an import hook.
MacroPy is also a great example of code that works on ASTs instead of raw source. If you wanted to build a distribution-time fixer, this might be handy, because then you could let Python 2.7 parse the Python 2.7 source so you don't have to.

If 3to2 doesn't work out, you can probably do something a lot simpler, by limiting what you write in your code to what your parser can handle. For example, you could set yourself the rule that a dict comprehension must be the only dict display on the line, and be followed by #dc. Then you just have to search for lines ending with #dc, find the braces, and parse the result—which means you only have to parse a small subset of Python that's a regular language, which means you can do it with simple regexps.

* Why would a tool designed to port 3.2+ code to 2.7 have a dict comprehension fixer? Because it was originally designed to port 3.1+ code to 2.6, and the author never removed the no-longer-needed fixers. So, you got lucky. :)",A,0
22844910,2014-04-03 17:33:07.383000+00:00,"Make filter a tuple and use str.startswith(), it takes either one string or a tuple of strings to test for:
filter = tuple(filter)

[s for s in to_check if s.startswith(filter)]

Demo:
>>> filter = (""/This/is/an/example"", ""/Another/example"")
>>> to_check = [""/This/is/an/example/of/what/I/mean"", ""/Another/example/this/is/"", ""/This/example"", ""/Another/freaking/example""]
>>> [s for s in to_check if s.startswith(filter)]
['/This/is/an/example/of/what/I/mean', '/Another/example/this/is/']

Be careful, when prefix-matching with paths, you generally want to append trailing path separators so that /foo/bar doesn't match /foo/bar_and_more/ paths.",A,3
35032137,2016-01-27 08:27:29.397000+00:00,"The attributes are misnamed, really; they have nothing to do with GET and POST.
In HTTP, parameters can either be provided as the querystring part of the URL -  /foo/bar?q=123 - or in the request body. Django calls these GET and POST, but it's perfectly valid to have a querystring on a POST, hence the confusion. (The opposite isn't true, though; a GET can't have a body).",A,1
23433991,2014-05-02 17:44:54.673000+00:00,"You'd use a regular expression with a replacement function:
import re

def change_string(string, replacement):
    def repl(match):
        o, c = match.group('open'), match.group('close')
        if o + c in ('..', ""''"", '()'):
            return '{}{}{}'.format(o, replacement, c)
        return match.group(0)
    return re.sub(r""(?P<open>['.(]).*?(?P<close>['.)])"", repl, string)

The repl() function verifies that the opener and closing characters match up before replacing.
Demo:
>>> str1 = "".X.""
>>> change_string(str1, 'Y')
'.Y.'
>>> change_string(""This is a larger 'example' with (more than one) replacement, including a 'false positive."", 'Y')
""This is a larger 'Y' with (Y) replacement, including a 'false positive.""
",A,3
17560465,2013-07-10 00:21:24.243000+00:00,"Text-copy-source-md5 is a hash of the source specified by the Node-copyfrom-rev and Node-copyfrom-path lines, not the actual text of the lines used to specify the source.
So, in your case, you'd have to fetch rev 63 of the appropriate file (whatever it is) and hash that.",A,1
27412474,2014-12-10 23:02:26.900000+00:00,"First, if you're trying to look up the entire word[0] in alphabet, rather than each character individually, you shouldn't be looping over the characters of word[0]. Just use alphabet.index(word[0]) directly.
From your comments, it sounds like you're trying to look up each transliterated-Burmese-script character in word[0]. That isn't possible unless you can write an algorithm to split a word up into those characters. Splitting it up into the ASCII bytes of the transliteration doesn't help at all.

Second, you probably shouldn't be using index here. When you think you need to use index or similar functions, 90% of the time, that means you're using the wrong data structure. What you want here is a mapping (presumably why it's called map.txt), like a dict, keyed by words, not a list of words that you have to keep explicitly searching. Then, looking up a word in that dictionary is trivial. (It's also a whole lot more efficient, but the fact that it's easy to read and understand can be even more important.)

Finally, I suspect that your map.txt is supposed to be read as a whitespace-separated list of transliterated characters, and what you want to find is the index into that list for any given word. 

So, putting it all together, something like this:
with open('map.txt', 'rb') as f:
    mapping = {word: index for index, word in enumerate(f.read().split())}
word_list = sorted(word_list, key=lambda word: mapping[word[0]])


But, again, that's only going to work for one-syllable words, because until you can figure out how to split a word up into the units that should be alphabetized (in this case, the symbols), there is no way to make it work for multi-syllable words.
And once you've written the code that does that, I'll bet it would be pretty easy to just convert everything to proper Unicode representations of the Burmese script. Each syllable still takes 1-4 code points in Unicode—but that's fine, because the standard Unicode collation algorithm, which comes built-in with Python, already knows how to alphabetize things properly for that script, so you don't have to write it yourself.
Or, even better, unless this is some weird transliteration that you or your teacher invented, there's probably already code to translate between this format and Unicode, which means you shouldn't even have to write anything yourself.",A,0
37324406,2016-05-19 13:17:56.223000+00:00,"Just call A.p(self) directly, this is not what super() was designed to support. Note that you have to pass in self manually as A.p() is an unbound method.
However, you want to reconsider your class design. Perhaps you want to introduce helper methods; one of these could then be called from A.p() and C.p() but not B.p()? That way B.p() wouldn't have to use super().p() at all, and simply just reuse these helper methods.",A,5
31802867,2015-08-04 07:11:31.813000+00:00,"You have instances of class1 in the self.group list. You are merely calling methods on those instances.
You added the instance with the test.add_card() method, a method on the class2 instance you created:
test = class2()
a1 = class1('a', '3')
test.add_card(a1)

The add_card method appended it to the self.group list:
def add_card(self, card):
    self.group.append(card)

so when you then loop over the list you access that same instance and can call methods on that instance the same way you could call the class1.add_card() method.",A,0
18022334,2013-08-02 16:57:18.627000+00:00,"The code snippet you posted does not do any of the Trie search. At all.
The function you are looking at permutes the given signature key (a string of zeros and ones) to find the nearest match; if no matches starting with the first character of the signature are found, it looks for items with the inverse value instead.
For your sample data, if you were to look for the signature 1101, there are no exact matches. But the trie prefix search will return results for a search for 1, for 11, and for 110. The search for 1101 fails, so digitReplacement is used to replace the last 1 with a 0, which does match, so 1100 is the result of getNearestSignatureKey() function..
To find matches, prefix matching is delegated to the trie object. This datatype is taken from the Biopython project, and is coded entirely in C; if you are so inclined, study the Trie_has_prefix function to see how that type searches for matching prefixes.
Documentation on that data type is sparse; the best we have is this autogenerated module page:

This module implements a trie data structure.  This allows an O(M)
  lookup of a string in a dictionary, where M is the length of the
  string.  It also supports approximate matches.
",A,1
23698511,2014-05-16 14:59:58.820000+00:00,"If you wanted to remove the last word from each nested string, use str.rpartition() or str.rsplit():
result = [[val[0].rpartition(' ')[0]] for val in lst]

or
result = [[val[0].rsplit(None, 1)[0]] for val in lst]

The latter can handle strings delimited with more than just 1 space.
Demo:
>>> lst = [[u'cat 1234'], [u'dog bird 5678'], [u'fish horse elephant 9012']]
>>> [[val[0].rpartition(' ')[0]] for val in lst]
[[u'cat'], [u'dog bird'], [u'fish horse elephant']]
>>> [[val[0].rsplit(None, 1)[0]] for val in lst]
[[u'cat'], [u'dog bird'], [u'fish horse elephant']]

If your strings are not nested inside lists, then we don't have to unwrap and rewrap; the [0] indexing can go, as well as creating a new nested list:
result = [val.rpartition(' ')[0] for val in lst]

or
result = [val.rsplit(None, 1)[0] for val in lst]
",A,5
23255173,2014-04-23 21:06:39.893000+00:00,"To remove consecutive duplicates, you could use itertools.groupby function:
#!/usr/bin/env python
import csv
from itertools import groupby
from operator import itemgetter

with open('data.csv', 'rb') as file, open('output.csv', 'wb') as output_file:
    writer = csv.writer(output_file)
    for row in csv.reader(file):
        writer.writerow(map(itemgetter(0), groupby(row)))

It reads the input csv file and writes it to the output csv file with consecutive duplicates removed.
If there could be adjacent duplicate 0, 1 at the very end of the row then remove duplicates only in row[:-1] (all but last columns) and append the last bit row[-1] to the result if you want to preserve it:
from itertools import islice

no_dups = map(itemgetter(0), groupby(islice(row, len(row)-1)))
no_dups.append(row[-1])
writer.writerow(no_dups)
",A,0
1505809,2009-10-01 18:57:50.780000+00:00,"Relevant parts from rfc2818 HTTP Over TLS:

If the hostname is available, the client MUST check it against the server's identity as presented in the server's Certificate message, in order to prevent man-in-the-middle attacks.
If the client has external information as to the expected identity of the server, the hostname check MAY be omitted. (For instance, a client may be connecting to a machine whose address and hostname are dynamic but the client knows the certificate that the server will
     present.) ...  In special cases, it may be appropriate for the client to simply ignore the server's identity, but it must be understood that this leaves the connection open to active attack.
If the hostname does not match the identity in the certificate, user oriented clients MUST either notify the user (clients MAY give the user the opportunity to continue with the connection in any case) or terminate the connection with a bad certificate error.
Automated clients MUST log the error to an appropriate audit log (if available) and SHOULD terminate the connection (with a bad certificate error). Automated clients MAY provide a configuration setting that disables this check, but MUST provide a setting which enables it.
",A,0
24594180,2014-07-06 08:50:15.440000+00:00,"You are ignoring the result of the .encode() call:
item['city'].encode('utf-8')

Strings are immutable, and are not encoded in-place. Even better, the type of the returned object is different. You'll need to assign the return value back:
item['city'] = item['city'].encode('utf-8')

However, you should not use codecs.open() for the CSV file. The csv module will always write bytestrings, not Unicode. 
By using a codecs.open() file object, an implicit decode takes place to get back to Unicode, and it is that that fails for you; it is why you get a UnicodeDecodeError exception, not an encode error:
  File ""C:\Python27\lib\codecs.py"", line 351, in write
    data, consumed = self.encode(object, self.errors)
exceptions.UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 11: ordinal not in range(128)

Use a regular open() call instead:
self.modelsCsv = csv.writer(open('Dees.csv', mode='wb'))

Note the 'wb'; the csv module handles line endings itself.",A,7
1887021,2009-12-11 10:01:13.267000+00:00,"Query caching only applies within a QuerySet. In other words, if you evaluate the same queryset object twice, query caching will operate. But if you are doing a query every 10 seconds, presumably this is via a cron that spawns a new process each time, so there is no way Django will cache anything.
It is possible that your database's own cache will come into operation if you're repeatedly performing exactly the same query. You should look at the documentation for your DBMS to see how to manage that properly.",A,9
22572758,2014-03-22 02:37:49.373000+00:00,"response.read() returns a bytestring. Python shouldn't die while printing a bytestring because no character conversion occurs, bytes are printed as is. 
You could try to print Unicode instead:
text = page.decode(response.info().getparam('charset') or 'utf-8')
print text
",A,1
30286184,2015-05-17 11:03:25.437000+00:00,"You're not using a RequestContext when rendering the template, so the request variable is not present. Use the render shortcut instead.
 return render(request, 'registration/logout.html')
",A,1
46754329,2017-10-15 11:05:00.863000+00:00,"You have an inconsistent variable. At the top of the script you define DJANGO_WSGI_MODULE, but in the call to gunicorn you are using DJANGO_WSGI_UNICORN which does not exist. Make sure you use the same name.",A,1
20229645,2013-11-26 22:49:09.290000+00:00,"Remove the @functools.wraps() decorator, this only applies to function decorators. With a newstyle class your decorator fails with:
>>> @higherorderclass
... class Foo(object):
...     def __init__(self):
...         print 'in foo init'
... 
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""<stdin>"", line 3, in higherorderclass
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python2.7/functools.py"", line 33, in update_wrapper
    setattr(wrapper, attr, getattr(wrapped, attr))
AttributeError: attribute '__doc__' of 'type' objects is not writable

Without the @functools.wraps() line your decorator works just fine:
>>> def higherorderclass(cl):
...     class wrapped(cl):
...         def __init__(self, *args, **kwds):
...             print 'in wrapped init'
...             super(wrapped, self).__init__(*args, **kwds)
...         def __repr__(self):
...             return 'in wrapped repr'
...     return wrapped
... 
>>> @higherorderclass
... class Foo(object):
...     def __init__(self):
...         print 'in foo init'
... 
>>> Foo()
in wrapped init
in foo init
in wrapped repr
",A,3
29637504,2015-04-14 21:13:04.440000+00:00,"
__dir__ returns a list of unique strings; it'll include the strings '__setitem__' and '__getitem__' without any reference to a specific implementation.
In other words, it doesn't matter if they refer to the TestClass or OrigClass implementation. Accessing an attribute by that name on TestClass will find the TestClass implementation.
__dir__ is purely informative and only used by the dir() command. Other tools (such as the Python help() system) may rely on dir(). 
You'll not get into a recursive loop, the method is entirely optional, there is no base implementation as dir() otherwise collects information about an object by other means.

You can use dir() on the self.data attribute, and append to the resulting list to create the TestClass version:
def __dir__(self):
    base = dir(self.data)
    base.extend(('_try_attr', 'data'))
    return base

The resulting list of strings will be sorted again by dir().",A,1
49393555,2018-03-20 20:28:50.110000+00:00,"First, the commas.
The nicest solution would be to wrap JsonLinesItemExporter so that it adds a comma at the end of each item. 
If the appropriate method isn't exposed in a way that you can override it, super it, and add the comma, you may have to reimplement the method in your subclass, or even monkeypatch the exporter class. Less nice.
Alternatively, you can hook the file you pass into the exporter to make writes do a replace('\n', ',\n'). This is hacky, so I wouldn't do it if you can hook the exporter instead, but it does have the virtue of being simple.

Now, the brackets at start and end of file. Without knowing the library you're using or the way you're using it, this will be pretty vague.
If you're using a single ""session"" of the exporter per file—that is, you open it at startup, write a bunch of items to it, then close it, and never re-open it and append to it, this is pretty easy. Let's assume you solved the first problem by subclassing the exporter class to hook its writes, something like this:
class JsonArrayExporter(JsonLinesItemExporter):
    def _write_bytes(self, encoded_bytes):
        encoded_bytes = _encoded_bytes.replace(b'\n', b',\n')
        returns super()._write_bytes(encoded_bytes)

I'm guessing at what the implementation looks like, but you've already discovered the right thing to do, so you should be able to translate from my guess to reality. Now, you need to add two methods like this: 
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._writebytes(b'[\n')

    def close(self):
        if not self.closed():
            self._writebytes(b']\n')
        super().close()

You may need a flush somewhere before the _writebytes if the exporter class has its own buffer inside it, but that's the only extra complexity I'd expect to see.
If you're reopening files and appending to them in each session, this obviously won't work. You could do something like this pseudocode in __init__:
if file is empty:
    write('[\n')
else:
    seek to end of file
    if last two bytes are ']\n':
        seek back 2 bytes

That has the advantage of being transparent to your client code, but it's a bit hacky. If your client code knows when it's opening a new file rather than appending to an old one, and knows when it's finishing off a file for good, it's probably cleaner to add addStartMarker and addEndMarker methods and call those, or just have the client manually write the brackets to the file before initializing/after closing the exporter.",A,1
20775981,2013-12-25 19:14:34.783000+00:00,"You could also use a regular expression:
>>> str1 = 'Wazzup1'
>>> import re
>>> bool(re.search(r'\d', str1))
True

Note: there might be a difference in how c.isdigit(), c in nums, int(c) and \d define what is a digit due to locale or Unicode.",A,0
46054749,2017-09-05 12:14:37.060000+00:00,"connect_ex returns an integer, which is an error code or 0 if there is no error. You need to read via the socket itself, not on the result of that call.
Also note, as mentioned in the docs, there is no read method; it is call recv.
stream_bytes += self.socket.recv(1024)

Also, as noted in the comments, you don't need to call self.connection.close, for the same reason.",A,0
15669040,2013-03-27 20:43:17.700000+00:00,"You are looking at URL-quoted data:
>>> from urllib2 import unquote
>>> unquote('rand_id%3A%3Ftmsid%3D1340496000_EP002960010145_11_0_10050_1_2_10036')
'rand_id:?tmsid=1340496000_EP002960010145_11_0_10050_1_2_10036'

You can split on the first = perhaps, then split on _:
>>> unquoted = unquote('rand_id%3A%3Ftmsid%3D1340496000_EP002960010145_11_0_10050_1_2_10036')
>>> unquoted.split('=', 1)[1].split('_')
['1340496000', 'EP002960010145', '11', '0', '10050', '1', '2', '10036']
>>> timestamp, event = unquoted.split('=', 1)[1].split('_')[:2]
>>> timestamp, event
('1340496000', 'EP002960010145')

If instead the data has multiple fields and you find a & in there too, you can perhaps better parse everything after the question mark as a URL query string instead using urlparse.parse_qs()
>>> from urlparse import parse_qs
>>> parse_qs(unquoted.split('?', 1)[1])
{'tmsid': ['1340496000_EP002960010145_11_0_10050_1_2_10036']}
>>> parsed = parse_qs(unquoted.split('?', 1)[1])
>>> timestamp, event = parsed['tmsid'][0].split('_', 2)[:2]
>>> timestamp, event
('1340496000', 'EP002960010145')
",A,3
36445062,2016-04-06 08:06:33.957000+00:00,"Methods on the type are only looked up if looking up the attribute on the object itself fails; compare this to instance attributes versus class attributes; this is just the same.
If you need a method to participate in the MRO, put it on the class itself. The metaclass can do this:
class Meta(type):
    def __new__(mcls, name, bases, body):
        cls = super(Meta, mcls).__new__(mcls, name, bases, body)
        if not hasattr(cls, 'method'):
            @classmethod
            def method(*_, **__):
                print('Meta')
            cls.method = method
        return cls

This sticks the method on the newly created class only if there is not already such a method; this allows your class (or any subclasses) to override the meta-class provided method and prevents any subclasses from getting the same where it could be inherited instead.",A,1
53303018,2018-11-14 14:55:27.357000+00:00,"There's no reason to put those into a set. Move the creation of m_evento_sanitario before those loops, then for each one instead of adding it to a set add it directly to the m2m:
for animal_json_id in animais_json:
    animal = get_object_or_404(Animal, id_animal=animal_json_id)
    m_evento_sanitario.animais.add(animal)

and similarly for produtos.",A,2
25953875,2014-09-20 22:25:31.127000+00:00,"Your weapon_fact doesn't return anything, so the default None is returned instead.
You also want to avoid masking the weptype name:
def weapon_fact(weptype, dmg_type):
    class SpecificWeapon(weapon):
        def __init__(self, name, dmg_mod):
            weapon.__init__(self, name, weptype, dmg_mod, dam_type)
            self.name = name
    return SpecificWeapon
",A,0
26643372,2014-10-30 01:05:27.567000+00:00,"The specific exception you asked about is because line doesn't exist outside the generator expression. If you want to access it, you need to keep it in the same scope as the print statement, like this:
for line in f:
    needle = line.strip()
    if needle in haystack:
        print line

But this isn't going to be particularly useful. It's just going to be the word from needle plus the newline at the end. If you want to print out the line (or lines?) from haystack that include needle, you have to search for that line, not just ask whether needle appears anywhere in the whole haystack.
To literally do what you're asking for, you're going to need to loop over the lines of haystack and check each one for needle. Like this:
with open('something.txt') as f:
    haystacks = list(f)

with open('d:\\Users\\something\\Desktop\\something\\dictionary\\entirelist.txt') as f:
    for line in f:
        needle = line.strip()
        for haystack in haystacks:
            if needle in haystack:
                print haystack


However, there's a neat trick you may want to consider: If you can write a regular expression that matches any complete line that includes needle, then you just need to print out all the matches. Like this:
with open('something.txt') as f:
    haystack = f.read()
with open('d:\\Users\\something\\Desktop\\something\\dictionary\\entirelist.txt') as f:
    for line in f:
        needle = line.strip()
        pattern = '^.*{}.*$'.format(re.escape(needle))
        for match in re.finditer(pattern, haystack, re.MULTILINE):
            print match.group(0)

Here's an example of how the regular expression works:
^.*Falco.*$


Debuggex Demo
Of course if you want to search case-insensitively, or only search for complete words, etc., you'll need to make some minor changes; see the Regular Expression HOWTO, or a third-party tutorial, for more.",A,0
52026436,2018-08-26 12:53:12.467000+00:00,"The defaultdict class implements a object.__reduce__() method where the second element of the returned tuple (the arguments for the constructor) is always going to be the factory object:
>>> d = NestedDict()
>>> d.__reduce__()
(<class '__main__.NestedDict'>, (<class '__main__.NestedDict'>,), None, None, <dict_itemiterator object at 0x110df59a8>)

That argument is then passed to the NestedDict() call to re-build the object. The exception is thrown because the NestedDict class doesn’t accept an argument. 
You can override the __reduce__ method in your subclass:
class NestedDict(defaultdict):
    def __init__(self):
        super().__init__(self.__class__)
    def __reduce__(self):
        return (type(self), (), None, None, iter(self.items()))

The above produces the exact same elements defaultdict.__reduce__() returns, except that the second element is now an empty tuple.
You could also just accept and ignore a single argument:
class NestedDict(defaultdict):
    def __init__(self, _=None):  # accept a factory and ignore it
        super().__init__(self.__class__)

The _ name is commonly used to mean I am ignoring this value.
An alternative implementation could just subclass dict and provide a custom __missing__ method; this method is called for keys not in the dictionary:
class NestedDict(dict):
    def __missing__(self, key):
        nested = self[key] = type(self)()
        return nested
    def __repr__(self):
        return f'{type(self).__name__}({super().__repr__()})'

This works exactly like your version, but doesn't need additional pickle support methods:
>>> d = NestedDict()
>>> d['foo']
NestedDict({})
>>> d['foo']['bar']
NestedDict({})
>>> d
NestedDict({'foo': NestedDict({'bar': NestedDict({})})})
>>> pickle.loads(pickle.dumps(d))
NestedDict({'foo': NestedDict({'bar': NestedDict({})})})
",A,2
19921967,2013-11-12 05:56:10.540000+00:00,"You can't parse a CSV file with BeautifulSoup, only HTML or XML.
If you want to use the charset guessing from BeautifulSoup on its own, you can. See the Unicode, Dammit section of the docs. If you have a complete list of all of the encodings that might have been used, but just don't know which one in that list was actually used, pass that list to Dammit.
There's a different charset-guessing library known as chardet that you also might want to try. (Note that Dammit will use chardet if you have it installed, so you might not need to try it separately.)
But both of these just make educated guesses; the documentation explains all the myriad ways they can fail.
Also, if each line is encoded differently (which is an even bigger mess than usual), you will have to Dammit or chardet each line as if it were a separate file. With much less text to work with, the guessing is going to be much less accurate, but there's nothing you can do about that if each line really is potentially in a different encoding. 
Putting it all together, it would look something like this:
encodings = 'utf-8', 'latin-1', 'cp1252', 'shift-jis'

def dammitize(f):
    for line in f:
        yield UnicodeDammit(line, encodings).unicode_markup

with open('foo.csv', 'rb') as f:
    for row in csv.reader(dammitize(f)):
        do_something_with(row)
",A,0
27736738,2015-01-02 02:31:15.900000+00:00,"You called the first function before handing it to the assertRaises() method. You can give the method additional arguments that it'll then pass on to the function-under-test:
self.assertRaises(TypeError, f, 1)

The better option would be to use self.assertRaises() as a context manager:
with self.assertRaises(TypeError):
    f(1)

You call the method 'normally' in that case, which reads a lot more natural.
See the TestCase.assertRaises() documentation:

Test that an exception is raised when callable is called with any positional or keyword arguments that are also passed to assertRaises().

and

If only the exception and possibly the msg arguments are given, return a context manager so that the code under test can be written inline rather than as a function:
with self.assertRaises(SomeException):
    do_something()

",A,0
26474730,2014-10-20 20:53:16.067000+00:00,"The problem is that, by writing class games(), you've explicitly asked Python to create an old-style class, instead of a new-style class. And you can't use super with old-style classes.
The solution is to just not do that. Write class games(object) to create a new-style class.
That will get you past the first error, but there's a second one: super takes two arguments, the type and the self instance. So, you need super(soccer, self).__init__().

The error message is a bit cryptic—you'd have to know that classobj is the type of any old-style class, while type is the type of any new-style class (without a custom metaclass) to understand it, and anyone who understands that wouldn't have had this question in the first place… But this turns out to be the kind of thing that's hard to explain in an error message without making people learn stuff they'd be better off not learning about. So, ultimately, Python solved the problem by getting rid of old-style classes entirely. If you were using Python 3.0 or newer, the first problem wouldn't exist (and the second one, you could write the much simpler super().__init__()).

If you want to know more about old-style and new-style classes, Unifying types and classes from the 2.2 docs is still the most detailed explanation, and The Inside Story on New-Style Classes from Guido's blog gives the most background.
But really, you don't need to know any of that stuff; all you need to know is that old-style classes are there for historical reasons, and you don't want to deal with them, so you should do one of the following:

Upgrade to 3.0 or later.
Always write class Foo(object): instead of class Foo: or class Foo():.
",A,1
22125817,2014-03-02 09:15:39.627000+00:00,"No need for regular expressions here.
Use str.rsplit():
output = inputstr.rsplit('.', 1)[0]

or str.rpartition():
output = inputstr.rpartition('.')[0]

str.rpartition() is the faster of the two but you need Python 2.5 or newer for it.
Demo:
>>> 'test.test1.test2'.rsplit('.', 1)[0]
'test.test1'
>>> 'test.test1.test2'.rpartition('.')[0]
'test.test1'
>>> 'test.test3.test4'.rsplit('.', 1)[0]
'test.test3'
>>> 'test.test3.test4'.rpartition('.')[0]
'test.test3'

And a time comparison against a million runs:
>>> from timeit import timeit
>>> timeit(""s.rsplit('.', 1)[0]"", ""s = 'test.test1.test2'"")
0.5848979949951172
>>> timeit(""s.rpartition('.')[0]"", ""s = 'test.test1.test2'"")
0.27417516708374023

where you can run 2 million str.rpartition() calls for the price of 1 million str.rsplit()'s.",A,8
19257843,2013-10-08 20:35:55.150000+00:00,"all() always returns True unless there is an element in the sequence that is False.
Your loop produces 0 items, so True is returned.
This is documented:

Return True if all elements of the iterable are true (or if the iterable is empty).

Emphasis mine.
Similarly, any() will always return False, unless an element in the sequence is True, so for empty sequences, any() returns the default:
>>> any(True for _ in '')
False
",A,7
2836491,2010-05-14 18:08:14.933000+00:00,"TDD for business apps works like this.

Write down a business requirement.
Write down a test for that requirement.
Write code that passes the test.

The trick is that there are many junky non-business requirements that don't need extensive testing.

""saves to a database"" isn't a business requirement.  It's technical.  
""activates a button on the GUI for some situation"" isn't a business requirement, it's part of the interface.
""backup"" isn't a business requirement; it's security or business continuity or something.

Consider a concrete example.
Requirement -- ""You can't borrow books until you've paid your fines.""
Tests.

Attempt to borrow book with fine.
Attempt to borrow book with no fine.

Code.
class FinesNotPaid( unittest.TestCase ):
    def setUp( self ):
        # load some fixtures with users, books and fines outstanding.
    def test_should_not_checkout_book( self ):
        x = TheCheckoutClass()
        x.checkoutBook( someBook )
        self.fail( ""Should have thrown error"" )

class FinesPaid( unittest.TestCase ):
    def setUp( self ):
        # load some fixtures with users, books and fines paid.
    def test_should_not_checkout_book( self ):
        x = TheCheckoutClass()
        x.checkoutBook( someBook )
        self.success(  )

class NoFines( unittest.TestCase ):
    etc.

These are Business Rules implemented by classes that are separate from your database and your GUI.  These are classes in an application domain layer.
Your CRUD rules are business rules.  You need to test them.  However, you don't need to test every database-related feature.  You need a few ""can I create and persist an object?"" tests.  You have to trust that the ORM, the Data Access layer and the Database actually work.  You don't write tests to exhaustively test the built-in features of the ORM.
Code coverage (100% or 80% or 10%) is largely meaningless.  Is software that has tests with 80% code coverage actually 20% more likely to fail?  It doesn't work that way.  Testing every line of code doesn't cover every logic path, so stop worrying and start testing.
Test the business use cases.  If they pass and there's untested code, then -- perhaps -- you wrote too much code.  ",A,8
23478570,2014-05-05 17:49:18.507000+00:00,"Redirect stdin to devnull, to avoid a subprocess to pause waiting for user input:
import os
from subprocess import Popen
try:
    from subprocess import DEVNULL # Python 3
except ImportError:
    DEVNULL = open(os.devnull, 'r+b', 0)

Popen(['pdflatex', 'file.tex'], stdin=DEVNULL, stdout=DEVNULL, stderr=DEVNULL)

I assume that you don't care about stdout/stderr also.",A,4
49472348,2018-03-25 03:48:45.813000+00:00,"There's another issue that isn't pointed out in any of the existing answers. Python is allowed to merge any two immutable values, and pre-created small int values are not the only way this can happen. A Python implementation is never guaranteed to do this, but they all do it for more than just small ints.

For one thing, there are some other pre-created values, such as the empty tuple, str, and bytes, and some short strings (in CPython 3.6, it's the 256 single-character Latin-1 strings). For example:
>>> a = ()
>>> b = ()
>>> a is b
True


But also, even non-pre-created values can be identical. Consider these examples:
>>> c = 257
>>> d = 257
>>> c is d
False
>>> e, f = 258, 258
>>> e is f
True

And this isn't limited to int values:
>>> g, h = 42.23e100, 42.23e100
>>> g is h
True

Obviously, CPython doesn't come with a pre-created float value for 42.23e100. So, what's going on here?
The CPython compiler will merge constant values of some known-immutable types like int, float, str, bytes,  in the same compilation unit. For a module, the whole module is a compilation unit, but at the interactive interpreter, each statement is a separate compilation unit. Since c and d are defined in separate statements, their values aren't merged. Since e and f are defined in the same statement, their values are merged.

You can see what's going on by disassembling the bytecode. Try defining a function that does e, f = 128, 128 and then calling dis.dis on it, and you'll see that there's a single constant value (128, 128)
>>> def f(): i, j = 258, 258
>>> dis.dis(f)
  1           0 LOAD_CONST               2 ((128, 128))
              2 UNPACK_SEQUENCE          2
              4 STORE_FAST               0 (i)
              6 STORE_FAST               1 (j)
              8 LOAD_CONST               0 (None)
             10 RETURN_VALUE
>>> f.__code__.co_consts
(None, 128, (128, 128))
>>> id(f.__code__.co_consts[1], f.__code__.co_consts[2][0], f.__code__.co_consts[2][1])
4305296480, 4305296480, 4305296480


You may notice that the compiler has stored 128 as a constant even though it's not actually used by the bytecode, which gives you an idea of how little optimization CPython's compiler does. Which means that (non-empty) tuples actually don't end up merged:
>>> k, l = (1, 2), (1, 2)
>>> k is l
False

Put that in a function, dis it, and look at the co_consts—there's a 1 and a 2, two (1, 2) tuples that share the same 1 and 2 but are not identical, and a ((1, 2), (1, 2)) tuple that has the two distinct equal tuples.

There's one more optimization that CPython does: string interning. Unlike compiler constant folding, this isn't restricted to source code literals:
>>> m = 'abc'
>>> n = 'abc'
>>> m is n
True

On the other hand, it is limited to the str type, and to strings of internal storage kind ""ascii compact"", ""compact"", or ""legacy ready"", and in many cases only ""ascii compact"" will get interned.

At any rate, the rules for what values must be, might be, or cannot be distinct vary from implementation to implementation, and between versions of the same implementation, and maybe even between runs of the same code on the same copy of the same implementation.
It can be worth learning the rules for one specific Python for the fun of it. But it's not worth relying on them in your code. The only safe rule is:

Do not write code that assumes two equal but separately-created immutable values are identical.
Do not write code that assumes two equal but separately-created immutable values are distinct.

Or, in other words, only use is to test for the documented singletons (like None) or that are only created in one place in the code (like the _sentinel = object() idiom).",A,3
14505194,2013-01-24 15:43:39.860000+00:00,"The class body is run before the class is constructed, yes.
The body of the class provides a temporary namespace, and all local names in that namespace are given as a dictionary to construct the class object, together with the base classes and a name for the class.
You can do this with the type() constructor too:
>>> Foo = type('Foo', (), {'a': 1})
>>> Foo.a
1

The class body is basically executed as a function, with the local namespace of that function being used to create the class attributes, the 3rd argument to type() above.
In python 3 you have a little more influence on that process with the __prepare__ hook on a metaclass. __prepare__ should be a class method that returns a initial namespace for the class body; use it to inject extra names into the generated class body before the class body is executed:
class MyMeta(type):
   @classmethod
   def __prepare__(mcl, name, bases):
       return {'a': 1}
",A,4
39943352,2016-10-09 11:59:33.060000+00:00,"Of course it's possible. You're already accessing the runconfig name, you can do exactly the same thing with the pk:
<a href=""{% url 'runconfig-detail' pk=run.config_name.id %}"">

However you should really pick a less confusing name for the foreign key; the field config_name doesn't point to a name, it points to a RunConfig instance.",A,2
30694583,2015-06-07 14:20:37.557000+00:00,"You need to test for tags, which are modelled as Element instances. Element objects have a name attribute, while text elements don't (which are NavigableText instances):
[x for x in dt.find_next_sibling('dd').contents if getattr(x, 'name', None) != 'br']

Since you appear to only have text and <br /> elements in that <dd> element, you may as well just get all the contained strings instead:
list(dt.find_next_sibling('dd').stripped_strings)

Demo:
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup('''\
... <dt>Term</dt>
... <dd>
...     +420 777 593 531<br/>
...     +420 776 593 531<br/>
...     +420 775 593 531<br/>
... </dd>
... ''')
>>> dt = soup.dt
>>> [x for x in dt.find_next_sibling('dd').contents if getattr(x, 'name', None) != 'br']
[u'\n    +420 777 593 531', u'\n    +420 776 593 531', u'\n    +420 775 593 531', u'\n']
>>> list(dt.find_next_sibling('dd').stripped_strings)
[u'+420 777 593 531', u'+420 776 593 531', u'+420 775 593 531']
",A,9
49336371,2018-03-17 13:09:06.203000+00:00,"You need to use self.kwargs, not kwargs.
course = Course.objects.get(pk=self.kwargs['course_id'])
",A,0
22993896,2014-04-10 16:36:26.283000+00:00,"The logging module was developed by a separate company in 2001, and was heavily based on Log4j. As such it follows the naming conventions the original author picked, which mirror the Log4j choices; the latter has a getLogger() method too.
Not until a year later did PEP 282 propose to add it to the standard library, by which time the naming convention was set in stone.
It is a known issue with the package, but it is not the only package to violate the PEP. From the linked Wiki:

PEP8 says - consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is most important.

So True, but can't not be changed, because of backward compatibility. logging2 maybe. -- techtonik
  
  
It's a low priority right now, unless there's an initiative to ensure the rest of the stdlib is made to conform to PEP8. -- VinaySajip



Last but not least, the styleguide itself has this to say on applying styleguides:

A Foolish Consistency is the Hobgoblin of Little Minds
A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is most important.
But most importantly: know when to be inconsistent -- sometimes the style guide just doesn't apply. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!
In particular: do not break backwards compatibility just to comply with this PEP!

'Fixing' logging would break backwards compatibility, which is just not worth it.",A,31
7337139,2011-09-07 16:14:33.873000+00:00,"Use python -m:
python -mfoo arg1 arg2
",A,2
23494714,2014-05-06 12:26:56.403000+00:00,"cmd in your example is a shell pipeline. subprocess.call() doesn't call the shell by default i.e., your command should fail unless you specify shell=True (use string argument in this case). To get output, you could call:
output = subprocess.check_output(cmd, shell=True)

You could avoid calling shell commands and use pure Python parser demonstrated in How to get .avi files length question instead:
#!/usr/bin/env python
from glob import glob
from hachoir_metadata import extractMetadata
from hachoir_parser import createParser

entries = [(path, extractMetadata(createParser(path)).get('duration'))
           for path in glob(u""*.wav"")]
print ""Min: path(%s), duration(%s)"" % min(entries, key=lambda (_,d): d)
print ""Max: path(%s), duration(%s)"" % max(entries, key=lambda (_,d): d)
print ""Total: duration(%s seconds)"" % sum(d.total_seconds() for _, d in entries)

To install the hachoir library, run:
$ pip install hachoir-{core,parser,metadata}
",A,1
8660243,2011-12-28 19:17:29.580000+00:00,"If it's YAML, then use a proper YAML module for this.  A language like Python (and I'm sure many others) have YAML modules available.  You probably already have Python installed; you'll need to add a YAML package like http://pyyaml.org/.
import yaml
with open(""some_file"",""r"") as source:
    objects= yaml.load( source )
# change your Cpanel::Easy::PHP5::Curl values
with open(""revised_file"",""w"") as target:
    yaml.dump( objects )

The only ""hard"" part about this is the namespace hierarchy that is being used.  It's not clear how this will wind up represented in Python.  You might need to define some packages and classes that reflect the namespace information.",A,1
365934,2008-12-13 23:31:55.753000+00:00,"Python 2.6+
next(iter(your_list), None)

If your_list can be None:
next(iter(your_list or []), None)

Python 2.4
def get_first(iterable, default=None):
    if iterable:
        for item in iterable:
            return item
    return default

Example:
x = get_first(get_first_list())
if x:
    ...
y = get_first(get_second_list())
if y:
    ...

Another option is to inline the above function:
for x in get_first_list() or []:
    # process x
    break # process at most one item
for y in get_second_list() or []:
    # process y
    break

To avoid break you could write:
for x in yield_first(get_first_list()):
    x # process x
for y in yield_first(get_second_list()):
    y # process y

Where:
def yield_first(iterable):
    for item in iterable or []:
        yield item
        return
",A,133
30149321,2015-05-10 08:33:36.830000+00:00,"All the template does when presented with any object is call str() (or unicode()) on it. Internally, the methods that implement those on the form class simply delegate to the as_table() method. So you can just call that directly:
return JsonResponse({'form': form.as_table()})

Note, though, that you might want to add other HTML, in which case a better approach might be to simply render a short template snippet, including the form, to a string - using render_to_string - and send that in the Json response.",A,1
12261700,2012-09-04 10:36:36.140000+00:00,"The are essentially the same, except for what you have to escape:
""'""
'""'

both work, but to incorporate multiple quote types you must escape the ones used to create the string:
""\""'""
'""\''

The two exist to make it easy for you to avoid having to escape your quotes, so the following two are easy:
'She said: ""Not so fast!""'
""Won't you come with us?""

Note that there are also tripple-quote variants:
""""""Now I can use either quote with more freedom: ' and "".""""""
'''Now I can use either quote with more freedom: ' and "".'''

These also allow newlines to be included without escaping:
""""""A
multiline
string
is
easy.
""""""

That last example would require you to use excessive \n escape sequences otherwise.",A,9
29845873,2015-04-24 11:06:03.217000+00:00,"What you're trying to do is impossible, because it doesn't make sense. The child has the same stdin as the parent, not a new one that you can write to. POSIX guarantees that after fork:

The child process shall have its own copy of the parent's file descriptors. Each of the child's file descriptors shall refer to the same open file description with the corresponding file descriptor of the parent.


Meanwhile, you're trying to write to the child's stdin from within the child. That makes even less sense. What do you expect writing to your own stdin to do?

Of course the child can write to its own stdout, which will be the same as the parent's stdout. But I suspect that's not what you want. What you want is for the parent to write to the child's stdin.
If so, you have to create a pipe before forking, then replace the child's stdin with the read side of that pipe (usually by using dup2), then write to the write side of that pipe. The CPython subprocess implementation is great example code for how to do that without relying on higher-level functions (even though it's in C, rather than in Python).
Something like this:
pr, pw = pipe()
pid = fork()
if pid == 0:
    os.close(pw)
    sys.stdin.close()
    os.dup2(pr, 0)
    os.execv(...)
else:
    os.close(pr)
    os.write(pw, ""sample"")
",A,5
317459,2008-11-25 14:11:35.293000+00:00,"""but how should one do it instead?""
Tough call.  The issue is that they are plugging in metadata (specifically column names) on the fly into a SQL statement.  I'm not a big fan of this kind of thing.  The sourcedest variable has two column names that are going to be updated. 
Odds are good that there is only one (or a few few) pairs of column names that are actually used.  My preference is to do this.
if situation1:
    stmt= ""INSERT INTO mastertickets (this, that) VALUES (?, ?)""
elif situation2:
    stmt= ""INSERT INTO mastertickets (foo, bar) VALUES (?, ?)""
else:
    raise Exception( ""Bad configuration -- with some explanation"" )
cursor.execute( stmt, (self.tkt.id, n) )

When there's more than one valid combination of columns for this kind of thing, it indicates that the data model has merged two entities into a single table, which is a common database design problem.  Since you're working with a product and a plug-in, there's not much you can do about the data model issues.",A,4
23735632,2014-05-19 10:39:44.867000+00:00,"You are using the glob syntax incorrectly; the [..] sequence works per character. The following glob would match your files correctly instead:
'1[5-8][0-9][0-9][0-9].*'

Under the covers, glob uses fnmatch which translates the pattern to a regular expression. Your pattern translates to:
>>> import fnmatch
>>> fnmatch.translate('[15000-18000].*')
'[15000-18000]\\..*\\Z(?ms)'

which matches 1 character before the ., a 0, 1, 5 or 8. Nothing else.
glob patterns are quite limited; matching numeric ranges is not easy with it; you'd have to create separate globs for ranges, for example (glob('1[8-9][0-9][0-9][0-9]') + glob('2[0-9][0-9][0-9][0-9]'), etc.).
Do your own filtering instead:
directory = ""/Users/Chris/Dropbox""

for filename in os.listdir(directory):
    basename, ext = os.path.splitext(filename)
    if ext != '.json':
        continue
    try:
        number = int(basename)
    except ValueError:
        continue  # not numeric
    if 18000 <= number <= 19000:
        # process file
        filename = os.path.join(directory, filename)
",A,9
29193061,2015-03-22 10:04:06.617000+00:00,"To remove lines that contain a word in the files given on the command-line:
#!/usr/bin/env python3
import fileinput

word = 'put your word here'
for line in fileinput.input(inplace=True):
    if word not in line:
       print(line, end='') # keep the line
",A,0
175065,2008-10-06 16:33:52.687000+00:00,I use Komodo Edit.  Check out the Open Komodo Edit.,A,42
3462733,2010-08-11 20:52:17.873000+00:00,"
Table locking enables many sessions to read from a table at the same time
To achieve a very high lock speed, MySQL uses table locking

""I would presume that row-level locking is better because"" [you lock less data].
First ""better"" is poorly defined in this page.  It appears that better means ""faster"".
Row-level locking cannot (in general) be faster because of contention for locks.  Locking each row of a large result set means the very real possibility of a conflict with another large result set query and a rollback.",A,1
36848621,2016-04-25 18:44:28.427000+00:00,"You could use a conditional expression:
b = b + 's' if not b.endswith('s') else b

Personally, I'd still stick with two lines, however:
if not b.endswith('s'):
    b += 's'
",A,8
14284591,2013-01-11 18:38:52.043000+00:00,"Use string formatting instead:
    def __str__(self)
        items = '\n'.join(['\t{0}{1}'.format(k, v)
            for k, v in self.adjencyList.iteritems()])
        return (
            ""Simple: {0.simple}\n""
            ""Directional: {0.directional}\n""
            ""{{\t{1}\n}}""
        ).format(self, items)
",A,4
3926666,2010-10-13 18:15:45.453000+00:00,"I can't understand what you're trying to do. There are a number of problems with your approach, though.
Firstly, there's no way of passing the POSTed values to the form instantiation. As a result, form.is_valid() will always be False.
Secondly, the result of node.edit_form() appears to be a single form, not a formset. So I don't know why you are passing it to the template context as formset and trying to iterate through formset.forms.",A,0
14269483,2013-01-10 23:59:34.013000+00:00,"First, get rid of the unnecessary parentheses; they just make the structure of your code harder to follow:
def b(self):
    if self.a() == 'Buy':
        return 'BUY SET'
    elif self.a() == 'Sell':
        return 'SELL SET''

Then remove that stray ' that causes it to raise a SyntaxError:
def b(self):
    if self.a() == 'Buy':
        return 'BUY SET'
    elif self.a() == 'Sell':
        return 'SELL SET'

Next, don't call self.a() twice. While it's harmless here, in real code there's a very good chance it will have side effects—or at least take long enough that you're wasting time:
def b(self):
    order = self.a()
    if order == 'Buy':
        return 'BUY SET'
    elif order == 'Sell':
        return 'SELL SET'

Next, if a returns anything but 'Buy' or 'Sell', you're going to return None. Is that really what you want? How you fix this depends on what you actually want. For example, you might want:
def b(self):
    order = self.a()
    if order == 'Buy':
        return 'BUY SET'
    elif order == 'Sell':
        return 'SELL SET'
    else:
        raise ValueError(""Unexpected order type '{}'"".format(order))

Or maybe:
def b(self):
    order = self.a()
    if order == 'Buy':
        return 'BUY SET'
    else: # We know there's nothing else it could be
        return 'SELL SET'

If you have more than two possibilities, it might be worth putting things into a dict instead of using an if/elif chain, or writing dynamic code. Examples:
def b(self):
    return {'Buy': 'BUY SET', 'Sell': 'SELL SET', 'Trade': 'TRADE SET',
            'Barter': 'BARTER SET', 'Steal': 'STEAL SET'}[self.a()]

def b(self):
    return '{} SET'.format(order.upper())

Note that these make different choices for the previous question. The dict version will raise a KeyError for an unknown value; the format version will just treat anything as valid.
Also, if you use a dict, you probably want to move it outside of the b function—e.g., make it a class attribute (which also encourages you to give it a nice name).
However, for only two choices, I think what you have is much clearer.
Finally, it's not very pythonic to give functions meaningless one-letter names; it would be better to do something like this:
def get_formatted_order(self):
    order = self.calculate_order()
    if order == 'Buy':
        return 'BUY SET'
    elif order == 'Sell':
        return 'SELL SET''
",A,14
25651198,2014-09-03 18:26:26.250000+00:00,"In example 2 you are printing the resulting filename exactly as many times as there are characters in the filename:
for filenames in os.listdir(r'C:\Users\output'):
    for filename in filenames:
        print(filenames)

Here filenames is one filename. Looping over that filename gives you individual characters, bound to filename (singular), but you then print filenames (plural) for each:
>>> for foo in ('bar', 'bazbaz'):
...     for character in foo:
...         print(character, foo)
b bar
a bar
r bar
b bazbaz
a bazbaz
z bazbaz
b bazbaz
a bazbaz
z bazbaz

I added the character print here to illustrate what is going on. A filename with 8 characters is printed 8 times, a filename with 7 characters (such as 'AAL.txt') is printed 7 times, etc.
In example 3 you are just printing the individual characters.
Remove the extra loop, you don't need it. Just loop over the values of os.listdir():
import os

for filename in os.listdir(r'C:\Users\output'):
    print(filename)
",A,3
23028677,2014-04-12 09:38:33.563000+00:00,"Browsers load the iframe content in a separate request. You'll have to do the same:
for iframe in iframexx:
    response = urllib2.urlopen(iframe.attrs['src'])
    iframe_soup = BeautifulSoup(response)

Remember: BeautifulSoup is not a browser; it won't fetch images, CSS and JavaScript resources for you either.",A,12
11428485,2012-07-11 08:25:20.577000+00:00,"Yes, the old function-based generic views were deprecated in 1.4. Use the class-based views instead.",A,6
32560722,2015-09-14 08:53:59.987000+00:00,pip will only install the Python libraries for connecting to a redis database. You need to install the redis server itself: probably the easiest way to do that on a Mac is to use Homebrew.,A,5
20081607,2013-11-19 20:43:04.550000+00:00,"Every warning that MySQL generates will be raised as a Warning by MySQLdb 1.2, unless you're using a use-result cursor. There is no code that discriminates between different warnings.
MySQLdb does not provide enough information to let Python's warnings module filter things out yourself in any way except by a regexp on the message. In particular, the level and code are already lost by the time the warnings filter gets to it.
So, here are your options:

Use something more flexible than MySQLdb. Its successor moist may still not be ready, but competitors like PyMySQL are.

Ignore warnings around every call that you know might print warnings you don't care about:
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    cursor.execute('DROP TABLE IF EXISTS sdfdsfds') # will not warn


Ignore warnings by message string regexp:
warnings.filterwarnings('ignore', 'unknown table')


Turn off MySQLdb's warning raising entirely, e.g., using the quick&dirty hack of setting the cursor._defer_warnings = True (see the code to see why this works). (Note that you can also use this flag to turn it back on and back off again, so you only skip warnings around certain commands. But if you're going to do that, use the warnings module.)

Fork, monkeypatch, or subclass MySQLdb to override its Cursor._warning_check function to discriminate in some way based on the warning level and/or code.",A,9
19261306,2013-10-09 01:10:00.377000+00:00,"I'll take a guess here, but I suspect I'm wrong.
You can't insert (""billy"", ""jim"") as a column in the database. This is intentional. The whole point of RDBMSs like sqlite is that each field holds exactly one value, not a list of values. You can't search for 'jim' in the middle of a column shared with other people, you can't join tables based on 'jim', etc.
If you really, really want to do this, you have to pick some way to convert the multiple values into a single string, and to convert them back on reading. You can use json.dumps/json.loads, repr/ast.literal_eval, or anything else that seems appropriate. But you have to write the extra code yourself. And you won't be getting any real benefit out of the database if you do so; you'd be better off just using shelve.
So, I'm guessing you don't want to do this, and you want to know what you want to do instead.
Assuming your schema looks something like this:
CREATE TABLE whois (Rid, Names);

What you want is:
CREATE TABLE whois (Rid);
CREATE TABLE whois_names (Rid, Name, FOREIGN KEY(Rid) REFERENCES whois(Rid);

And then, to do the insert:
tt = [(rid,(""billy"",""jim""))]
for rid, names in tt:
    c.execute('INSERT INTO whois VALUES (?)', (rid,))
    for name in names:
        c.execute('INSERT INTO whois_names VALUES (?, ?)', (rid, name))

Or (probably faster, but not as interleaved):
c.executemany('INSERT INTO whois VALUES (?)', (rid for rid, names in tt))
c.executemany('INSERT INTO whois_names VALUES (?, ?),
              (rid, name for rid, names in tt for name in names))
",A,1
47759938,2017-12-11 19:23:42.300000+00:00,"I don't know where you read that advice, but it is completely wrong. A circular import is a circular import, no matter what form of the import statement you use.
The solution here is not to import the models at all into core. You don't seem to need them there, as you don't refer to them.",A,1
426804,2009-01-09 02:01:08.977000+00:00,"Inverted Index.
Since this has a loop, I'm sure it fails the O(n) test.  However, when your result set has n rows, it's impossible to avoid iterating over the result set.  The query, however, is two hash lookups.
from collections import defaultdict

country = [ ""England"", ""USA"" ]

author=  [ (""Milton"", ""England""), (""Shakespeare"",""England""), (""Twain"",""USA"") ]

title = [ (""Milton"", ""Paradise Lost""), 
    (""Shakespeare"", ""Hamlet""),
    (""Shakespeare"", ""Othello""),
    (""Twain"",""Tom Sawyer""),
    (""Twain"",""Huck Finn""),
]

inv_country = {}
for id,c in enumerate(country):
    inv_country.setdefault(c,defaultdict(list))
    inv_country[c]['country'].append( id )

inv_author= {}
for id,row in enumerate(author):
    a,c = row
    inv_author.setdefault(a,defaultdict(list))
    inv_author[a]['author'].append( id )
    inv_country[c]['author'].append( id )

inv_title= {}
for id,row in enumerate(title):
    a,t = row
    inv_title.setdefault(t,defaultdict(list))
    inv_title[t]['title'].append( id )
    inv_author[a]['author'].append( id )

#Books by authors from England
for t in inv_country['England']['author']:
    print title[t]
",A,1
53570668,2018-12-01 12:17:18.860000+00:00,"But the bit you quote from the docs explicitly states the UserManager class assumes that you have both username and email fields. Since you don't, you would need to create your own subclass that defines the create_user method but doesn't expect a username parameter.
However, in your code you don't actually need to call this method. The main reason for doing so is that it sets the password; but you actually do that separately anyway. So you could just create the user like a normal model instance:
user = User(email=email)
user.set_password(password)
user.save()

Also note, your validation of passwords should go into the form's clean() method, not the view.",A,1
12478266,2012-09-18 13:42:37.880000+00:00,"If your code has to call specific private methods that cannot be overridden, use a name that starts with two underscores:
class A(object):
    def __init__(self):
        print(""initializing A"")
        self.__a()
    def __a(self):
        print(""A.a()"")

class B(A):
    def __init__(self):
        super().__init__()
        # add stuff for B
        self.bnum=3 # required by B.a()        
    def __a(self):
        print(""B.__a(), bnum=%i""%self.bnum)

Python ""mangles"" such method names by adding in the class name (plus an underscore) to minimize the chances subclasses overwrite them with their own versions.
The PEP 8 Python Style Guide has this to say about private name mangling:

If your class is intended to be subclassed, and you have attributes
  that you do not want subclasses to use, consider naming them with
  double leading underscores and no trailing underscores. This invokes
  Python's name mangling algorithm, where the name of the class is
  mangled into the attribute name. This helps avoid attribute name
  collisions should subclasses inadvertently contain attributes with the
  same name.
Note 1: Note that only the simple class name is used in the mangled
  name, so if a subclass chooses both the same class name and attribute
  name, you can still get name collisions.
Note 2: Name mangling can make certain uses, such as debugging and
  __getattr__(), less convenient. However the name mangling algorithm
  is well documented and easy to perform manually.
Note 3: Not everyone likes name mangling. Try to balance the need to
  avoid accidental name clashes with potential use by advanced callers.
",A,4
24313486,2014-06-19 18:08:11.650000+00:00,Django creates the tables. It does not create the database that the tables live in; you must do that.,A,3
21879573,2014-02-19 11:47:41.207000+00:00,"You have to produce lists with two columns; if your two columns are coming from two separate iterables, use zip() to join them:
week1 = xrange(1, 101) #for column1
week2 = xrange(1, 101) #for column2

cur.executemany(""INSERT INTO stud (classID, othercolumn) VALUES(%s, %s)"", zip(week1, week2))
db.commit()

zip(week1, week2) produces a sequence of tuples; each a pair of values taken from both input sequences: [(week1[0], week2[0]), (week1[1], week2[1]), ...].",A,1
21989341,2014-02-24 13:33:54.527000+00:00,"It appears you are using Django templates, judging by the syntax.
Instead of {% ifequal .. %}, use the {% if ... %} tag with the == operator, which lets you use boolean operators to combine tests:
<ul>
   {% if g_share == ""on"" and fb_share == ""on"" and tw_share == ""on"" %}
      <li> Share: </li>
   {% endif %}
</ul>
",A,2
54467987,2019-01-31 19:32:40.403000+00:00,"You didn't instantiate Workload.
w1 = Workload()
",A,2
37436490,2016-05-25 12:01:18.613000+00:00,"There are quite a few errors here. Your main one is that you have overridden the __init__ method so that it does not accept any parameters, but then you are trying to call it with parameters. Obviously, your subclass method should accept the same parameters as the base class, and pass those parameters into the super call - usually that is done with *args, **kwargs.
However it does not look like you should be overriding __init__ at all here. The only code you have in that overridden method most definitely does not belong there. Fields must always be defined at class level; they will simply not work if you define them inside a method. Move those fields out, and remove that method altogether.
class PraiseReport(CFCModel):
    username = ndb.StringProperty(repeated=True)
    report = ndb.StringProperty(required=True)
",A,2
53481630,2018-11-26 12:57:39.483000+00:00,"You can't name your model line_info then use that same name for the list of instances. line_info is now a local variable, it can't also be a global name, which is why you get a UnboundLocalError exception when you remove your line_info = None line.
Use a different name for the line_info.query.all() result. You do not need to set anything to None first:
from flask import render_template
from app import app
from model import line_info

@app.route('/')
def search_func():
    lines = Line_info.query.all()
    return render_template(""search_var.html"", line_info=lines)

You can still use line_info in the template, although that's not really recommended either.
Instead, I recommend you rename your model to follow the Python naming conventions; Use CamelCase, with uppercase initials, no _ underscores:
class LineInfo(db.Model):
    # ...

Now it is much easier to distinguish from local variables.",A,0
27247954,2014-12-02 11:10:53.713000+00:00,"You have misunderstood how for loops work in Python.
When you do for counter in DaysOfTheWeek, counter takes in turn each value from DaysOfTheWeek. That means the first time it will be ""Mon"", then ""Tue"", etc. So when you try to add it to 0, it fails with that error.
But you shouldn't be adding it to anything: that's the point. Instead, do this:
for day in DaysOfTheWeek:
    ...
    print(day, "": "", Meals[random_meal],"" and a number of "",NumberOfSides[random_meal],""."")

and you don't need counter=0 or counter+=1.",A,0
26800821,2014-11-07 12:00:36.503000+00:00,"The __init__ file in that project is empty. You need to import the nested pycvss module:
from pycvss import pycvss

c = pycvss.Cvss()

The documentation is rather unclear about this; I'd file a documentation issue with the project.
Personally, I'd add one line to the __init__.py file here:
from pycvss import Cvss

and update the documentation to illustrate how to import that class.
Note that the project expects the enum library to be available. If you are not using Python 3.4 or newer, you'll have to install the enum34 backport package too.",A,2
15695918,2013-03-29 01:20:55.757000+00:00,"Your add function doesn't have anything that will increment self.size. So it's whatever you set it to in __init__, which is presumably 0.
So, when the list actually is empty, __len__ returns 0 because self.head is None.
And after you add an element, it still returns 0 because self.size is 0.

Also, your code has at least one other problem in it. Look at this:
elif value > self.tail.data:
    self.tail = newNode
    newNode.prev = self.tail
    newNode.next = None
    return newNode

Clearly, newNode.prev is going to end up pointing at itself, rather than the previous tail.
There are lots of things that can help judge the correctness of code—unit tests, code reviews by someone who didn't work on it, stepping through it with an interactive visualizer, formal proofs, etc.—but the number of hours you worked on it is not one of those things.",A,1
44267248,2017-05-30 16:16:17.380000+00:00,"You just need a simple HTML form.


<form action=""{% url 'user_authentication' %}"" method=""GET"">
  <div>
    <label for=""username"">Username</label>
    <input name=""username"" id=""username"">
  </div>
  <div>
    <label for=""password"">Password</label>
    <input name=""password"" id=""password"">
  </div>
  <div>
    <input type=""submit"">
  </div>
</form>



This will send your username and password in the GET params (obviously in a real site you would use POST and get the data from request.POST in Django).
Note, this is all basic web development, and nothing specific to Django; you may benefit from doing an introductory HTML tutorial.",A,0
4069790,2010-11-01 14:32:00.977000+00:00,"Use the Python logging module. Then use the Django debug toolbar, which will catch and display all the things you send to the log.",A,2
24916171,2014-07-23 16:38:01.903000+00:00,"Apart from the problems of infinite recursion, it's a mistake to model something as a plain class and then later think about how you would convert it into a Django model. This is because of the object-relational impedence mismatch: Django models are meant to be stored in the database, and relational dbs never match up perfectly with oop concepts.
Luckily, there exists a very good algorithm for spring hierarchical data, which is MPTT, and a good Django implementation of that, django-mptt.",A,1
311360,2008-11-22 13:51:25.087000+00:00,"Look at the array module.
class Bit( object ):
    def __init__( self, size ):
        self.bits= array.array('B',[0 for i in range((size+7)//8)] )
    def set( self, bit ):
        b= self.bits[bit//8]
        self.bits[bit//8] = b | 1 << (bit % 8)
    def get( self, bit ):
        b= self.bits[bit//8]
        return (b >> (bit % 8)) & 1

FWIW, all of the //8 and % 8 operations can be replaced with >>3 and &0x07.  This may lead to slightly better speed at the risk of some obscurity.
Also, changing 'B' and 8 to 'L' and 32 should be faster on most hardware.  [Changing to 'H' and 16 might be faster on some hardware, but it's doubtful.]",A,8
17300592,2013-06-25 14:50:08.120000+00:00,"You use a list comprehension:
L = [float('nan') if el[0] == 1 else el for el in L]

The if .. else .. part is called a conditional expression.
This replaces the list with a new list. If you have multiple references to the same list, you can replace the elements of the list instead with a slice assignment:
L[:] = [float('nan') if el[0] == 1 else el for el in L]

Now all elements in L will be replaced with all the elements produced by the list comprehension. The difference between the two expressions is subtle but crucial; the first rebinds L to point to a new list, the second retains that list but only replaces the elements contained in the list itself.",A,2
21255069,2014-01-21 10:11:43.263000+00:00,"You don't need transactions at all here. If an IntegrityError is raised, that means the database update couldn't even be done, so there is nothing to roll back.
Transactions are useful if you want to roll back all the updates if a single one fails.",A,0
21756682,2014-02-13 14:14:34.827000+00:00,"The line filename = sys.argv[1] runs first. Your len() test is not reached.
Move setting the filename and tree into the main() function, and don't use globals here:
def f1(tree):
    ...

def main():
    if len(sys.argv) < 2:
        print 'usage: extract.py [file ...]'
        sys.exit(1)

    filename = sys.argv[1]
    tree = etree.parse(filename)

    f1(tree)

if __name__ == '__main__':
    main()
",A,3
16405743,2013-05-06 19:24:15.340000+00:00,"Your first problem is that applyCoder can't work as written.
buildCoder builds a dict that only has entries for letters. But applyCoder tries to look up anything that's not in string.punctuation, or == ' ', or in str(range(10)). I think you wanted string.digits there (because str(range(10)) is '[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]'), but it's still going to blow up if you give it, say, a newline, which a file called story.txt is almost guaranteed to have.
The simple fix is to just check for l in string.ascii_uppercase or l in string.ascii_lowercase. But there's an even better fix: Instead of trying to come up with a convoluted way to express the same filter in reverse, or repeating yourself, just try it:
for l in text:
    new_text += coder.get(l, l)

This will return coder[l] if l is in the map, or the default value, l, if it's not.

Having fixed that, the function runs, and successfully outputs something. But it doesn't output the right something. Why? 
Well, look at this:
if currentMatch>wordsFound:
    currentMatch=wordsFound
    bestShift=i

So, every time you find a better match than the initial wordsFound of 0, you… throw away the currentMatch value and leave wordsFound untouched. Surely you wanted wordsFound = currentMatch, not the other way around, right?

Having fixed both of these problems:
$ ln -s /usr/share/dict/words words.txt
$ echo -e ""This is a test.\n\nIs it good enough? Let's see.\n"" | rot13 > story.txt
$ python caesar.py
Loading word list from file...
   235886 words loaded.
Loading word list from file...
   235886 words loaded.
18
Loading word list from file...
   235886 words loaded.
This is a test. 

Here's some text. Is it enough? Let's see.

So, it's obviously doing some unnecessary repetitive work somewhere, but other than that, it works.

It's probably more important to learn how to debug problems like this yourself than to get the answer to this problem, so here's a couple of suggestions.
I found the problems by adding some extra print statements. The important one is here:
if currentMatch>wordsFound:
    print i, currentMatch, wordsFound
    currentMatch=wordsFound
    bestShift=i

You'll see that wordsFound never changes from 0. And that it picks a shift with 1 match as the best even after having found one with 18 matches. So obviously, something is wrong.
But I didn't know where to put that one. I added a dozen or so print lines all over the place. That's the simplest way to debug simple code.
For more complex code, where there's just way too much to print, you may want to write to a log file (ideally using logging) that you can parse after the fact. Or, better, use much simpler input data, and run in the debugger and/or an interactive visualizer (like this one).
Or, better, strip things down until you find the part that doesn't work. For example, if you know that shift 18 should be better than shift 12, try calling applyShift with 12 and 18 and see what they each return.
Even if these steps don't get you to the answer, they will get you to a better question to post on SO.",A,1
26768509,2014-11-05 22:38:11.760000+00:00,"You can do this with a counter and a dictionary:
index = 1
word_to_index = {}
for word in sentence.split():
    if word in word_to_index:
        # already seen
        continue
    word_to_index[word.lower()] = index
    index += 1

This assigns a unique index to each (lowercased) word; now you can use those to output your numbers:
print sentence
for word in sentence.split():
    print word_to_index[word.lower()],

If you want to use some more Python standard library magic, use a collections.defaultdict() object combined with itertools.count():
from collections import defaultdict
from itertools import count

word_to_index = defaultdict(count(1).next)

print sentence
for word in sentence.split():
    print word_to_index[word.lower()],

For Python 3, you'd have to use count(1).__next__, as well as the more obvious switch to using print() as a function (print(word_to_index[word.lower()], end=' ')).
This will autogenerate indices for each word. Demo of the latter approach:
>>> from collections import defaultdict
>>> from itertools import count
>>> sentence = ""The cat sat on the mat""
>>> word_to_index = defaultdict(count(1).next)
>>> print sentence
The cat sat on the mat
>>> for word in sentence.split():
...     print word_to_index[word.lower()],
... 
1 2 3 4 1 5
",A,4
23780676,2014-05-21 10:34:13.133000+00:00,"Your are adding formatted strings; adding strings places them one after the other.
Add the numbers first:
total = (
    pack[packageSelect-1][""adult""] * noAdult + 
    pack[packageSelect-1][""child""] * noChild + 
    AREAS.get(delivery))
print(""That is "" + str(formatC(total)) + ""for "" + pack[packageSelect][""name""] +  "" package for "" + str(noAdult) + "" "" + adult + "" and "", str(noChild) + "" "" + child + "",  delivered to "" + str(deliveryArea.title()) + ""."")

You may want to study how you can format strings with str.format(); it would make your printing code a whole lot simpler and more readable:
print(""That is {total} for {pack['name']} package for {noAdult} {adult} and {noChild} {child}, delivered to {area}."".format(
    total=formatC(total), pack=pack[packageSelect], noAdult=noAdult, adult=adult,
    noChild=noChild, child=child, area=deliveryArea.title()))
",A,1
32503926,2015-09-10 13:58:03.360000+00:00,"At issue here is that dict.items() returns a dictionary view in Python 3. It is as if you called dict.viewitems() in Python 2.
Either convert the dictionary view to a list first, or use the sorted() function:
edges = list(arestas.items())
edges.sort(key=lambda x: x[1])

or
edges = arestas.items()
edges = sorted(edges, key=lambda x: x[1])

Since the latter includes an implicit conversion to list anyway, it is the better choice unless you have a need to still access the unsorted list too.",A,4
37124693,2016-05-09 20:16:23.860000+00:00,"You have an unindented triple-quoted string literal right before the elif:
"""""" Mildly poisonous """"""
elif num >= -10 and num < 0:

""""""..."""""" string literals are not multi-line comments. They create strings, and only because you then ignore the string object produced does Python ignore the line. They are still part of the Python syntax; you can't ignore indentation rules when you use them.
Use proper # comments instead:
# Toxic
if num >= -25 and num < -10:
    # ...
# Mildly poisonous
elif num >= -10 and num < 0:
    # ...
# Healthy
else:
    # ...

Since comments are ignored altogether by the syntax, it doesn't matter how they are indented.
If you must use """""" ... """""" triple-quoted strings as 'block comments', you must indent them to be part of the if or elif block they are placed in:
"""""" Toxic """"""
if num >= -25 and num < -10:
    # ...
    """""" Mildly poisonous """"""
elif num >= -10 and num < 0:
    # ...
    """""" Healthy """"""
else:
    # ...
",A,3
24208685,2014-06-13 15:14:13.920000+00:00,"Special methods are always called on the type; for instances, that is the class. See the Special method lookup:

For custom classes, implicit invocations of special methods are only guaranteed to work correctly if defined on an object’s type, not in the object’s instance dictionary.

This means that you cannot add __str__ or __repr__ methods to the instance and expect those to be used.",A,2
12987242,2012-10-20 09:51:22.027000+00:00,"You could use an explicit if statement:
for item in items:
    if item > 3:
       # ...

Or you could create a generator if you need a name to iterate later, example:
filtered_items = (n for n in items if n > 3)

Or you could pass it to a function:
total = sum(n for n in items if n > 3)

It might be matter of taste but I find a for-loop combined with inlined genexpr such as for x in (y for y in items if y > 3): to be ugly compared to the above options.",A,2
12586576,2012-09-25 15:49:08.670000+00:00,"You have two % characters in there that are not meant to be formatting characters. You need to double it to make Python ignore it:
result = 'Div.1 :: %s [ %s p ] [ %s%% ] [ %s Dom ] <--> [ %s Dom ] [ %s%% ] [ %s p ] %s ::' % (attacker, attackerpoint, attackerbar, attackerdom, deffenderdom, deffenderbar, deffenderpoint, deffender)
",A,3
18258082,2013-08-15 17:20:58.060000+00:00,"For historical (hysterical?) reasons Python's bool type is a subclass of int, and True is equal to 1 and False is equal to 0.
They hash to the same location as well:
>>> True == 1
True
>>> hash(True) == hash(1)
True
>>> False == 0
True
>>> hash(False) == hash(0)
True

Since both True and 1 are considered equal and they hash to the same slot, both set and dict treat them as the same thing.
You'll see the same with a dictionary:
>>> True in {1: 'foo'}
True
>>> 1 in {True: 'foo'}
True

This behaviour extends to other numbers too; floating point values equal to integer values will show the same behaviour:
>>> {1.0, 1, 2.0, 2}
{1, 2}

but at least the reasons why that happens are a little more.. obvious.",A,7
15022654,2013-02-22 11:01:44.830000+00:00,"You really want to switch to using lxml here, because then you can use an XPath query:
from lxml import etree as ET

os.chdir(""/Users/paulsallen/workspace/fforce/FForce Dev Account/config/objects/"")
objs = glob.glob(""*.object"")

fields_dict = {}

for filename in objs:
    root = ET.parse(filename).getroot()
    required = root.xpath('.//n:fullName[../n:required/text()=""true""]/text()',
        namespaces={'n': tree.nsmap[None]})
    fields_dict[os.path.splitext(filename)[0]] = required

With that code you end up with a dictionary of lists; each key is a filename (without the extension), each value is a list of required fields.
The XPath query looks for fullName elements in the default namespace, that have a required element as sibling with the text 'true' in them. It then takes the contained text of each of those matching elements, which is a list we can store in the dictionary.",A,3
15191821,2013-03-03 22:43:40.413000+00:00,"Just split on whitespace:
line.split()

str.split() with no arguments will split on variable-width whitespace, and remove leading and trailing whitespace as needed. Whitespace is any tab, space, newline or carriage return:
>>> '10      AccessibleComputing     0       381202555       2010-08-26T22:38:36Z    OlEnglish       7181920 #F3#    [[Help:Reverting|Reverted]] edits by [[Special:Contributions/76.28.186.133|76.28.186.133]] ([[User talk:76.28.186.133|talk]]) to last version by Gurch  #REDIRECT#F0#[[Computer#F0#accessibility]]#F0#{{R#F0#from#F0#CamelCase}}        lo15ponaybcg2sf49sstw9gdjmdetnk ,Computer_accessibility'.split()
['10', 'AccessibleComputing', '0', '381202555', '2010-08-26T22:38:36Z', 'OlEnglish', '7181920', '#F3#', '[[Help:Reverting|Reverted]]', 'edits', 'by', '[[Special:Contributions/76.28.186.133|76.28.186.133]]', '([[User', 'talk:76.28.186.133|talk]])', 'to', 'last', 'version', 'by', 'Gurch', '#REDIRECT#F0#[[Computer#F0#accessibility]]#F0#{{R#F0#from#F0#CamelCase}}', 'lo15ponaybcg2sf49sstw9gdjmdetnk', ',Computer_accessibility']
",A,6
50797484,2018-06-11 12:14:04.850000+00:00,"player_or_pc != 'u' or player_or_pc != 'c' is always true:

if player_or_pc is 'u', it is not equal to 'c', so one of the two conditions is true
if player_or_pc is 'c', it is not equal to 'u', so one of the two conditions is true
any other value both conditions are true

Use and:
while player_or_pc != 'u' and player_or_pc != 'c':

or use ==, put the whole in parentheses, and use not in front:
while not (player_or_pc == 'u' or player_or_pc == 'c'):

at which point it is clearer to use a membership test:
while player_or_pc not in {'u', 'c'}:
",A,4
1271761,2009-08-13 12:48:52.660000+00:00,"""Over time, however, those constant changes in design seemed to mean I was spending a whole lot more time changing my tests than I was on writing the code itself""
Good.  You should spend a lot of time on testing.  It's important, and it's how you demonstrate that your code is right.  ""Less code than test"" is a good benchmark.
That means that you were writing effective tests that demonstrated your expectations for the underlying technology.
You may want to consider doing this.

Some tests are ""essential"" or ""core"" or ""enduring"" features of the application independent of any technology choices.  Focus on these.  The should never change.
Some tests confirm the technology or implementation choices.  These change all the time.  Perhaps you should segregate these so that the technology changes lead to focused changes here.
",A,4
50958140,2018-06-20 22:53:39.237000+00:00,"The closure captures are just a distraction here, and so is handling 90 and ss in different ways,1 so let's throw all of that out and just solve the problem you're struggling with.
First, you want a function that does one thing if called with an argument, and a different thing if called without an argument. That's easy; just use a parameter with a default value, or use *args.
Second, the ""different thing"" you want to do is to return a function that recursively does the same thing as your function, except for sticking an extra 'o' in the result. When defining a recursive data structure—whether it's a recursive closure, or a recursive defaultdict, the easiest way to do it is usually to use recursion.
For example:
def foo(x=None, *, _chain=0):
    if x is None:
        def bar(*args):
            return foo(*args, _chain=_chain+1)
        return bar
    else:
        return 'f' + 'o'*_chain + x

You could rewrite bar as a decorator, and return bar(foo), but that seems unnecessary. It wouldn't change the fact that the key here is that foo returns a function that calls foo.

1. If you don't know how to do that, just if isinstance(x, str):. Or, if you want to get fancy, look up functools.singledispatch.",A,2
426461,2009-01-08 23:23:16.863000+00:00,"Have you considered Adobe AIR? 
Bruce Eckel said: Try combining the power of Python with the polish of Adobe Flash to create a desktop application.",A,3
49913042,2018-04-19 05:01:42.867000+00:00,"The problem is that you're calling this on the wrong object.
It's a bit confusing that you named a StringVar object combobox, and a Combobox object qtype. But it's the Combobox object that has a list of strings that you can set. A StringVar just has a single string—in this case, the current selection of the Combobox.
The error message may be a little opaque. The reason it says StringVar instance has no attribute __setitem__ is that x[key] = value actually calls x.__setitem__(key, value), so if the type of x (StringVar in your case) has no such method, that's what the error says.
I don't know if you can see a Combobox's values using the dict-style syntax or not (the support for that is a big haphazard…). If it doesn't work, you presumably need to configure it, or possibly even configure the underlying Listbox. But, either way, it's the Combobox that you need to operate on.",A,0
32851223,2015-09-29 18:25:46.087000+00:00,"It looks like the server is responding differently depending on the client User-Agent. If you specify a custom User-Agent header the server responds with a PDF:
import requests
import shutil

url = 'http://www.prociv.pt/cnos/HAI/Setembro/Inc%C3%AAndios%20Rurais%20-%20Hist%C3%B3rico%20do%20Dia%2028SET.pdf'
headers = {'User-Agent': 'curl'}  # wink-wink
response = requests.get(url, headers=headers, stream=True)

if response.status_code == 200:
    with open('result.pdf', 'wb') as output:
        response.raw.decode_content = True
        shutil.copyfileobj(response.raw, output)

Demo:
>>> import requests
>>> url = 'http://www.prociv.pt/cnos/HAI/Setembro/Inc%C3%AAndios%20Rurais%20-%20Hist%C3%B3rico%20do%20Dia%2028SET.pdf'
>>> headers = {'User-Agent': 'curl'}  # wink-wink
>>> response = requests.get(url, headers=headers, stream=True)
>>> response.headers['content-type']
'application/pdf'
>>> response.headers['content-length']
'466191'
>>> response.raw.read(100)
'%PDF-1.5\r\n%\xb5\xb5\xb5\xb5\r\n1 0 obj\r\n<</Type/Catalog/Pages 2 0 R/Lang(pt-PT) /StructTreeRoot 37 0 R/MarkInfo<</'

My guess is that someone abused a Python script once to download too many files from that server and you are being tar-pitted based on the User-Agent header alone.",A,2
26352925,2014-10-14 04:59:14.003000+00:00,"The shebang #!/usr/bin/env python3 might be incorrect if the script is actually Python 2 script, not Python 3 script. json accepts bytestrings in Python 2. In Python 3, the input must be Unicode string.
The json code that fails is inside boto package.
boto's setup.py claims Python 3 support but It is possible that boto scripts are incompatible with Python 3 yet.",A,2
22412829,2014-03-14 18:18:42.057000+00:00,"Yes, this is a bug in Python; see issue 17073. Any rowid over sys.maxint (e.g. 2147483647 on 64-bit systems) would overflow:
$ python
Python 2.6.6 (r266:84292, Dec 27 2010, 00:02:40) 
[GCC 4.4.5] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import sys
>>> import sqlite3
>>> conn = sqlite3.connect(':memory:')
>>> sqliteobj = conn.cursor()
>>> sqliteobj.execute('CREATE TABLE BlockedNumbers (MobileNumber integer PRIMARY KEY NOT NULL, BlockedTimes integer);')
<sqlite3.Cursor object at 0xb73fb7a0>
>>> sqliteobj.execute(""INSERT INTO BlockedNumbers (MobileNumber,BlockedTimes) VALUES (?, '1')"", (sys.maxint,))
<sqlite3.Cursor object at 0xb73fb7a0>
>>> sqliteobj.lastrowid
2147483647
>>> sqliteobj.execute(""INSERT INTO BlockedNumbers (MobileNumber,BlockedTimes) VALUES (?, '1')"", (sys.maxint + 1,))
<sqlite3.Cursor object at 0xb73fb7a0>
>>> sqliteobj.lastrowid
-2147483648

Python versions 2.6.9, 2.7.4, 3.2.4 and 3.3.1 all include the fix for this bug.",A,2
18204835,2013-08-13 09:01:38.110000+00:00,"You have another Database module or package in your path somewhere, and it is getting imported instead.
To diagnose where that other module is living, add:
import Database
print Database.__file__

before the from Database import Database line; it'll print the filename of the module. You'll have to rename one or the other module to not conflict.",A,3
20773658,2013-12-25 14:15:09.543000+00:00,You shouldn't be importing models into settings. But you should be sure to add myapp to the INSTALLED_APPS setting.,A,0
16337639,2013-05-02 12:05:35.910000+00:00,"The underlying urllib3 library logs all new connections and URLs with the logging module, but not POST bodies. For GET requests this should be enough:
import logging

logging.basicConfig(level=logging.DEBUG)

which gives you the most verbose logging option; see the logging HOWTO for more details on how to configure logging levels and destinations.
Short demo:
>>> import requests
>>> import logging
>>> logging.basicConfig(level=logging.DEBUG)
>>> r = requests.get('http://httpbin.org/get?foo=bar&baz=python')
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): httpbin.org
DEBUG:requests.packages.urllib3.connectionpool:""GET /get?foo=bar&baz=python HTTP/1.1"" 200 353

The following messages are logged:

INFO: New connections (HTTP or HTTPS)
INFO: Dropped connections
INFO: Redirects
WARN: Connection pool full (if this happens often increase the connection pool size)
WARN: Retrying the connection
DEBUG: Connection details: method, path, HTTP version, status code and response length
",A,52
14903651,2013-02-15 21:32:58.627000+00:00,"To be honest, the easiest way is just to assign them:
e.a = a
e.b = b
e.c = c

You can't really do it more dynamically, because a variable doesn't know its own name. You would have to pass them as keyword arguments, in which case you can just update the namespace's __dict__ directly:
def dump_into_namespace(e, **kwargs):
    e.__dict__.update(kwargs)

and you would have to call it as:
dump_into_namespace(e, a=a, b=b, c=c)
",A,4
29194893,2015-03-22 13:22:37.157000+00:00,"I don't understand why you're trying to use cycle for only one thing. That doesn't make sense.
If you want to set a class for the first element only, use {% if forloop.first %}.",A,1
49321109,2018-03-16 12:44:48.943000+00:00,"I think you have misunderstood what foreign keys do. They give you access to the whole instance of the related model, not to an individual field. There is no reason to have two or three FKs pointing to the same model unless you want them to point to different instances, but even then you wouldn't need to use different to_field values.
Your model should just be:
class IFileTable(models.Model):
    intermediate_file = models.ForeignKey(IntermediateFile)

and now, given an instance of IFileTable, you can do:
my_ifiletable.intermediate_file.title
my_ifiletable.intermediate_file.year
my_ifiletable.intermediate_file.uploaded_at
",A,2
16876609,2013-06-01 19:47:34.593000+00:00,"This doesn't really make sense. A query is not something that is worth caching: it's just a stateless object that is easily recreated in a line of code. Caching is for actual data, which is expensive to fetch from the datastore.
A better solution if you need to easily reference a series of queries might be to simply store them in a dictionary, either at module level or as a class attribute of the relevant model.",A,1
2768144,2010-05-04 18:56:27.233000+00:00,"Surely this will work:
Profile.objects.all().annotate(viewcount=Count('profileview')).order_by('-viewcount')

As Botondus says, the Profile model is aware of the backwards ForeignKey relationship.",A,4
29085748,2015-03-16 19:52:25.977000+00:00,"You could just use a function, no class required:
shapes = {shape.__name__.lower(): shape 
          for shape in Shape.__subclasses__()}

def MagicShape(kind, color, size):
    try:
        return shapes[kind](color, size)
    except KeyError:
        raise ValueError(kind)

The class.__subclasses__() method here returns all subclasses of Shape, making for a quick and handy way to build a map from kind string to class.
Remember that creating a class is just another call. There is no difference between:
class Foo(object):
    def __init__(self, arg1, arg2):
        pass

and
def Foo(arg1, arg2):
    return something_that_is_an_instance

from the caller's point of view; they'd just use:
result = Foo(value1, value2)

for both the class and the function.",A,4
12085880,2012-08-23 06:25:33.107000+00:00,"You can use the zope.schema.getValidationErrors() function:
 newInstance = createContentInContainer(folder, id, **schemavalues)
 errors = getValidationErrors(IMyType, newInstance)
 if errors:
     # Schema not validated; errors is a sequence of (field, exception) tuples
     # field is None if the error is for an invariant.
",A,1
50863688,2018-06-14 18:21:53.950000+00:00,"me?fields=id,name,picture is a GraphQL query, and picture is a User Picture object, which comes with a few parameters.
One of those is type, which lets you specify a predefined size for the picture:
me?fields=id, name, picture.type(large)

The default is to give you picture.type(small).
Or you could use the width and height fields to request a specific size:
me?fields=id, name, picture.width(125).height(150)

The returned image dimensions can be larger than the specified size; the idea is that the image will 'fit' the dimensions.
You could even request multiple sizes, each returned under a different name; I'm showing just the picture definitions here, not the whole me?fields=... specification:
picture.type(small).as(smallpic),picture.width(1024).height(768).as(hdpic)

at which point the result with have URLs for both smallpic and hdpic.",A,2
18520575,2013-08-29 20:40:57.723000+00:00,"It seems very silly to do this in Python when you could just write mv *.pdf convert.pdf in an sh-compatible shell (the default on most non-Windows systems) or rename *.pdf convert.pdf in cmd (the default on Windows). But if you really want to, you can do it in two ways.
import glob, os
for name in glob.iglob('*.pdf'):
    os.rename(name, 'convert.pdf')

import os
for name in os.listdir('.'):
    if os.path.splitext(name)[1] == '.pdf':
        os.rename(name, 'convert.pdf')
",A,3
12540263,2012-09-22 02:43:12.780000+00:00,"You use symmetric difference to find C but the edge points (0,9) have only one neighbor. You could fix it by using left, right differences for edge points:
n = len(A) # 10 in your case
C = [0.] * n

if n > 1:
   C[0] = (A[1] - A[0]) / 2.
   C[-1] = (A[-1] - A[-2]) / 2.

for i in xrange(1, n-1):
    C[i] = (A[i+1] - A[i-1]) / 2.

Whether it is appropriate depends on how you'd like to use the C list later.
If you need C[0] = C[1] then it is even simpler:
n = len(A)
C = [0.] * n

for i in xrange(1, n-1):
    C[i] = (A[i+1] - A[i-1]) / 2.

if n > 1:
   C[0] = C[1]
   C[-1] = C[-2] # for symmetry

if n == 0 i.e., A is empty then C is empty.
if n == 1 i.e., A has exactly one element then C has exactly one element: C[0] = 0.",A,1
15707510,2013-03-29 16:41:18.787000+00:00,"You need to loop over the nested structure to apply the filters only to the inner values. Dict comprehensions work fine for that:
dict_new = {kouter: {kinner: tuple(filter(bool, vinner)) for kinner, vinner in vouter.iteritems()} 
            for kouter, vouter in dict_old.iteritems()}

Demo:
>>> {kouter: {kinner: tuple(filter(bool, vinner)) for kinner, vinner in vouter.iteritems()} 
...             for kouter, vouter in dict_old.iteritems()}
{'a': {'a1': (1, 1), 'a2': (1, 1, 1)}, 'b': {'b1': (1, 1, 1, 1), 'b2': (1, 1)}}
",A,4
51585137,2018-07-29 22:41:00.383000+00:00,"You know how to copy a sequence to another sequence one by one, right?
new_string = ''
for ch in old_string:
    new_string = new_string + ch

If you want to copy the sequence in reverse, just add the new values onto the left instead of onto the right:
new_string = ''
for ch in old_string:
    new_string = ch + new_string

That's really the only trick you need.

Now, this isn't super-efficient, because string concatenation takes quadratic time. You could solve this by using a collections.deque (which you can append to the left of in constant time) and then calling ''.join at the end. But I doubt your teacher is expecting that from you. Just do it the simple way.

Of course you have to loop over TextImage applying this to every string in every sublist in the list. That's probably what they're expecting you to use [:] for. But that's easy; it's just looping over lists.",A,1
26104888,2014-09-29 16:46:20.577000+00:00,"You are POST-ing an empty body, and only using query parameters. requests can send duplicate keys just fine, both in request parameters and in a POST body. All you have to do is use a sequence of key-value tuples instead of a dictionary:
params = [
    (""token"", token),
    (""title"", ""title""),
    (""opt"", """"),
    (""opt"", 'value'),
    (""token"": token),
]

These will be encoded for you when used as query parameters or as application/x-www-form-urlencoded POST body.
To send these as a POST body, use the data keyword argument:
requests.post('http://mypage.com/add', data=params,
              headers = headers, proxies = proxy, timeout = max_timeout)

or use params to send these as query parameters (in the URL):
requests.post('http://mypage.com/add', params=params,
              headers = headers, proxies = proxy, timeout = max_timeout)
",A,3
48936441,2018-02-22 20:16:20.020000+00:00,"This is indeed a scoping issue, as documented in the Jinja2 template reference:

Scoping Behavior
Please keep in mind that it is not possible to set variables inside a block and have them show up outside of it. This also applies to loops.
[...]
As of version 2.10 more complex use cases can be handled using namespace objects which allow propagating of changes across scopes[.]

So you could use the namespace() class as a work-around:
{% set ns = namespace(items=0) %}
{% for line in current_order.order_lines %}
    {% set ns.items = ns.items + line.quantity %}
{% endfor %}

{{ ns.items }}

That said, it is much better if you calculated the item count up front and passed this into the template as part of the current_order object or additional context.
Another option is to use the sum() filter to sum those quantities:
{% for line in current_order.order_lines %} #loops twice in current test
    <!-- render order line -->
{% endfor %}

{{ current_order.order_lines|sum(attribute='quantity') }}
",A,4
42957126,2017-03-22 16:09:53.270000+00:00,"Yes, don't re-invent the wheel. Use the standard library instead; you want to use the collections.Counter() class here:
from collections import Counter

def my_dedup_count(l):
    return Counter(l).items()

You may want to just return the counter itself and use all functionality it provides (such as giving you a key-count list sorted by counts).
If you expected only consecutive runs to be counted (so ['a', 'b', 'a'] results in [('a', 1), ('b', 1), ('a', 1)], then use itertools.groupby():
from itertools import groupby

def my_dedup_count(l):
    return [(k, sum(1 for _ in g)) for k, g in groupby(l)]
",A,3
40748470,2016-11-22 17:50:49.047000+00:00,"No, the name getinput in the nested function doesn't create a reference. It is looked up each time _tryagain_ is called, because it is a global. Not that this matters, as a module is cleared as a whole when Python exits, there is no real chance for a memory leak here.
However, you are using recursion to ask users for input, and your code is hard to follow. Use a simple loop instead, see Asking the user for input until they give a valid response.",A,2
28919046,2015-03-07 19:31:31.087000+00:00,"You don't define a new book dictionary inside the loop, so each time through the loop you're just reusing the same dictionary over and over: and you just end up assigning the same one to multiple keys.
Instead, make sure you define a new book dict at the start of each iteration:
booklist = {}
for line in file:
    book = {}
    ...
    booklist['isbn'] = book
",A,4
33507389,2015-11-03 19:25:35.380000+00:00,"PIL is not a standard library; you installed it for Python 2, but that installation is not (and cannot be) used by Python 3.
Install PIL (or rather, the Pillow fork) explicitly in Python 3 as well:
python3 -m ensurepip  # optional, makes sure pip is installed
python3 -m pip install Pillow
",A,4
14800487,2013-02-10 17:29:04.353000+00:00,"You could use PunktSentenceTokenizer directly (it is used to implement sent_tokenize()):
from nltk.tokenize.punkt import PunktSentenceTokenizer

text = 'Rabbit say to itself ""Oh dear! Oh dear! I shall be too late!""'
for start, end in PunktSentenceTokenizer().span_tokenize(text):
    length = end - start
    print buffer(text, start, length), start, length

You could use text[start:end] instead of buffer(text, start, end - start) if you don't mind copying of each sentence.",A,4
29845754,2015-04-24 11:00:21+00:00,"There's no such thing as ""actionscript3 compressed JSON"".
The fact that the data came from JSON originally is irrelevant; it's stored in AMF, which is what matters. And AMF isn't compressed JSON. It doesn't even handle the same data types as JSON (although it almost includes all of JSON's data types as a subset).
Adobe published a spec for AMF (or at least for AMF3, the version that you've got), so you can read the spec and write your own AMF deserializer.
But you don't have to; there are already parsers for AMF on PyPI and elsewhere. I don't know which one is best for your use, and StackOverflow isn't a good place to find recommendations for particular libraries anyway, but you can just search PyPI or Google.",A,1
52519003,2018-09-26 13:32:13.593000+00:00,"You should not use Base64 here. The site you link to gives you the hex values of the digest bytes. Use the HMAC.hexdigest() method to get the same value in hex in Python:
>>> key = b'secret'
>>> message = b'123'
>>> digester = hmac.new(key, message, hashlib.sha1)
>>> digester.hexdigest()
'b14e92eb17f6b78ec5a205ee0e1ab220fb7f86d7'

put differently, your code outputs the correct value, but as Base64-encoded data:
>>> digester.digest()
b'\xb1N\x92\xeb\x17\xf6\xb7\x8e\xc5\xa2\x05\xee\x0e\x1a\xb2 \xfb\x7f\x86\xd7'
>>> base64.urlsafe_b64encode(digester.digest())
b'sU6S6xf2t47FogXuDhqyIPt_htc='

and the value you generated online contains the exact same bytes as the hex digest, so we can generate the same base64 output for that:
>>> bytes.fromhex('b14e92eb17f6b78ec5a205ee0e1ab220fb7f86d7')
b'\xb1N\x92\xeb\x17\xf6\xb7\x8e\xc5\xa2\x05\xee\x0e\x1a\xb2 \xfb\x7f\x86\xd7'
>>> base64.urlsafe_b64encode(bytes.fromhex('b14e92eb17f6b78ec5a205ee0e1ab220fb7f86d7'))
b'sU6S6xf2t47FogXuDhqyIPt_htc='
",A,2
41042286,2016-12-08 14:52:53.720000+00:00,"You should not be using a for loop at all. Just remove that one item:
def clear(self, r):
    self.__list_item.remove(r)

You are trying to remove r from the list as many times as you have total items in the list..",A,0
47122497,2017-11-05 14:08:40.083000+00:00,"You've told the form to expect the user as the first positional parameter, so you need to pass it there.
form2 = class_model(request.user, data=request.POST)
",A,0
8554220,2011-12-18 19:32:20.317000+00:00,"Why do you want the signal to be an instance method? There doesn't seem to be any reason for it. You just need to ensure that when you write the signal function itself, it passes the relevant instance - eg as an instance kwarg - exactly as the built-in pre-save and post-save signals do.
Added after edit to question But that is exactly how signals are supposed to work. There's a single signal function, which sends the signal along with any association information, and any number of receivers that listen to the signal. But the receivers themselves aren't associated with particular instances - they can't be, an instance only exists when you actually instantiate it(!) and otherwise it's just a row in a database.
Perhaps your receiver function could query for the relevant objects itself, and update them there - or even better, do an update query to change them in-place.",A,1
41351526,2016-12-27 20:05:11.547000+00:00,"The first positional argument to a forms.DecimalField is not the label, but the max value - which is why the error complains about comparing a string (that value) with an int (the actual value of the field).
Generally you shouldn't use positional args with form field classes - use kwargs consistently:
amount = forms.DecimalField(
    label=_(""Parts now inside storage""),
    ...)
",A,2
40576888,2016-11-13 17:45:27.390000+00:00,"You need to compare Python 3's str.translate() with Python 2's unicode.translate(). Both take a mapping from codepoint (an integer) to a replacement (either another integer or a single-character Unicode e string).
The str type has a static method str.maketrans() that takes the characters-to-delete (the second argument to Python 2's str.translate()) as the third argument, to produce such a map. Use that here:
map = str.maketrans('', '', 'aeiouAEIOU')
a = data_list[i].translate(map)

This outputs a dictionary mapping each of the vowel codepoints to None:
>>> str.maketrans('', '', 'aeiouAEIOU')
{97: None, 101: None, 105: None, 111: None, 117: None, 65: None, 69: None, 73: None, 79: None, 85: None}
",A,6
10934230,2012-06-07 14:51:02.650000+00:00,"You should really use a proper templating system. Jinja2 is included with AppEngine.
However in the meantime your problem is that your templates are ASCII but your data is not (can't tell if it's utf-8 or unicode). Easy solution is to prefix each template string with u to make it Unicode.
But, you should really use a proper templating system.",A,6
37426027,2016-05-25 00:32:28.180000+00:00,"You are slicing the outer list:
student_scores[4:5][0]

Slicing produces a new list, in this case with one element, an the [0] selects that nested list:
>>> student_scores = [74.0, 96.0, 72.0, 88.0, ['71', '80', '83', '77', '90', '88', '95', '71', '76', '94'], 80.0, 74.0, 98.0, 77.0]
>>> student_scores[4:5]
[['71', '80', '83', '77', '90', '88', '95', '71', '76', '94']]
>>> student_scores[4:5][0]
['71', '80', '83', '77', '90', '88', '95', '71', '76', '94']

Perhaps you want to use student_scores[4][0] (no slicing, just the 4th element) instead? That'd produce '71':
>>> student_scores[4][0]
'71'

You are also skipping student_scores[5], and will get an IndexError for student_scores[9], which doesn't exist.
You probably want to avoid typing all those direct references; specify your weights as a sequence and use zip() with a generator expression and sum() to calculate the weighted sum:
def weighted_total_score(student_scores):
    weights = .1, .1, .15, .05, .3, .08, .08, .09, .05                 
    return sum(int(s[0] if isinstance(s, list) else s) * w
               for s, w in zip(student_scores, weights))

This uses isinstance(s, list) to detect the one list object and extract the first value from that.
If you need the average of the the nested list, calculate that on the spot:
def average(string_scores):
    return sum(map(int, string_scores), 0.0) / len(string_scores)

def weighted_total_score(student_scores):
    weights = .1, .1, .15, .05, .3, .08, .08, .09, .05                 
    return sum(int(average(s[0]) if isinstance(s, list) else s) * w
               for s, w in zip(student_scores, weights))

The average() function here converts each string in the list to an integer, then sums those integers and divides the result by the length of the list. The sum() is started with a floating point 0.0 to force the total to be a float, this makes sure the division is also producing a float, this only matters on Python 2.",A,2
13935537,2012-12-18 14:52:48.403000+00:00,"By using re.split():
>>> re.split(r'(this|into|ones)', ""Let's split this string into many small ones"")
[""Let's split "", 'this', ' string ', 'into', ' many small ', 'ones', '']

By putting the words to split on in a capturing group, the output includes the words we split on.
If you need the spaces removed, use map(str.strip, result) on the re.split() output:
>>> map(str.strip, re.split(r'(this|into|ones)', ""Let's split this string into many small ones""))
[""Let's split"", 'this', 'string', 'into', 'many small', 'ones', '']

and you could use filter(None, result) to remove any empty strings if need be:
>>> filter(None, map(str.strip, re.split(r'(this|into|ones)', ""Let's split this string into many small ones"")))
[""Let's split"", 'this', 'string', 'into', 'many small', 'ones']

To split on words but keep them attached to the following group, you need to use a lookahead assertion instead:
>>> re.split(r'\s(?=(?:this|into|ones)\b)', ""Let's split this string into many small ones"")
[""Let's split"", 'this string', 'into many small', 'ones']

Now we are really splitting on whitespace, but only on whitespace that is followed by a whole word, one in the set of this, into and ones.",A,3
52640661,2018-10-04 06:49:06.983000+00:00,There is no need for JSON here at all. You have already serialized the data so it is in form-encoded format; doing JSON.stringify just adds another pointless level of serialization. Remove that line and send data directly in your Ajax function.,A,1
4741660,2011-01-19 22:42:42.543000+00:00,"Result is a dictionary. Iterating through a dictionary with for x in mydict just gives the keys, not the values. So you need to do {% for item, value in mydict.items %}.
Secondly, {{ result.item.username }} makes no sense at all. item is a the value of a key into the result dict, but you can't do that sort of indirect lookups in Django templates. However, luckily we have the value in the value variable.
Thirdly, as Kabie points out, each value is actually a single-element list. So you'll need to access the first item in that list, and then the username member of that.
Putting it all together:
{% for item, value in result.items %}
<h1> {{ item }} </h1>
<p> {{ value.0.username }} </p>
{% endfor %}
",A,1
46625057,2017-10-07 20:59:23.653000+00:00,"The namespace in a function does not produce attributes. __call__ is a local variable inside f(), nothing more.
def <identifier>(...): binds the name identifier in the current namespace, and in a function that means it assigns a local name, just like some_name = ... would do.
At most, you can return that object:
>>> def f():
...     def __call__(x):
...         return x
...     return __call__  # return the local variable
...
>>> f()
<function f.<locals>.__call__ at 0x10e946400>
>>> f()(1)
1

It would be really surprising if all local variables in a function became attributes instead, even before you had actually executed the function!
However, even if you gave a function object a __call__ attribute, it still would not be used. You can only use a class to create an object with a __call__ method:
class Foo:
    def __call__(self, x):
        return x

foo = Foo()
foo()  # returns 1

That's because __call__ is a special method, and thus subject to the Special method lookup rules, which dictate that Python would look for the method on the type of the object, not the object itself.
When Python evaluates foo(...), if foo is not a function object Python will execute type(foo).__call__(foo, ...). Python does this to make sure that the Foo.__call__ method is not used when you create your instance with Foo(); classes are callables too but defining Foo.__call__ should not clash with that.",A,4
24268760,2014-06-17 16:17:49.213000+00:00,"
The L postfixes signify long integers. They are the same thing as (short) integers really; there really is no need to convert these. It is only their repr() output that includes the L; print the value directly or write it to a file and the L postfix is not included.
Unicode values can be encoded to UTF-8 with the unicode.encode() method:
encoded = unicodestr.encode('utf8')


Your beef is with the list representation here; you logged all rows, and Python containers represent their contents by calling repr() on each value. These representations are great for debugging as their types are made obvious.
It depends on what you do with these values next. It is generally a good idea to use Unicode throughout your code, and only encode at the last moment (when writing to a file, or printing or sending over the network). A good many methods handle this for you. Printing will encode to your terminal codec automatically, for example. When adding to an XML file, most XML libraries handle Unicode for you. Etc.",A,2
20275588,2013-11-28 22:25:35.283000+00:00,"The following would match a hex RGB string:
import re

_rgbstring = re.compile(r'#[a-fA-F0-9]{6}$')

def isrgbcolor(value):
    return bool(_rgbstring.match(value))

This only returns True if a string starting with # followed by exactly 6 hex digits is passed in.
Demo:
>>> isrgbcolor('#FAF0E6')
True
>>> isrgbcolor('#FAF0')
False
>>> isrgbcolor('FAF0E6')
False
>>> isrgbcolor('#NotRgb')
False

If you want to support the 3-digit CSS format as well, update the pattern:
_rgbstring = re.compile(r'#[a-fA-F0-9]{3}(?:[a-fA-F0-9]{3})?$')

This matches a hash followed by 3 hex digits, plus an optional 3 extra digits.",A,6
21892404,2014-02-19 21:05:07.717000+00:00,"No need for proxy settings. You can use a single virtual server, but run each application under a certain subdirectory: for example, with Django or another wsgi app, you would simply set WSGIScriptAlias to the relevant dir.
A big clarification, though: Django is not in any way a server. In fact, you need Apache or an equivalent in order to serve Django properly.",A,0
2876803,2010-05-20 18:41:43.943000+00:00,"You shouldn't need to write an xml file - one of the benefits of Haystack is that it does that for you. Once your searchindex classes are defined, just run ./manage.py build_solr_schema and copy the resulting xml to your solrconfig file.",A,-4
15848532,2013-04-06 08:08:33.017000+00:00,"You could define a set of chains, applied with functools.reduce()
from functools import reduce

chains = (
    (doStuff1,),
    (doStuff1, doStuff2),
    (doStuff1, doStuff2, doStuff3),
)

for textfile in path:
    for chain in chains:
        data = reduce(lambda data, func: func(data), chain, textfile)
        calculateandPrint(data)

The reduce() call effectively does func3(func2(func1(textfile)) if chain contained 3 functions.
I assumed here that you wanted to apply calculateandPrint() per textfile in path after the chain of functions has been applied.
Each iteration of the for chain in chains loop represents one of your doStuffx loop bodies in your original example, but we only loop through for textfile in path once.
You can also swap the loops; adjusting to your example:
for chain in chains:
    for doc in documents:
        doc.list_strippedtext = reduce(lambda data, func: func(data), chain, doc.text)
    bow = createBOW(documents)  
",A,3
27721860,2014-12-31 13:32:05.057000+00:00,"You could remove the first part until no more dashes remain; that'd be the domain name to remove from the hostname:
hostname = domain
while '-' in domain:
    domain = domain.partition('.')[-1]
hostname = hostname[:-len(domain) - 1]

or the other way around, remove the last part if it doesn't contain dashes, with str.rpartition():
hostname = domain
while True:
    first, _, end = hostname.rpartition('.')
    if '-' in end:
        break
    hostname = first

Using a regular expression looking for any part that only contains letters and dots:
import re

hostname = re.sub(r'\.[a-z.]+$', '', domain)

Demo:
>>> domain = 'ab-test-db-dev.0002-colo1-vm234.abc.domain.com'
>>> hostname = domain
>>> while '-' in domain:
...     domain = domain.partition('.')[-1]
... 
>>> hostname[:-len(domain) - 1]
'ab-test-db-dev.0002-colo1-vm234'
>>> domain = 'ab-test-db-dev.0002-colo1-vm234.abc.domain.com'
>>> hostname = domain
>>> while True:
...     first, _, end = hostname.rpartition('.')
...     if '-' in end:
...         break
...     hostname = first
... 
>>> hostname
'ab-test-db-dev.0002-colo1-vm234'
>>> import re
>>> re.sub(r'\.[a-z.]+$', '', domain)
'ab-test-db-dev.0002-colo1-vm234'
",A,2
19013012,2013-09-25 18:57:31.250000+00:00,"
I was wondering, if this is the fastest way possible?

No, of course not. You can implement it a lot faster in hand-coded assembly than in Python. So what?
If the ""do something..."" is not trivial, and there are many matches, the cost to do something 100000 times is going to be a lot more expensive than the cost of looping 500000 times, so finding the fastest way to loop doesn't matter at all.
In fact, just calling split two to three each loop instead of remembering and reusing the result is going to swamp the cost of iteration, and not passing a maxsplit argument when you only care about two results may as well.

So, you're trying to optimize the wrong thing. But what if, after you fix everything else, it turns out that the cost of iteration really does matter here?
Well, you can't use a comprehension directly to speed things up, because comprehensions are for expressions that return values, not statements to do things.
But, if you look at your code, you'll realize you're actually doing three things: splitting each string, then filtering out the ones that don't match, then doing the ""do something"". So, you can use a comprehension for the first two parts, and then you're only using a slow for loop for the much smaller list of values that passed the filter.
It looks like you tried this, but you made two mistakes.
First, you're better off with a generator expression than a list comprehension—you don't need a list here, just something to iterator over, so don't pay to build one.
Second, you don't want to split the string three times. You can probably find some convoluted way to get the split done once in a single comprehension, but why bother? Just write each step as its own step.
So:
split_items = (item.split(';') for item in items)
filtered_items = (item for item in split_items 
                  if item[0] == ""string_value"" or item[1] == ""string_value"")
for item in filtered_items:
    do something...

Will this actually be faster? If you can get some real test data, and ""do something..."" code, that shows that the iteration is a bottleneck, you can test on that real data and code. Until then, there's nothing to test.",A,3
36730031,2016-04-19 21:42:59.553000+00:00,"You most likely have a file with one or more DOS EOF (CTRL-Z) characters in it, ASCII codepoint 0x1A. When Windows opens a file in text mode, it'll still honour the old DOS semantics and end a file whenever it reads that character. See Line reading chokes on 0x1A.
Only by opening a file in binary mode can you bypass this behaviour. To do so and still count lines, you have two options:

read in chunks, then count the number of line separators in each chunk:
def bufcount(filename, linesep=os.linesep, buf_size=2 ** 15):
    lines = 0
    with open(filename, 'rb') as f:
        last = ''
        for buf in iter(f.read, ''):
            lines += buf.count(linesep)
            if last and last + buf[0] == linesep:
                # count line separators straddling a boundary
                lines += 1
            if len(linesep) > 1:
                last = buf[-1]
    return lines

Take into account that on Windows os.linesep is set to \r\n, adjust as needed for your file; in binary mode line separators are not translated to \n.
Use io.open(); the io set of file objects open the file in binary mode always, then do the translations themselves:
import io

with io.open(filename) as f:
    lines = sum(1 for line in f)

",A,1
51227190,2018-07-07 21:50:33.810000+00:00,"This is a limitation of argparse—but one that's partially lifted in 3.7.
Unix tools generally don't claim to support intermixing of options and arguments, even though they often do. The problem is that combining it with some other features, like subcommands, leads to ambiguity. So, typically, libraries that support any of those features punt on the problem and don't allow intermixing. Or they do something kind of hacky—allowing options at the end, at the start, and in certain hard-to-predict cases but not others in the middle.
That's what argparse originally did. But 3.7 adds Intermixed parsing.
You have to manually call parse_intermixed_args instead of parse_args.
And if you try to use this with any of the features it doesn't go well with, you'll get an exception (even if there's no ambiguity for the particular set of args you pass—which should make it easier to debug).
But otherwise, it'll work as expected: options (together with their values, of course) can be freely mixed with positional arguments anywhere in the command line.

Unfortunately, I don't know of a drop-in backport on PyPI to get 3.7 argparse in earlier versions; the semi-official argparse backport is mainly for pre-2.7/3.2 versions that don't have it at all, and only backports the 3.4 version.",A,5
51430384,2018-07-19 19:22:10.347000+00:00,"Usually, you don't need this in an expression, so you just call next(it), ignoring the results, to consume and discard the first element.

However, if the iterator might be empty, you have to decide what you want to happen:

Maybe you want to raise StopIteration, in which case next(it) is fine.
Maybe you want to raise something else, in which case you next(it) inside an except StopIteration: raise SomethingElse().
Maybe you just want to leave the iterator empty, in which case you can call next(it, None). 

You can find examples of these in the stdlib and docs. For example, if you scan through the recipes in itertools:
def pairwise(iterable):
    ""s -> (s0,s1), (s1,s2), (s2, s3), ...""
    a, b = tee(iterable)
    next(b, None)
    return zip(a, b)

This is doing exactly what you want to do—skip the first element of b. And if iterable is empty, you don't want an error here; you just want to iterate nothing. So, next(b, None).

What if you need to do this in the middle of an expression?
Then you can write a function that skips the first element:
def skip_first(iterable):
    it = iter(iterable)
    next(it, None)
    return it

(Again, you have to decide what you want to happen for an empty iterable.)
This returns a first-skipped version of the iterator, so you can use it inline. (It also mutates the iterator you passed in, of course, but you normally only use on a temporary value that you're not keeping any references to, so that's not a problem.)
Or, if you need to return a generator instead of an arbitrary iterator (usually you don't):
def skip_first(iterable):
    it = iter(iterable)
    next(it, None)
    yield from it

Or you can use the more general version of the same idea, itertools.islice. The following have the same effect:
it = skip_first(it)
it = itertools.islice(it, 1, None)


While we're on the itertools recipes, it's worth looking at consume:
def consume(iterator, n=None):
    ""Advance the iterator n-steps ahead. If n is None, consume entirely.""
    # Use functions that consume iterators at C speed.
    if n is None:
        # feed the entire iterator into a zero-length deque
        collections.deque(iterator, maxlen=0)
    else:
        # advance to the empty slice starting at position n
        next(islice(iterator, n, n), None)

Forget the None part; the interesting bit is that it skips n elements with an islice and a next. (Notice that it's mutating iterator in-place, not returning something.)",A,3
35080352,2016-01-29 08:52:21.600000+00:00,"
FileNotFoundError: [Errno 2] No such file or directory: 'command'

command is a shell builtin. subprocess.Popen does NOT run the shell by default.
To run the shell, pass shell=True:
>>> import subprocess
>>> subprocess.check_output('command -v python', shell=True)
b'/usr/bin/python\n'

To find the full path to an executable, you could use shutil.which() instead:
>>> import shutil
>>> shutil.which('python')
'/usr/bin/python'
",A,2
53447195,2018-11-23 13:00:09.757000+00:00,"GitHub is blocking your repeated requests. Do not scrape sites in quick succession, many website operators actively block too many requests. As a result, the content that is returned no longer matches your XPath query.
You should be using the REST API that GitHub provides to retrieve project stats like the number of contributors, and you should implement some kind of rate limiting. There is no need to retrieve the same number 100 times, contributor counts do not change that rapidly. 
API responses include information on how many requests you can make in a time window, and you can use conditional requests to only incur rate limit costs when the data actually has changed:
import requests
import time
from urllib.parse import parse_qsl, urlparse

owner, repo = 'tipsy', 'profile-summary-for-github'
github_username = '....'
# token = '....'   # optional Github basic auth token
stats = 'https://api.github.com/repos/{}/{}/contributors'

with requests.session() as sess:
    # GitHub requests you use your username or appname in the header
    sess.headers['User-Agent'] += ' - {}'.format(github_username)
    # Consider logging in! You'll get more quota
    # sess.auth = (github_username, token)

    # start with the first, move to the last when available, include anonymous
    last_page = stats.format(owner, repo) + '?per_page=100&page=1&anon=true'

    while True:
        r = sess.get(last_page)
        if r.status_code == requests.codes.not_found:
            print(""No such repo"")
            break
        if r.status_code == requests.codes.no_content:
            print(""No contributors, repository is empty"")
            break
        if r.status_code == requests.codes.accepted:
            print(""Stats not yet ready, retrying"")
        elif r.status_code == requests.codes.not_modified:
            print(""Stats not changed"")
        elif r.ok:
            # success! Check for a last page, get that instead of current
            # to get accurate count
            link_last = r.links.get('last', {}).get('url')
            if link_last and r.url != link_last:
                last_page = link_last
            else:
                # this is the last page, report on count
                params = dict(parse_qsl(urlparse(r.url).query))
                page_num = int(params.get('page', '1'))
                per_page = int(params.get('per_page', '100'))
                contributor_count = len(r.json()) + (per_page * (page_num - 1))
                print(""Contributor count:"", contributor_count)
            # only get us a fresh response next time
            sess.headers['If-None-Match'] = r.headers['ETag']

        # pace ourselves following the rate limit
        window_remaining = int(r.headers['X-RateLimit-Reset']) - time.time()
        rate_remaining = int(r.headers['X-RateLimit-Remaining'])
        # sleep long enough to honour the rate limit or at least 100 milliseconds
        time.sleep(max(window_remaining / rate_remaining, 0.1))

The above uses a requests session object to handle repeated headers and ensure that you get to reuse connections where possible.
A good library such as github3.py (incidentally written by a requests core contributor) will take care of most of those details for you.
If you do want to persist on scraping the site directly, you do take a risk that the site operators block you altogether. Try to take some responsibility by not hammering the site continually.
That means that at the very least, you should honour the Retry-After header that GitHub gives you on 429:
if not r.ok:
    print(""Received a response other that 200 OK:"", r.status_code, r.reason)
    retry_after = r.headers.get('Retry-After')
    if retry_after is not None:
        print(""Response included a Retry-After:"", retry_after)
        time.sleep(int(retry_after))
else:
    # parse OK response
",A,0
19600735,2013-10-25 23:07:17.640000+00:00,"Nothing based on a pathname will work, because your package, or your entire site-packages, could be, say, a zipfile, or the files may have been installed somewhere other than inside the package, etc.
The pkgutil module, in particular the get_data function, is the stdlib way to get the contents of a data file that's part of a package.
See Installing Package Data, and Installing Additional Files for more info on making sure the files actually end up there in the first place.
If you want to use setuptools instead of just the stdlib, pkg_resources can make things easier/more flexible.",A,5
14363251,2013-01-16 16:37:45.163000+00:00,"Unfortunately, ElementTree namespace support is rather patchy.
You'll need to use an internal method from the xml.etree.ElementTree module to get a namespace map out:
_, namespaces = ET._namespaces(root, 'utf8')

namespaces is now a dict with URIs as keys, and prefixes as values.
You could switch to lxml instead. That library implements the same ElementTree API, but has augmented that API considerably.
For example, each node includes a .nsmap attribute which maps prefixes to URIs, including the default namespace under the key None.",A,2
52329960,2018-09-14 10:21:15.483000+00:00,"Your list doesn't contain any strings with single quotes. I think you are confusing the repr() representation of the strings with their values.
When you print a Python standard library container such as a list (or a tuple, set, dictionary, etc.) then the contents of such a container are shown their repr() representation output; this is great when debugging because it makes it clear what type of objects you have. For strings, the representation uses valid Python string literal syntax; you can copy the output and paste it into another Python script or the interactive interpreter and you'll get the exact same value.
For example, s here is a string that contains some text, some quote characters, and a newline character. When I print the string, the newline character causes an extra blank line to be printed, but when I use repr(), you get the string value in Python syntax form, where the single quotes are part of the syntax, not the value. Note that the newline character also is shown with the \n syntax, exactly the same as when I created the s string in the first place:
>>> s = 'They heard him say ""Hello world!"".\n'
>>> print(s)
They heard him say ""Hello world!"".

>>> print(repr(s))
'They heard him say ""Hello world!"".\n'
>>> s
'They heard him say ""Hello world!"".\n'

And when I echoed the s value at the end, the interactive interpreter also shows me the value using the repr() output.
So in your list, your strings do not have the ' characters as part of the value. They are part of the string syntax. You only need to replace the "" characters, they are part of the value, because they are inside the outermost '...' string literal syntax. You could use str.replace('""', '') to remove them:
[value.replace('""', '') for value in my_list]

or, you could use the str.strip() method to only remove quotes that are at the start or end of the value:
[value.strip('""') for value in my_list]

Both work just fine for your sample list:
>>> my_list = ['""3""', '""45""','""12""','""6""']
>>> [value.replace('""', '') for value in my_list]
['3', '45', '12', '6']
>>> [value.strip('""') for value in my_list]
['3', '45', '12', '6']

Again, the ' characters are not part of the value:
>>> first = my_list[0].strip('""')
>>> first         # echo, uses repr()
'3'
>>> print(first)  # printing, the actual value written out
3
>>> len(first)    # there is just a single character in the string
1

However, I have seen that you are reading your data from a tab-separated file that you hand-parse. You can avoid having to deal with the "" quotes altogether if you instead used the csv.reader() object, configured to handle tabs as the delimiter. That class automatically will handle quoted columns:
import csv

with open(inputfile, 'r', newline='') as datafile:
    reader = csv.reader(datafile, delimiter='\t')
    for row in reader:
        # row is a list with strings, *but no quotes*
        # e.g. ['3', '45', '12', '6']

Demo showing how csv.reader() handles quotes:
>>> import csv
>>> lines = '''\
... ""3""\t""45""\t""12""\t""6""
... ""42""\t""81""\t""99""\t""11""
... '''.splitlines()
>>> reader = csv.reader(lines, delimiter='\t')
>>> for row in reader:
...     print(row)
...
['3', '45', '12', '6']
['42', '81', '99', '11']
",A,1
38439689,2016-07-18 14:46:08.020000+00:00,"Or a combination of filter and exclude.
MetricAssociation.objects.filter(metric=metric).exclude(specifics=None)
",A,0
33274349,2015-10-22 06:04:25.013000+00:00,"There are two steps:

parse the time string into a date/time object e.g., How to parse dates with -0400 timezone string in python? and
Parsing date with timezone from an email?
import time

time_string = ""20151021|133102-0400""
tt = time.strptime(time_string[:15], ""%Y%m%d|%H%M%S"")
hours, minutes = divmod(int(time_string[16:]), 100)
utc_offset = (hours * 60 + minutes) * 60
if time_string[15] == '-':
    utc_offset = -utc_offset

get ""seconds since the Epoch"" that corresponds to the date/time object: Converting datetime.date to UTC timestamp in Python
from calendar import timegm

posix_timestamp = timegm(tt) - utc_offset



I want the code to run regardless of the native timezone of the machine it is running on

time.mktime() expects local time as an input. Don't use it if the input time string does not represent local time. You also shouldn't use it if the time string has the utc offset (mktime() may produce a wrong result for ambiguous times or past/future dates).",A,0
17225333,2013-06-20 23:09:50.257000+00:00,"If you don't need it to be human-readable/editable, the easiest solution is to just use pickle.
To write:
with open(the_filename, 'wb') as f:
    pickle.dump(my_list, f)

To read:
with open(the_filename, 'rb') as f:
    my_list = pickle.load(f)


If you do need them to be human-readable, we need more information.
If my_list is guaranteed to be a list of strings with no embedded newlines, just write them one per line:
with open(the_filename, 'w') as f:
    for s in my_list:
        f.write(s + '\n')

with open(the_filename, 'r') as f:
    my_list = [line.rstrip('\n') for line in f]


If they're Unicode strings rather than byte strings, you'll want to encode them. (Or, worse, if they're byte strings, but not necessarily in the same encoding as your system default.)
If they might have newlines, or non-printable characters, etc., you can use escaping or quoting. Python has a variety of different kinds of escaping built into the stdlib.
Let's use unicode-escape here to solve both of the above problems at once:
with open(the_filename, 'w') as f:
    for s in my_list:
        f.write((s + u'\n').encode('unicode-escape'))

with open(the_filename, 'r') as f:
    my_list = [line.decode('unicode-escape').rstrip(u'\n') for line in f]


You can also use the 3.x-style solution in 2.x, with either the codecs module or the io module:*
import io

with io.open(the_filename, 'w', encoding='unicode-escape') as f:
    f.writelines(line + u'\n' for line in my_list)

with open(the_filename, 'r') as f:
    my_list = [line.rstrip(u'\n') for line in f]

* TOOWTDI, so which is the one obvious way? It depends… For the short version: if you need to work with Python versions before 2.6, use codecs; if not, use io.",A,97
23909478,2014-05-28 10:44:13.340000+00:00,"has_js=1 is just another cookie name-value pair (most likely a Drupal JavaScript probe cookie). 
All cookies your browser sends are concatenated in the one Cookie header, separated by ; semicolons. Cookie parameters are part of the Set-Cookie header to influence what the browser does when storing the values, and never part of the Cookie header sent back to the server.
You can just add it to your session:
session.cookies['has_js'] = '1'
",A,1
12352144,2012-09-10 12:53:23.087000+00:00,"That's dead code; it's a local variable, so it is not ever going to influence anything else.
If it had been a instance variable, then dispatcher.send could have read the value while executing, but it's completely superfluous here.
The asyncore module is relatively ancient, the line has been there since it was first committed to the python codebase in 1999.",A,6
21842524,2014-02-18 01:16:57.070000+00:00,"Your decapitalizeTokens function works on a single character. You're passing it a list of strings. If you want to call it on every character of every string in that list, you need to loop over the list, and then loop over each string, somewhere.
You can do this with explicit loop statements, like this:
words = []
for token in tokens:
    word = ''
    for char in token:
        word += decaptializeTokens(char)
    words += word

… or by using comprehensions:
words = [''.join(decapitalizeTokens(char) for char in token) 
         for token in cleanTokens]


However, I think it would make far more sense to move the loops into the decapitalizeTokens function—both based on its plural name, and on the fact that you have exactly the same loops in the similarly-named depunctuateTokens function. If you build decapitalizeTokens the same way you built depunctuateTokens, then your existing call works fine:
words = decapitalizeTokens(cleanTokens)


As a side note, the built-in lower method on strings already does what you want, so you could replace this whole mess with:
words = [token.lower() for token in cleanTokens]

… which would also fix a nasty bug in your attempt. Consider what, say, decapitalizeTokens would do with a digit or a space.
And, likewise, depunctuateTokens can be similarly replaced by a call to the translate method. For example (slightly different for Python 2.x, but you can read the docs and figure it out):
punctuation=""""""!""#$%&'()*+,-./:;<=>?@[\]^_`{|}~""""""
punctmap = {ord(char): None for char in punctuation}
cleanTokens = [token.translate(punctmap) for token in cleanTokens]
",A,2
4772085,2011-01-23 03:10:14.887000+00:00,"Why do you write ""e-mail""?  Don't the template variables have to be a Python variable names?  ""e_mail""?
http://docs.djangoproject.com/en/dev/topics/templates/#variables
It says  

Variable names consist of any
  combination of alphanumeric characters
  and the underscore (""_"").
",A,2
50296116,2018-05-11 15:59:42.700000+00:00,"Pass in the dictionary using the **kwargs call syntax to unpack your dictionary into separate arguments:
SimpleNamespace(**d)

This applies each key-value pair in d as a separate keyword argument. 
Conversely, the closely releated **kwargs parameter definition in the __init__ method of the class definition shown in the Python documentation captures all keyword arguments passed to the class into a single dictionary again.
Demo:
>>> from types import SimpleNamespace
>>> d = {'a': 1, 'b':2}
>>> sn = SimpleNamespace(**d)
>>> sn
namespace(a=1, b=2)
>>> sn.a
1
",A,8
49200317,2018-03-09 18:50:27.970000+00:00,"When you click on an input object of type image, two form values are set, the button name plus .x for the first, and .y for the other.
However, pressing Enter in a regular text input field will also submit a form, so you don't have to click on a submit button. I'd just leave the value empty altogether.
There is not much flexibility in the way robobrowser handles form submits, to avoid using the submit button you'd have to delete it from the form outright:
del signin.fields['btnSubmit']

before submitting.
If you must submit using the image button, then you'll have to teach Robobrowser how to handle that type; currently it has no handling for these. The following adds that:
from functools import wraps
from robobrowser.forms import form
from robobrowser.forms.fields import Submit, Input

class ImageSubmit(Submit):
    def serialize(self):
        return {self.name + '.x': '0', self.name + '.y': '0'}

def include_image_submit(parse_field):
    @wraps(parse_field)
    def wrapper(tag, tags):
        field = parse_field(tag, tags)
        if type(field) is Input:  # not a subclass, exactly this class
            if field._parsed.get('type') == 'image':
                field = ImageSubmit(field._parsed)
        return field
    return wrapper

form._parse_field = include_image_submit(form._parse_field)

at which point you can use browser.submit_form(signin, signin['btnSubmit']) to submit the form and the correct fields will be included.
I've submitted a pull request to the robobrowser project to add image submit support.",A,1
27642809,2014-12-24 22:48:58.500000+00:00,"To estimate number of lines in a file:
def line_size_hint(filename, learn_size=1<<13):
    with open(filename, 'rb') as file:
        buf = file.read(learn_size)
        return len(buf) // buf.count(b'\n')

number_of_lines_approx = os.path.getsize(filename) // line_size_hint(filename)

To find the exact number of lines, you could use wc-l.py script:
#!/usr/bin/env python
import sys
from functools import partial

print(sum(chunk.count('\n') for chunk in iter(partial(sys.stdin.read, 1 << 15), '')))
",A,3
32075720,2015-08-18 14:48:49.310000+00:00,"It's doing exactly what you're telling it to: outputting the string representation of the entire row. A row is a list, and a list always includes list delimiters.
You don't want that at all. You have one element, which is already a string; you should grab that element and append it, without calling str.
for row in reader:
    your_list.append(row[0])

Also note that your original writing function is very un-Pythonic. You should never iterate through len(range(something)): the very ugliness of that should tell you that it's not the way to do things. Instead, just iterate through the thing.
for item in text:
    spamwriter.writerow([item])

Finally, you might consider that, since you are writing a single element per row and reading it back, CSV may not be the appropriate format. A simple text write and read would be easier.",A,0
14866432,2013-02-14 01:36:58.183000+00:00,"If you just want the tag names, use a values_list query:
tags = UserTag.objects.filter(users=request.user).values_list('name', flat=True)
",A,3
1181966,2009-07-25 12:00:01.033000+00:00,"Models are just classes, and in Python class instances are dynamic. This means you can add any property you like to an instance by simply assigning it.
myinstance = MyModel.objects.get(pk=1)
myinstance.mycomparisonattribute = True

However I don't see why this is any better than using your comparison method. Alternatively, use a custom template tag that can do comparisons - SmileyChris's smart if tag is excellent.",A,1
13644168,2012-11-30 11:16:43.423000+00:00,"The iter() function can be passed a function and a sentinel, use that to read a file in 820 byte chunks:
for chunk in iter(lambda: f.read(820), ''):
    # chunk is now 820 bytes long, until the last chunk which *could* be shorter.

Every iteration, the lambda function will be called, reading 820 bytes, until f.read(820) returns an empty string (signifying EOF).
The chunk is just a string, so you can use slicing to get your filename:
filename = chunk[2:16]

Used together:
with open(""file"", ""rb"") as f:
    for chunk in iter(lambda: f.read(820), ''):
        open(chunk[2:16], 'wb').write(chunk)
",A,4
22478532,2014-03-18 11:50:25.360000+00:00,"If the number of letters is variable, it's easiest to use a regular expression:
import re

characters, numbers = re.search(r'([A-Z]+)(\d+)', inputstring).groups()

This assumes that:

The letters are uppercase ASCII
There is at least 1 character, and 1 digit in each input string.

You can lock the pattern down further by using {3, 4} instead of + to limit repetition to just 3 or 4 instead of at least 1, etc.
Demo:
>>> import re
>>> inputstring = 'FED590498'
>>> characters, numbers = re.search(r'([A-Z]+)(\d+)', inputstring).groups()
>>> characters
'FED'
>>> numbers
'590498'
",A,5
23523791,2014-05-07 16:53:47.543000+00:00,"Your function doesn't have a fh keyword argument. It has a fh_id keyword argument though.
Either fix your function signature (rename fh_id to fh) or your call (use fh_id instead of fh).",A,1
19422858,2013-10-17 09:26:38.783000+00:00,"Your while loop does not do the same thing as your for loop; the for loop starts at 1 and always increments i.
Move the i += 1 before the even test:
i = 0

while(i < 10):
    i += 1
    if i % 2 == 0:   
        continue
    print i

because 0 % 2 == 0 is True, so you always continue, skipping the i += 1 and print i statements.",A,2
20911416,2014-01-03 19:37:30.297000+00:00,"The first obvious problem is this line:
tutor_var.trace('w', lambda name, index, mode: validate_enter_0_2)

You've created a function that takes three variables, and returns the validate_enter_0_2 function as a function object. That doesn't do any good.
You want to create a function that calls the validate_enter_0_2 function. Like this:
tutor_var.trace('w', lambda name, index, mode: validate_enter_0_2())

You have the exact same problem with name_var and will also need to fix it there, of course.

On top of that, you don't actually have a function named validate_enter_0_2 to call, because you defined it as validate_enter_2. This means your validation function just raises a NameError instead of doing useful. Or, if you have a validate_enter_2 function defined somewhere else in your code, it calls the wrong function. (This is one reason that cryptic names like enter_0_2 and enter_2 are not a good thing.)

There's at least one other problem with your code: You're repeatedly trying to use name_var, which is a StringVar object, as if it were a string. You can't do that. And if you actually look at the console output, Tkinter will tell you this, with tracebacks like this:
Exception in Tkinter callback
Traceback (most recent call last):
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk/Tkinter.py"", line 1410, in __call__
    return self.func(*args)
  File ""tkval2.py"", line 25, in callback
    if len(name_var) > 10 or any(l not in string.ascii_letters for l in name_var):
AttributeError: StringVar instance has no attribute '__len__'

And that exception is happening before you get a chance to create the new Entry.
To fix that, you need to call get on the StringVar whenever you want to get its value, like this:
if len(name_var.get()) > 10 or any(l not in string.ascii_letters for l in name_var.get())


Finally, as I explained in the answer to your other question, your trace validator isn't going to get called until something changes. This means you will either need to call it explicitly, or explicitly name_var.set(''), or just start the button off disabled. As written, it will start off enabled, and only disable if you type something and then erase it.

I'm not sure whether those are the only problems with your code, but all of them will certainly prevent your validation from working as expected. So, you need to fix all of them, and any other errors in your code, before it will work.

From your comments:

I am however wondering how to create a pop up message displaying an error…

When do you want to do that? What condition do you want to check, and when do you want to check it?
At any rate, as in most GUIs, the way to ""pop up"" anything like this is a dialog. Dialog Windows in the Tkinter book explains everything you need to know. But you don't need to copy-paste all that code, or write it from scratch; the stdlib comes with Tkinter helper modules that do most of the work for you. In your case, you probably just want tkMessageBox.

… and something that forces the user to re enter their name

Force them how? Just erasing the existing Entry contents would leave them with an empty box to fill in, which would also disable the button. Is that what you want?
Anyway, guessing at what you want, it could look something like this:
def callback():
    if len(name_var.get()) > 10:
        tkMessageBox.showerror(""Bad name"", ""Your name is too long. What's wrong with good American names like Joe?"")
        name_var.set('')
        return
    # the rest of your code here

In the callback function (called when they click the button after typing their name), instead of just checking some condition and printing something out, I check a condition and pop up an error dialog, clear out the existing name, and return early instead of creating the second half of the form. I didn't handle your other condition (any non-ASCII letters), but it should be obvious how to add that.
However, validation like this might be better done through actual validation—instead of making them wait until they click the button, catch it as soon as they try to type the 11th character, or a space or accented character, or whatever else you don't like. Then you can pop up a message box, and either disable the button until they fix it, reject/undo the change (which is easier with a validatecommand function than with a trace function, as shown in my answer to your previous question).
One last thing: Instead of a message box, it may be better to just embed the error as, say, a Label that appears in the form itself (maybe with the error description in red, with a big flag icon). This is common in web apps and in more modern GUIs because it provides more immediate feedback, and is less obtrusive to the user flow.",A,1
26720798,2014-11-03 18:39:47.623000+00:00,"You can't do that; objects don't track what names reference them.
Any object can have between 1 and unlimited names or other objects referencing them, as well, so what name would you pick if there are more than one or the object is referenced by a list or a dictionary? Or what if you passed those objects directly into your method with lens1.propagate_rays(Ray(args), Ray(different_args))?
The best you can do is give your objects a name attribute, then reference that in the __repr__ of the object:
class Ray:
    def __init__(name, *other_args):
        self.name = name

    def __repr__(self):
        return '<Ray({!r}, ....>'.format(self.name)
",A,1
48769020,2018-02-13 14:25:03.323000+00:00,"The mocks defined in decorators are passed to the decorated function. I don't know what response is supposed to be but you need to accept an argument for the get and post objects.
@mock.patch('requests.get', side_effect=mocked_requests_get)
@mock.patch('requests.post', side_effect=mocked_requests_post)
def test_zeros(self, post_mock, get_mock):
    self.assertEqual(0, 0) 
",A,1
23413867,2014-05-01 18:16:43.277000+00:00,"First of all, you marked the post() function with a decorator, telling Flask what URLs to route to that function:
@app.route('/blog/<path:path>')
def post(path):

See the <path:path> element in there? Now anything past the /blog/ part of the URL is taken and passed to post() as the path argument, so if you accessed http://localhost:8000/blog/foo/bar/baz, then Flask would take the path past /blog/ and call post('foo/bar/baz'). That's where path comes from.
It is the responsibility of the template to produce the HTML:
return render_template('post.html',post = post)

This looks up the file post.html in your templates directory, and executes it as a Jinja2 template page. It can access the post variable because you passed it in. It is that template that does all the HTML work; the template contains a line {{ post.html|safe }} to include the result from the Post.html() method.
Your Post class uses a cached property for the .html() method. This means two things:

it is a property, which means you can indirectly invoke the method by treating it as an attribute. post.html will look up the attribute on the post object, Python will notice it is a descriptor object and call the special __get__ method on it, which in turn calls the Post.html method.
The Werkzeug @cached_property decorator caches the result on the instance. It makes use of the difference between class attributes and instance attributes.
When first looked up, post.html is not found on the instance, so Python then turns to the Post class. It finds the html descriptor there, invokes it, and the cached_property descriptor calls the actual Post.html method. The result of that call is then stored as an instance attribute. After this, every time post.html is looked up, there is an attribute on the instance itself that can be used.
",A,2
32549647,2015-09-13 12:26:42.957000+00:00,"You're iterating through the results value of the JSON, but as you can see from your sample output, there is no such value. You need to iterate through crs.features.
$.each (json.crs.features, function (i, val) {
    name.push(val.properties.name);
    zip.push(val.properties.zip);
}
",A,1
4221171,2010-11-19 00:46:35.583000+00:00,"It depends on your PYTHONPATH setting and the structure of your projects and apps.
We have many, many projects.  Each with several apps.  All are on our PYTHONPATH, so the project name is essential.
If you have only one project, and the top-level project directory is on your PYTHONPATH, then each app can be resolved separately and you can't use the project name.",A,2
46620393,2017-10-07 12:58:12.110000+00:00,"There are quite a few things here that don't make any sense.
Firstly, you can't tell your payment provider to make a callback to your site with a 127.0.0.1 address. That's just your localhost; but obviously the gateway is elsewhere on the Internet. It needs to have an actual address for your site which it can call.
Secondly, unrelated to your question but still a very serious problem, you must absolutely not use global variables like this. Those will be shared by all users of your site, so the amounts will get all mixed up. I don't know anything about this payment provider, but I'm absolutely certain that it will provide the amount in the parameters of the callback.",A,0
307637,2008-11-21 02:26:01.647000+00:00,"Look at pyXSD for Python tools that are similar to JAXB.
XSD's are used to create Python classes.  Python objects are used to emit XML. ",A,8
27202952,2014-11-29 13:43:10.647000+00:00,"Just use the key argument to the sorted() function or list.sort() method:
sorted_list = sorted(list_of_objects, key=function_that_calculates)

The function_that_calculates is called for each entry in list_of_objects and its result informs the sort.
If you meant that each object has a method, you can use a lambda or the operator.methodcaller() object to call the method on each element:
sorted_list = sorted(list_of_objects, key=lambda obj: obj.method_name())

or
from operator import methodcaller

sorted_list = sorted(list_of_objects, key=methodcaller('method_name'))

Note that in Python, there is no such thing as a private attribute; your sorting function can still just access it. The leading underscore is just a convention. As such, sorting by a specific attribute can be done with either a lambda again, or using the operator.attrgetter() object:
sorted_list = sorted(list_of_objects, key=lambda obj: obj._variable_name)

or
from operator import attrgetter

sorted_list = sorted(list_of_objects, key=attrgetter('_variable_name'))
",A,5
16321556,2013-05-01 15:51:12.630000+00:00,"Instead of using fileinput, open the file directly yourself:
import sys
try:
    fileobj = open(sys.argv[1], 'r')
except IndexError:
    fileobj = sys.stdin

with fileobj:
    data = fileobj.read()
",A,5
27867015,2015-01-09 18:35:14.967000+00:00,"
Why Python cuts '^' character and how to avoid it?

Python does not cut ^ character. Popen() passes the string (resize_command) to CreateProcess() Windows API call as is.
It is easy to test:
#!/usr/bin/env python
import sys
import subprocess

subprocess.check_call([sys.executable, '-c', 'import sys; print(sys.argv)'] +
                      ['^', '<-- see, it is still here'])

The latter command uses subprocess.list2cmdline() that follows Parsing C Command-Line Arguments rules to convert the list into the command string -- it has not effect on ^. 
^ is not special for CreateProcess(). ^ is special if you use shell=True (when cmd.exe is run).
if and only if the command line produced will be interpreted by cmd, prefix each shell metacharacter (or each character) with a ^ character. It includes ^ itself.",A,3
38672715,2016-07-30 10:10:00.123000+00:00,"Python does not have a built-in name nan, nor is there a keyword.
It looks as if you forgot to import it; numpy defines such a name:
from numpy import nan

From the local name df I infer you are probably using pandas; pandas' documentation usually uses np.nan, where np is the numpy module imported with import numpy as np. See their 10 Minute to pandas intro for example.",A,9
7672997,2011-10-06 10:22:17.350000+00:00,"
Should it be a separate module (by what name)? 

demo/some_useful_name.py
A demo directory contains demo scripts.  Similarly, a test directory contains all your unit tests.

Should it be located in the package's root directory? 

No.  It's not part of the package.  It's a demo.

Out of the package?

Yes.

In init.py?

Never.

A package has two lives.  (1) as uninstalled source, (2) in the lib/site-packages as installed code.
The ""source"" should include README, setup.py, demo directory, test directory, and the package itself.
The top-level ""source"" setup.py should install just the package.  The demo and test don't get installed.  They get left behind as part of the download.",A,3
2566527,2010-04-02 12:05:11.643000+00:00,You don't need to write your own tag. The built-in {% with %} tag does this.,A,4
30013539,2015-05-03 11:58:13.037000+00:00,"str.replace() returns a new string object with the replacements made; the words values are not affected.
I'd use str.strip() to remove the p and parentheses from the line:
for line in file:
    if not line.startswith('p'):
        continue  # skip these lines
    words = line.strip('\np()').split(',')
    x, y, z = map(float, words)
    mesh.verts.append(Blender.NMesh.Vert(x, y, z))

Here we skip any lines without a p at the start, and str.strip() removes any of the characters found in the argument from the start and end of a string; so any p, ( and ) characters from both the start and end are removed (as well as newlines, which are always present when reading lines from a file):
>>> 'p(-7,-24,63)\n'.strip('\np()')
'-7,-24,63'
>>> 'p(-7,-24,63)\n'.strip('\np()').split(',')
['-7', '-24', '63']
>>> x, y, z = map(float, 'p(-7,-24,63)\n'.strip('\np()').split(','))
>>> x, y, z
(-7.0, -24.0, 63.0)
",A,2
275587,2008-11-09 04:06:02.223000+00:00,"I'm not sure I understand the question.
The SQLObject documentation lists six kinds of connections available.  Further, the database connection (or scheme) is specified in a connection string.  Changing database connections from SQLite to MySQL is trivial.  Just change the connection string.
The documentation lists the different kinds of schemes that are supported.",A,0
48793410,2018-02-14 17:53:51.167000+00:00,"It is certainly possible, but your code has inter-related dependencies. Setting the url property looks up self.subdomain, which calls the subdomain property which in turn looks up the value of self.__subdomain - which doesn't exist yet.
You should at least set self.subdomain first. However, you should reconsider having getters and setters for subdomain at all; just set the an attribute directly. Either way, you should avoid the double-underscore prefix, since this does name mangling and leads to behaviour you may not expect; if you must set a hidden attribute, use a single underscore prefix.",A,2
2591717,2010-04-07 10:46:48.820000+00:00,"Almost all of your list is ""Framework"".
Almost none of that is functionality a user actually sees.
My to do list never looks like that.  Never.
Consequently, I have this advice.
STOP.
Do this instead.

Find a framework that does this.  
Focus on the the actual users and their actual use cases. 

Now your todo list will have actual use cases that an actual user will actually care about.  That's what should be on your todo list.  Not framework technology.  Use cases.",A,3
34634401,2016-01-06 13:31:46.330000+00:00,"You've accepted a name parameter in your class, which the unit test is not expecting or passing. Remove that.",A,3
25568835,2014-08-29 13:00:51.527000+00:00,"os.chmod() takes a filename, but you are trying to pass in the file object instead.
You could use output.name here:
os.chmod(output.name, 0o777)
os.system(""screen -m -d bash -c ./"" + output.name)    

or you could first store the filename you generate in a variable:
filename = '{0}{1}.sh'.format(outputBase, at)
with open(filename, 'w') as output:
    output.write('#!/bin/sh\n')
    output.write('\n'.join(outputData))
    output.write('\n')
os.chmod(filename, 0o777)
os.system(""screen -m -d bash -c ./{0}"".format(filename))    

I also used the file object as a context manager here; the with statement closes the file for you when the block exits.",A,1
26182628,2014-10-03 16:04:55.417000+00:00,"For a search of a literal value inside a repeated property, you just use the normal =:
MyModel.gql('WHERE barfoo = :1', foobar)

And for the ""is any of this list in the listproperty"", you can use exactly the same IN as your original query. Do note though that IN is generally very inefficient in the GAE datastore.
(Note, there is never any benefit in using GQL over the db/ndb APIs. It can't do anything that those APIs can't do, and in fact the statements are converted to exactly the same underlying remote procedure calls.)",A,1
49852541,2018-04-16 08:15:29.587000+00:00,"Your form does not post to the view that saves the data; it posts directly to the view_content view, which ignores the data completely.
You can set the form action to ""."" to get it to post back to the same view that rendered it.
(As an additional point, you should avoid hard-coding URLs. Use the {% url %} tag in templates, and the reverse() function in views, to output URLs based on view names.)",A,0
23803991,2014-05-22 10:17:34.813000+00:00,"You are looking at a subclass of unicode most likely. I think you have an instance of suds.sax.text.Text() here. Since this is a subclass, isinstance(obj, unicode) is True.
It's value is already a unicode string, so decoding will trigger an implicit encode first with ASCII, which will indeed fail.
You can convert the type to unicode by simply using:
unicode(obj[cur_key]['value'])

but take into account the value can still be XML escaped; the .unescape() method returns an unescaped version (it'll return self if the value wasn't escaped to begin with).
When comparing with another string, make sure they are exactly equal. Unicode values can contain many 'hidden' values, like zero-width characters or combining characters that can be represented in combined form as well. Use repr(value) to get an escaped representation that makes such codepoints more obvious.
The suds.sax.text.Text() class doesn't special-case equality testing, so your != test would work just the same as if the value was a unicode string instead of the subclass.",A,9
22710420,2014-03-28 10:37:23.560000+00:00,"You need to write line separators after your headers:
import sys
sys.stdout.write(""Content-type: text/html\n\n"")
response = ""dadada""
sys.stdout.write(response)
sys.stdout.write(""</br>"")

You can use print; it'll add line separators for you:
print ""Content-type: text/html""
print
response = ""dadada""
print response + '</br>'

If you are not married to CGI scripts, give micro-web frameworks like Flask, Bottle or Pyramid  a try instead; these take care of most of the HTTP details for you.",A,1
14245367,2013-01-09 19:56:13.583000+00:00,"There probably is a better answer… But when I've been faced with this problem, I had a file that I already wanted to access different parts of separately, which gave me an easy solution to this problem.
For example, say chunkyfoo.bin is a file consisting of a 6-byte header, a 1024-byte numpy array, and another 1024-byte numpy array. You can't just open the file and seek 6 bytes (because the first thing numpy.fromfile does is lseek back to 0). But you can just mmap the file and use fromstring instead:
with open('chunkyfoo.bin', 'rb') as f:
    with closing(mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ)) as m:
        a1 = np.fromstring(m[6:1030])
        a2 = np.fromstring(m[1030:])

This sounds like exactly what you want to do. Except, of course, that in real life the offset and length to a1 and a2 probably depend on the header, rather than being fixed comments.
The header is just m[:6], and you can parse that by explicitly pulling it apart, using the struct module, or whatever else you'd do once you read the data. But, if you'd prefer, you can explicitly seek and read from f before constructing m, or after, or even make the same calls on m, and it will work, without affecting a1 and a2.
An alternative, which I've done for a different non-numpy-related project, is to create a wrapper file object, like this:
class SeekedFileWrapper(object):
    def __init__(self, fileobj):
        self.fileobj = fileobj
        self.offset = fileobj.tell()
    def seek(self, offset, whence=0):
        if whence == 0:
            offset += self.offset
        return self.fileobj.seek(offset, whence)
    # ... delegate everything else unchanged

I did the ""delegate everything else unchanged"" by generating a list of attributes at construction time and using that in __getattr__, but you probably want something less hacky. numpy only relies on a handful of methods of the file-like object, and I think they're properly documented, so just explicitly delegate those. But I think the mmap solution makes more sense here, unless you're trying to mechanically port over a bunch of explicit seek-based code. (You'd think mmap would also give you the option of leaving it as a numpy.memmap instead of a numpy.array, which lets numpy have more control over/feedback from the paging, etc. But it's actually pretty tricky to get a numpy.memmap and an mmap to work together.)",A,3
16863908,2013-05-31 18:49:31+00:00,"Still convert them to floats, but without the % sign:
float(value.strip(' \t\n\r%'))

The .strip() removes any extra whitespace, as well as the % percent sign, you don't need that to be able to compare two values:
>>> float('5.265%  '.strip(' \t\n\r%'))
5.265
>>> float('2.1545%'.strip(' \t\n\r%'))
2.1545

float() itself will normally strip away whitespace for you but by stripping it yourself you make sure that the % sign is also properly removed, making this a little more robust when handling data from files.",A,5
20692072,2013-12-19 21:30:47.850000+00:00,"You actually have two problems here.

The big one is that print sorted(file_handle) reads and sorts the whole rest of the file and prints that out. You're doing that once per line. So, what happens is that you read the first line, split it, ignore the result, sort and print all the lines after the first, and then you're done.
What you want to do is accumulate all the words as you go along, then sort and print that. Like this:
def sorting(self):
    filename = (""food.txt"")
    file_handle = open(filename, ""r"")
    words = []
    for line in file_handle:
        words += line.split()
    file_handle.close()
    print sorted(words)

Or, if you want to print the sorted list one line at a time, instead of as a giant list, change the last line to:
print '\n'.sorted(words)


For the second, more minor problem, the one you asked about, you just need to strip off the newlines. So, change the words += line to this:
words += line.strip().split()

However, if you had solved the first problem, you wouldn't even have noticed this one. If you have a line like ""one two three\n"", and you call split() on it, you will get back [""one"", ""two"", ""three""], with no \n to worry about. So, you don't actually even need to solve this one.

While we're at it, there are a few other improvements you could make here:

Use a with statement to close the file instead of doing it manually.
Make this function return the list of words (so you can do various different things with it, instead of just printing it and returning nothing).
Take the filename as a parameter instead of hardcoding it (for similar flexibility).
Maybe turn the loop into a comprehension—but that would require an extra ""flattening"" step, so I'm not sure it's worth it.
If you don't want duplicate words, use a set rather than a list.
Depending on the use case, you often want to use rstrip() or rstrip('\n') to remove just the trailing newline, while leaving, say, paragraph indentation tabs or spaces. If you're looking for individual words, however, you probably don't want that.
You might want to filter out and/or split on non-alphabetical characters, so you don't get ""that."" as a word. Doing even this basic kind of natural-language processing is non-trivial, so I won't show an example here. (For example, you probably want ""John's"" to be a word, you may or may not want ""jack-o-lantern"" to be one word instead of three; you almost certainly don't want ""two-three"" to be one word…)
The self parameter is only needed in methods of classes. This doesn't appear to be in any class. (If it is, it's not doing anything with self, so there's no visible reason for it to be in a class. You might have some reason which would be visible in your larger program, of course.)

So, anyway:
def sorting(filename):
    words = []
    with open(filename) as file_handle:
        for line in file_handle:
            words += line.split()
    return sorted(words)

print '\n'.join(sorting('food.txt'))
",A,2
9041375,2012-01-27 23:20:25.607000+00:00,"json strings always use "", not ' so '\u05d9\u05d7\u05e4\u05d9\u05dd' is not a json string.
If you load a valid json text then all Python strings in it are Unicode so you don't need to decode anything. To display them you might need to encode them using a character encoding suitable for your terminal.
Example
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import json

d = json.loads(u'''{""title"": ""\u05d9\u05d7\u05e4\u05d9\u05dd""}''')
print d['title'].encode('utf-8') # -> יחפים

Note: it is a coincidence that the source encoding (specified in the first line) is equal to the output encoding (the last line) they are unrelated and can be different.
If you'd like to see less \uxxxx sequences in a json text then you could use ensure_ascii=False:
Example
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import json

L = ['יחפים']
json_text = json.dumps(L) # default encoding for input bytes is utf-8
print json_text # all non-ASCII characters are escaped
json_text = json.dumps(L, ensure_ascii=False)
print json_text # output as-is

Output
[""\u05d9\u05d7\u05e4\u05d9\u05dd""]
[""יחפים""]
",A,1
14107095,2013-01-01 00:45:40.867000+00:00,"Your refresh_type value is not update, it's 'update'. The rest of your variables suffer from the same ailment, they have quotes as part of the string value.
Use print ""refresh_type is:"", repr(refresh_type) as a diagnostic aid in this.
You could use:
if refresh_type == ""'update'"":

but you have a more fundamental problem, one where you end up with extra quotes in your string values.
To illustrate, a short Python interpreter session:
>>> print 'update'
update
>>> print ""'update'""
'update'
>>> ""'update'"" == 'update'
False
>>> foo = ""'update'""
>>> print repr(foo)
""'update'""
",A,4
9063728,2012-01-30 12:14:47.040000+00:00,"The question doesn't make sense.
The user can only see one response at a time. If the response to your form is the PDF, that's what they'll see in their browser.",A,1
29968632,2015-04-30 13:08:31.003000+00:00,"You haven't quite understood how classes and instances work.
Calling the class is what you do when you need a new character. Every time you call Main_Character(), you get a whole new instance - with the default values as set in __init__. If you had characters for each of your friends, you would call it one time for each one. You then would need to keep each of those instances in a variable, so you can reference them again each time.
So, for instance:
my_character = Main_Character()
unfiltered_name=input(""Please enter the name of your character:"")
my_character.Char_Name(unfiltered_name)
print(""Welcome,"", my_character.character_name,""!"", ""Here are your current stats!"")
print(my_character)
",A,2
21146487,2014-01-15 19:30:07.827000+00:00,"The problem is that you don't have a child named groupId, you have a child named {http://maven.apache.org/POM/4.0.0}groupId, because etree doesn't ignore XML namespaces, it uses ""universal names"". See Working with Namespaces and Qualified Names in the effbot docs.",A,4
40957895,2016-12-04 11:00:20.073000+00:00,"You are making several mistakes. You didn't read the documentation close enough; the cmp function option is gone in Python 3. You also implemented your cmp function incorrectly, and last but not least, you don't need to use a cmp function at all, you can use a key function to extract the minimum of the two exact values in the lists you are sorting.
You need to pay close attention to the section you appear to be reading, The Old Way Using the cmp Parameter:

In Py3.0, the cmp parameter was removed entirely (as part of a larger effort to simplify and unify the language, eliminating the conflict between rich comparisons and the __cmp__ methods). 

The error message is a little confusing here, but list.sort() takes no positional arguments. From the list.sort() documentation:

sort(*, key=None, reverse=None)

The * indicates no positional arguments are accepted, and there is no cmp option.
Going back to the Sorting Howto:

When porting code from Python 2.x to 3.x, the situation can arise when you have the user supplying a comparison function and you need to convert that to a key function. The following wrapper makes that easy to do: 
def cmp_to_key(mycmp):

[...]
In Python 2.7, the cmp_to_key() tool was added to the functools module. 

and again from the list.sort() documentation:

The functools.cmp_to_key() utility is available to convert a 2.x style cmp function to a key function.

You also incorrectly implemented your cmp function; there is no need to iterate, and you have to return -1, 0 or 1 to indicate relative order (so you would have to use cmp(min(list1), min(list2)) if there still was a built-in cmp() function).
However, you can sort your data without a (slow) cmp function. All you need to do is to extract the min() of the two values of each list, in a sort key:
lines.sort(key=lambda l: min(l[0], l[2]))
",A,2
2277710,2010-02-17 01:23:35.120000+00:00,"Modified version of @swanson's answer (not tested):
from collections import defaultdict
from itertools   import chain

# generate set of all possible words
lines = open('data.txt').readlines()
words = set(chain.from_iterable(line.split() for line in lines))

# parse input into groups
groups = defaultdict(list)
for line in lines:    
    for word in words:
        if word in line:
           groups[word].append(line)
",A,0
14182276,2013-01-06 12:48:33.423000+00:00,"You could use a mapping to turn letters to numbers. Such a mapping is easy to generate from the string.ascii_uppercase string:
import string
letter_to_index = {l: i for i, l in enumerate(string.ascii_uppercase)}

Using letter_to_index it is then trivial to turn a letter (e.g. E), into a number (4):
>>> letter_to_index['E']
4

Using a space-separated input, this becomes:
letters = []
for coordinate in code.split():
    row, column = letter_to_index[coordinate[0]], letter_to_index[coordinate[1]]
    letters.append(grid[row][column])

print ''.join(letters)

Other techniques use ord() to get the ASCII ordinal for a letter (subtract ord('A') to make it 0-based) or using .index() on string.ascii_uppercase (which would be slower).
Comparing their relative speed with the timeit module is the best way to figure out which one is the fastest technique (if you so care):
>>> import timeit
>>> def lookup_map(coordinate):
...     return letter_to_index[coordinate[0]], letter_to_index[coordinate[1]]
... 
>>> def lookup_ord(coordinate, orda=ord('A')):
...     return ord(coordinate[0]) - orda, ord(coordinate[1]) - orda
... 
>>> def lookup_index(coordinate, upper=string.ascii_uppercase):
...     return upper.index(coordinate[0]), upper.index(coordinate[1])
... 
>>> coords = [random.choice(string.ascii_uppercase[:6])+random.choice(string.ascii_uppercase[:6]) for _ in range(1000)]
>>> timeit.timeit('[lookup(c) for c in coords]', 'from __main__ import coords, lookup_map as lookup', number=10000)
3.883746862411499
>>> timeit.timeit('[lookup(c) for c in coords]', 'from __main__ import coords, lookup_ord as lookup', number=10000)
4.582481861114502
>>> timeit.timeit('[lookup(c) for c in coords]', 'from __main__ import coords, lookup_index as lookup', number=10000)
9.782031059265137

The dictionary (mapping) approach is fastest; using ord() is close behind (if you compute ord('A') only once), and using .index() takes almost three times as long.",A,1
48320760,2018-01-18 11:55:01.203000+00:00,"You should not be using string interpolation to write SQL queries. Quite apart from the problem you're having, it leaves you open to SQL injection. Use parameters instead:
sql = 'INSERT OR IGNORE INTO ' + self.__article_url_table_name + ' (url) values (?)'
c.execute(sql, (href,))
",A,5
8913766,2012-01-18 16:33:07.373000+00:00,"After three years of Django development, I've learned the following.
The ORM is the access layer.  Nothing more is needed.  
50% of the business logic goes in the model.  Some of this is repeated or amplified in the Forms.
20% of the business logic goes in Forms.  All data validation, for example, is in the forms.  In some cases, the forms will narrow a general domain (allowed in the model) to some subset that's specific to the problem, the business or the industry.
20% of the business logic winds up in other modules in the application.  These modules are above the models and forms, but below the view functions, RESTful web services and command-line apps.
10% of the business logic winds up in command-line apps using the management command interface.  This is file loads, extracts, and random bulk changes.
It's very important that view functions and RESTful web services do approximately nothing.  They use models, forms, and other modules as much as possible.  The view functions and RESTful web services are limited to dealing with the vagaries of HTTP and the various data formats (JSON, HTML, XML, YAML, whatever.)
Trying to invent Yet Another Access Layer is a zero-value exercise.",A,25
47496728,2017-11-26 13:36:44.153000+00:00,"You don't have a local variable called id in your get method; Python is assuming you're referring to the global built-in id() function, hence the error.
According to your URLs, you are capturing a keyword argument called pk. So you should get that from the kwargs dict:
Project.objects.get(id=kwargs['pk'])...
",A,2
12079491,2012-08-22 18:47:00.007000+00:00,"The process is called URL encoding:
>>> urllib.quote('https://dl.dropbox.com/u/94943007/file.kml', '')
'https%3A%2F%2Fdl.dropbox.com%2Fu%2F94943007%2Ffile.kml'
",A,2
18351889,2013-08-21 07:58:21.623000+00:00,"You can't really do that in the query. But there doesn't seem to be a need to: it's a simple addition, it can be done in Python.",A,0
897218,2009-05-22 10:20:42.670000+00:00,"""[I] would have to parse the data myself""
Why?   CGI has a parser you can call explicitly.
Read the uploaded stream and save it in a local disk file.  
For blazing speed, use a StringIO in-memory file.  Just be aware of the amount of memory the upload will take.
Use cgi.parse(mylocalfile).",A,2
20145927,2013-11-22 13:19:26.820000+00:00,"Add another level, with a tuple (just the comma):
(k, v), = d.items()

or with a list:
[(k, v)] = d.items()

or pick out the first element:
k, v = d.items()[0]

The first two have the added advantage that they throw an exception if your dictionary has more than one key, and both work on Python 3 while the latter would have to be spelled as k, v = next(iter(d.items())) to work.
Demo:
>>> d = {'foo': 'bar'}
>>> (k, v), = d.items()
>>> k, v
('foo', 'bar')
>>> [(k, v)] = d.items()
>>> k, v
('foo', 'bar')
>>> k, v = d.items()[0]
>>> k, v
('foo', 'bar')
>>> k, v = next(iter(d.items()))  # Python 2 & 3 compatible
>>> k, v
('foo', 'bar')
",A,58
15462805,2013-03-17 15:54:17.917000+00:00,"The core reason is float -> str conversion that is not shown in your question (timestamp to a string). Example:
from datetime import datetime
from itertools import count


def main():
    for i in count():
        timestamp = (datetime.utcnow() - datetime(1970, 1, 1)).total_seconds()
        s_timestamp = str(timestamp)
        dt_from_str = datetime.utcfromtimestamp(float(s_timestamp))
        now = datetime.utcnow()
        if now < dt_from_str:
            dt = datetime.utcfromtimestamp(timestamp)
            for o in [now, dt_from_str, dt, timestamp, s_timestamp, i]:
                print(repr(o))
            break

main()

Output:
datetime.datetime(2013, 3, 17, 15, 52, 38, 355006)
datetime.datetime(2013, 3, 17, 15, 52, 38, 360000)
datetime.datetime(2013, 3, 17, 15, 52, 38, 355003)
1363535558.355003
'1363535558.36'
688

Notice that dt (datetime(2013, 3, 17, 15, 52, 38, 355003)) is less than now (datetime(2013, 3, 17, 15, 52, 38, 355006)) as it should and the rounding occurs in str(timestamp) expression ('1363535558.36').",A,0
27656372,2014-12-26 11:16:36.480000+00:00,"You have to wrap it in quotes.
var description = ""{{object.description|clean_text }}""

However there is already a built-in filter which does exactly what you want, escapejs:
var description = ""{{object.description|escapejs }}""
",A,2
52952409,2018-10-23 15:10:30.670000+00:00,"The context manager helps close the object.
You don't have to create the object used as a context manager, at the same time you use with to enter the context, though. The open() and gzip.open() calls return a new object that happens to be a context manager, and you can create them before you enter the context:
if fname.endswith('.gz'):
    f = gzip.open(fname,'rt')
else:
    f = open(fname, 'r')

with f:
    # do something

In both cases, the object returns self on entering the context, so there is no need to use as f here.
Also, functions are first-class citizens, so you can also use a variable to store the function and then call that in the with statement to create the context manager and file object:
if fname.endswith('.gz'):
    opener = gzip.open
else:
    opener = open

with opener(fname, 'rt') as f:  # yes, both open and gzip.open support mode='rt'
    # do something

This doesn't really buy you anything over the other method here, but you could use a dictionary to map extensions to callables if you so desire.
The bottom line is that with calls context-manager hook methods, nothing less, nothing more. The expression after with is supposed to supply such a manager, but creating that object is not subject to the context management protocol.",A,9
5351075,2011-03-18 11:08:39.233000+00:00,"The IEEE754 is pretty clear on the precision of doubles:
http://en.wikipedia.org/wiki/IEEE_754-2008
You have 52 bits of precision plus an additional assumed bit.
You have exponents from -1022 to 1023, about 11 bits, including a sign.
The 64th bit is the overall sign for the number.
We'll ignore subnormalized numbers.  
You're asking about exponents between -1022 and 0.  This means you have about 10 of the available 11 bits of exponent available to you.
You have 52+1 bits of mantissa available.
This is about 62 bits of usable precision to represent 2**62 distinct values from 
",A,2
53407282,2018-11-21 07:39:48.827000+00:00,"You are passing in a dictionary, but the networkx.pagerank() function doesn't take a dictionary. From the documentation:

G (graph) – A NetworkX graph. Undirected graphs will be converted to a directed graph with two directed edges for each undirected edge.

You can use networkx.Graph() to convert your dictionary:
G = nx.Graph(matrix)
pr = nx.pagerank(G, alpha=0.85)

Demo:
>>> import networkx as nx
>>> matrix = {'1': {'2': 23, '4': 56}, '4': {'2': 22, '7': 5}}
>>> G = nx.Graph(matrix)
>>> nx.pagerank(G, alpha=0.85)
{'1': 0.2459279727012903, '4': 0.36673529905297914, '2': 0.2459279727012903, '7': 0.14140875554444032}
",A,1
16482079,2013-05-10 12:17:53.460000+00:00,"You'd need to generate the sequence of halved numbers:
def halved(n):
    while n:
        yield n
        n >>= 1

Then use turn that into a list:
list(halved(32))

or just directly sum it:
sum(halved(32))

You'd have to use math.log() to turn that into a range()-suitable value:
import math
sum(n >> i for i in range(int(math.log(n, 2)) + 1))
",A,4
7422164,2011-09-14 19:50:48.473000+00:00,">>> hex(ord('A'))
'0x41'

Is that the kind of thing you want?
Maybe something like this?
for o in range(128):
    print chr(o), hex(o)

Or maybe 
import string
for c in string.printable:
    print c, hex(ord(c))
",A,1
41824860,2017-01-24 09:46:01.543000+00:00,"For a series of properties that follow a specific pattern, I'd use a __getattr__ hook:
def __getattr__(self, name):
    # translate a\d names into _a\d
    if name.startswith('a') and name[1:].isdigit():
        return getattr(self, '_' + name)

This presumes that it isn't easier to just set those attributes directly (so adding a0 instead of _a0), e.g. when you also have a corresponding __setattr__ that handles gating or transformations on setting those same attributes.",A,0
49351977,2018-03-18 19:25:18.357000+00:00,"First, PyRun_String (or anything that can simulate eval or exec) can of course do it, but that seems like cheating; you're not building a generator function in C, you're building one in Python and then calling that in C.
Anyway, the reason you can't figure out how to build a generator function with the C API is that there's no C API to do it. Or, rather, there is a C API to build generators out of CPython frame objects running Python generator code objects (and to build generator functions out of generator code objects, but that part you can even do from Python; it's just the types.FunctionType constructor), but that won't do you any good. (Unless you just want to write C code that builds Python bytecode for a generator, which would be cheating just as much as PyRun_String, and more work.)
So, if you want to build a generator function in C, you have to do it manually. It is clearly possible to do this, as proved by the fact that Cython can do it (up to some limit--e.g., inspect.isgeneratorfunction and inspect.isgenerator will return False on gen_func and gen_func()). But it's not easy, and I'm not sure what it gets you.
The core problem is that CPython implements generators by freezing CPython frames and passing them around (hence the API). C code doesn't use CPython frames, it uses the C stack. (Even if you used setjmp/longjmp and explicit stack copying to build C coroutines, you'd be fighting with the way CPython itself uses the C stack.)
So, the only viable option I can think of it to build an iterator class (which this answer shows how to do) and then implement the rest of the generator protocol on top of that. It'll be basically the same as implementing the generator protocol in Python, but storing your state on your PyObject struct instead of in your object dict, just like translating any other class to C.
If you want to see what Cython does, it's essentially that, although you have to wade through a lot of boilerplate to see it. Create a file genpyx.pyx:
def gen_func():
    yield None

Then cythonize genpyx.pyx, and look at the created genpyx.c file. (Look for __pyx_gb_6genpyx_2generator, and most of the other stuff right near it.) Despite Cython having a mechanism to partially fake up frames so it can do tracebacks through Cython code with Python on either end, it's still storing all the state explicitly in a struct it passes around through the functions, just as you'd have to. Cython does support two of the quasi-documented generator attributes gi_running and gi_yieldfrom, which is a nice idea, but it can't fake gi_frame and gi_code (any more than extension functions try to fake __code__), and it doesn't fake being an instance of types.GeneratorType (which you could sort of do, but it would be as dangerous as with any other non-heap type).
And meanwhile, if your simulated generator doesn't have any use the value of yield, what's the point in implementing a send that takes and ignores an argument, checks that an otherwise-unnecessary first-run flag is set, and then does the same thing as __next__? Trying to implement as much of the generator protocol as possible is necessary if you're building Cython and need something to compile Cython generator bodies to, but YAGNI if you're just translating things like gen_func manually.",A,1
31190619,2015-07-02 17:03:58.973000+00:00,"You can't improve O(n) time-complexity because if the input is all 9s then you have to change n nines to zero and prepend one at the beginning:
def increment(digits):
    for i in reversed(range(len(digits))):
        if digits[i] != 9:
           digits[i] += 1
           break
        else:
           digits[i] = 0
    else: # no break, all 9s
        digits.insert(0, 1) # 

Example:
>>> a = [1, 2, 3]
>>> increment(a)
>>> a
[1, 2, 4]

The array grows in ""all 9s"" case:
>>> a = [9, 9, 9]
>>> increment(a)
>>> a
[1, 0, 0, 0]
",A,2
20078869,2013-11-19 18:11:35.743000+00:00,"Your ''.join() expression is filtering, removing anything non-ASCII; you could use a conditional expression instead:
return ''.join([i if ord(i) < 128 else ' ' for i in text])

This handles characters one by one and would still use one space per character replaced.
Your regular expression should just replace consecutive non-ASCII characters with a space:
re.sub(r'[^\x00-\x7F]+',' ', text)

Note the + there.",A,196
37525216,2016-05-30 12:03:36+00:00,"If you want JSON, you need to actually create some JSON.
 ctx['regions'] = json.dumps(cntr_rgns)
",A,1
7537987,2011-09-24 08:48:36.257000+00:00,"Your code in the question should work as is. If it doesn't then either your actual code is different (e.g., you might use stdout=PIPE that may change the child buffering behavior) or it might indicate a bug in the child application itself such as the read-ahead bug in Python 2 i.e., your input is sent correctly by the parent process but it is stuck in the child's internal input buffer.
The following works on my Ubuntu machine:
#!/usr/bin/env python
import time
from subprocess import Popen, PIPE

LINE_BUFFERED = 1

#NOTE: the first argument is a list
p = Popen(['cat'], bufsize=LINE_BUFFERED, stdin=PIPE,
          universal_newlines=True)
with p.stdin:
    for cmd in [""doSomething1\n"", ""doSomethingElse\n""]:
        time.sleep(1) # a delay to see that the commands appear one by one
        p.stdin.write(cmd)
        p.stdin.flush() # use explicit flush() to workaround
                        #   buffering bugs on some Python versions
rc = p.wait()
",A,0
47471089,2017-11-24 10:19:27.753000+00:00,"By yielding the value, the caller of the function has access to the intermediate results, making this memory efficient.
For example, the following would give you all results up to n as well, but requires that you create a list first to hold those values:
def fibonacci(n):
    results = []
    curr = 1
    prev = 0
    counter = 0
    while counter < n:
        results.append(curr)
        prev, curr = curr, prev + curr
        counter += 1
    return results

If my code then asked for fibonacci(10 ** 9), that would not only take a long time to produce, but would also require a significant amount of memory.
The generator option gives the caller access to the results immediately, which means they can use just that one intermediate result in another part of the program.)
This makes the generator function far more versatile and flexible. You could sum the fibonacci results, without having to rewrite the generator, for example:
sum_of_fibonacci_10 = sum(fibonacci(10))

This is still memory efficient, the summing takes place as the results are produced, at no point do all 10 results have to exist in memory. Just the current intermediate value, and the sum total up to now, are required.
If you wanted to just print the values instead, you can do so in a loop:
for next_value in fibonacci(10):
    print(next_value)

and the code is still just as efficient, and we still did not have to change the generator function.
So generators let you maintain and share intermediate state, saving you memory. This can be done with a class too, but a generator is much more readable. The following would do the same, but is not nearly as easy to follow:
class FibonacciGenerator:
    def __init__(self, n):
        self.n = n
        self.curr = 1
        self.prev = 0
        self.counter = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.counter >= self.n:
            raise StopIteration
        self.prev, self.curr = self.curr, self.prev + self.curr
        self.counter += 1
        return self.prev
",A,1
317055,2008-11-25 11:24:31.607000+00:00,"Option 1 does not work, actually.
Option 2 is the only choice.  RNG's absolutely require that you generate the values in sequence from a single seed.
Your ""create a new generator with a new seed"" breaks the mathematical foundation.  What you get then totally depends on your seeds, which -- sadly -- won't be very random.",A,8
15477803,2013-03-18 13:16:40.363000+00:00,"The sax parser itself is supposed to provide your content handler with a locator. The locator has to implement certain methods, but it can be any object as long as it has the right methods. The xml.sax.xmlreader.Locator class is the interface a locator is expected to implement; if the parser provided a locator object to your handler then you can count on those 4 methods being present on the locator.
The parser is only encouraged to set a locator, it is not required to do so. The expat XML parser does provide it.
If you subclass xml.sax.handler.ContentHandler() then it'll provide a standard setDocumentHandler() method for you, and by the time .startDocument() on the handler is called your content handler instance will have self._locator set:
from xml.sax.handler import ContentHandler

class MyContentHandler(ContentHandler):
    def __init__(self):
        ContentHandler.__init__(self)
        # initialize your handler

    def startElement(self, name, attrs):
        loc = self._locator
        if loc is not None:
            line, col = loc.getLineNumber(), loc.getColumnNumber()
        else:
            line, col = 'unknown', 'unknown'
        print 'start of {} element at line {}, column {}'.format(name, line, col)
",A,4
2208276,2010-02-05 15:33:44.423000+00:00,"How could this work? Aggregation is done in the database, but your calculation is obviously in Python.
You'll have to do the sum in Python as well:
aggregated_delta = sum([m.delta() for m in MyModel.objects.all()])
",A,1
19191885,2013-10-04 22:43:21.253000+00:00,"The pickle module already imports _pickle if available. It is the C-optimized version of the pickle module, and is used transparently.
From the pickle.py source code:
# Use the faster _pickle if possible
try:
    from _pickle import *
except ImportError:
    Pickler, Unpickler = _Pickler, _Unpickler

and from the pickle module documentation:

The pickle module has an transparent optimizer (_pickle) written in C. It is used whenever available. Otherwise the pure Python implementation is used.

In Python 2, _pickle was known as cPickle, but has been updated to allow the transparent use as an implementation detail.",A,39
26080062,2014-09-27 23:13:43.260000+00:00,"You are only testing if the first card in the players hand is any of he guesses. You need to test each card in the hand:
while not any(guess in PH for guess in [guess1, guess2, guess3, guess4]):

This takes each guessed card and tests that card against the hand in turn. any() stops looping over the guesses when a match is found.
A better idea still is to use set intersections:
guesses = {guess1, guess2, guess3, guess4}
while not guesses.intersection(PH):
    # ask for new guesses

You want to avoid having to type out your 'ask for a guess' code twice; start a loop with while True and use break to end the loop when a correct guess has been made:
suits = ('Hearts', 'Diamonds', 'Clubs', 'Spades')

while True:
    guess = raw_input(""Make a guess:"")
    guesses = {'{} of {}'.format(guess, suit) for suit in suits}
    if guesses.intersection(PH):
        # correct guess!
        break
    print ""You do not have a card like that in your hand.""

I used a set comprehension to build the guesses in a loop.",A,1
31450778,2015-07-16 09:46:18.570000+00:00,"Use one exception block and special case your handling there; you can just use isinstance() to detect a specific exception type:
try:
    os.makedirs(path)
except Exception as e:
    if isinstance(e, WindowsError) and ""directory already exists"" in e:
        print ""Folder already exists, moving on.""
    else:
        print e
        error = 1

Note that I'd not rely on the container-like nature of exceptions here; I'd test the args attribute explicitly:
if isinstance(e, WindowsError) and e.args[0] == ""directory already exists"":
",A,4
2366092,2010-03-02 19:32:56.547000+00:00,"You have to read in pieces.  Not because of memory constraints, but because of the parsing requirements.  18MiB fits in memory easily.  On a 4Gb machine it fits in memory 200 times over.
Here's the usual design pattern.

Read the first 4 bytes only.  Use struct to unpack just those bytes.
Confirm the sync bytes and get the header length.
If you want the rest of the header, you know the length, read the rest of the bytes.
If you don't want the header, use seek to skip past it.
Read the first four bytes of a record to get the number of SV Observations.  Use struct to unpack it.
Do the math and read the indicated number of bytes to get all the SV Observations in the record.
Unpack them and do whatever it is you're doing.  
I strongly suggest building namedtuple objects from the data before doing anything else with it.

If you want all the data, you have to actually read all the data.
""and without reading an 18 MiB file one byte at a time)?""  I don't understand this constraint.  You have to read all the bytes to get all the bytes.
You can use the length information to read the bytes in meaningful chunks.  But you can't avoid reading all the bytes.
Also, lots of reads (and seeks) are often fast enough.  Your OS buffers for you, so don't worry about trying to micro-optimize the number of reads.
Just follow the ""read length -- read data"" pattern.",A,6
24020212,2014-06-03 16:24:09.630000+00:00,"Any name that is not a local (has not been assigned to in the current scope) is instead assumed to be a global. The name is looked up every time the code runs.
So at runtime the name is looked up in the global namespace, which is just a dictionary. If the name doesn't exist at that time, a NameError exception is raised.
You can see this when you disassemble a function; bytecode is shown when using the dis module:
>>> import dis
>>> def foo():
...     bar = 'baz'  # local
...     bar  # reference the local
...     baz  # reference something else; e.g. a global
... 
>>> dis.dis(foo)
  2           0 LOAD_CONST               1 ('baz')
              3 STORE_FAST               0 (bar)

  3           6 LOAD_FAST                0 (bar)
              9 POP_TOP             

  4          10 LOAD_GLOBAL              0 (baz)
             13 POP_TOP             
             14 LOAD_CONST               0 (None)
             17 RETURN_VALUE        

bar is a local (it was assigned to in the block), while baz is a global. The local is referenced by LOAD_FAST, while the global is referenced by LOAD_GLOBAL.
To do this, function objects have a function.__globals__ reference linking it to the module globals mapping; see the User-defined functions section in the datamodel documentation:
>>> foo.__globals__ is globals()
True

Python also cleans up globals when deleting modules; to prevent circular references holding up finalisation globals are rebound to None at that time (although this behaviour changed in Python 3.4). If you however maintain a reference to tester, your code will find those None values.
Your tester instance, still referencing the original class and it's methods, are still referencing their module through their function.__globals__ references. So, although you deleted the sys.modules reference to the module, triggering the module cleanup, the globals dictionary is still referenced by the class methods. This globals dictionary now holds None values for each global.",A,5
42459616,2017-02-25 17:59:42.540000+00:00,"You can't set a starting position; it'll always start where the given sequence starts.
You can move the cycle along a few steps before you use it for whatever you need it for. Use itertools.islice() to skip some items:
from itertools import islice

starting_at_three = islice(positions, 2, None)

You pass in the iterable, then a start and stop value; None here means that the islice() iterator continues forever or until the underlying positions iterator is exhausted.
Demo:
>>> from itertools import islice, cycle
>>> positions = cycle([1, 2, 3, 4])
>>> starting_at_three = islice(positions, 2, None)
>>> next(starting_at_three)
3
>>> next(starting_at_three)
4
>>> next(starting_at_three)
1

The other option is to pass in a different sequence; you could pass in [3, 4, 1, 2] for example.",A,6
26763833,2014-11-05 17:52:40.827000+00:00,"You are asking Python to create a new string from index 0 to index 0. And that's an empty slice.
The end value of a slice is exclusive. Python picks all characters from the starting index that come before the end index. Here -0 is the same thing as 0 (negative zero is the same thing as positive zero).
Note that that is different from using actual negative values; [0:-1] would take the length and replace the negative value with len(object) - 1 and use that as the end value, but this doesn't work for -0, because that is not a negative value!
You can use None instead of 0 if you wanted to slice to the end:
substring = word[:-value if value else None]
",A,2
24558329,2014-07-03 15:54:14.760000+00:00,"When you use files, the POST data is encoded as multipart/form-data, which is what you want when posting form data that includes large binary blobs of content.
If you use just data on the other hand, then application/x-www-form-urlencoded is used instead (unless you pass in just a string, then data is posted as-is).
For file uploads, you probably want the former, as multipart/form-data can also encode the mime-type and filename of the file data. Looking at the Facebook Ad API adimage enpoint their samples use curl -F, which is documented to use multipart/form-data for the upload.
Remember that also need to include a access_token in the POST:
requests.post(endpoint,
    files={'pic.jpg': open('pic.jpg', 'rb')},
    data={'access_token': '___'})

This uses both files and data, requests will combine both into one multipart/form-data POST body.
Also see What does enctype='multipart/form-data' mean? and application/x-www-form-urlencoded or multipart/form-data?",A,3
35372611,2016-02-12 21:38:40.423000+00:00,"You could use the following dict comprehension:
with open(number_file) as fileobj:
    result = {row[0]: row[1:] for line in fileobj for row in (line.split(),)}

where the for row in (one_element_tuple,) is effectively an assignment.
Or you could use a nested generator expression to handle the splitting of each line:
with open(number_file) as fileobj:
    result = {row[0]: row[1:] for row in (line.split() for line in fileobj)}

However, if your file is really tab-delimited, don't fear the csv module:
import csv

with open(number_file) as fileobj:
    result = {row[0]: row[1:] for row in csv.reader(fileobj, delimiter='\t')}
",A,7
18970233,2013-09-23 22:54:31.580000+00:00,"
Does Python have a nice and short closure/lambda syntax for it?

Yes, but you don't want it in this case.
The closest equivalent to that Ruby code is:
new_values = map(print, [1, 2, 3])

That looks pretty nice when you already have a function lying around, like print. When you just have some arbitrary expression and you want to use it in map, you need to create a function out of it with a def or a lambda, like this:
new_values = map(lambda x: print(x), [1, 2, 3])

That's the ugliness you apparently want to avoid. And Python has a nice way to avoid it: comprehensions:
new_values = [print(x) for x in values]

However, in this case, you're just trying to execute some statement for each value, not accumulate the new values for each value. So, while this will work (you'll get back a list of None values), it's definitely not idiomatic.
In this case, the right thing to do is to write it explicitly—no closures, no functions, no comprehensions, just a loop:
for x in values:
    print x
",A,35
30284479,2015-05-17 07:35:35.960000+00:00,"As the docs for bytes explains:

While bytes literals and representations are based on ASCII text, bytes objects actually behave like immutable sequences of integers.

In other words, if b = b'abcd', then b[0] or next(iter(b)) is 97, not b'a'.
If you want to construct a bytes object from an iterable of small integers, don't use b'', just call the constructor:
bytes(next(iterator) for i in range(4))

Of course a much simpler way to do what you wanted is to just slice the bytes:
bchain[:4]

Or, if you want to work on any iterable of small integers, not just a bytes, use islice:
bytes(itertools.islice(bchain, 4))


As J.F. Sebastian points out, this is the reason PEP 467 includes iterbytes. Assuming it's added to Python 3.5, you could use b.iterbytes()--or, if b is just ""some bytes-like iterable"", memoryview(b).iterbytes()--instead of iter(b), and then you wouldn't have this problem in the first place.",A,2
12218674,2012-08-31 15:45:41.470000+00:00,"No, you cannot set the docstring of a function from within the body of the function, not without executing code either outside of the function, or inside the function (which requires calling the function first).
The way python normally sets the docstring is to take the first line of the function suite (everything indented under the def line) and if that's a string literal, it removes that from the suite and makes that the docstring. Python then compiles the rest of the suite into the function code object, and creates a new function() object by passing in the compiled code object and the docstring (among others).
If you use an expression that happens to produce a string instead, then that 'trick' doesn't work; python will ignore that expression, leaving it as part of the function suite to compile. Because the function suite itself is compiled and not executed, you cannot 'reach out' and set the docstring to be used on the function object during compilation time.
The only way you can dynamically set a function's docstring is by referring to a function object directly and setting it's __doc__ or func_doc variable (the two are aliases). Sure, that can be done in the function suite, but that's rather pointless, the docstring will be wrong until you do so.
Note that when you import a function from a module, you already get the docstring too:
>>> import itertools
>>> print itertools.groupby.__doc__
groupby(iterable[, keyfunc]) -> create an iterator which returns
(key, sub-iterator) grouped by each value of key(value).

There is no need to import the docstring separately.",A,1
6690393,2011-07-14 08:23:40.810000+00:00,"Use in:
while not roomNumber in (""L1"", ""L2"", ""L3"", ""L4"", ""L5""):
",A,2
15663414,2013-03-27 15:59:24.523000+00:00,"Use csv to read the first file, and a dictionary to store the file1 rows. The second file is in FASTA format, so we only take the lines starting with ?:
import csv

with open('file1', 'rb') as file1:
    file1_data = dict(line.split(None, 2)[1::-1] for line in file1 if line.strip())

with open('file2', 'rb') as file2, open('output', 'wb') as outputfile:
    output = csv.writer(outputfile, delimiter='|')
    for line in file2:
        if line[:1] == '?':
            row = line.strip().split('|')
            key = row[0][1:]
            if key in file1_data:
                 output.writerow(row + [file1_data[key]])

This produces:
?dog|parallel|numbering|position|23
?cat|parallel|nuucers|position|24

for your input example.",A,3
3963247,2010-10-18 20:46:35.730000+00:00,"Use the __range operator:
...filter(current_issue__isnull=True, created_at__range=(start_date, end_date))
",A,192
23674838,2014-05-15 09:53:30.030000+00:00,"The Python documentation gives you pure-python equivalents for both functions; they are trivial to translate to JavaScript:
function any(iterable) {
    for (var index = 0; index < iterable.length; ++index) {
        if (iterable[index]) return true;
    }
    return false;
}

and
function all(iterable) {
    for (var index = 0; index < iterable.length; ++index) {
        if (!iterable[index]) return false;
    }
    return true;
}

Recent browser versions (implementing ECMAScript 5.1, Firefox 1.5 and IE 9) have native support in the form of Array.some and Array.every; these take a callback that determines if something is 'true' or not:
some_array.some(function(elem) { return !!elem; });
some_array.every(function(elem) { return !!elem; });

The Mozilla documentation I linked to has polyfills included to recreate these two methods in other JS implementations.",A,20
19925173,2013-11-12 09:17:40.667000+00:00,"What makes you think assets is magic in some way? It's not, it's simply the name of a directory. You don't have a directory called that, so you shouldn't use it. Use {% static 'images/pythonlogo.jpeg' %} etc.",A,1
16629617,2013-05-18 22:32:54.707000+00:00,"Each group is an iterable, and turning that into a list exhausts it. You cannot turn an iterable into a list twice.
Store the list as a new variable:
for key, group in groupby(sorted(words, key = len), len):
    grouplist = list(group)
    if len(grouplist) > 1:
        print grouplist

Now you consume the iterable only once:
>>> for key, group in groupby(sorted(words, key = len), len):
...     grouplist = list(group)
...     if len(grouplist) > 1:
...         print grouplist
... 
['this', 'that', 'them', 'what']
",A,3
12628636,2012-09-27 19:22:31.510000+00:00,"You are dealing with two files here.
You open fn for writing in the current directory, but you rename the file '.\out\' + fn.
When opening fn, make sure you use the correct directory:
f = open(r'.\out\' + fn ,mode)

Note that on Windows, you can use the / separator as well, which is easier to deal with as you don't have to use raw strings or escape the slashes. Also, it's better to use os.path.join() to deal with directories and files:
filename = os.path.join('./out', fn)

then work with filename throughout the function.",A,2
51374141,2018-07-17 05:55:26.757000+00:00,"Your outer while True: loop is effectively for upper_bound in range(b+1): written more obscurely, so it obviously runs b+1 times.
Your inner loop is for i in range(upper_bound):, and upper_bound averages (b+1)/2, so it runs (b+1)/2 times per outer loop, or (b+1) * (b+1)/2 total.
You haven't shown us the definition of print_01_codes('', a), or told us what the values of a and b are.

If a is a collection of some kind, and print_01_codes loops over the values in a (just as, e.g., print('', a) would), that loop runs a times per inner loop, or (b+1) * (b+1)/2 * a total.
If a is just a number, and it's just printed (just as, e.g., print('', a) would), that takes log(a, 10) time.

Meanwhile, all of that code in anotherfunc is irrelevant, because that function never gets called.
So, your time is not O(a * b), but either O(a * b**2) or O(log a * b**2).",A,0
853611,2009-05-12 16:12:37.977000+00:00,"We do this by having a ""Request"" table in the database.
When the upload arrives, we create the uploaded File object, and create a Request.
We start the background batch processor.
We return a 200 ""we're working on it"" page -- it shows the Requests and their status.
Our batch processor uses the Django ORM.  When it finishes, it updates the Request object.  We can (but don't) send an email notification.  Mostly, we just update the status so that the user can log in again and see that processing has completed.

Batch Server Architecture notes.
It's a WSGI server that waits on a port for a batch processing request.  The request is a REST POST with an ID number; the batch processor looks this up in the database and processes it.
The server is started automagically by our REST interface.  If it isn't running, we spawn it.  This makes a user transaction appear slow, but, oh well.  It's not supposed to crash.  
Also, we have a simple crontab to check that it's running.  At most, it will be down for 30 minutes between ""are you alive?"" checks.  We don't have a formal startup script (we run under Apache with mod_wsgi), but we may create a ""restart"" script that touches the WSGI file and then does a POST to a URL that does a health-check (and starts the batch processor).
When the batch server starts, there may be unprocessed requests for which it has never gotten a POST.  So, the default startup is to pull ALL work out of the Request queue -- assuming it may have missed something.  ",A,6
22411795,2014-03-14 17:24:25.237000+00:00,"You assigned a string value to os, it is no longer bound to the module:
os = 'some string'

Rename that to something else.",A,3
32855618,2015-09-29 23:57:11.577000+00:00,"
I want to replace the date 2015-05-01 (from and to) with the current date

To replace the date, you could use string formatting. To post the request and to print the response into stdout:
#!/usr/bin/env python2
import urllib2
import sys
from datetime import date
from contextlib import closing
from shutil import copyfileobj
from urllib import quote

url = 'http://fme.discomap.eea.europa.eu/fmedatastreaming/AirQuality/AirQualityUTDExport.fmw'
data = ""POSTDATA=FromDate={now}&ToDate={now}"".format(now=quote(date.today()))
data += ""&Countrycode=&InsertedSinceDate=&UpdatedSinceDate=&Pollutant=PM10&Namespace=&Format=XML&UserToken="" 
with closing(urllib2.urlopen(url, data)) as response:
    copyfileobj(response, sys.stdout)

In general, consider urllib.urlencode(), to create application/x-www-form-urlencoded data instead of doing it manually.",A,0
12515327,2012-09-20 14:55:38.007000+00:00,"You'll need to use an absolute filename, not a local file:
import os

try:
    currdir = os.path.abspath(__file__)
except NameError:  # We are the main py2exe script, not a module
    import sys
    currdir = os.path.abspath(os.path.dirname(sys.argv[0]))
DBfile = os.path.join(currdir, 'GDP.mdb')
",A,3
27680241,2014-12-28 20:20:10.887000+00:00,"Why are you running sqlmigrate? The correct command to run migrations is just migrate.
sqlmigrate simply prints the SQL that would be executed in a migration, as the documentation shows.",A,0
17098594,2013-06-13 22:59:34.550000+00:00,"You can't really ""monkeypatch"" a function the way you can a class, object, module, etc.
Those other things all ultimately come down to a collection of attributes, so replacing one attribute with a different one, or adding a new one, is both easy and useful. Functions, on the other hand, are basically atomic things.*
You can, of course, monkeypatch the builtins module by replacing the sum function. But I don't think that's what you were asking. (If you were, see below.)
Anyway, you can't patch sum, but you can write a new function, with the same name if you want, (possibly with a wrapper around the original function—which, you'll notice, is exactly what decorators do).

But there is really no way to use sum(['s','t','a','c','k']) to do what you want, because sum by default starts off with 0 and adds things to it. And you can't add a string to 0.**
Of course you can always pass an explicit start instead of using the default, but you'd have to change your calling code to send the appropriate start. In some cases (e.g., where you're sending a literal list display) it's pretty obvious; in other cases (e.g., in a generic function) it may not be. That still won't work here, because sum(['s','t','a','c','k'], '') will just raise a TypeError (try it and read the error to see why), but it will work in other cases.
But there is no way to avoid having to know an appropriate starting value with sum, because that's how sum works.
If you think about it, sum is conceptually equivalent to:
def sum(iterable, start=0):
    reduce(operator.add, iterable, start)

The only real problem here is that start, right? reduce allows you to leave off the start value, and it will start with the first value in the iterable:
>>> reduce(operator.add, ['s', 't', 'a', 'c', 'k'])
'stack'

That's something sum can't do. But, if you really want to, you can redefine sum so it can:
>>> def sum(iterable):
...     return reduce(operator.add, iterable)

… or:
>>> sentinel = object()
>>> def sum(iterable, start=sentinel):
...     if start is sentinel:
...         return reduce(operator.add, iterable)
...     else:
...         return reduce(operator.add, iterable, start)

But note that this sum will be much slower on integers than the original one, and it will raise a TypeError instead of returning 0 on an empty sequence, and so on.

If you really do want to monkeypatch the builtins (as opposed to just defining a new function with a new name, or a new function with the same name in your module's globals() that shadows the builtin), here's an example that works for Python 3.1+, as long as your modules are using normal globals dictionaries (which they will be unless you're running in an embedded interpreter or an exec call or similar):
import builtins
builtins.sum = _new_sum

In other words, same as monkeypatching any other module.
In 2.x, the module is called __builtin__. And the rules for how it's accessed through globals changed somewhere around 2.3 and again in 3.0. See builtins/__builtin__ for details.

* Of course that isn't quite true. A function has a name, a list of closure cells, a doc string, etc. on top of its code object. And even the code object is a sequence of bytecodes, and you can use bytecodehacks or hard-coded hackery on that. Except that sum is actually a builtin-function, not a function, so it doesn't even have code accessible from Python… Anyway, it's close enough for most purposes to say that functions are atomic things.
** Sure, you could convert the string to some subclass that knows how to add itself to integers (by ignoring them), but really, you don't want to.",A,12
34687408,2016-01-08 22:49:55.743000+00:00,"You could use augmented assignment, -=:
a[4] -= 3

Or just re-assign the result of a subtraction back to the index (which is what -= does anyway):
a[4] = a[4] - 3
",A,5
14671481,2013-02-03 10:23:58.007000+00:00,"From the iterparse() docs:

Note: iterparse() only guarantees that it has seen the “>” character
  of a starting tag when it emits a “start” event, so the attributes are
  defined, but the contents of the text and tail attributes are
  undefined at that point. The same applies to the element children;
  they may or may not be present. If you need a fully populated element,
  look for “end” events instead.

Drop inReport* variables and process ReportHost only on ""end"" events when it fully parsed. Use ElementTree API to get necessary info such as cvss_base_score from current ReportHost element.
To preserve memory, do:
import xml.etree.cElementTree as etree

def getelements(filename_or_file, tag):
    context = iter(etree.iterparse(filename_or_file, events=('start', 'end')))
    _, root = next(context) # get root element
    for event, elem in context:
        if event == 'end' and elem.tag == tag:
            yield elem
            root.clear() # preserve memory

for host in getelements(""test2.nessus"", ""ReportHost""):
    for cvss_el in host.iter(""cvss_base_score""):
        print(cvss_el.text)
",A,3
29599630,2015-04-13 07:07:03.777000+00:00,"Your checkC is a list of tuples—not a list of lists of tuples, as you claim: So, when you do for i in checkC:, each i is just a tuple, like (15, 12).
So, i[1] is perfectly valid—it's the second element of (15, 12), which is 12. But i[1][1] is not—it's be the second element of the second element of (15, 12), which is the second element of 12, which is meaningless, because 12 is an int and has no elements. Hence the error.
But I don't know why you're trying to index i at all. You want to look up the tuple (15, 12), as a single value; you don't want to look up 15 and 12 separately. So just use i, not i[anything].

That will get rid of your actual error, because if i in userCoords is legal—but it's still useless. There's no way that a tuple like (15, 12) can be in usedCoords, because usedCoords only holds lists of tuples, not tuples.
What you want to ask is whether i is in any of the sublists that are in usedCoords:
if any(i in sublist for sublist in usedCoords):
    print('Used')

If you don't understand the any function and generator expressions or other comprehensions, you should read through the official tutorial, starting with Iterators; it explains it much nicer than any spur-of-the-moment StackOverflow answer can. But until then, you can always write the loop and check explicitly instead:
for sublist in usedCoords:
    if i in sublist:
        print('Used')
        break
else:
    print('Not used')


Meanwhile, your loop has a problem as well. For each member of checkC, you set tf to True if it's found, False if it's not. So, at the end of the loop, tf will only tell you whether the last member was found. If you want to know whether any of the members are found, you can't keep resetting tf = False; once it's True, it has to stay True forever. So, the whole thing becomes:
tf = False
for i in checkC:
    if any(i in sublist for sublist in usedCoords):
        tf = True
        print('used')
        break
if tf == False:
    usedCoords.append(checkC)
    tf = True

Which can be simplified to:
for i in checkC:
    if any(i in sublist for sublist in usedCoords):
        print('used')
        break
else:
    usedCoords.append(checkC)
tf = True

And, even further, to:
if any(i in sublist for i in checkC for sublist in usedCoords):
    print('used')
else:
    usedCoords.append(checkC)
",A,1
18797455,2013-09-14 01:38:07.787000+00:00,"There is no do … while loop in Python. I believe somewhere in the FAQ it explains why, and how to work around it.
But that doesn't matter because what you've written isn't a do … while loop, it's just a plain while loop. The whole point of a do … while in every language that has one is that you put the test condition after the loop body, instead of before, which guarantees that the loop will run at least once, and which allows you to avoid doing pre-loop setup that you'd have to duplicate inside the loop.
So, to translate your code to Python:
number = 18
while number == 1:
    if number%2==0:
        number = number/2
    else:
        number = (number*3)+1
print(number)

However, it's worth noting that since number = 18 before the loop, it will never be == 1 the first time through, so this is just going to skip the whole thing and print out 18.
You may have wanted while number != 1 or while number > 1, but you have to figure out what you actually want to write before anyone can write it.",A,1
53021779,2018-10-27 12:11:20.770000+00:00,"You do not need to use indices at all. Just start a new story list whenever you have a new title, and append the previous one to list_of_stories:
story = []
list_of_stories = []
titles = set(find_title(fname))

for line in read_lines_in_text(fname):
    if line in titles:
        # start a new story, append the previous
        if story:
            list_of_stories.append(story)
        story = [line]
    elif story:  # a story has been started
        story.append(line)

# handle the last story
if story:
    list_of_stories.append(story)

When using a generator function, you really want to avoid treating it as a random access sequence with index numbers.
Note that we also avoid reading fname more than once just to get the titles; the titles variable is a set of title strings returned by find_title(), stored as a set for fast membership testing.",A,3
20634401,2013-12-17 12:40:29.403000+00:00,You don't seem to be comparing strings with strings. You're comparing a string with an Interface object. Maybe you meant {% ifequal interface app_interface.title %}?,A,1
647811,2009-03-15 13:32:39.363000+00:00,"Follow these steps.

Run it with except Exception, e: print repr(e).
See what exception you get.
Change the Exception to the exception you actually got.

Also, remember that the exception, e, is an object.  You can print dir(e), e.__class__.__name__, etc.to see what attributes it has.
Also, you can do this interactively at the >>> prompt in Python.  You can then manipulate the object directly -- no guessing.",A,15
16939689,2013-06-05 12:13:38.733000+00:00,"As mgilson points out, you can use update to update a set in-place from another set. That actually works out slightly quicker:
def union():
    i = set(range(10000))
    j = set(range(5000, 15000))
    return i.union(j)

def update():
    i = set(range(10000))
    j = set(range(5000, 15000))
    i.update(j)
    return i

timeit.Timer(union).timeit(10000)   # 10.351907968521118
timeit.Timer(update).timeit(10000)  # 8.83384895324707
",A,4
8582939,2011-12-20 22:44:00.417000+00:00,"Stop writing circular imports.  It's simple.  thing cannot possible depend on everything that's in otherthing.  
1) search for other questions exactly like yours.
2) read those answers.
3) rewrite otherthing so that thing depends on part of otherthing, not all of otherthing.  ",A,-5
36140471,2016-03-21 20:01:06.837000+00:00,"You can use string formatting to generate your placeholders:
statement = ""SELECT * FROM table WHERE column IN ({0})"".format(
    ', '.join(['%s'] * len(filterList)))
cursor.execute(statement, filterList)

You may still want to test that filterList is not empty, and omit the WHERE clause altogether in that case:
statement = ""SELECT * FROM table""
if filterList:
    statement += "" WHERE column IN ({0})"".format(
        ', '.join(['%s'] * len(filterList)))
cursor.execute(statement, filterList)
",A,3
19285564,2013-10-10 01:23:30.497000+00:00,"There are three different obvious ways to define integer division over the negative numbers, and three corresponding ways to define remainder:

floored division, where the quotient is floored toward negative infinity and the remainder has the same sign as the divisor.
truncated division, where the quotient is truncated toward zero and the remainder as the same sign as the dividend.
Euclidean division, where the quotient is truncated in whichever direction leaves the remainder positive.

All three of these preserve the fundamental law of integer division:
dividend = divisor * quotient + remainder

So, none of the three are ""right"" or ""wrong"".*
Of course that doesn't stop people from having holy wars. Knuth argued for floored division on the basis that it was ""mathematically correct"". Wirth argued for truncated division because it's ""less surprising"". Raymond Boute argued that integer division is defined in terms of the Euclidean algorithm. I won't try to settle a decades-old holy war by arguing that all three of them, including two of the most important people in the field, are wrong… 
Some languages solve this problem by having two different functions.** 
Let's just say that Python picked up Knuth's definition, so its modulus operator has the sign of the divisor.

* Of course picking non-matching definitions of quotient and remainder is another story. Or, worse, specifying one and leaving the other implementation-defined, as C did before C99.
** This is especially fun because they're not always consistent about which one is which. At least when they're called rem and mod, rem is the one that goes with div, while mod is whichever of floored or Euclidean doesn't go with div, when when they're called rem and remainder or mod and modulo…",A,6
18135764,2013-08-08 20:43:43.457000+00:00,"The problem here is that you try to access values in report2 that you haven't created yet. 
It's pretty much the same problem you'd get with a variable. Imagine you had this code:
i = 1
if i == 1:
    a += 2
else:
    a = 2

You can't do a += 2 because there is no value there.
So to fix this, you need to deal with the case where you're seeing a key for the first time. You can use defaultdict instead of dict, or the setdefault method, or an explicit in check, or a try, but usually one of the first two will be easier.",A,0
43068777,2017-03-28 11:49:23.190000+00:00,"You appended the bytes in the following order
my_bytes.append(dummySrvId & 0xff)  # -4 (offset from end)
my_bytes.append(dummySrvId >> 8)    # -3
my_bytes.append(srvIdFrom & 0xff)   # -2
my_bytes.append(srvIdFrom >> 8)     # -1

but in your Python test to decode, you then shift the wrong bytes, effectively swapping the MSB and LSB parts:
srvIdFrom = (
    my_bytes[len(my_bytes)-4] << 8   #  -4 is dummySrvId & 0xff
  | my_bytes[len(my_bytes)-3] << 0   #  -3 is srvIdFrom >> 8
)
srvIdTo = (
    my_bytes[len(my_bytes)-2] << 8   #  -2 is srvIdFrom & 0xff
  | my_bytes[len(my_bytes)-1] << 0   #  -1 is srvIdFrom >> 8
)

The latter does not match your C code either; you shift the little-endian bytes (offsets -3 and -1):
srvIdFrom = payload[len-4]  | payload[len-3] << 8;
srvIdTo = payload[len-2]  | payload[len-1] << 8;

and indeed, if you swap the shifting and match your C code to decode, you get the correct values:
>>> my_bytes[len(my_bytes)-4] | my_bytes[len(my_bytes)-3] << 8
1234
>>> my_bytes[len(my_bytes)-2] | my_bytes[len(my_bytes)-1] << 8
5678

so something else is wrong. Your Python encoding work is fine.",A,0
20943083,2014-01-06 04:37:37.977000+00:00,"
I have an idea for a fix where I do another for loop, get each character, and append each non-comma to another array and then convert to int. But this sounds complex…

You're right—that would work, but it's not the easiest way to do it.
Just use the split method on strings, like this:
for line in info:
    solution.append(line.split("",""))

Now, instead of each line being ['1234,123,345\n'], it will be ['1234', '123', '345\n'].
This gets you close, but those are still strings, not ints. An inner loop to convert each one to an int, just as you suggested, will work great. But it might be easier to read with a for loop in a list comprehension, rather than in a for statement. Like this:
for line in info:
    solution.append([int(number) for number in line.split("","")])

If you don't understand the comprehension, here it is as a statement:
for line in info:
    numbers = []
    for number in line.split("",""):
        numbers.append(int(number))
    solution.append(numbers)

And, since the int function can ignore whitespace, int('345\n') is 345, so you don't have to do anything else.
But if you did have to do something else, rstrip would be the easiest answer: it removes all whitespace from the end of a string. So:
for line in info:
    solution.append([int(number) for number in line.rstrip().split("","")])
",A,1
26684194,2014-10-31 22:13:03.013000+00:00,"The third argument is there just for symmetry with __pow__.
The argument was included in the original 'add in-place operator equivalents' commit but there is no support to use it from Python code, other than calling the __ipow__ method directly.
For example, the INPLACE_POWER opcode handling passes in None as the third argument:
case INPLACE_POWER:
    w = POP();
    v = TOP();
    x = PyNumber_InPlacePower(v, w, Py_None);
    Py_DECREF(v);
    Py_DECREF(w);
    SET_TOP(x);
    if (x != NULL) continue;
    break;

Most likely it is there to make implementing __ipow__ as an alias for __pow__ trivial even from C code.",A,2
142251,2008-09-26 22:00:25.923000+00:00,"In Python 3.0:
import io

with io.StringIO() as f:
    f.write('abcdef')
    print('gh', file=f)
    f.seek(0)
    print(f.read())
",A,24
30259475,2015-05-15 12:23:26.700000+00:00,"Yes, the files are opened in binary mode. See the default_stream_factory() function source:

def default_stream_factory(total_content_length, filename, content_type,
                           content_length=None):
    """"""The stream factory that is used per default.""""""
    if total_content_length > 1024 * 500:
        return TemporaryFile('wb+')
    return BytesIO()


So either you get a TemporaryFile() object opened in wb+ mode (write and read, binary), or an in-memory BytesIO object.",A,0
52286686,2018-09-12 02:37:15.367000+00:00,"
given that the Queue q was created in the Main process, does the data flow like this?

A => Main => B

No. As explained in the docs, a Queue is just an auto-synchronizing wrapper around a Pipe. When you pass a Queue to a child, you're just passing that Pipe and some locks.
And the Pipe is just a wrapper around an operating system pipe. When you pass a Pipe to a child, you're just passing the pipe's file descriptor/handle.
Ignoring the locks, process A is basically just writing to a pipe, and process B is just reading from it.
The locks do make things a bit more complicated (and may also mean that process A spins up a hidden background thread), but they still don't involve the main process at all.
Unless the main process calls a method on a queue, it has nothing to do with that queue at all.",A,5
18907529,2013-09-20 01:25:50.797000+00:00,"If you're just trying to replace each --- with -, and replace each -- into -… that's easy to translate directly into Python:
def checkio(line):
    return line.replace('---', '-').replace('--', '-')

(If you want to fix replace any string of two or more - characters with a single -, whether there are 2, 3, or 45, then it's a job for regular expressions. But otherwise, you're just making the problem more complicated. You don't want to just use ""magic code"" you don't understand—and, while it's definitely worth learning regular expressions at some point, you don't have to learn them to solve this problem.)",A,0
296539,2008-11-17 19:11:03.587000+00:00,Just an idea: Use Python as embeddable scripting language to plot your graphs. Python has a plethora of plotting libraries.,A,3
21212967,2014-01-19 03:50:24.613000+00:00,"The problem with this:
subprocess.call(""python /home/pi/test.py"")

Is that it looks through your PATH from an executable named ""python /home/pi/test.py""—all one program, with the space in the name. You want to pass a list of arguments, not a command line. As the docs explain:

args is required for all calls and should be a string, or a sequence of program arguments. Providing a sequence of arguments is generally preferred, as it allows the module to take care of any required escaping and quoting of arguments (e.g. to permit spaces in file names). If passing a single string, either shell must be True (see below) or else the string must simply name the program to be executed without specifying any arguments.

So, do this:
subprocess.call([""python"", ""/home/pi/test.py""])

That will look for an executable named ""python"", and run it with ""/home/pi/test.py"" as its first argument (and with ""python"" as its zeroth argument), which is what you want.
It may still not work—python has to be on the PATH of your special Apache user, through an accessible path, and executable. Often it's better to pass sys.executable or ""/usr/bin/python"" or whatever is appropriate.
Meanwhile, it would be easier (as in not completely impossible) to debug this if you didn't just call call and ignore the result. For example:
ret = call([""python"", ""/home/pi/test.py""])
if ret:
    print ""Running test.py returned"", ret

… or …
try:
    check_call([""python"", ""/home/pi/test.py""])
except Exception as e:
    print ""Tried to run test.py, got back"", repr(e)

Or… do you actually want its output to pass straight through to the CGI? If not, you probably want to capture its output and do something to it, like this:
try:
    output = check_output([""python"", ""/home/pi/test.py""])
except Exception as e:
    print ""Tried to run test.py, got back"", repr(e)
else:
    print ""test.py said"", output
",A,0
51141816,2018-07-02 18:23:29.213000+00:00,"Firstly, delete that get_object() method in your AnimalDetailView. It is doing nothing other than causing a completely pointless update query.
Now, in your MedhistoryListView, you want to set the queryset to be the set of objects related to the Animal identified by the PK in the URL. So, define the get_queryset() method:
def get_queryset(self, *args, **kwargs):
    return MedicalHistory.objects.filter(animal_id=self.kwargs['pk'])
",A,0
7533505,2011-09-23 18:48:53.783000+00:00,"You want to use aggregation:
from django.db.models import Sum

def list_invoices(request):
    invoices = Item.objects.all().annotate(subtotal=Sum('invoiceline__price'))
",A,1
8492493,2011-12-13 16:21:15.960000+00:00,"You could use q.queue attribute to access the underlying sequence:
>>> q = Queue()
>>> q.put(1)
>>> q.queue
deque([1])
",A,1
210234,2008-10-16 20:45:04.240000+00:00,"If you need to transform keys or values before creating a dictionary then a generator expression could be used. Example:
>>> adict = dict((str(k), v) for k, v in zip(['a', 1, 'b'], [2, 'c', 3])) 

Take a look Code Like a Pythonista: Idiomatic Python.",A,12
34711427,2016-01-10 22:16:16.103000+00:00,This is exactly what the ManyToManyField is for.,A,1
1302755,2009-08-19 21:35:23.573000+00:00,I guess you could always subclass (or monkeypatch) the ChangeList class and replace get_query_set() with a dummy. I can't work out why you'd want to do that though - it would mean you wouldn't get any data in the changelist page.,A,0
21707932,2014-02-11 16:49:39.153000+00:00,"Sure, assign None to it first:
a = 5
c = None
if a == 7:
    c = 10
if c:
    print c

None tests as False in a boolean context, so if c: still works as written.
You could also catch the NameError exception:
try:
    print c
except NameError:
    pass

or use the globals() and locals() functions:
if 'c' in locals():
    # in a function

if 'c' in globals():
    # outside a function

but that's just plain ugly and unnecessary.",A,7
41509215,2017-01-06 15:49:51.330000+00:00,"Just split once, and test if the list is long enough:
def __init__(self, input):
    self.__input = input

    parts = input['text'].split(None, 1)
    self.__command = parts[0] if parts else None
    self.__command_string = parts[1].lower() if len(parts) > 1 else None

    self.__text = None
    if self.__command_string:
        self.__text = self.__command_string.partition(' ')[-1]

I used None as the first argument to str.split() to have it split on arbitrary whitespace; this gives you an automatic strip as well.
If you do still need to handle exceptions, do not use a blanket except: block. Catch specific exceptions only, like IndexError for an indexing expression where the list might not be long enough.
Also, I'd recommend against using __ double-underscore names. Use those only if you are aiming to have your class subclassed by 3rd-party code where it is important that the internal implementation is not accidentally affected by whatever the subclass defines.",A,4
9808831,2012-03-21 16:30:25.057000+00:00,"You don't seem to have understood how web applications work. They don't wait for signals - or, rather, that's all they do. Every page served by a Web service is in response to a signal, ie a request. Your web service just needs to respond to normal requests in the normal way. ",A,1
20820757,2013-12-29 00:07:57.647000+00:00,"Yes, if you pass in app to the Redis() constructor you need to have the configuration loaded.
Postpone passing in app, call redis.init_app(app) after loading the configuration:
app = Flask(__name__)

manager = Manager(app)
db = SQLAlchemy()
redis = Redis()

def attach_extensions(app):
    db.init_app(app)
    redis.init_app(app)

then when configuration has loaded:
from myapp import app, manager, attach_extensions

@manager.command
def debug():
    app.config.from_pyfile(""debug.cfg"")
    attach_extensions(app)
    app.run()
",A,3
45058408,2017-07-12 13:00:58.027000+00:00,"Your db container, running Postgres, only exists locally. It isn't deployed to Heroku, and in any case Heroku does not support docker-compose, so the Django container has no knowledge of what ""db"" refers to.
You should use the normal pattern of overriding the database settings via dj-database-url so that in production your app uses the Postgres add-on as specified in the environment variables.",A,1
39612324,2016-09-21 09:17:05.737000+00:00,"This is not a JSON file; this is a file containing individual lines of JSON. You should parse each line individually.
for row in infile:
  data = json.loads(row)
  writer.writerow(data)
",A,0
19551743,2013-10-23 20:33:12.123000+00:00,"Getting back an empty string from s.recv() means the other side called close or shutdown on the connection (or it was interrupted in some other way—the other side quit, or someone ripped out the Ethernet cord).
And looking at your server code, if you say you want 3 guesses, it will try to recv 3 times… but the client will only send once, after which it will close the connection. Which means the next recv on the server will return an empty string, which is exactly what you're seeing.
To fix this, you need to send as many guesses as the other side is expecting. Something like this:
#Make Guesses
for i in range(int(guesses)):
    print(s.recv(1024).decode())
    currentGuess = sys.stdin.readline()
    s.send(currentGuess.encode())
    print(s.recv(80).decode())
s.close()


Meanwhile, although it isn't causing this particular problem, your code can't work reliably. Sockets are byte streams, not message streams. There is no guarantee that each send will match up with a recv on the other side. The recv may get half of a send, or two sends bundled up together. On some platforms, when you're using localhost, and sending small messages, and not sending things too quickly, everything just happens to work out all the time. On other platforms, things work most of the time but occasionally don't. On others, they fail frequently.
On top of that, as the docs explain, send is never guaranteed to send its entire string; that's why it returns the number of bytes actually sent. You can solve this problem by just using sendall, but that won't help with the other problem.
You have to build a message stream on top of the TCP byte stream, by designing and implementing a protocol. That may sound scary, but a very simple protocol, ""All messages end with a newline"", can be implemented just by using the makefile method, like this:
s.connect((""127.0.0.1"", 4007))
f = s.makefile('r+b')

Now, each time you were doing either of these:
foo = s.recv(80)
s.send(bar)

… instead do these:
foo = f.readline().strip()
f.write(bar + '\n')


One last problem that isn't affecting you, but should be fixed:
Calling encode and decode with no argument is almost always a bad idea, especially with network bytes. You're asking Python to decode the data with whatever happens to be in sys.getdefaultencoding(). Can you be sure that the default encoding is the same on the client and server? Can you be sure that it can handle anything the user might type? On a typical Windows machine, it may be something like 'cp1252', meaning that if the user types a Chinese character, the client will quit with a UnicodeError, which isn't very user-friendly. The right thing to do is to explicitly use 'utf-8'. (Which means your protocol is now ""All messages are in UTF-8, and end with a newline."")
If you're using makefile as suggested above, you can solve this even more easily by letting the file object do the encoding for you. Just do this:
f = s.makefile('r+', encoding='utf-8')

Then you can leave out all the decode and encode calls.",A,1
40361091,2016-11-01 13:30:17.827000+00:00,"Your ""subcategory"" URL requires two arguments - category and subcategory - but you are only providing one.",A,2
10825882,2012-05-31 00:04:25.593000+00:00,"It is not generally safe to install this package over the top of Xcode. It was designed for people who want the compiler and its dependencies without having to install Xcode. If you've installed the Command Line Tools for Xcode, it will overwrite many of the files, and probably break all kinds of things.
If you only care about iOS development, you don't need the Command Line Tools for Xcode at all, so it won't matter even if they're completely broken. But in that case, you'd be better off not installing those Command Line Tools, instead of trying to install both.
A much safer alternative is to install a compiler somewhere other than /usr. You can obviously do this by building it yourself, but that's a lot of work. You may want to look into the Brew apple-gcc42 package, which is specifically designed to supplement Xcode 4.2+'s Command Line Tools by adding the missing tools (all in /usr/local/bin instead of /usr/bin, and with the suffix -4.2). Or, if you're already using MacPorts or Fink, try their packages instead. (If you need help telling different packages to use /usr/local/bin/gcc-4.2 instead of /usr/bin/gcc, etc., you should probably ask a new question.)
Another obvious alternative is to use Ruby 1.9.3, which doesn't have this problem, instead of 1.9.2. (And if you don't want to get into the hairy details of building things yourself, Brew's got a package for that, too…)",A,4
24886718,2014-07-22 11:55:09.623000+00:00,"You'll need a custom filter. Something like:
@register.filter
def check_page_perms(page, perms):
    return 'can_access_page_%s' % page.name in perms

and use it:
{% if page|check_page_perms:perms %}
",A,1
51392194,2018-07-18 01:18:48.580000+00:00,"Since you're on macOS, your computer already had a Python 2.7, pre-installed by Apple. If you're on macOS 10.13, it's 2.7.10; older versions of course have older versions.
Meanwhile, you've installed Python 2.7.14. You didn't tell us how—python.org installer, Anaconda, Homebrew, whatever—but that's fine.
The problem is that the Apple Python 2.7.10 is still your ""primary"" 2.7, so you somehow ended up with a pip 9.0.1 that installed its packages for your 2.7.14, but thinks it's supposed to run with the Apple 2.7.10 instead. That's why it's looking in /usr/local/lib/python2.7/site-packages, which is the site-packages for Apple's 2.7.10, not for your 2.7.14. And you either don't have pip for the Apple 2.7.10, or have an older version. Hence the error.

Headaches in dealing with multiple Python installations—especially multiple installations of the same version—are why the Python Packaging User Guide suggests that you:

Use python -m pip to run pip.
Use virtual environments if at all possible.

I don't know how you normally make sure you're running your 2.7.14 instead of Apple's 2.7.10, but whatever command you run, if you do the same thing with a -m pip, it's guaranteed to use your 2.7.14 rather than Apple's 2.7.10. For example, if you normally type python2, use python2 -m pip instead of pip2.
Meanwhile, if you activate a virtual environment, both python and pip (and other things like 2to3) are going to be the versions that go with that environment, no matter what else you happen to have installed and how confusing your overall system setup is.",A,3
21842759,2014-02-18 01:39:36.850000+00:00,"You define the function as taking one parameter:
def finddiscount(discount):

(and later redefine it, but let's ignore that for now, since the second definition also takes one parameter).
But then you call it with no arguments:
finddiscount()

Presumably you wanted this:
finddiscount(quantity)


There are a number of other problems with your code. You shouldn't be defining all of this stuff inside a finddiscount function definition. And you definitely shouldn't be defining a local function named finddiscount inside a global function with the same name.
Also, you usually want your functions to actually return something, not just print a value and then do nothing. (For example, you might want to return the discount, so some later code can apply that discount to the price.)
But this will solve the problem you asked about.",A,1
5085281,2011-02-22 23:27:10.450000+00:00,"This is what ""Mock Objects"" are for.
You can use a mock version of logging which will properly buffer the log messages so that you can later make assertions about them.",A,2
14122989,2013-01-02 13:30:12.650000+00:00,"The immediate cause of your error message is because you're passing a list object to the Django serializer, but it's expecting a QuerySet.
What I would do is to build up the data structure as a series of nested Python dictionaries, and then convert it all to JSON at the end with json.dumps(). (Note that's the actual built-in json library, which you've shadowed with the serializer.) Something like (untested):
serializer = serializers.get_serializer('python')
vaccine_list = serializer.serialize(vaccines)
for i, v in enumerate(vaccines):
    diseases = v.diseases.all()
    disease_list = serializer.serialize(diseases)
    vaccine_list[i]['fields']['diseases'] = disease_list
data = json.dumps(vaccine_list)
",A,2
33260256,2015-10-21 13:09:53.150000+00:00,"I think it's the other fields that are the issue here. Their values are empty strings, not the string ""None"":
payload = {
    ""list_class_values[notice][zipstate]"": """",
    ""list_class_values[notice][type][]"": """",
    ""list_class_values[notice][keywords]"": ""colorado""
}

The form field names are otherwise correct; the syntax is a convention used by Ruby on Rails and PHP, but is otherwise not a standard. Servers that support the syntax parse the keys out into array maps (dictionaries in Python terms). See Form input field names containing square brackets like field[index]
Note that you need to pass this to the data argument for a POST body (there is no payload keyword argument, you should get an exception):
r = requests.post(url='http://www.example.com', data=payload, headers=headers)
",A,1
28840367,2015-03-03 19:32:54.277000+00:00,"You are not producing a list of lists. You are resetting self.newLists each loop iteration, to a single list with 3 elements:
for line in g:
    matrix = line.split()
    JD = matrix[2]
    mintime = matrix[5]
    maxtime = matrix[7]
    self.newLists = [JD, mintime, maxtime]

You need to instead use list.append() to add those 3 elements to a list you set once, outside of the loop:
self.newLists = []
for line in g:
    matrix = line.split()
    JD = matrix[2]
    mintime = matrix[5]
    maxtime = matrix[7]
    self.newLists.append([JD, mintime, maxtime])

Your simplify method is adding the individual characters of mintime to your output lists:
for sub in self.newLists:
    date = sub[0]
    if date not in dates:
        dates[date] = []
    dates[date].extend(sub[1])

You want to use list.append() there, not list.extend(). That loop can be simplified using dict.setdefault() rather than test for the key manually:
for date, mintime, maxtime in self.newLists:
    dates.setdefault(date, []).append(mintime)
",A,2
49620349,2018-04-02 23:57:36.987000+00:00,"The main problem with your code is that you're not actually sending the strings.
You've defined a structure that includes two c_wchar_p members—or, in C terms, wchar_t * pointers. You send those pointers over, and but never send the data they're pointing to. That can't possibly work.
You can't just send C structs containing pointers in any language. You have to write some higher-level protocol that includes the actual strings instead of the pointers in some way, and then write the code that serializes and deserializes to the struct. Which is much easier to do with functions like struct.pack instead of around a ctypes.Structure. And even easier to do on top of a higher-level protocol like netstrings, and even easier if you just use a text-based protocol with some human-readable framing, like newline-escaped JSON texts with newlines as delimiters.

If you really want a binary protocol based on just dumping fixed-sized structures to the network, your structs have to be fixed-sized and self-contained. For example, if your packet_type were always at most 4 characters, and your sensor_name at most 30 characters, and it were acceptable to waste space for shorter names, you could do this:
class app_packet(Structure):
    _fields_ = [('packet_type',c_wchar*4),
                ('sensor_name',c_wchar*30),
                ('value',c_float)]

Now the characters are embedded directly in the structure, so it will work.

Except that it won't really work, because your data types aren't network-portable. A wchar_t can be either 2 bytes or 4 bytes—not just between different platforms, but even between binaries built with different compilers or flags on the same platform. (Plus, they're of course native-endian.) If you really want embedded 2-byte or 4-byte strings, you have to be explicit about it: use c_uint16 or c_uint32, encode with s.encode('utf-16') or s.encode('utf-32'), then either memcpy or cast and slice-copy. But then of course they're not strings within your code until you pull them out, cast them back, and decode them, at which point you might as well be using a proper protocol in the first place.

Also, it's still not clear why you'd ever want handshake data to be stored in a structure like this in the first place. Why not just pass it around as a tuple (or namedtuple or normal class) with two strings and a float, and serialize/deserialize right as it goes over/comes in from the network.
You mentioned in a comment that you need them to be mutable, but that doesn't explain it; it makes even less sense that you'd want mutable handshake data. And besides, you can trivially just make a new tuple with different strings instead of having string-like members that you can mutate in place.",A,0
54494227,2019-02-02 14:58:38.167000+00:00,"You should not convert bytes objects to strings with str(bytes_value). You created a printable representation of the object.
The proper way to convert from bytes to str is to decode the bytes to Unicode. If you have UTF-8 bytes, decode with that codec with the bytes.decode() method:
string_value = bytes_value.decode('utf8')

You can also specify the encoding if you want to use the str() function, see the  str(bytes_value, encoding) form in the documentation:
string_value = str(bytes_value, 'utf8')

If you accidentally used str(bytes_value) and can't now get the original value by fixing that error and re-running your code, you can recover the original value by using ast.literal_eval():
bytes_representation = str(bytes_value)  # ""b'....'""
recovered_bytes_value = ast.literal_eval(bytes_representation)

This should only be used to recover data, not as a production-level serialisation mechanism. ast.literal_eval() is quite slow, and not safe from denial-of-service attacks when used on user-supplied input (it is possible to crash Python or significantly slow it down with bad input).
If you are using an API that should really work on bytes but for some reason is only accepting strings (usually a warning sign of a incorrectly designed and implemented API), then either use a binary-to-ASCII encoding (including base64 / base16 / base32 / base85) or decode the binary data as Latin-1.
This is even more important if you are doing this to encrypt data. The printable representation of a bytes() object only ever uses ASCII characters, always starts with b' or b"", and always ends in ' or "". Any non-printable bytes (more than half of all 256 possible byte values) are represented with a limited range of \x{hh} and \{l} escape sequences. All this makes it much easier for a determined attacker to break your encryption. A best-practices encryption library will let you encrypt bytes directly. In fact, it is usually preferred to encrypt bytes.",A,1
9109127,2012-02-02 08:23:50.643000+00:00,"You could use subprocess:
from subprocess import check_output as qx

output = qx(['sqlite3', 'foo.db', 'select * from sqlite_master'])
print output

Or sqlite3 module:
import sqlite3

conn = sqlite3.connect('foo.db')
for row in conn.execute('select * from sqlite_master'):
    print row

If you still want to use envoy then you could fix it as:
import envoy

r = envoy.run([[""sqlite3"", ""foo.db"", ""select * from sqlite_master""]])
print r.std_out
",A,1
24289613,2014-06-18 15:33:09.660000+00:00,"You are missing a closing parenthesis on a preceding line:
sum = int(raw_input('What is the next score? ')
#        ^         ^                          ^^ ?
#        |          \-------- matched -------/ |
#         \-------------- missing --------------/

You do so again later on in the program:
sum = sum + int(raw_input('What is the next score? ')
#              ^         ^                          ^^ ?
#              |          \-------- matched -------/ |
#              \-------------- missing --------------/

You may want to take a look at Asking the user for input until they give a valid response for tips on how to handle user input better.",A,1
27453084,2014-12-12 22:26:22.553000+00:00,"You can use the precision field to set a maximum field width:
formatter = logging.Formatter('%(levelname).1s %(message)s')

.1 sets the field width to at most one character, truncating the level to the first character:
>>> for level in ('CRITICAL', 'ERROR', 'INFO', 'WARNING', 'DEBUG'):
...     print '%(level)-.1s %(message)s' % {'level': level, 'message': 'Hello world!'}
... 
C Hello world!
E Hello world!
I Hello world!
W Hello world!
D Hello world!

See the String Formatting Operations documentation:

Conversion: 's'
Meaning: String (converts any Python object using str()).
Notes: (6)

[...] The precision determines the maximal number of characters used.

",A,21
51236320,2018-07-08 22:02:47.837000+00:00,"First, since your goal is to build a dict, just build the dict on the fly, instead of building a bunch of separate variables and then putting them in a dict at the end.
You can also use a Counter instead of a plain dict so you don't need to worry about checking whether the color is already there.
While we're at it, there's no need to call str on something that's already a string, and you've got a bunch of unnecessary parens all over the place.
So:
from collections import Counter
dictfordf = Counter()
dictfordf['key'] = 40006
for i in range(len(testlist)-1):
    if testlist[i] == testlist[i+1]:
        print(""No Change"")
    else:
        print(""Change to: "" + testlist[i+1])
        dictfordf[testlist[i+1]] += 1

It's a little hacky to store a value for 'key' that really isn't a count, so you might want to consider using a defaultdict, or setdefault on a normal dict, instead. But I don't think it's too bad.
Of course if 'key' could be one of the elements in testlist, this is going to increment the key. But then if that's possible, it's not clear what should happen in that case, so it's not clear how you'd want to fix it.

Meanwhile, you can make things a little concise by iterating over adjacent pairs. See the pairwise recipe in the itertools docs. But of course this adds the definition of pairwise to your code (or you can import it from a third-party lib like more-itertools or toolz).
So:
from collections import Counter
from itertools import tee

def pairwise(iterable):
    ""s -> (s0,s1), (s1,s2), (s2, s3), ...""
    a, b = tee(iterable)
    next(b, None)
    return zip(a, b)

dictfordf = Counter()
dictfordf['key'] = 40006
for prev, current in pairwise(testlist):
    if prev == current:
        print(""No Change"")
    else:
        print(""Change to: "" + current)
        dictfordf[current] += 1


You can abstract things further by using either groupby, or the unique_justseen recipe from itertools. I think this will obscure rather than clarify where you print the outputs—but, assuming you understand the pairwise version, it's worth reading up on both of them, and trying to write both alternatives, at least as an exercise.",A,4
33843862,2015-11-21 13:23:44.613000+00:00,"The range(0, iterations) object produces integers, not tuples. You are asking the for loop to unpack each integer into two variables:
for this_index, this_object in range(0, iterations):

Unpacking an integer fails:
>>> this_index, this_object = 0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'int' object is not iterable

You can just iterate over the indices and extract objects in a second:
for this_index in range(iterations):
    this_object = objects_clone[this_index]
    # ...

If you wanted to enumerate over all objects except the last, perhaps you wanted to slice to ignore the last element:
for this_index, this_object in enumerate(objects_clone[:-1]):
    # ...
",A,1
50768146,2018-06-08 20:42:59.933000+00:00,"The feature you're asking for is called forward (type) references, and it has been added to Python as of 3.7 (in PEP 563).1 So this is now valid:
from __future__ import annotations
class C:
    def spam(self, other: C) -> C:
        pass

Notice the __future__ statement. This will be necessary until 4.0.
Unfortunately, in Python 3.6 and earlier, this feature is not available, so you have to use string annotations, as explained in Jim Fasarakis Hilliard's answer.
Mypy already supports forward declarations, even when run under Python 3.6—but it doesn't do you much good if the static type checker says your code is fine but the interpreter raises a NameError when you try to actually run it.

1. This was already discussed as a possible feature in PEP 484, but deferred until later, after people had more experience using forward declarations in annotations. PEP 563/Python 3.7 is that ""later"".",A,2
12691407,2012-10-02 13:39:30.200000+00:00,"You could use Python Launcher for Windows to run easily your_python_script.py using different Python version.
Also pylauncher allows you to indicate in the script what Python version it expects (in the #!-line (shebang)) and run it automatically.",A,1
15505487,2013-03-19 16:44:50.727000+00:00,"Read and write the data with the csv module, and keep track of when the 'next' group starts:
import csv

def grouped(reader):
    start = end = next(reader)
    print start, end
    for row in reader:
        if int(row[1]) - 1 != int(end[1]):
            yield (start, end)
            start = end = row
        else:
            end = row
    yield (start, end)

with open('inputfile.csv', 'rb') as inf, open('outputfile.csv', 'wb') as outf:
    inputcsv = csv.reader(inf, delimiter='\t')
    outputcsv = csv.writer(outf, delimiter='\t')
    for start, stop in grouped(inputcsv):
        outputcsv.writerow(start + stop[1:])

This writes:
1   2345    2348
1   2412    2414

to outputfile.csv for your input.
This solution never keeps more than 3 rows of data in memory, so you should be able to throw any size of CSV file at it.",A,2
29219921,2015-03-23 20:39:16.663000+00:00,".sendline() is equivalent to .sendline('') that is equivalent to .send('') + .send(self.linesep) that is likely has the same effect as .send('\n') ('\n' == '\012', '\r' == '\015'). You could also try .send('\r\n') (I'm not sure who is responsible for the terminal line discipline).",A,0
14107094,2013-01-01 00:45:27.143000+00:00,"If your professor wants you to invent your own LCS algorithm, you're done. Your algorithm is not the most optimal one ever created, but it's in the right complexity class, you clearly understand it, and you clearly didn't copy your implementation from the internet. You might want to be prepared to defend your algorithm, or discuss alternatives. If I were your prof, I'd give you an A if:

You turned in that program.
You were able to explain why there's no possible O(N) or O(N log M) alternative.
You were able to participate in a reasonable discussion about other algorithms that might have a better lower bound (or significantly lower constants, etc.), and the time/space tradeoffs, etc., even if you didn't know the outcome of that discussion in advance.

On the other hand, if your professor wants you to pick one of the well-known algorithms and write your own implementation, you probably want to use the standard LP algorithm. It's a standard algorithm for a reason—which you probably want to read up on until you understand. (Even if it isn't going to be on the test, you're taking this class to learn, not just to impress the prof, right?)
Wikipedia has pseudocode for a basic implementation, then English-language descriptions of common optimizations. I'm pretty sure that writing your own Python code based on what's on that page wouldn't count as plagiarism, or even as a trivial port, especially if you can demonstrate that you understand what your code is doing, and why, and why it's a good algorithm. Plus, you're writing it in Python, which has much better ways to memoize than what's demonstrated in that article, so if you understand how it works, your code should actually be substantially better than what Wikipedia gives you.
Either way, as I suggested in the comments, I'd read A survey of longest common subsequence algorithms by Bergroth, Hakonen, and Raita, and search for similar papers online.",A,10
1106928,2009-07-09 23:17:23.193000+00:00,"This is something you usually do in your view functions.
aList = [""a"", ""b"", ""c""]
listAndFlags = [ (item,item in aList) for item in someQuerySet ]

Now you have a simple two-element list that you can display
{% for item, flag in someList %}
    <tr><td class=""{{flag}}"">{{item}}</td></tr>
{% endfor %}
",A,2
40727674,2016-11-21 19:15:04.157000+00:00,"The **{0: 'constant', 1:'lambert'} passes in two default key-value pairs as keyword arguments to the defaultdict() constructor. However, the same constructor would treat a dictionary as the second argument as defaults too.
As such, the ** can be omitted altogether, in both 2 and 3:
collections.defaultdict(lambda: 'blinn', {0: 'constant', 1:'lambert'})

Demo in Python 2, showing you get the same output:
>>> import sys
>>> sys.version_info
sys.version_info(major=2, minor=7, micro=12, releaselevel='final', serial=0)
>>> import collections
>>> collections.defaultdict(lambda: 'blinn', **{0: 'constant', 1:'lambert'})  == collections.defaultdict(lambda: 'blinn', {0: 'constant', 1:'lambert'})
True
>>> collections.defaultdict(lambda: 'blinn', **{0: 'constant', 1:'lambert'})
defaultdict(<function <lambda> at 0x102426410>, {0: 'constant', 1: 'lambert'})
>>> _[0]
'constant'
>>> collections.defaultdict(lambda: 'blinn', {0: 'constant', 1:'lambert'})
defaultdict(<function <lambda> at 0x102426410>, {0: 'constant', 1: 'lambert'})
>>> _[0]
'constant'

and in Python 3:
>>> import sys
>>> sys.version_info
sys.version_info(major=3, minor=6, micro=0, releaselevel='beta', serial=3)
>>> import collections
>>> collections.defaultdict(lambda: 'blinn', {0: 'constant', 1:'lambert'})
defaultdict(<function <lambda> at 0x10e3589d8>, {0: 'constant', 1: 'lambert'})
>>> _[0]
'constant'
",A,2
12551565,2012-09-23 10:29:50.167000+00:00,"You want to return your personId value; printing it to the console won't help in a web application:
@app.route('/getpersonbyid', methods = ['POST'])
def getPersonById():
    personId = int(request.form['personId'])
    return str(personId)  # back to a string to produce a proper response

The parenthesis around int are meaningless noise, btw, they are not needed in Python and are in this case ignored by the parser.
I'm assuming you are doing something meaningful with the personId value in the view; otherwise using int() on the value is a little pointless.
You can also use the MultiDict.get() method to have the value converted for you:
personId = request.form.get('personId', type=int)

Now personId will be set to an integer, or None if the field is not present in the form or cannot be converted to an integer.",A,49
51938335,2018-08-20 20:53:55.330000+00:00,"It is possible to do what you want to do, as long as you're willing to rely on multiple undocumented internals of your Python interpreter (in my case, CPython 3.7)—but it isn't going to do you any good.

The iterator is not exposed to locals, or anywhere else (not even to a debugger). But as pointed out by Patrick Haugh, you can get at it indirectly, via get_referrers. For example:
for ref in gc.get_referrers(seq):
    if isinstance(ref, collections.abc.Iterator):
        break
else:
    raise RuntimeError('Oops')

Of course if you have two different iterators to the same list, I don't know if there's any way you can decide between them, but let's ignore that problem.

Now, what do you do with this? You've got an iterator over seq, and… now what? You can't replace it with something useful, like an itertools.chain(seq, [1, 2, 3]). There's no public API for mutating list, set, etc. iterators, much less arbitrary iterators.
if you happen to know it's a list iterator… well, the CPython 3.x listiterator does happen to be mutable. The way they're pickled is by creating an empty iterator and calling __setstate__ with a reference to a list and an index:
>>> print(ref.__reduce__())
(<function iter>, ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],), 7)
>>> ref.__setstate__(3) # resets the iterator to index 3 instead of 7
>>> ref.__reduce__()[1][0].append(10) # adds another value

But this is all kind of silly, because you could get the same effect by just mutating the original list. In fact:
>>> ref.__reduce__()[1][0] is seq
True

So:
lst = list(range(10))
for elem in lst:
  print(elem, end=' ')
  if elem % 2:
    lst.append(elem * 2)
print()

… will print out:
0 1 2 3 4 5 6 7 8 9 2 6 10 14 18 

… without having to monkey with the iterator at all.

You can't do the same thing with a set.
Mutating a set while you're in the middle of iterating it will affect the iterator, just as mutating a list will—but what it does is indeterminate. After all, sets have arbitrary order, which is only guaranteed to be consistent as long as you don't add or delete. What happens if you add or delete in the middle? You may get a whole different order, meaning you may end up repeating elements you already iterated, and missing ones you never saw. Python implies that this should be illegal in any implementation, and CPython does actually check it:
s = set(range(10))
for elem in s:
  print(elem, end=' ')
  if elem % 2:
    s.add(elem * 2)
print()

This will just immediately raise:
RuntimeError: Set changed size during iteration

So, what happens if we use the same trick to go behind Python's back, find the set_iterator, and try to change it?
s = {1, 2, 3}
for elem in s:
    print(elem)
    for ref in gc.get_referrers(seq):
        if isinstance(ref, collections.abc.Iterator):
            break
    else:
        raise RuntimeError('Oops')
    print(ref.__reduce__)

What you'll see in this case will be something like:
2
(<function iter>, ([1, 3],))
1
(<function iter>, ([3],))
3
(<function iter>, ([],))

In other words, when you pickle a set_iterator, it creates a list of the remaining elements, and gives you back instructions to build a new listiterator out of that list. Mutating that temporary list obviously has no useful effect.

What about a tuple? Obviously you can't just mutate the tuple itself, because tuples are immutable. But what about the iterator?
Under the covers, in CPython, tuple_iterator shares the same structure and code as listiterator (as does the iterator type that you get from calling iter on an ""old-style sequence"" type that defines __len__ and __getitem__ but not __iter__). So, you can do the exact same trick to get at the iterator, and toreduce` it.
But once you do, ref.__reduce__()[1][0] is seq is going to be true again—in other words, it's a tuple, the same tuple you already had, and still immutable.",A,4
45192260,2017-07-19 13:35:37.757000+00:00,"No, that's not how it works. You need to namespace the templates, even if they're inside an app.
--app
----templates
------app
--------index.html

or, just keep the single project-level templates file, but still use namespaces:
--project
----templates
------index.html
------app
--------index.html

Either way, you now reference the template as ""app/index.html"".
Note, you should really use the render shortcut:
def index(request):
    context = {}
    return render(request, 'app/index.html', context)

Also note, it's not usual to have a project-level views.py. Normally that would be inside an app.",A,1
23301182,2014-04-25 19:11:46.120000+00:00,"It's trivial to create one with the @contextlib.contextmanager decorator:
from contextlib import contextmanager

@contextmanager
def dummy_manager(ob):
    yield ob

That's it; this creates a context manager that does nothing but hand you ob back, and the __exit__ handler does exactly nothing.
I'd use it like this:
f = open(args.infile, ""r"") if args.infile else dummy_manager(sys.stdin)
with f:
    process(f)
",A,2
33281322,2015-10-22 12:41:55.967000+00:00,"Your tuple has only two elements. Just reference those directly:
list1 = x[0]
list2 = x[1]

or
list1, list2 = x

This creates additional references to the two lists contained in x, not copies. If you need new list objects with the same contents, create copies:
list1, list2 = x[0][:], x[1][:]

See How to clone or copy a list?",A,2
45689675,2017-08-15 09:00:34.180000+00:00,"Don't use any here. A generator isn't the right tool either. You actually want a list comprehension to collect all the matching words.
matching = [word for word in bannedWords if word in mystring]
if matching:
    print (""BANNED WORD(S) DETECTED : "", ','.join(matching))
",A,3
28651206,2015-02-21 20:53:44.167000+00:00,"variants is presumably a list. In python you use square brackets to index a list:
if product.variants[0].inventory_management:
",A,1
246093,2008-10-29 08:08:23.503000+00:00,"See How would you display an array of integers as a set of ranges? (algorithm)
My answer to the above question:
void ranges(int n; int a[n], int n)
{
  qsort(a, n, sizeof(*a), intcmp);
  for (int i = 0; i < n; ++i) {
    const int start = i;
    while(i < n-1 and a[i] >= a[i+1]-1)
      ++i;
    printf(""%d"", a[start]);
    if (a[start] != a[i])
      printf(""-%d"", a[i]);
    if (i < n-1)
      printf("","");
  }
  printf(""\n"");
}
",A,3
41620482,2017-01-12 18:33:39.320000+00:00,"Rather than use a decorator, you'd want to create a context manager. A context manager is told when a block of code (the context) exits, so you can clean up after yourself at that point.
Ironically, there is a decorator that makes it easy to write a context manager, called @contextlib.contextmanager:
from contextlib import contextmanager

@contextmanager
def _persistent_temp_file(suffix=None, prefix=None, dir=None, text=False):
    fd = None
    try:
        fd, temp_file = tempfile.mkstemp(suffix=suffix, prefix=prefix, dir=dir, text=text)
        yield temp_file
    finally:
        if fd is not None:
            os.close(fd)

Note the yield temp_file line; that's the point at which this function is paused, and the temp_file value is then returned from the resulting context manager __enter__ method, and becomes available to the as clause.
Then use this in a with statement:
def write_zip(test_name: str) -> str:
    with _persistent_temp_file() as zip_path:
        with zipfile.ZipFile(zip_path, 'w') as zf:
            for fpath in _list_files(test_name):  # _list_files() not shown
                zf.write(fpath, arcname=os.path.basename(fpath))
    return zip_path

Not that you need to re-invent the persistent-temp-file wheel here. You could just use the tempfile.NamedTempFile() object here instead:
from tempfile import NamedTempFile

def write_zip(test_name: str) -> str:
    with NamedTempFile(delete=False) as temp_file:
        with zipfile.ZipFile(temp_file, 'w') as zf:
            for fpath in _list_files(test_name):  # _list_files() not shown
                zf.write(fpath, arcname=os.path.basename(fpath))
    return temp_file.name
",A,1
50483863,2018-05-23 08:49:54.427000+00:00,"You can't have that with an abstract model. A ForeignKey needs to point to an ID in an actual table, and an abstract model does not represent a table.
There are two options for achieving what you want. The first is to use a concrete base model - which has the disadvantage of an extra database join each time to get the extended data. The second is to use generic relations rather than a foreign key.",A,1
51913324,2018-08-18 23:39:54.383000+00:00,"First, you're not importing a timezone module, you're importing the timezone class out of the datetime module. So, trying to install a timezone module, even if there were such a thing, wouldn't help anything.
Meanwhile, as the docs say, the timezone class was added in Python 3.2, so you don't have it in 2.7.
There used to be a backport of the datetime module from around Python 3.4 to 2.7, which would have solved your problem, but it seems to have been abandoned years ago.

So, that leaves modifying your code so that it doesn't need timezone.
Fortunately, the only instance of timezone you're ever using is the UTC one, and that's pretty easy to work around.
A timezone class is just a pre-built implementation of a tzinfo class. In 2.x, you have to implement your own tzinfo class. But the trivial class UTC example in the docs happens to be exactly what you want:
ZERO = timedelta(0)
HOUR = timedelta(hours=1)

class UTC(tzinfo):
    """"""UTC""""""

    def utcoffset(self, dt):
        return ZERO

    def tzname(self, dt):
        return ""UTC""

    def dst(self, dt):
        return ZERO

utc = UTC()

Now, everywhere you used timezone.utc, just use utc instead, and you're done.",A,0
17460043,2013-07-04 00:10:23.793000+00:00,"The obvious way to do this is with datetime. But you apparently want to avoid that for some strange reason. Well, you can use time or calendar, or various third-party libraries, or custom code instead.
Here it is with time:
import time

def yesterday():
    now = time.time()
    yesterday = time.localtime(now - 86400) # seconds/day
    start = time.struct_time((yesterday.tm_year, yesterday.tm_mon, yesterday.tm_mday,
                              0, 0, 0, 0, 0 yesterday.tm_isdst))
    today = time.localtime(now)
    end = time.struct_time((today.tm_year, today.tm_mon, today.tm_mday,
                            0, 0, 0, 0, 0 today.tm_isdst))
    return time.mktime(start), time.mktime(end)

If you run this during a leap second, on a platform that tracks leap seconds, it will give today instead of yesterday. You can check for that easily (basically, if today == yesterday, subtract another day), but I don't think it's worth it.
Also, if you run this during DST crossover in a timezone where the crossover happens between midnight and 01:00 or 23:00 (depending on your hemisphere), it will get the wrong day. For example, in Brazil, if you ran this code during the second 23:00-00:00 hour on 16 February 2013, it would return the start of the day that includes the time 24 hours ago… which is today, rather than yesterday. The same workaround works here.",A,1
39757681,2016-09-28 21:07:37.703000+00:00,"The specific exception is caused by trying to store a unicode value in a date datatype that is part of a struct. The conversion of the Python type to Spark internal representation expected to be able to call date.toordinal() method.
Presumably you have a dataframe schema somewhere that consists of a struct type with a date field, and something tried to stuff a string into that.
You can trace this based on the traceback you do have. The Apache Spark source code is hosted on GitHub, and your traceback points to the pyspark/sql/types.py file. The lines point to the StructField.toInternal() method, which delegates to the self.dataType.toInternal() method:
class StructField(DataType):
    # ...
    def toInternal(self, obj):
        return self.dataType.toInternal(obj)

which in your traceback ends up at the DateType.toInternal() method:
class DateType(AtomicType):
    # ...
    def toInternal(self, d):
        if d is not None:
            return d.toordinal() - self.EPOCH_ORDINAL

So we know this is about a date field in a struct. The DateType.fromInternal() shows you what Python type is produced in the opposite direction:
def fromInternal(self, v):
    if v is not None:
        return datetime.date.fromordinal(v + self.EPOCH_ORDINAL)

It is safe to assume that toInternal() expects the same type when converting in the other direction.",A,5
28697827,2015-02-24 14:03:38.340000+00:00,"Your original code uses the fact that it is the name __metaclass__ in the class body is used as the meta class, but the 2to3 fixer only looks for straight assignments:
__metaclass__ = MetaClassName

rather than a class __metaclass__ statement or other manner of defining the name (from somemodule import MetaClassName as __metaclass__ would work in a Python 2 class body and 2to3 would miss that too).
You can fix this by moving the meta classes to separate class definitions:
class BaseMeta(type):
    def __new__(cls, classname, bases, dict):
        new = type.__new__(cls, classname, bases, dict)
        new.classname = classname
        print (""BaseMeta::new. Called."")
        return new                 

class Base(object):
    __metaclass__ = BaseMeta

class HeirMeta(BaseMeta):
    def __new__(self, *args):
        new = BaseMeta.__new__(self, *args)
        print (""HeirMeta::new. Called."")
        return new

class Heir(Base):    
    __metaclass__ = HeirMeta

    @classmethod
    def define(cls, nexttype):
        print (""Heir::define. Called."")

class HeirOfHeir(Heir):
    pass

Heir.define(HeirOfHeir)

You'll have to do this to define metaclasses in Python 3 anyway, as the mechanism to define metaclasses was changed to determining the metaclass before the class body is run rather than during (so that a metaclass can influence that step too).
Now 2to3 will correctly detect that there is a __metaclass__ attribute on your classes and rewrite those to use the new Python 3 syntax:
stackoverflow-2.7 $ bin/python -m lib2to3 fixed.py 
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
RefactoringTool: Refactored fixed.py
--- fixed.py    (original)
+++ fixed.py    (refactored)
@@ -5,8 +5,8 @@
         print (""BaseMeta::new. Called."")
         return new                 

-class Base(object):
-    __metaclass__ = BaseMeta
+class Base(object, metaclass=BaseMeta):
+    pass

 class HeirMeta(BaseMeta):
     def __new__(self, *args):
@@ -14,9 +14,7 @@
         print (""HeirMeta::new. Called."")
         return new

-class Heir(Base):    
-    __metaclass__ = HeirMeta
-
+class Heir(Base, metaclass=HeirMeta):    
     @classmethod
     def define(cls, nexttype):
         print (""Heir::define. Called."")
RefactoringTool: Files that need to be modified:
RefactoringTool: fixed.py

and the refactored code works as expected:
stackoverflow-2.7 $ bin/python -m lib2to3 -o ../stackoverflow-3.4 -nw --no-diffs fixed.py 
lib2to3.main: Output in '../stackoverflow-3.4' will mirror the input directory '' layout.
RefactoringTool: Skipping implicit fixer: buffer
RefactoringTool: Skipping implicit fixer: idioms
RefactoringTool: Skipping implicit fixer: set_literal
RefactoringTool: Skipping implicit fixer: ws_comma
RefactoringTool: Refactored fixed.py
RefactoringTool: Writing converted fixed.py to ../stackoverflow-3.4/fixed.py.
RefactoringTool: Files that were modified:
RefactoringTool: fixed.py
stackoverflow-2.7 $ cd ../stackoverflow-3.4
stackoverflow-3.4 $ bin/python -V
Python 3.4.2
stackoverflow-3.4 $ bin/python fixed.py 
BaseMeta::new. Called.
BaseMeta::new. Called.
HeirMeta::new. Called.
BaseMeta::new. Called.
HeirMeta::new. Called.
Heir::define. Called.
",A,8
37506620,2016-05-29 05:44:26.297000+00:00,"If both Postman and requests are receiving an error, then there is more context than what HttpRequester is showing. There are a number of headers that I'd expect to be set almost always, including User-Agent and Content-Length, that are missing here.
The usual suspects are cookies (look for Set-Cookie headers in earlier requests, preserve those by using a requests.Session() object), the User-Agent header and perhaps a Referrer header, but do look for other headers like anything starting with Accept, for example.
Have HttpRequester post to http://httpbin.org/post instead for example, and inspect the returned JSON, which tells you what headers were sent. This won't include cookies (those are domain-specific), but anything else could potentially be something the server looks for. Try such headers one by one if cookies are not helping.",A,0
14041003,2012-12-26 13:06:01.663000+00:00,"There are no <div class=""top""> elements with text on that page, so items is an empty list. Remove the text=True filter:
items = soup.findAll('div', {""class"": ""top""})

and extract all text from it:
item_text = u' '.join(item.stripped_strings)
if textcontent and item_text:            
    spamwriter.writerow([time.strftime(""%Y-%m-%d""),time.strftime(""%B""),time.strftime(""%A"") , item_text, textcontent])

or, integrated into your existing code:
with open('TMO_DE_2012-12-26.csv', 'wb') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter=',')
    spamwriter.writerow([""Date"",""Month"",""Day of Week"",""Device Name"",""Price""])
    items = soup.findAll('div', {""class"": ""top""})
    prices = soup.findAll('strong', {""class"": ""preis-block""})
    for item, price in zip(items, prices):
        textcontent = u' '.join(price.stripped_strings)
        item_text = u' '.join(item.stripped_strings)
        if item_text and textcontent:            
            spamwriter.writerow([time.strftime(""%Y-%m-%d""),time.strftime(""%B""),time.strftime(""%A""), item_text.encode('utf8'),textcontent.encode('utf8')])
",A,0
14006773,2012-12-22 21:36:36.780000+00:00,"You didn't copy the code correctly; linked page has this line:
gpsd = gps(mode=WATCH_ENABLE) #starting the stream of info

Note that it's just gps(), not gps.gps(); at the top of the script all names from the gps module are imported into the current namespace, making the gps() a local name.
Make sure you do have the statement from gps import * at the top of your script, from the error message it appears that you did not import it correctly (the NameError indicates that there is nothing named gps imported in your script).",A,2
17402833,2013-07-01 10:57:56.470000+00:00,"You are opening a relative path, but your working directory is not what you think it is.
Use an absolute path instead:
train = os.path.join('c:/Documents and Settings', train)

Without an absolute path, Python uses the current working directory. What that directory is depends on how you ran your script, and is not something you should rely on.",A,5
10445463,2012-05-04 08:36:48.153000+00:00,"You can't do that. Use normal database fields for structured data, not JSON blobs.
If you need to search on JSON data, consider using a noSQL database like MongoDB.",A,0
30775818,2015-06-11 08:44:06.593000+00:00,"You have 5 codepoints. One of those codepoints is outside of the Basic Multilingual Plane which means the UTF-16 encoding for those codepoints has to use two code units for the character.
In other words, the client is relying on an implementation detail, and is doing something wrong. They should be counting codepoints, not codeunits. There are several platforms where this happens quite regularly; Python 2 UCS2 builds are one such, but Java developers often forget about the difference, as do Windows APIs.
You can encode your text to UTF-16 and divide the number of bytes by two (each UTF-16 code unit is 2 bytes). Pick the utf-16-le or utf-16-be variant to not include a BOM in the length:
title = u'test'
len_in_codeunits = len(title.encode('utf-16-le')) // 2

If you are using Python 2 (and judging by the u prefix to the string you may well be), take into account that there are 2 different flavours of Python, depending on how you built it. Depending on a build-time configuration switch you'll either have a UCS-2 or UCS-4 build; the former uses surrogates internally too, and your title value length will be 6 there as well. See Python returns length of 2 for single Unicode character string.",A,6
28164422,2015-01-27 06:22:02.617000+00:00,"tzlocal module finds pytz timezone that corresponds to your OS local timezone on *nix and Win32:
from datetime import datetime
from tzlocal import get_localzone # $ pip install tzlocal

print(datetime.now(get_localzone()))
# -> 2015-01-27 07:20:52.163408+01:00

Your distribution may patch pytz module to use OS tz database instead of embed one (e.g., Ubuntu does it).",A,1
19191796,2013-10-04 22:33:22.147000+00:00,"There is no need to create a dynamic number of arguments. Use one instead; it is assigned the whole row and you can then use indexes to access items:
for row in ws.iter_rows():
    # row is now a tuple
    first_value = row[0]
",A,5
17152507,2013-06-17 16:35:37.977000+00:00,"You have non-unique labels; you can use a defaultdict to generate numbers on first access, combined with a counter:
from collections import defaultdict
from itertools import count
from functools import partial

label_to_number = defaultdict(partial(next, count(1)))
[(label_to_number[label], label) for label in labels]

This generates a count in order of the labels first occurrence in labels.
Demo:
>>> labels = [""brown"", ""black"", ""blue"", ""brown"", ""brown"", ""black""]
>>> label_to_number = defaultdict(partial(next, count(1)))
>>> [(label_to_number[label], label) for label in labels]
[(1, 'brown'), (2, 'black'), (3, 'blue'), (1, 'brown'), (1, 'brown'), (2, 'black')]

Because we are using a dictionary, the label-to-number lookups are constant cost, so the whole operation will take linear time based on the length of the labels list.
Alternatively, use a set() to get unique values, then map these to a enumerate() count:
label_to_number = {label: i for i, label in enumerate(set(labels), 1)}
[(label_to_number[label], label) for label in labels]

This assigns numbers more arbitrarily, as set() objects are not ordered:
>>> label_to_number = {label: i for i, label in enumerate(set(labels), 1)}
>>> [(label_to_number[label], label) for label in labels]
[(2, 'brown'), (3, 'black'), (1, 'blue'), (2, 'brown'), (2, 'brown'), (3, 'black')]

This requires looping through labels twice though.
Neither approach requires you to first define a dictionary of labels; the mapping is created automatically.",A,7
15118777,2013-02-27 17:47:33.567000+00:00,"A generator function can be less efficient than building a list directly.
You can just build the list in the holes_between() function and return that:
def holes_between(intervals):
    prec = intervals[0][1] + 1 # Bootstrap the iteration
    result = []
    for low, high in intervals[1:]:
        if prec <= low - 1:
            result.append((prec, low - 1))
        prec = high + 1
    return result

but do measure the differences, using the timeit module.
If you have some typical input, you'd test that with:
import timeit

def holes_between_list(intervals):
    prec = intervals[0][1] + 1 # Bootstrap the iteration
    result = []
    for low, high in intervals[1:]:
        if prec <= low - 1:
            result.append((prec, low - 1))
        prec = high + 1
    return result

def holes_between_generate(intervals):
    prec = intervals[0][1] + 1 # Bootstrap the iteration
    for low, high in intervals[1:]:
        if prec <= low - 1:
            yield (prec, low - 1)
        prec = high + 1

intervals = [ ... ] # fill in some test data

print 'As list:', timeit.timeit(
    'holes_between(intervals)',
    'from __main__ import intervals, holes_between_list as holes_between')

print 'Using a generator:', timeit.timeit(
    'list(holes_between(intervals))',
    'from __main__ import intervals, holes_between_generate as holes_between')

The lower value is the faster method for your test data.",A,3
51708106,2018-08-06 12:51:28.517000+00:00,You are not using Jinja. That is an error from the Django template language. In that language the tag you need is called verbatim.,A,1
43271532,2017-04-07 06:54:47.407000+00:00,"open() is a tool to open any file on the filesystem. It is not a tool to find Python modules.
Python's import machinery is fairly complex (it can be extended, adjusting how modules are found or loaded) and out of the box supports cached bytecode files with different extensions (.pyc, .pyd, in a __bytecache__ directory or not), loading from .zip files, and loading native extensions, all from a series of configurable directories listed on sys.path. This machinery is also there to allow you to override modules with different versions, simply by putting them in a different location on the search path.
The vast majority of use-cases for the open() function, however, do not need this machinery, you want to open your cat pictures from your Desktop, and not have to worry about the cat.py module in a different directory.
You can re-use the module resolution behaviour of the import machinery by using the importlib.util.find_spec() module:
from importlib.util import find_spec

module_spec = find_spec('f')
if module_spec is not None:
    with open(module_spec.origin) as module_source:
        exec(module_source.read())

This does require that your Desktop folder is on your sys.path module search path.",A,1
38260354,2016-07-08 06:39:06.547000+00:00,"Your wabbits() function takes two arguments:
def wabbits(n, k):
#           1  2

but your code calls it with just one:
return wabbits(n-2)*k + wabbits(n-1)
#              ^^^              ^^^^

You need to pass in a value for k as well. You could just pass in the current value of k:
def wabbits(n, k):
    if n == 1:
        return 1
    if n == 2:
        return 1
    return wabbits(n-2, k)*k + wabbits(n-1, k)
    #              1    2              1    2

and indeed that produces 19:
>>> def wabbits(n, k):
...     if n == 1:
...         return 1
...     if n == 2:
...         return 1
...     return wabbits(n-2, k)*k + wabbits(n-1, k)
...
>>> wabbits(5, 3)
19
",A,4
2003512,2010-01-05 01:13:21.293000+00:00,"Most of the overall design patterns are completely applicable:
Separation of Concerns; Model-Control-Presentation (in all of it's variants); Layered Architecture; Input-Process-Output, etc.
Once you've decomposed a big problem into smaller problems, the smaller problems are a matter of working through the various transformations from source to destination representation.
I find that the input-process-output pattern and transformation pipeline patterns help.",A,2
52292433,2018-09-12 09:57:54.617000+00:00,"You call the downloadPDF but you ignore its return value. You would need to return the result of calling that method. However, this will not work because you cannot return a response from get_context_data; that method, as the name implies, must return a dictionary context and not a response.
You need to move this code into the get method.",A,1
24221824,2014-06-14 16:11:18.280000+00:00,"Not without providing your own formatter class. Creating one that reverses the optional and positional arguments can be done with a subclass, but note that this relies on internal methods that can change between Python versions.
The following class will work in Python 2 and 3, as the overridden method is still called in the same way, but only for text fitting within the terminal width (using the COLUMNS environment variable, defaulting to 80 columns minus 2 characters padding):
class SwappedOrderHelpFormatter(argparse.HelpFormatter):
    def _format_actions_usage(self, actions, groups):
        # split optionals from positionals
        optionals = []
        positionals = []
        for action in actions:
            if action.option_strings:
                optionals.append(action)
            else:
                positionals.append(action)
        return super(SwappedOrderHelpFormatter, self)._format_actions_usage(
            positionals + optionals, groups)

Tell argparse to use this when creating your argument parser:
parser = argparse.ArgumentParser(
    # ....
    formatter_class=SwappedOrderHelpFormatter,
    # ....
    )

If your text is not going to fit most terminal widths, then you need to re-define the argparse.HelpFormatter._format_usage() method entirely, and swap the order of the optionals and positionals variables.
Demo:
>>> parser = argparse.ArgumentParser(prog='PROG', formatter_class=SwappedOrderHelpFormatter)
>>> parser.add_argument('-l', action='store_true')
_StoreTrueAction(option_strings=['-l'], dest='l', nargs=0, const=True, default=False, type=None, choices=None, help=None, metavar=None)
>>> parser.add_argument('-R')
_StoreAction(option_strings=['-R'], dest='R', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)
>>> parser.add_argument('STR_file')
_StoreAction(option_strings=[], dest='STR_file', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)
>>> parser.add_argument('PDB_file')
_StoreAction(option_strings=[], dest='PDB_file', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)
>>> parser.print_help()
usage: PROG STR_file PDB_file [-h] [-l] [-R R]

positional arguments:
  STR_file
  PDB_file

optional arguments:
  -h, --help  show this help message and exit
  -l
  -R R
",A,0
27835492,2015-01-08 08:04:01.450000+00:00,"It is ok to use a naive datetime object that represents local time if you use it only for displaying the timestamp.
datetime.fromtimestamp(ts_from_external_source) should work during DST transitions (local time may be ambiguous but POSIX timestamps are not if we ignore leap seconds). Though it might fail for dates from the past/future if the local timezone had/will have different UTC offset at the time and the underlying C library does not use a historical timezone database such as the tz database (Linux, OS X use it. python on Windows -- might not).
datetime.fromtimestamp(ts_from_external_source) should be ok for recent dates in most timezones.
You could use Europe/Moscow timezone with dates from 2010-2015 for testing (the timezone rules had changed in that period).
You could provide the tz database using pytz module:
from datetime import datetime, timedelta
from tzlocal import get_localzone # $ pip install tzlocal
import pytz # $ pip install pytz

tz = get_localzone() # get the local timezone as pytz timezone (with historical data)
utc_dt = datetime(1970, 1, 1, tzinfo=pytz.utc) + timedelta(seconds=ts_from_external_source)
dt = utc_dt.astimezone(tz)

Or:
dt = datetime.fromtimestamp(ts_from_external_source, tz)

Run these tests to see whether datetime + timedelta and fromtimestamp() produce different results on your platform.",A,1
22556506,2014-03-21 10:41:58.737000+00:00,"You can apply the list as separate arguments:
print(*L)

and let print() take care of converting each element to a string. You can, as always, control the separator by setting the sep keyword argument:
>>> L = [1, 2, 3, 4, 5]
>>> print(*L)
1 2 3 4 5
>>> print(*L, sep=', ')
1, 2, 3, 4, 5
>>> print(*L, sep=' -> ')
1 -> 2 -> 3 -> 4 -> 5

Unless you need the joined string for something else, this is the easiest method. Otherwise, use str.join():
joined_string = ' '.join([str(v) for v in L])
print(joined_string)
# do other things with joined_string

Note that this requires manual conversion to strings for any non-string values in L!",A,67
33912104,2015-11-25 08:56:22.177000+00:00,"To get the uptime in human-readable format in Python:
>>> from datetime import timedelta
>>> ns = 45900482044637
>>> print(timedelta(microseconds=round(ns, -3) // 1000))
12:45:00.482045
",A,1
15397276,2013-03-13 21:57:15.723000+00:00,"If you just want to know why it's not working:
First, main isn't setting a global a, but a local one. If you want to make it global, you have to be explicit:
def main():
    global a
    a = 1
    # ...

But this won't fix the problem, because ""global"" in Python is per-module. In other words, that just creates a foo.a, but your code in bar.Bar will be looking for bar.a, which will not exist.
You could import foo and then access foo.a if you wanted. (In this case, the circular dependency shouldn't be a problem, and the if __name__ == '__main__' won't get executed twice.) Or you could assume that foo has been imported, and use sys.modules['foo'].a. Or you could inject a into bar's dictionary from foo. Or lots of other tricks. But these are all horrible things to do.
If you can't make a global work, the right answer is almost always to not use a global. (In fact, even when you can make a global work, that's usually the right answer.)
So, how do you do that? Without knowing more about the details, it's hard to guess whether a belongs as an explicit module global, a class attribute, an instance attribute, a parameter to incr, etc. But figure out which one makes the most sense given what a represents, and do that.

In the comments, you suggested that you have a bunch of these configuration variables. If you wrap them all up in a dict and pass that dict to each object's constructor, that will make things simpler.
More importantly, a dict is a reference to a mutable object. Copying it just makes another reference to that same object. 
main.py:
def main():
    configs = {}
    configs['gravity'] = 1.0
    rock = rps.Rock(configs)
    rock.do_stuff()
    configs['gravity'] = 2.1
    rock.do_stuff()
if __name__ == '__main__':
    main()

rps.py:
class Rock(object):
    def __init__(self, configs):
        self.configs = configs
    def do_stuff(self):
        print('Falling with gravity {}.'.format(self.configs['gravity']))

When you run this, it'll print something like:
Falling with gravity 1.0.
Falling with gravity 2.1.

You haven't modified the value configs['gravity'] (that's an immutable float; you can't modify it), but you have replaced it with a different value (because configs is a mutable dict; you can modify it just fine). You can try printing out things like id(self.configs), id(self.configs['gravity']), etc. in various places if the identity isn't clear.",A,4
40460561,2016-11-07 08:15:54.493000+00:00,"First of all, forgo the list comprehension; feed the values directly into sum() with a generator expression:
sum(1 for a in A if a in arr)

If A is a set, use the set.intersection() method to extract the common values, then take the length of the result:
len(A.intersection(arr)))

This is faster than trying to test arr for each value. This does produce a new set() object first however, but you were creating a list before so that's not much difference.
At that point it is far simpler just to subtract the second length:
len(A.intersection(arr)) - len(B.intersection(arr))

You can avoid creating sets altogether by looping over arr and testing each value in arr against either A or B; this too is faster because set membership tests are O(1) constant time:
sum(1 if v in A else -1 if v in B else 0 for v in arr)

Your method, of testing a in arr for every value in the set A, requires a full list scan of arr if the value a is not present; this makes membership testing against a list a O(N) linear time problem, and you do so for every value in A and for every value in B, so you end up with O((A+B) * N) == O(KN) time. Testing each value in arr against the set is only O(N * 1) == O(N) time.
Moreover, if values in arr are not unique, your approach would actually lead to to the wrong answer; you'd only count happy or unhappy numbers once, while the problem requires them to be counted each time they appear.",A,2
50297186,2018-05-11 17:15:11.280000+00:00,"You’re creating three sub frames of your parent, storing each of them as self.frame (so all but the last one are lost), and not placing any of them anywhere. 
So, you’ve correctly placed the canvas on one of these invisible frames, but that doesn’t do any good.
I’m not sure what you’re trying to do with all these separate frames.

If you really need three sibling frames, you have to store them in separate variables, or in a list, or something, and you need to place them.
If you need one sibling frame, just create it once instead of three times, and again, you need to place it.
If you need three or one child frames instead of sibling frames, create them with self rather than self.master.
If you don’t need any sibling or child frames at all, don’t create them, and just place the canvas on self instead of self.frame.
",A,1
53938012,2018-12-26 22:56:05.947000+00:00,"Well, you don't ever seem to be setting the username. You set a timestamp, which doesn't exist as a field, but not the actual username field.
    model_instance = symbol_form.save(commit=True)
    model_instance.userame = request.user
    model_instance.save()

As an aside, that field should be called user, as it points to the whole User object not just the username.",A,1
49225076,2018-03-11 21:04:46.543000+00:00,"I think what you're looking for is Mypy, a static type checker for Python 2.7+/3.4+. It's the type checker that Python 3.6's annotation system is designed around, but they've been careful to make sure it can be used with older versions of Python. (In fact, part of the motivation for type hints in the first place is that Guido wanted to use Mypy to help guide upgrading a large codebase from 2.7 to 3.5.)
Of course you can't use 3.6 syntax in older versions of Python. In 3.5, parameters can be annotated, but not locals. In 3.4, annotations are limited. In 2.7, annotations don't exist at all.
If you read the docs, there are a few ways around this, but the basic idea is that you put all the annotations into comments in ""public"" code, while you write ""typeshed"" files full of out-of-line annotations for ""internal"" code.
The good news is that, because Mypy has been blessed by the core language, other static type checkers—and related tools like IDE indexers or deeper static analyzers—are adapting to do things the same way, so whatever you use for your 2.7 or 3.4 code is probably going to work with your favorite IDE or vim plugin or analyzer or whatever (if not today, then soon).",A,1
32398771,2015-09-04 13:01:58.307000+00:00,"Two steps:

Convert the time string into an aware datetime object (or a naive datetime object that represents time in UTC). 
>>> from dateutil import parser
>>> parser.parse(""30/Aug/2015:05:13:53 +0200"".replace(':', ' ', 1))
datetime.datetime(2015, 8, 30, 5, 13, 53, tzinfo=tzoffset(None, 7200))

You've already done it. See How to parse dates with -0400 timezone string in python? on how to do it using only stdlib.
Convert an aware datetime object to ""seconds since the Epoch"":
>>> from datetime import datetime
>>> from dateutil import tz
>>> td = d - datetime(1970, 1, 1, tzinfo=tz.tzutc())
>>> td
datetime.timedelta(16677, 11633)
>>> (td.microseconds + (td.seconds + td.days * 86400) * 10**6) // 10**6
1440904433

Use / and enable  from __future__ import division, to get fractions of a second. If you don't need to support fractions; you could simplify the formula:
>>> td.seconds + td.days * 86400
1440904433

If you get a utc time on the 1st step using only stdlib then you don't need dateutil.tz here. See Converting datetime.date to UTC timestamp in Python

Here's a Python 3 solution for visitors from a search engine:
>>> from datetime import datetime
>>> d = datetime.strptime(""30/Aug/2015:05:13:53 +0200"", ""%d/%b/%Y:%H:%M:%S %z"")
>>> d.timestamp()
1440904433.0
",A,3
23341378,2014-04-28 12:38:20.910000+00:00,"The @a.setter decorator ignores the docstring and it is not copied to the resulting property object; set the docstring on the getter instead.
See the property documentation:

If given, doc will be the docstring of the property attribute. Otherwise, the property will copy fget‘s docstring (if it exists).

Emphasis mine.
Your code results in:
>>> class Foo:
...     a = None
...     @property
...     def a(self):
...         pass
...     @a.setter
...     def a(self, v):
...         '''
...         >>> 1 == 1
...         False
...         '''
...         pass
...
>>> Foo.a
<property object at 0x101a21050>
>>> Foo.a.__doc__ is None
True

Set the docstring on the getter however and you get:
>>> class Foo:
...     a = None
...     @property
...     def a(self):
...         '''
...         >>> 1 == 1
...         False
...         '''
...         pass
...     @a.setter
...     def a(self, v):
...         pass
... 
>>> Foo.a
<property object at 0x101a210a8>
>>> Foo.a.__doc__
'\n        >>> 1 == 1\n        False\n        '

Another, ugly workaround would be to recreate the property with the docstring being copied from the setter, explicitly:
class Foo:
    a = None

    @property
    def a(self):
        pass

    @a.setter
    def a(self, v):
        '''
        >>> 1 == 1
        False
        '''
        pass

    a = property(a.fget, a.fset, doc=a.fset.__doc__)
",A,5
20381739,2013-12-04 17:17:12.900000+00:00,"To download all files from a remote folder over ssh, you could use ftp.listdir() to list the files followed by ftp.get() for each file:
#!/usr/bin/env python
import os
import sys
from contextlib import closing

from paramiko import SSHConfig, SSHClient

# specify hostname to connect to and the remote/local paths
hostname, remote_dirname, destdir = sys.argv[1:]

# load parameters to setup ssh connection
config = SSHConfig()
with open(os.path.expanduser('~/.ssh/config')) as config_file:
    config.parse(config_file)
d = config.lookup(hostname)

# connect
with closing(SSHClient()) as ssh:
    ssh.load_system_host_keys() #NOTE: no AutoAddPolicy() 
    ssh.connect(d['hostname'], username=d.get('user'))
    with closing(ssh.open_sftp()) as sftp:
        # cd into remote directory
        sftp.chdir(remote_dirname)
        # cd to local destination directory
        os.chdir(destdir)
        # download all files in it to destdir directory
        for filename in sftp.listdir():
            sftp.get(filename, filename)
",A,2
43632891,2017-04-26 11:28:03.183000+00:00,"You should not use a list comprehension at all here. List comprehensions are great at building a list of values, and should not be used for general looping. Using a list comprehension for the side-effects is a waste of memory on a perfectly good list object.
List comprehensions are also expressions, so can only contain other expressions. del is a statement and can't be used inside an expression.
Just use a for loop:
# use a tuple if you need a literal sequence; stored as a constant
# with the code object for fast loading
for key in ('selected', 'actual', 'previous', 'forecast'):
    del event[key]

or rebuild the dictionary with a dictionary comprehension:
# Use a set for fast membership testing, also stored as a constant
event = {k: v for k, v in event.items()
         if k not in {'selected', 'actual', 'previous', 'forecast'}}

The latter creates an entirely new dictionary, so other existing references to the same object won't see any changes.
If you must use key deletion in an expression, you can use object.__delitem__(key), but this is not the place; you'd end up with a list with None objects as a result, a list you discard immediately.",A,10
30505956,2015-05-28 11:57:35.160000+00:00,"In your settings, ON_HEROKU will never actually be equal to True, because environment variables are always strings. This means that your db settings will never be set.
Instead you should just check for boolean truth:
if ON_HEROKU:
    ...
",A,0
216386,2008-10-19 12:53:42.110000+00:00,"""holds the primary application class (most functional routines)""
As in singular -- one class?
I'm not surprised that the One Class Does Everything design isn't working.  It might not be what I'd call object-oriented.   It doesn't sound like it follows the typical MVC design pattern if your functionality is piling up in a single class.
What's in this massive class?  I suggest that you can probably refactor this into pieces.  You have two candidate dimensions for refactoring your application class -- if, indeed, I've guessed right that you've put everything into a single class.

Before doing anything else, refactor into components that parallel the Real World Entities.  It's not clear what's in your ""state.py"" -- wether this is a proper model of real-world entities, or just mappings between persistent storage and some murky data structure in the application.  Most likely you'd move processing out of your application and into your model (possibly state.py, possibly a new module that is a proper model.)
Break your model into pieces.  It will help organize the control and view elements.  The most common MVC mistake is to put too much in control and nothing in the model.
Later, once your model is doing most of the work, you can look at refactor into components that parallel the GUI presentation.  Various top-level frames, for example, should probably have separate cotrol objects.  It's not clear what's in ""GUI.py"" -- this might be a proper view.  What appears to be missing is a Control component.
",A,2
6953307,2011-08-05 08:12:12.030000+00:00,"Plone redirects to the login form because you raise Unauthorized. If you want different behaviour you'll need to do something different. 
In this case, you could directly redirect the user to a new page with an error message tailored to the situation. ",A,2
54952971,2019-03-01 22:09:03.110000+00:00,"Because whenever you go to index with a GET, you always overwrite whatever's saved in the session. If you have nothing in the GET request, you overwrite the session value with none.
You should probably check that the symbol is in request.GET and not in request.session before setting it.",A,0
26003138,2014-09-23 19:25:11.257000+00:00,"If you want a general solution to detecting Morse code, you are going to have to take a look at what it looks like as a waveform (tom10's link to this question should help here if you can install numpy and matplotlib; if not, you can use the stdlib's csv module to export a file that you can use in your favorite spreadsheet program); work out how you as a human can distinguish dots, dashes, and spaces; turn that into an algorithm (a series of steps that even a literal-minded moron can follow); then turn that algorithm into code. Or you may be able to find a library that's already done this for you.
But for your specific case, you only need to detect exact copies of the contents of dot.wav and dash.wav within your larger file. (At least assuming you're not using any lossy compression, which usually you aren't in .wav files.) So, this is really just a substring search.
Think about how you'd detect the strings 'dot' and 'dash' within a string like 'dash dash   dash dash dash   dot dash dot   dot dot dot   dot '. For such a simple problem, you could use a stupid brute-force algorithm, and it would be fine:
def find(haystack, needle, start):
    for i in range(start, len(haystack)):
        if haystack[i:i+len(needle)] == needle:
            return i
    return len(haystack)

def decode_morse(morse):
    i = 0
    while i < len(morse):
        next_dot = find(morse, 'dot', i)
        next_dash = find(morse, 'dash', i)
        if next_dot < next_dash:
            if next_dot < len(morse):
                yield '.'
            i = next_dot
        else:
            if next_dash < len(morse):
                yield '-'
            i = next_dash

Now, if you're searching a list of numbers instead of a string, how does this have to change? Barely at all; you can slice a list, compare two lists, etc. just like you can with strings.
The only real problem you'll run into is that you don't have the whole list in memory at once, just 20 frames at a time. What happens if a dot starts in frame 19 and ends in frame 20? If your files aren't too big, this is easy to solve: just read all the frames into memory in one giant list, then search the whole thing. But otherwise, you have to do some buffering.
For example (ignoring error handling and dealing with the end of the file properly, and dealing only with dashes for simplicity—of course you have to do both of those properly in your real code):
buf = []
while True:
    while len(buf) < 2*len(dash):
        buf.extend(waveFile.readFrames(20))
    next_dash = find(buf, dot)
    if next_dash < len(buf):
        yield '.'
        buf = buf[next_dash:]
    else:
        buf = buf[-len(dash):]

We're making sure we always have at least two dash lengths in our buffer. And we always keep the leftover after the first dot or dash (if one was found) or a full dash length (if not) in the buffer, and add the next buffer to that. That's actually overkill; think it through and think through out exactly what you need to make sure we never miss a dash that falls between two buffers. But the point is, as long as you get that right, you can't miss any dots or dashes.",A,0
49753451,2018-04-10 12:17:25.870000+00:00,"You can use a StringField and then convert to integer in the validation method.
class StrikeSerializer(serializers.ModelSerializer):
    civilians = serializers.IntegerField(default='', required=False)

    def validate_civilians(self, value):
        if not value:
            return 0
        try:
            value = int(value)
        except ValueError:
            raise serializers.ValidationError('You must supply an integer')
       return value
",A,5
25411583,2014-08-20 18:07:23.877000+00:00,"intersection = set(*times[:1]).intersection(*times[1:])
",A,0
52249646,2018-09-10 00:18:31.680000+00:00,"The program doesn't ""stop recursion"" unless you return from every function on the stack.1
And you're not doing that. No matter what the recursive call to dfs returns, you just ignore it and continue on with your loop:
for i in lst:
    dfs(word, gen+i)

… and then fall off the end of the loop and the if statement and:
return False

So, those recursive calls have no effect except to waste CPU time. The only way you can ever return anything but False at any level is if gen == word: at that level. So, when that isn't true at the top level, you always return False.

The fix is simple: When one of the recursive calls returns True, you want to return True as well: the value is found if it matches gen, or if it's found in a depth-first search on gen+'a', or if it's found in a depth-first search on gen+'b'. So:
for i in lst:
    if dfs(word, gen+i):
        return True

Now you only continue the loop and fall off the end if none of the rercusive calls is true.

1. Unless you raise an unhandled exception.",A,0
17525923,2013-07-08 11:52:41.657000+00:00,"You need to pass in unicode text:
u'technologieën in °C'

Do make sure you use the # -*- coding: utf-8 -*- comment at the top, and make sure your text editor is actually using that codec. If your editor saves the file as Latin-1 encoded text, use that codec in the header, etc. The comment communicates to Python how to interpret your source file, especially when it comes to parsing string literals.
Alternatively, use escape codes for anything non-ASCII in your Unicode literals:
u'technologie\u00ebn in \u00b0C'

and avoid the issue of what codec to use in the first place.
I urge you to read:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder

before you continue.
Most fonts will support the °, but if you see a box displayed instead, then you have a font issue and need to switch to a font that supports the characters you are trying to display.  For example, if Ariel supports your required characters, then use:
matplotlib.rc('font', family='Arial')

before plotting.",A,11
32782048,2015-09-25 12:18:31.047000+00:00,"No, there is no need for the explicit str() calls, the %s formatter includes a str() call already:
>>> class Foo(object):
...     def __str__(self):
...         return ""I am a string for Foo""
... 
>>> '%s' % Foo()
'I am a string for Foo'

This is also explicitly documented in the String Formatting Operations section:

's'
  String (converts any Python object using str()).
",A,6
11391756,2012-07-09 08:49:40.297000+00:00,"Because you don't have a date field in your Contract model, so nothing is returned.
Because the datastore is schemaless, GQL can't know whether or not any of the entities in your model might have a particular field. So, unlike SQL, it doesn't raise an error when you reference a non-existent field: it just performs the query, and if nothing is found, it returns nothing.
I'm not sure why you want to query it in any case. And even if you did, you should really get it by the key that's returned from the put() call, rather than doing a query: the eventual consistency model means that it's possible for the data not to be fully persisted by the next statement, so a query wouldn't see it whereas a direct get by key is always guaranteed to do so.",A,1
4525304,2010-12-24 09:09:51.743000+00:00,"Plone 4 requires python 2.6, so it won't work with python 2.4. See http://plone.org/documentation/faq/plone-versions",A,4
37314784,2016-05-19 05:45:13.533000+00:00,"if you want to find 7**x recursively:
def max_siblings(depth, degree=7, total=1):
    """"""How many siblings maximum at the given *depth*.""""""
    return max_siblings(depth-1, degree, total*degree) if depth else total

If you want to find ((7**(depth+1)) - 1) // 6 recursively:
def max_nodes(depth, degree=7, total=1):
    return max_nodes(depth-1, degree, total+max_siblings(depth)) if depth else total

Example:
for depth in range(5): 
    print(max_nodes(depth))

Output:
1
8
57
400
2801

You could cache max_siblings() computations using @lru_cache(maxsize=None) decorator",A,0
2799482,2010-05-09 22:50:59.030000+00:00,"nose (suggested by @Paul Hankin)
#!/usr/bin/env python
# file: test_pairs_nose.py
from nose.tools import eq_ as eq

from mymodule import f

def test_pairs(): 
    for input, output in [ (2, 332), (234, 99213), (9, 3), ]:
        yield _test_f, input, output

def _test_f(input, output):
    try:
        eq(f(input), output)
    except AssertionError:
        if input == 9: # expected failure
            from nose.exc import SkipTest
            raise SkipTest(""expected failure"")
        else:
            raise

if __name__==""__main__"":
   import nose; nose.main()

Example:
$ nosetests test_pairs_nose -v
test_pairs_nose.test_pairs(2, 332) ... ok
test_pairs_nose.test_pairs(234, 99213) ... ok
test_pairs_nose.test_pairs(9, 3) ... SKIP: expected failure

----------------------------------------------------------------------
Ran 3 tests in 0.001s

OK (SKIP=1)

unittest (approach similar to @doublep's one)
#!/usr/bin/env python
import unittest2 as unittest
from mymodule import f

def add_tests(generator):
    def class_decorator(cls):
        """"""Add tests to `cls` generated by `generator()`.""""""
        for f, input, output in generator():
            test = lambda self, i=input, o=output, f=f: f(self, i, o)
            test.__name__ = ""test_%s(%r, %r)"" % (f.__name__, input, output)
            setattr(cls, test.__name__, test)
        return cls
    return class_decorator

def _test_pairs():
    def t(self, input, output):
        self.assertEqual(f(input), output)

    for input, output in [ (2, 332), (234, 99213), (9, 3), ]:
        tt = t if input != 9 else unittest.expectedFailure(t)
        yield tt, input, output

class TestCase(unittest.TestCase):
    pass
TestCase = add_tests(_test_pairs)(TestCase)

if __name__==""__main__"":
    unittest.main()

Example:
$ python test_pairs_unit2.py -v
test_t(2, 332) (__main__.TestCase) ... ok
test_t(234, 99213) (__main__.TestCase) ... ok
test_t(9, 3) (__main__.TestCase) ... expected failure

----------------------------------------------------------------------
Ran 3 tests in 0.000s

OK (expected failures=1)

If you don't want to install unittest2 then add:
try:    
    import unittest2 as unittest
except ImportError:
    import unittest
    if not hasattr(unittest, 'expectedFailure'):
       import functools
       def _expectedFailure(func):
           @functools.wraps(func)
           def wrapper(*args, **kwargs):
               try:
                   func(*args, **kwargs)
               except AssertionError:
                   pass
               else:
                   raise AssertionError(""UnexpectedSuccess"")
           return wrapper
       unittest.expectedFailure = _expectedFailure    
",A,11
40588302,2016-11-14 12:00:50.093000+00:00,"__init__ is not a constructor. It is an initialisation method, called after the instance was already constructed for you (the actual constructor method is called __new__()).
You can always call it again from your code if you need to re-initialise, this isn't a style violation. In fact, it is used in the Python standard library; see the multiprocessing.heap.Heap() implementation for example:
def malloc(self, size):
    # return a block of right size (possibly rounded up)
    assert 0 <= size < sys.maxsize
    if os.getpid() != self._lastpid:
        self.__init__()                     # reinitialize after fork

or the threading.local implementation, which uses a context manager to defer initialisation.
There is otherwise nothing special about the __init__ method itself. It is merely automatically called by type.__call__ (after creating the instance with instance = cls.__new__(cls, *args, **kwargs), cls.__init__(instance, *args, **kwargs) is called if it is available).",A,11
35269289,2016-02-08 12:05:44.357000+00:00,"Python socket module is a thin wrapper around your platform's socket API. The issue is unrelated to Python.
It is not necessary that you get No Route to Host error. Moreover it is common that a firewall just drops received packets (for a filtered port) that may manifest as a timeout error in your code. See Drop vs. Reject (ignore the conclusion but read the explanation of what is happening).
To workaround, make multiple concurrent connections and set a fixed timeout or use raw-sockets and send the packets yourself (you could use scapy, to investigate the behavior).",A,0
14954385,2013-02-19 09:45:16.723000+00:00,"You are using a PHP-ism by naming the inputs score[{{quiz.id}}]. Don't do that. Just call them all score, and your browser will do the right thing by putting them all in the same POST value which you can then get via request.POST.getlist('score').",A,2
1191649,2009-07-28 02:25:46.997000+00:00,"If you have a Linux box, you already have /dev/random.",A,2
6222491,2011-06-03 02:49:17.163000+00:00,"http://www.lugod.org/presentations/gtk/gui.html
Google is full of tutorials on C-callable GUI frameworks.  That's just the first one.",A,0
16101416,2013-04-19 09:26:28.763000+00:00,"Pillow 2.0 supports Python 3, including 3.3:

Note: Pillow >= 2.0.0 supports Python versions: 2.6, 2.7, 3.2, 3.3; Pillow < 2.0.0 supports Python versions: 2.4, 2.5, 2.6, 2.7.

Pillow is a fork of PIL that adds (much) better packaging and patches and improvements to make PIL work on Python 3.
Windows installers and eggs are included.",A,5
42799568,2017-03-15 01:24:02.900000+00:00,"You pass in a seed to the global random object each time you call f(), because all top-level functions in the random module feed into a singleton object. This means that by the time f3 is created, the seeds set in f2 and f1 have been superseded, the seeds are not independent from the global random object. Importing random again for each f() call does not give you a new state, as really only binds names anew each time once the module object itself is loaded (only on the first import).
If you want to have a seeded random generator per function, you need to create individual random.Random() instances:
import random
import time

def f(p):
    seeded_random = random.Random(time.time())
    def g():
        return 1 if seeded_random.random() < p else 0
    return g

From the random module documentation:

The functions supplied by this module are actually bound methods of a hidden instance of the random.Random class. You can instantiate your own instances of Random to get generators that don’t share state.
",A,3
9821670,2012-03-22 11:49:22.293000+00:00,"It works as expected on BeautifulSoup 3.2.0:
>>> soup.find('title',text=re.compile(""Page"")) 
u'Page title'
>>> soup.findAll('title',text=re.compile(""Page"")) 
[u'Page title']
",A,0
24631816,2014-07-08 12:25:00.687000+00:00,"First of all, you should always use binary mode for pickle files. On platforms where this matters (e.g. Windows), opening a file in text mode means that all line terminators are translated; \n becomes \r\n on write, and \r\n becomes \n again on reading.
On Python 2 the default pickle protocol is ASCII-based, but that doesn't mean that the contents of the values are not going to be affected. For your w -> rb example, most likely a value with a \n embedded was written out as \r\n, then read as \r\n meaning the length of the data changed, triggering the error message because certain quoting expectations were not met (the closing quote was not read because the string length changed).
The fact that you didn't run into this specific exception with the other non-binary combinations does not mean you didn't have problems anyway. Values could still end up being corrupted.
All other protocol versions are binary based, meaning you can break the protocol in more creative ways still.",A,2
15655265,2013-03-27 09:17:34.557000+00:00,"No, the difference is not a question of performance. In both cases, the entire module must be parsed, and any module-level code will be executed. The only difference is in namespaces: in the first, all the names in the imported module will become names in the current module; in the second, only the package name is defined in the current module.
That said, there's very rarely a good reason to use from foo import *. Either import the module, or import specific names from it.",A,16
50659958,2018-06-02 18:30:20.510000+00:00,"This question doesn't really have anything to do with videos.
For a start, STATIC_URL is for assets that are, well, static - that is, they are provided with the application and don't change. For things that are dynamically uploaded by users, they are indeed saved to the media folder and so you would need MEDIA_URL.
Secondly, you aren't actually using the value from the model instance. Your template needs to iterate through the elements in the object list you send from the view, and for each one needs to access its video attribute.
When you do this, you can in fact use the file field's url attribute, which avoids the need to use MEDIA_URL at all.
{% for video in object_list %}
    <video id=""intro"" type=""video/mp4"" controls preload autoplay>
        <source src=""{{ video.file.url }}"" type=""video/mp4"">
    </video>
{% endfor %}

And finally, you need something that is actually serving your media files at MEDIA_URL; Django won't do this for you. In development only, you can configure the dev server to do this; in production you'll need to set up your server to do it directly.",A,0
30456379,2015-05-26 10:48:23+00:00,"You'd consult the Python style guide; the following entries are applicable:

Compound statements (multiple statements on the same line) are generally discouraged.
Yes:
if foo == 'blah':
    do_blah_thing()
do_one()
do_two()
do_three()

Rather not:
if foo == 'blah': do_blah_thing()
do_one(); do_two(); do_three()


and

While sometimes it's okay to put an if/for/while with a small body on
  the same line, never do this for multi-clause statements. Also avoid
  folding such long lines!
Rather not:
if foo == 'blah': do_blah_thing()
for x in lst: total += x
while t < 10: t = delay()

Definitely not:
if foo == 'blah': do_blah_thing()
else: do_non_blah_thing()

try: something()
finally: cleanup()

do_one(); do_two(); do_three(long, argument,
                             list, like, this)

if foo == 'blah': one(); two(); three()


You should also not use parentheses around the condition, and put whitespace around the operators.
In your case, in Python you'd also use exception handling rather than test (ask for forgiveness, rather than permission):
num1 = 10
num2 = 10
try:
    print(num1 / num2)
except ZeroDisionError:
    print(""Cannot divide by zero"")
",A,3
31767534,2015-08-02 00:32:31.023000+00:00,"First of all: the abs() calls are entirely redundant if you are squaring the result anyway.
Next, you may be reading the profile output wrong; don't mistake the cumulative times with the time spent only on the function call itself; you are calling abs() many many times so the accumulated time will raise rapidly.
Moreover, profiling adds a lot of overhead to the interpreter. Use the timeit module to compare the performance between approaches, it gives you overall performance metrics so you can compare apples with apples.
It is not that the abs() function is slow; it is calling any function that is 'slow'. Looking up the global name is slower than looking up locals, and then you need to push the current frame on the stack, execute the function, then pop the frame from the stack again.
You can alleviate one of those pain points by making abs() a local name outside the loop:
_abs = abs
for i in range(radius + 1):
    # ...
    total += (_abs(x1 - x) ** 2 + _abs(y1 - y) ** 2) ** 0.5

Not that abs() really is taking such a huge toll on your performance, really, not when you time your functions properly. Using a radius of 1000 to make 100 repeats practical, timeit comparisons give me:
>>> from timeit import timeit
>>> def picalc(radius = 10000000):
...     total = 0
...     x = 0
...     y = radius
...     for i in range(radius + 1):
...         x1 = i
...         y1 = (radius ** 2 - x1 ** 2) ** 0.5
...         total += ((x1 - x) ** 2 + (y1 - y) ** 2) ** 0.5
...         x = x1
...         y = y1
... 
>>> def picalc_abs(radius = 10000000):
...     total = 0
...     x = 0
...     y = radius
...     for i in range(radius + 1):
...         x1 = i
...         y1 = (radius ** 2 - x1 ** 2) ** 0.5
...         total += (abs(x1 - x) ** 2 + abs(y1 - y) ** 2) ** 0.5
...         x = x1
...         y = y1
... 
>>> def picalc_abs_local(radius = 10000000):
...     total = 0
...     x = 0
...     y = radius
...     _abs = abs
...     for i in range(radius + 1):
...         x1 = i
...         y1 = (radius ** 2 - x1 ** 2) ** 0.5
...         total += (_abs(x1 - x) ** 2 + _abs(y1 - y) ** 2) ** 0.5
...         x = x1
...         y = y1
... 
>>> timeit('picalc(1000)', 'from __main__ import picalc', number=100)
0.13862298399908468
>>> timeit('picalc(1000)', 'from __main__ import picalc_abs as picalc', number=100)
0.14540845900774002
>>> timeit('picalc(1000)', 'from __main__ import picalc_abs_local as picalc', number=100)
0.13702849800756667

Notice how there is very little difference between the three approaches now.",A,6
10774257,2012-05-27 13:14:47.127000+00:00,"The easiest way is to use effbot.org's Console module:
import Console

c = Console.getconsole()
c.text(0, -1, 'And this is the string at the bottom of the console')

By specifying -1 for the second (line) argument you are addressing the last line of the console.
Because the Console module only works on Windows, to get this to work on UNIX terminals as well, you should take a look at the wcurses library, which provides a partial curses implementation that'll work on Windows. You'd drive it the same as the stdlib curses module; use the first on Windows, the latter on UNIX.",A,1
13274842,2012-11-07 17:20:48.147000+00:00,"You would unit-test the is_valid() function itself. You then only have to test for the fact that .run() raises a ValueError exception when passed invalid input:
with self.assertRaises(ValueError):
    objectundertest.run(invalid_parameter)

Since you already have a unittest for is_valid() itself, your unittests for .run() do not need to focus on it's functionality. You can focus on the functionality unique to .run().
In your case BaseAction is a unit providing a service to subclasses; I'd test this as a mock subclass:
class MockAction(BaseAction):
    run_called = None

    def do_run(self, param):
        self.run_called = param

so you can check if run indeed called do_run with the expected param value.",A,2
31428743,2015-07-15 11:07:36.793000+00:00,"There is no need for any parsing at the framework level. The body of the post request is always available in request.body, so you can access it directly:
result = json.loads(request.body)
",A,1
7896765,2011-10-25 22:43:27.897000+00:00,"Though it is not strictly necessary for an url you could store it as Unicode.
BeautifulSoup works with Unicode.
>>> from BeautifulSoup import BeautifulSoup
>>> soup = BeautifulSoup(""""""<a href=""ascii"">"""""", fromEncoding=""utf-8"")
>>> isinstance(soup('a', href=True)[0]['href'], unicode)
True

>>> soup = BeautifulSoup(""""""<a href=""αβγ"">"""""", fromEncoding=""utf-8"")
>>> soup('a', href=True)[0]['href']
u'\u03b1\u03b2\u03b3'

In both cases the url is unicode. 
You could call isinstance() or type() to find out what type the url has.

You could specify encoding=None to get Unicode:
i.renderContents(encoding=None)

In general it might be helpful to use dir(obj), help(obj.method) in a interactive Python console. See also Printing Document.",A,1
45140984,2017-07-17 09:50:13.580000+00:00,"AutoFields are set by the database itself, so don't get a value until after you save. But you have not saved at that point, because you passed commit=False to the form save; this creates an instance in memory but does not send it to the db yet.
If you want this to work, you will have to remove that commit=False and accept the (tiny) cost of saving to the db twice.",A,1
19818966,2013-11-06 17:41:23.023000+00:00,"any() is exactly what you need here, combined with a generator expression:
any(func(i) for i in big_list)

This will stop iterating for the first value where func(i) returns a true value. As soon as a True value is found, you've proven that there is a value that is true in the input sequence (""is there any value that is true?"" -> yup, we found at least one).
For and, you'd use all() instead:
all(func(i) for i in big_list)

which will return False the moment a func(i) falsey value is found. If one false value is found, then you have proven that there is at least one value that is not true, so they cannot all be true.
Note that these two functions are given a generator expression:
(func(i) for i in big_list)

This is evaluated lazily; every time you ask for the next value of a generator expression, it'll evaluate the loop and execute the func(i) expression once. It will not produce the whole list at once, it'll produce items one by one.
Your reduce(operator.or_, [func(1), func(0)]) expression has to build the whole input list before it can call reduce(). The reduce() method will process the whole input list, it will not short-circuit, because it has no knowledge of what operation is being applied to the input values. You could give reduce() a generator expression as well, but it won't stop iterating once the outcome is set (on the first true value for or or the first false value for and), again because reduce() has no specialist knowledge of the operation being performed.",A,4
20378589,2013-12-04 14:56:50.700000+00:00,"The problem is that facts is a class variable, not an instance one. You should define it inside __init__ (and remove the useless local variable declaration there):
class Fruit:

   def __init__(self, FruitName):
       self.name = FruitName
       self.facts = []

   def addfact(self, FruitName):
       ...

Note Python is not Java, there is no need to ""declare"" attributes at the class level.",A,3
13825783,2012-12-11 17:53:53.757000+00:00,"By using a full or relative path. You are specifying just a filename, with no path, and that means that it'll be saved in the current directory.
To save the file in the Pics2 directory, relative from the current directory, use:
fig.savefig('Pics2/forcing' + str(forcing) + 'damping' + str(damping) + 'omega' + str(omega) + 'set2.png')

or better still, construct the path with os.path.join() and string formatting:
fig.savefig(os.path.join(('Pics2', 'forcing{0}damping{1}omega{2}set2.png'.format(forcing, damping, omega)))

Best is to use an absolute path:
path = '/Some/path/to/Pics2'
filename = 'forcing{0}damping{1}omega{2}set2.png'.format(forcing, damping, omega)
filename = os.path.join(path, filename)
fig.savefig(filename)
",A,7
38068846,2016-06-28 06:43:11.423000+00:00,"Use the itertools.product() function to generate the values over a variable number of ranges:
for values in product(*(range(*b) for b in boundaries_list)):
    # do things with the values tuple, do_whatever(*values) perhaps

Don't try to set a variable number of variables; just iterate over the values tuple or use indexing as needed.
Using * in a call tells Python to take all elements of an iterable and apply them as separate arguments. So each b in your boundaries_list is applied to range() as separate arguments, as if you called range(b[0], b[1], b[2]).
The same applies to the product() call; each range() object the generator expression produces is passed to product() as a separate argument. This way you can pass a dynamic number of range() objects to that call.",A,2
14348454,2013-01-15 22:54:31.423000+00:00,"Yes, don't create so many copies. Just reference the subdict:
result = example_dict
search_key = ['level_one', 'level_two_a']
for term in search_key:
    result = result[term]

As long as you are not altering the result dict, making a copy is pointless. Since you discard the previous copy and make a new one on every iteration, you are wasting CPU time as well as memory.
Even if you did have to modify result and don't want those changes to affect example_dict, you only need to copy the final result value after looping.",A,1
36312298,2016-03-30 14:45:34.800000+00:00,"Generators can only be iterated over once. Each group in the groupby() result is such a generator. Calling list() on a generator causes it to yield all values, after which the generator is exhausted, empty, and can not be reset; a second list() call on the same object then simply results in an empty list, since the generator no longer yields values to put in the list.
Store the result of list() in a new variable instead:
for key, group in gs:
    group_result = list(group)
    print(key, group)
    print(group_result)
    print(key, group, group_result)

Now you call list() on the generator just the once, and reuse the result.",A,3
4258711,2010-11-23 17:05:01.973000+00:00,"The code you have posted looks for all a elements with the bpImage class. But your example has the bpImage class on the img element, not the a. You just need to do:
soup.find(""img"", { ""class"" : ""bpImage"" })
",A,1
2051125,2010-01-12 17:55:42.157000+00:00,"Are you asking about struct.unpack( 'b', someByte )?
A ""byte"" is a 1-character string.  
The value 0xff is an integer, and already unpacked.
If you're finding the 4-character value 0xff in an input file, you're best served by eval() since your input file contains Python code, 0xff.",A,-7
16994562,2013-06-07 23:56:32.080000+00:00,"Yes, file objects are iterators.
Like all iterators, you can only loop over them once, after which the iterator is exhausted. Your file read pointer is at the end of the file. Re-open the file, or use .seek(0) to rewind the file pointer if you need to loop again.
Alternatively, try to avoid looping over a file twice; extract what you need into another datastructure (list, dictionary, set, heap, etc.) during the first loop.",A,3
27305006,2014-12-04 22:04:21.367000+00:00,"You could consider both cases to be the same (ignore the difference). Your script just reads two lines from stdin. stdin may be redirected from a file or it can be attached to the console, your script can work the same in many cases:
print(""Read the first line from stdin"", input())
answer = input(""Did you like that? (Yes/No): "") # read 2nd line
if answer == ""Yes"":
    print(""You liked it!"")

See executable code example.

Q: What I wanted was to read some inputs from a file OR from the console (depending on the parameters used when the app was ran). Some other lines I wanted them to be always read from the console. 

To read from the console directly regardless whether stdin is redirected from a file or not, you could use msvcrt.getwch(). See example usage in getpass.win_getpass().
If you have issues with accepting Unicode input; install win_unicode_console package. You can enable it globally for your python installation or for a specific user or for a script as a whole or temporarily using win_unicode_console.enable()/.disable(). To force it to use console, you could set sys.stdin=None temporarily if stdin is redirected or call ReadConsoleW() yourself (cumbersome).",A,0
53133558,2018-11-03 17:02:11.697000+00:00,"The problem is probably that your main URLpattern matches everything. So when you make the Ajax request, it is served by the view that returns a full page, not the one that returns a single value.
Make sure you use beginning and terminating anchors:
url(r'^$', views.main, name = 'main'),
url(r'^calc/$', views.calc, name = 'calc')

Or use the new path syntax:
path('', views.main, name = 'main'),
path('calc/', views.calc, name = 'calc')
",A,1
50476472,2018-05-22 21:07:25.017000+00:00,"There's no way to fully replace how Python wraps the stdin and stdout pipes in sys.stdin and sys.stdout. (There is some limited control, like the -u option`, but that's it.)
However, you're asking for multiple other things, all of which are possible, and some of which might be what you want.

First, sys.stdin is just a normal TextIOWrapper, as returned by open (in text mode). This means you can access its underlying binary object, or the raw unbuffered file object underneath that, or the OS file descriptor underneath that, the same as for any other text file:
sys.stdin.buffer
sys.stdin.buffer.raw
sys.stdin.fileno()

Often, that's all you need. You wouldn't want to actually replace sys.stdin = sys.stdin.buffer. That would break input, the fileinput module, and who knows what else that's expecting stdin to be a text file. But you can just use sys.stdin.buffer instead of sys.stdin, and it's roughly the equivalent of what I think you were looking for from perl's binmode.

And you want a TextIOWrapper with different options wrapped around the same underlying file, you can do that too.
For the simplest cases, the easiest way to do that is just to call open the way you wanted to call open, passing the file descriptor:
sin = open(sys.stdin.fileno(), <your open arguments here>)

For less trivial cases, you may need to read the TextIOWrapper docs (and BufferedReader and other types in the io module) and wrap exactly the part you want in exactly the way you want. But usually, open will be good enough.
Either way, you can just use sin in place of sys.stdin.
Or, since this now is a perfectly good TextIOWrapper (assuming you opened in text mode), it is safe and reasonable to replace sys.stdin:
sys.stdin = sin

… and now input, etc., will continue to work—and will work the way you wanted them to.",A,2
19049612,2013-09-27 11:28:23.977000+00:00,"You're including punctuation, numbers and spaces in the count,  which you're not supposed to be doing.",A,0
1224421,2009-08-03 20:24:53.647000+00:00,"If your file confuses the C libraries, then your results are expected.
The OS thinks it's 180Mb.
However, there are null bytes scattered around, which can confuse the C stdio libraries.
Try opening the file with ""rb"" and see if you get different results.",A,5
2005753,2010-01-05 11:41:07.107000+00:00,"Don't over-think it.
You don't need to fancy ""loopback"" device.
You're right about simulating the USB host.  You don't need to create a fancy USB loopback device or any silliness like that.

Design your client.
Design a ""generic"" USB interface layer that would simply passes requests to a real USB device driver.  Don't code this yet.
Code a USB interface layer that mocks the host instead of using the real USB device.
This has the same interface as your generic USB interface.  It's a proper subclass.
However, it doesn't actually use the USB device driver.  Instead it acts like it's the
web server on the USB device.  
Clearly, it's not a complete simulation.  It's just enough for you to test your client.
Each key feature that the client must interact with is represented with enough canned
responses that you're sure the client works.  
When your client works, code the real USB interface layer that really passes requests 
from client to device.  This is that you'll use in production and to do integration
tests with the real USB device.

Your client software now has two configurations -- all handled in software:

The real driver -- for production.
The mock driver -- for unit testing.

It shouldn't be any more complex than this.  And, you can now unit test your entire client using your mock host.",A,5
50963597,2018-06-21 08:13:59.463000+00:00,"A couple of things here. Firstly, you can do date calculations using datetime.timedelta.  And secondly, you don't want to check that the date is equal to ""today - 7"", you want to check it is greater than that and less than or equal to today. So:
seven_days_ago = today - timedelta(days=7)
for x in query:
    if seven_days_ago < x[1].date() <= today:
        ...
",A,2
17422431,2013-07-02 09:49:11.833000+00:00,"You are removing an item from the list returned by .items(). You need to delete from the dictionary instead:
result.popitem(last=False)

OrderedDict.popitem() returns the first or last key-value, after deleting it. Setting last to False signals you wanted to remove the first. 
Note that you can test against just the first value:
if result.values()[0] < score:

I'd also just use key assignment instead of .update() for just one element:
result[str(line)] = score  # instead of temp=str(line); result.update({temp:score})
",A,6
15325856,2013-03-10 18:24:51.293000+00:00,"The second example:
>>> L = [(('a', 'c'), -3), (('a', 'd'), -7), (('c', 'd'), -4)]
>>> zip(*[(a[0], a[1], b) for a, b in L])
[('a', 'a', 'c'), ('c', 'd', 'd'), (-3, -7, -4)]

It first flattens each item and then transposes the list.",A,3
25065353,2014-07-31 17:30:06.250000+00:00,"One very quick optimization would be to make names a set, since list lookup is linear (O(n)) whereas set lookup is constant (O(1)):
names = set()
doubleList = []
for number, object1 in enumerate(allinstances):
    if object1.name not in names:
        names.add(object1.name)
",A,1
17319214,2013-06-26 11:49:31.943000+00:00,"Python decodes all file data to Unicode values. You didn't specify an encoding to use, so Python uses the default for your system, the cp1252 Windows Latin codepage.
However, that is the wrong encoding for your file data. You need to specify an explicit codec to use:
data = open(""Ausstellungen.txt"",mode=""r"", encoding='UTF8')

What encoding to use exactly, is unfortunately something you need to figure out yourself. I used UTF-8 as an example codec.
Be aware that some versions of RoboCopy have problems producing valid output.
If you don't yet know what Unicode is, or want to know about encodings, see:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder

The reason you see the error crop up for different parts of your file is that your data contains more than one codepoint that the cp1252 encoding cannot handle.",A,0
2120425,2010-01-22 20:53:21.320000+00:00,"Mutual imports usually mean you've designed your models incorrectly.
When A depends on B, you should not have B also depending on A.
Break  B into two parts.
B1 - depends on A.
B2 - does not depend on A.
A depends on B1.  B1 depends on B2.  Circularity removed.",A,2
30711502,2015-06-08 14:13:38.923000+00:00,"The whole thing could be replaced by a generator expression:
def allNumbers(string):
    return ''.join(x for x in string if x.isdigit())
",A,4
16539379,2013-05-14 09:19:04.937000+00:00,"That's not what cycle is for: it's for alternating between two or more alternatives. You don't want that at all.
Instead, just use the forloop attributes:
{% for child in children %}
    <li class=""{% if forloop.first %}active{% elif forloop.last %}last{% endif %}"">...</li>
{% endfor %}

Although I suppose you don't want the first one to always be active, but you haven't given any information about how you do want to determine where 'active' goes.",A,0
15790339,2013-04-03 14:35:47.303000+00:00,"I'm not sure why you need to do this at all, but you are definitely overcomplicating things. You can just add the field dynamically to the form when you instantiate it:
class MultiSelectForm(forms.Form):
    def __init__(self, *args, **kwargs):
        dynamic_field = kwargs.pop('dynamic_field')
        super(MultiSelectForm, self).__init__(*args, **kwargs)
        self.fields[dynamic_field['name']] = forms.MultipleChoiceField(
            choices=dynamic_field['choices'], 
            empty_permitted=False
            required=dynamic_field['required'],
            widget=MultiSelect(attrs=dynamic_field['attrs']))

and then pass a dictionary on instantiation:
my_form = MultiSelectForm(
    dynamic_field={'name': 'countries', 'choices': (mychoices), 'required': True})
",A,2
43369474,2017-04-12 12:25:06.363000+00:00,"Python's copy module will not produce copies of immutable objects, that'd be very inefficient. decimal.Decimal() objects are immutable, so they just return self for copy operations:
>>> from decimal import Decimal
>>> d = Decimal()
>>> d.__copy__() is d
True
>>> d.__deepcopy__({}) is d
True

See the decimal module documentation:

A decimal number is immutable.

Because they are immutable, there is no point in creating a copy; everywhere you could use the copy, you could safely use the original too, but without wasting the memory on two completely identical objects that can never diverge.",A,6
35553970,2016-02-22 12:40:15.013000+00:00,"
I have searched high and low over the last few days for clues about how I'd then sort all the lines in the file by datetime.

def get_time(line):
    match = re.search(r""</a>\s*(.+?)\s*<br><br>"", line)
    if match:
        return datetime.strptime(match.group(1), ""%I:%M %p - %d %b %Y"")
    return datetime.min

lines.sort(key=get_time)

It assumes that the time is monotonous in the given time period (e.g., no DST transitions) otherwise you should convert the input time to UTC (or POSIX timestamp) first.",A,1
12891905,2012-10-15 08:34:50.730000+00:00,"No, you cannot use pip to install anything but python distributions.
Use another installation tool; buildout can manage both Python distributions and other installation tasks with python extensions called recipes. 
Alternatively, you could package up your jQuery dependency as a python distribution, but such a distribution would be very tightly bound to your application. The Plone community does do this, but they include configuration for the Plone CMS to integrate the libraries into their registries; see plone.app.jquery for example, the contents of the package include more than just jQuery.",A,0
27986480,2015-01-16 14:45:56.897000+00:00,"multiprocessing has a thread pool that doesn't start other processes:
#!/usr/bin/env python
from multiprocessing.pool import ThreadPool
from time import time as timer
from urllib2 import urlopen

urls = [""http://www.google.com"", ""http://www.apple.com"", ""http://www.microsoft.com"", ""http://www.amazon.com"", ""http://www.facebook.com""]

def fetch_url(url):
    try:
        response = urlopen(url)
        return url, response.read(), None
    except Exception as e:
        return url, None, e

start = timer()
results = ThreadPool(20).imap_unordered(fetch_url, urls)
for url, html, error in results:
    if error is None:
        print(""%r fetched in %ss"" % (url, timer() - start))
    else:
        print(""error fetching %r: %s"" % (url, error))
print(""Elapsed Time: %s"" % (timer() - start,))

The advantages compared to Thread-based solution:

ThreadPool allows to limit the maximum number of concurrent connections (20 in the code example)
the output is not garbled because all output is in the main thread 
errors are logged
the code works on both Python 2 and 3 without changes (assuming from urllib.request import urlopen on Python 3).
",A,24
20672084,2013-12-19 01:54:27.297000+00:00,"
Should setuputils even be responsible for creating the service on the machine, or rather should this be handled by an external package manager like dpkg/apt/rpm

Almost certainly the latter.
distutils/setuptools is not designed to handle things like this.
There's some configuration information that's sufficient for installing site-packages, shared data, executables, and maybe a few other things in ways that make sense on your platforms. But there's nowhere near enough to handle things like installing init scripts.
These tools are designed to handle not just slightly different early-2010s-era Ubuntu-like linux distros, but a wide variety of different platforms. On non-Ubuntu-like distros (and pre-lucid Ubuntu) there is no Upstart, but there is SysV-style init. On some other *nixes, there isn't even SysV-style init, but there is BSD-style init. On OS X, while SysV-style init does exist, it's heavily deprecated and launchd is used instead. On Windows, there isn't anything even remotely similar, but there are completely different ways to set up ""services"" and ""run-at-startup"" programs and related concepts.
On top of that, on many platforms, the package manager wants to be able to own all startup scripts, and you don't want to violate that expectation on a user/sysadmin's behalf without him specifically asking for it.
So, you need a platform-specific package for each platform. If you just create a PyPI package and a .deb for Ubuntu Precise or whatever you use, if some Fedora or Mac or Ubuntu Natty user gets jealous, they'll either do it themselves, or ask you.",A,1
43022183,2017-03-25 21:30:48.330000+00:00,"You'd create a new list for each iteration of the outer for loop:
v=[1,2,3]
b = []
for s in range(0,5):
    result = []
    for i in range (0,3):
         tot= np.multiply(v[i],s)
         result.append(tot)
    b.append(result)
print (b)

You could just use * to multiply values, and you can iterate directly over v (no need to use a range)`:
v = [1, 2, 3]
b = []
for s in range(5):
    result = []
    for i in v:
        result.append(i * s)
    b.append(result)

You can replace both loops with list comprehensions:
b = [[i * s for i in v] for s in range(5)]
",A,0
23700306,2014-05-16 16:31:17.927000+00:00,"In Python 2, all objects are orderable.
CPython (the original Python implentation) orders None before numbers, then numbers before everything else. Objects that don't implement ordering themselves, are compared by their type name; so function sorts before instances of the class Zippedeedoodah.  Other implementations are free to pick different orders.
Python does this to support sorting of heterogeneous lists. Quoting the Comparisons expressions documenation:

The operators <, >, ==, >=, <=, and != compare the values of two objects. The objects need not have the same type. If both are numbers, they are converted to a common type. Otherwise, objects of different types always compare unequal, and are ordered consistently but arbitrarily.

In Python 3, only objects that explicitly support ordering are supported, everything else throws an exception:
>>> def a(x): return x+3 
... 
>>> a < 4
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unorderable types: function() < int()
",A,13
20751711,2013-12-23 21:52:20.020000+00:00,"First, you've got the base case wrong:
if n == 1:
    return fn

After all, repeat(fn, 1) is just a function that applies fn once—that's fn.
Now, if the base case is when n == 1, the recursive case is almost always going to be something where you pass n - 1 to yourself.
So, what's the difference between repeat(fn, n) and repeat(fn, n-1)? If you can't figure it out, expand a simple case out in your head or on paper:
repeat(fn, 3)(x): fn(fn(fn(x)))
repeat(fn, 2)(x): fn(fn(x))

And now it's obvious: repeat(fn, n) is the same thing as fn(repeat(fn, n-1)), right? So:
else:
    def new_fn(x):
        return fn(repeat(fn, n-1)(x))
    return new_fn


However, as filmor points out in the comments, it would be easier to use partial here:
def repeat3(fn, n, x):
    if n == 1:
        return fn(x)
    else:
        return fn(repeat3(fn, n-1, x))

def repeat(fn, n):
    return functools.partial(repeat3, fn, n)
",A,4
29834351,2015-04-23 21:09:00.047000+00:00,"The docs for multiprocessing.Queue apparently don't say this directly, but it's strongly implied in two  places.
First, the Queue docs say:

Queue implements all the methods of queue.Queue except for task_done() and join().

And earlier, in Exchanging objects between processes:

The Queue class is a near clone of queue.Queue.

And, if you follow the link:

Constructor for a FIFO queue. maxsize is an integer that sets the upperbound limit on the number of items that can be placed in the queue. Insertion will block once this size has been reached, until queue items are consumed. If maxsize is less than or equal to zero, the queue size is infinite.

So, yes, -1 is documented to mean infinite… although maybe not in the most direct way possible.
You could file a doc bug on this. Maybe all of the classes should explicitly say ""The interface is exactly like <relevant other class, with link> except as specified below"", instead of leaving you to find that information elsewhere in the docs?

Since you actually asked two questions, not one, here's where SEM_VALUE_MAX is defined:
In PyInit__multiprocessing, the top-level code for the _multiprocessing module, it's added dynamically to the __dict__ of the SemLock type.
The value it gets set to is platform-specific, but it's generally going to be your platform's SEM_VALUE_MAX or INT_MAX. The former is defined in POSIX 2004 as part of limits.h, and just specifies:

The maximum value a semaphore may have.

The source comments may be a bit confusing.
I don't know about NetBSD, but IIRC, old FreeBSD did the same: the value is provided as a signed integer (actually, it's just a #define preprocessor macro, but that makes it an int-typed constant), but the APIs that use it take unsigned values. In C, you can just pass -1 to a function that takes an unsigned int and it's the same as passing UINT_MAX (4294967295). Python doesn't have those casting rules, so the module just converts a -1 to INT_MAX (2147483647), which is safer, and you're not going to have more than 2.1 billion semaphores. I believe current FreeBSD and its descendants, like OS X, just give you USHORT_MAX (32767), which makes this unnecessary, but there may be other OS's that do the same.",A,3
15202477,2013-03-04 13:17:10.240000+00:00,"Your backend authenticate() method returns UserProfile (the class) instead of userprofile (the instance of that class belonging to the user).
Although there's nothing actually wrong in the middleware, it would be clearer if you kept to the same naming convention - which is the normal Python one - and referred to request.userprofile instead of request.UserProfile.",A,0
13417982,2012-11-16 13:52:26.940000+00:00,"You are interpreting the HTML returned as a list; by looping over text you create individual characters:
>>> for ch in 'sample':
...     print ch,
s a m p l e

Just interpolate the html variable directly:
<body>
    {{ html }}
</body>
",A,3
20026201,2013-11-17 01:45:04.140000+00:00,"You have an outer wrapping to traverse here; your results object is a dictionary with one key 'd', referencing a dictionary value which has a 'results' key, and that is a list of dictionaries, and there we find a dictionary with the 'Web' key finally:
for result in results['d']['results']:
    for item in result.get('Web', []):
        print item.get(u'DisplayUrl')

Demo:
>>> for result in results['d']['results']:
...     for item in result.get('Web', []):
...         print item.get(u'DisplayUrl')
... 
www.test.com
www.speakeasy.net/speedtest
",A,1
23130845,2014-04-17 10:35:45.573000+00:00,"The Jinja context is a namespace; so yes, you have to use a dictionary, otherwise the values are not bound to names for you to refer to in your template.",A,1
15752354,2013-04-01 21:38:07.090000+00:00,"No, Python doesn't allow this. Assignment is a statement only, and can't be embedded in an expression.",A,2
359506,2008-12-11 14:23:17.213000+00:00,"There is an easy way to execute a sequence of commands.
Use the following in subprocess.Popen
""command1; command2; command3""

Or, if you're stuck with windows, you have several choices.

Create a temporary "".BAT"" file, and provide this to subprocess.Popen
Create a sequence of commands with ""\n"" separators in a single long string.

Use """"""s, like this.
""""""
command1
command2
command3
""""""

Or, if you must do things piecemeal, you have to do something like this.
class Command( object ):
    def __init__( self, text ):
        self.text = text
    def execute( self ):
        self.proc= subprocess.Popen( ... self.text ... )
        self.proc.wait()

class CommandSequence( Command ):
    def __init__( self, *steps ):
        self.steps = steps
    def execute( self ):
        for s in self.steps:
            s.execute()

That will allow you to build a sequence of commands.",A,19
30615661,2015-06-03 09:11:54.897000+00:00,"Use a loop and track the maximum value found so far per country. You'll have to parse those dates into datetime objects so that you can easily compare them:
from datetime import datetime

max_dates = {}
for entry in list_of_dicts:
    date = datetime.strptime(entry['date'], '%d-%b-%y')
    country = entry['country']
    if country not in max_dates or date > max_dates[country][0]:
        max_dates[country] = (date, entry)

result = [entry for date, entry in max_dates.values()]

Demo:
>>> from datetime import datetime
>>> list_of_dicts = [
...     {'country' : 'India', 'date' : '18-Mar-14'},
...     {'country' : 'India', 'date' : '18-Apr-14'},
...     {'country' : 'India', 'date' : '18-May-14'},
...     {'country' : 'Australia', 'date' : '18-Mar-14'},
...     {'country' : 'Australia', 'date' : '18-Apr-14'},
...     {'country' : 'Australia', 'date' : '18-May-14'},
...     {'country' : 'China', 'date' : '18-Mar-14'},
...     {'country' : 'China', 'date' : '18-Apr-14'},
...     {'country' : 'China', 'date' : '18-May-14'}
... ]
>>> max_dates = {}
>>> for entry in list_of_dicts:
...     date = datetime.strptime(entry['date'], '%d-%b-%y')
...     country = entry['country']
...     if country not in max_dates or date > max_dates[country][0]:
...         max_dates[country] = (date, entry)
... 
>>> [entry for date, entry in max_dates.values()]
[{'date': '18-May-14', 'country': 'China'}, {'date': '18-May-14', 'country': 'Australia'}, {'date': '18-May-14', 'country': 'India'}]
",A,3
31185907,2015-07-02 13:25:54.290000+00:00,"Don't encode the line; print Unicode text directly:
#!/usr/bin/env python3
from itertools import islice

with open('instagram_info.txt', encoding='utf-8-sig') as file:
    print(""reading  images URLs file"")
    for line in islice(file, 50): # read no more than 50 lines from the file
        print(""Line: "", line, end='')
",A,0
20280856,2013-11-29 07:56:36.053000+00:00,"You actually have two problems here. Fixing either one of them would actually eliminate your immediate error, but you need to fix both of them.

When I try to run this library from the command prompt by typing regetron

You shouldn't have a script named regetron and also have a module or package named regetron. Fix it by renaming your script. But if you want to understand why it's causing a problem:
The current working directory is always part of sys.path. So, you're in the directory with regetron.py in, and you run it with regetron. That means that regetron.py is on the path. So when you import regetron, it finds your script, not the package. Or, when you from regetron import engine, it finds your script, and tries to import a variable/function/class/whatever named engine from it, rather than finding the package and trying to import a module underneath it.


I added c:\Python27\Lib\site-packages\regetron to %PYTHONPATH%

Never add a package's directory to sys.path.
Since site-packages is already on your sys.path, the code in regetron/engine.py is already available as regetron.engine. You don't want it to also be available as engine. This will cause all kinds of problems.

So, rename you script to something else, remove regetron from %PYTHONPATH%, and everything will be fine.
But you may want to (re-)read the section on Packages in the tutorial.",A,0
21216422,2014-01-19 11:57:06.620000+00:00,"The timeit.Timer class can be used in two different ways.
It can either take source code to be compiled an executed—in which case, the code is executed in a fresh environment where only the setup code has been run, or it can take a callable, in which case the callable is just called (in your current environment, like any other callable).
So, you have two options:
u = timeit.Timer(""np.arange(1000)"", setup='import numpy as np')

… or …
u = timeit.Timer(lambda: np.arange(1000))

In the first case, the fact that you happen to have done an import numpy as np is irrelevant; it has no effect on the environment in which np.arange(1000) is compiled and executed (and thus you must include it in the setup=... bit).
In the second case, the fact that you've done an import numpy as np obviously is relevant—it affects the environment in which your code, including the lambda: np.arange(1000), gets evaluated.",A,12
2370725,2010-03-03 11:25:59.793000+00:00,"Floating-point division a/b is not mathematical division a ÷ b, except in very rare* circumstances.  
Generally, floating point division a/b is a ÷ b + ε.  
This is true for two reasons.

Float numbers (except in rare cases) are an approximation of the decimal number.
a is a + εa.  
b is b + εb.
Float numbers uses a base 2 encoding of the digits to the right of the decimal place.  When you write 3.1, this is expanded to a base-2 approximation that differs from the real value by a small amount.  
Real decimal numbers have the same problem, by the way.  Write down the decimal expansion of 1/3.  Oops.  You have to stop writing decimal places at some point.  Binary floating point numbers have the same problem.
Division has a fixed number of binary places, meaning the answer is truncated.  If there's a repeating binary pattern, it gets chopped.  In rare cases, this doesn't matter.  In general, you've introduced error by doing division.

Therefore, when you do something like repeatedly add 1/k values you're computing
1 ÷ k + ε
And adding those up.  Your result (if you had the right range) would be
n × (1 ÷ k + ε) = n ÷ k + n × ε
You've multiplied the small error, ε, by n.   Making it a big error.  (Except in rare cases.)
This is bad.  Very bad.  All floating point division introduces an error.  Your job as a programmer is to do the algebra to avoid or defer division to prevent this.  Good software design means good algebra to prevent errors being introduced by the division operator.
[* The rare cases. In rare cases, the small error happens to be zero.  The rare cases occur when your floating point values are small whole numbers or fractions that are sums of powers of two 1/2, 1/4, 1/8, etc.  In the rare case that you have a benign number with a benign fractional part, the error will be zero.]  ",A,8
20085884,2013-11-20 01:52:13.357000+00:00,"There is no persistent_lru_cache in the stdlib. But you can build one pretty easily.
The functools source is linked directly from the docs, because this is one of those modules that's as useful as sample code as it is for using it directly.
As you can see, the cache is just a dict. If you replace that with, say, a shelf, it will become persistent automatically:
def persistent_lru_cache(filename, maxsize=128, typed=False):
    """"""new docstring explaining what dbpath does""""""
    # same code as before up to here
    def decorating_function(user_function):
        cache = shelve.open(filename)
        # same code as before from here on.

Of course that only works if your arguments are strings. And it could be a little slow.
So, you might want to instead keep it as an in-memory dict, and just write code that pickles it to a file atexit, and restores it from a file if present at startup:
    def decorating_function(user_function):
        # ...

        try:
            with open(filename, 'rb') as f:
                cache = pickle.load(f)
            except:
                cache = {}
        def cache_save():
            with lock:
                with open(filename, 'wb') as f:
                    pickle.dump(cache, f)
        atexit.register(cache_save)

        # …
        wrapper.cache_save = cache_save
        wrapper.cache_filename = filename

Or, if you want it to write every N new values (so you don't lose the whole cache on, say, an _exit or a segfault or someone pulling the cord), add this to the second and third versions of wrapper, right after the misses += 1:
            if misses % N == 0:
                cache_save()


See here for a working version of everything up to this point (using save_every as the ""N"" argument, and defaulting to 1, which you probably don't want in real life).
If you want to be really clever, maybe copy the cache and save that in a background thread.
You might want to extend the cache_info to include something like number of cache writes, number of misses since last cache write, number of entries in the cache at startup, …
And there are probably other ways to improve this.
From a quick test, with save_every=1, this makes the cache on both get_pep and fib (from the functools docs) persistent, with no measurable slowdown to get_pep and a very small slowdown to fib the first time (note that fib(100) has 100097 hits vs. 101 misses…), and of course a large speedup to get_pep (but not fib) when you re-run it. So, just what you'd expect.",A,5
1044777,2009-06-25 16:03:54.363000+00:00,"I would say that if you're worried about 'enterprise-level' support, you should be looking more at Django. Although you can debate the relative technical merits of the two frameworks, there's no doubt that Django has the bigger uptake, and there are quite a few large companies using it.
One additional reason is that IBM have (just in the last couple of days) released a Django driver for DB2, so you should have no problem using your existing database with the Django ORM.",A,2
49988443,2018-04-23 19:18:23.537000+00:00,"Before doing anything, note that there is something very wrong with your connection, and diagnosing that and getting it fixed is far better than working around it. But sometimes, you just have to deal with a broken server, and even sending keepalives doesn't help. So, what can you do?
The trick is to download a chunk at a time, then abort the download—or, if the server can't handle aborting, close and reopen the connection.
Note that I'm testing everything below with ftp://speedtest.tele2.net/5MB.zip, which hopefully this doesn't cause a million people to start hammering their servers. Of course you'll want to test it with your actual server.
Testing for REST
The entire solution of course relies on the server being able to resume transfers, which not all servers can do—especially when you're dealing with something badly broken. So we'll need to test for that. Note that this test will be very slow, and very heavy on the server, so do not testing with your 3GB file; find something much smaller. Also, if you can put something readable there, it will help for debugging, because you may be stuck comparing files in a hex editor.
def downit():
    with open('5MB.zip', 'wb') as f:
        while True:
            ftp = FTP(host='speedtest.tele2.net', user='anonymous', passwd='test@example.com')
            pos = f.tell()
            print(pos)
            ftp.sendcmd('TYPE I')
            sock = ftp.transfercmd('RETR 5MB.zip', rest=pos)
            buf = sock.recv(1024 * 1024)
            if not buf:
                return
            f.write(buf)

You will probably not get 1MB at a time, but instead something under 8KB. Let's assume you're seeing 1448, then 2896, 4344, etc.

If you get an exception from the REST, the server does not handle resuming—give up, you're hosed.
If the file goes on past the actual file size, hit ^C, and check it in a hex editor.


If you see the same 1448 bytes or whatever (the amount you saw it printing out) over and over again, again, you're hosed.
If you have the right data, but with extra bytes between each chunk of 1448 bytes, that's actually fixable. If you run into this and can't figure out how to fix it by using f.seek, I can explain—but you probably won't run into it.


Testing for ABRT
One thing we can do is try to abort the download and not reconnect.
def downit():
    with open('5MB.zip', 'wb') as f:
        ftp = FTP(host='speedtest.tele2.net', user='anonymous', passwd='test@example.com')
        while True:
            pos = f.tell()
            print(pos)
            ftp.sendcmd('TYPE I')
            sock = ftp.transfercmd('RETR 5MB.zip', rest=pos)
            buf = sock.recv(1024 * 1024)
            if not buf:
                return
            f.write(buf)
            sock.close()
            ftp.abort()

You're going to want to try multiple variations:

No sock.close.
No ftp.abort.
With sock.close after ftp.abort.
With ftp.abort after sock.close.
All four of the above repeated with TYPE I moved to before the loop instead of each time.

Some will raise exceptions. Others will just appear to hang forever. If that's true for all 8 of them, we need to give up on aborting. But if any of them works, great!
Downloading a full chunk
The other way to speed things up is to download 1MB (or more) at a time before aborting or reconnecting. Just replace this code:
buf = sock.recv(1024 * 1024)
if buf:
    f.write(buf)

with this:
chunklen = 1024 * 1024
while chunklen:
    print('   ', f.tell())
    buf = sock.recv(chunklen)
    if not buf:
        break
    f.write(buf)
    chunklen -= len(buf)

Now, instead of reading 1442 or 8192 bytes for each transfer, you're reading up to 1MB for each transfer. Try pushing it farther.
Combining with keepalives
If, say, your downloads were failing at 10MB, and the keepalive code in your question got things up to 512MB, but it just wasn't enough for 3GB—you can combine the two. Use keepalives to read 512MB at a time, then abort or reconnect and read the next 512MB, until you're done.",A,1
24914634,2014-07-23 15:26:11.910000+00:00,"__abstractmethods__ is a descriptor to support Abstract Base Classes; it wraps a slot that is empty by default (so the descriptor raises an attribute error). Most of all, it is an implementation detail of how CPython handles abstract methods.
The attribute is used to track what methods are abstract, so that instances can be blocked from being created if they don't provide concrete implementations:
>>> import abc
>>> class FooABC(metaclass=abc.ABCMeta):
...     @abc.abstractmethod
...     def bar(self):
...         pass
... 
>>> FooABC.__abstractmethods__
frozenset({'bar'})
>>> class Foo(FooABC): pass
... 
>>> Foo()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: Can't instantiate abstract class Foo with abstract methods bar

The abc.ABCMeta implementation sets the __abstractmethods__ attribute, and type() uses it to check for any abstract methods that should have been implemented but are not.",A,4
1795642,2009-11-25 09:16:06.777000+00:00,"The documentation explains this.
Middleware classes are processed in the order they appear. The CommonMiddleware class is higher up than your LogMiddleware class, so is processed first. It performs a redirect because your URL doesn't end with a slash, so returns an HttpResponseRedirect. 
If a request middleware returns a response, as in this case, no further request middleware classes are processed, so 'start' is not logged. However, response middleware classes are always processed, so 'end' is logged.",A,14
11626890,2012-07-24 08:21:24.400000+00:00,"This question doesn't have anything to do with BeautifulSoup.
The problem is that, as the documentation explains, match only matches at the beginning of the string. Since the digits you want to find are at the end of the string, it returns nothing.
To match on a digit anywhere, use search - and you probably want to use the \d entity for digits.
matches = re.search(r'\d+', theURL)
",A,5
1706558,2009-11-10 09:09:45.333000+00:00,"Member.objects.filter(group__in=bob.group_set.all()).exclude(pk=bob.pk)

Edit I didn't notice that you didn't have a ManyToMany relationship set up between Member and Group. You'll need to add that:
class Group(Model):
   name = CharField(max_length=30)
   members = ManyToManyField(Member, through='Membership')

now syncdb and it should work. ",A,1
2194814,2010-02-03 19:27:46.990000+00:00,"I would do this.
Starting with this code.
def foo(
        flab_nickers, # a series of under garments to process
        has_polka_dots=False,
        needs_pressing=False  # Whether the list of garments should all be pressed
   ):
    ...

I would write a parser that grabs the function parameter definitions and builds the following:
def foo(
        flab_nickers, 
        has_polka_dots=False,
        needs_pressing=False,
   ):
   """"""foo

   :param flab_nickers: a series of under garments to process
   :type flab_nickers: list or tuple
   :param has_polka_dots: default False
   :type has_polka_dots: bool
   :param needs_pressing: default False, Whether the list of garments should all be pressed
   :type needs_pressing: bool
   """"""
    ...

That's some pretty straight-forward regex processing of the various arguments string patterns to fill in the documentation template.
A lot of good Python IDEs (for example PyCharm) understand the default Sphinx param notation and even flag vars/methods in the scope that IDE thinks does not conform to the declared type.
Note the extra comma in the code; that's just to make things consistent.  It does no harm, and it might simplify things in the future.
You can also try and use the Python compiler to get a parse tree, revise it and emit the update code.  I've done this for other languages (not Python), so I know a little bit about it, but don't know how well supported it is in Python.
Also, this is a one-time transformation.
The original in-line comments in the function definition don't really follow DRY because it's a comment, in an informal language, and unusable by any but the most sophisticated tools.  
The Sphinx comments are closer to DRY because they're in the RST markup language, making them much easier to process using ordinary text-parsing tools in docutils.
It's only DRY if tools can make use of it.
Useful links:
https://pythonhosted.org/an_example_pypi_project/sphinx.html#function-definitions
http://sphinx-doc.org/domains.html#id1",A,8
2759523,2010-05-03 16:24:04.523000+00:00,"You probably want to use
request.POST.getlist('categories')

which will return all the selected values for that form field.",A,6
20824014,2013-12-29 10:05:19.693000+00:00,"The code does not produce valid JSON, as you can see from your example: there is no comma between the lists. 
You don't want to produce JSON by concatenating strings. Instead, build up lists or dicts in Python, then dump to JSON at the end:
data = []
if users:
  data.append([{'label': user.first_name, 'value': '/'+ user.username + '/'} for user in users])
if cols:
  data.append([{'label': col.name, 'value': col.get_absolute_url()} for col in cols])
if fes:
  data.append([{'label': fe.name, 'value': fe.get_absolute_url()} for fe in fes])
json_str = json.dumps(data)
",A,1
15395033,2013-03-13 19:46:53.930000+00:00,"You can manipulate the result of pb.property_list(), so this would work:
 properties = pb.property_list()
 properties[:] = [e for e in properties if e.name() != '__scatter__']
",A,1
2367102,2010-03-02 21:53:23.350000+00:00,"lxml supports XPath:
from lxml import etree
root = etree.fromstring(""...your xml..."")

el, = root.xpath(""//div[text() = 'the string']/preceding-sibling::*[9]"")
",A,3
25888809,2014-09-17 11:00:57.763000+00:00,"offset is a datetime.timedelta object. If you need just the seconds, extract them with timedelta.total_seconds():
return utc_datetime + offset.total_seconds()

Your function signature however, suggests it was expecting you to feed it a datetime.datetime() object, in which case you shouldn't change this function, but the code that calls it. Clearly you are giving it an integer instead.",A,7
19145477,2013-10-02 19:44:07.123000+00:00,"
I already tried the audioop module as it has got something that converts ADPCM to linear streams but I neither know what linear streams are nor how to use the function that converts them.

The short version: ""Linear"" is what you want.* So, the function you want is adpcm2lin.

How do you use it?
Almost everything in audioop works the same way: you loop over frames, and call a function on each frame. If your input data has some inherent frame size, like when you're reading from an MP3 file (using an external library), or your output library demands some specific frame size, you're a bit constrained on how you determine your frames. But when you're dealing with raw PCM formats, the frames are whatever size you want, from a single sample to the whole file.**
Let's do the whole file first, for simplicity:
with open('spam.adpcm', 'rb') as f:
    adpcm = f.read()
pcm, _ = audioop.adpcm2lin(adpcm, 2, None)

If your adpcm file is too big to load into memory and process all at once, you'll need to keep track of the state, so:
with open('spam.adpcm', 'rb') as f:
    state = None
    while True:
        adpcm = f.read(BLOCKSIZE)
        if not adpcm:
            return
        pcm, state = audioop.adpcm2lin(adpcm, 2, state)
        yield pcm


Of course I'm assuming that you don't need to convert the sample rate or do anything else. If you do, any such conversions should come after the ADPCM decompression.***

* The long version: ""Linear"" means the samples are encoded directly, rather than mapped through another algorithm. For example, if you have a 16-bit A-to-D, and you save the audio in an 8-bit linear PCM file, you're just saving the top 8 bits of each sample. That gives you a very dynamic range, so quieter sounds get lost in the noise. There are various companding algorithms that give you a much wider dynamic range for the same number of bits (at the cost of losing other information elsewhere, of course); see μ-law algorithm for details on how they work. But if you can stay in 16 bits, linear is fine.
** Actually, with 4-bit raw ADPCM, you really can't do a single sample… but you can do 2 samples, which is close enough.
*** If you're really picky, you might want to convert to 32-bit first, then do the work, then convert back to 16-bit to avoid accumulating losses. But when you're starting with 4-bit ADPCM, you aren't going for audiophile sound here.",A,4
5020576,2011-02-16 18:30:32.850000+00:00,"
Does this have to come after I define the function?

Yes.

q = (1-p)**(n-x)
TypeError: unsupported operand type(s) for -: 'int' and 'str'

You have two - operations.  One of those two is a mixture of int and str data.
Let's go through each operand.
1 - int
p - result of input(), and therefore a string
n - result of input(), and therefore a string
x - result of input(), and therefore a string
You probably want to do something to convert the results of input() to a proper floating-point value.  float() works well for this.",A,2
27906182,2015-01-12 16:09:29.303000+00:00,"You are using a generator expression in your list comprehension:
sum(samples[i-j] for j in range(n))

Generator expressions require a new frame to be created each time you run one, just like a function call. This is relatively expensive.
You don't need to use a generator expression at all; you only need to slice the samples list:
sum(samples[i - n + 1:i + 1])

You can specify a second argument, a start value for the sum() function; set it to 0.0 to get a float result:
sum(samples[i - n + 1:i + 1], 0.0)

Together these changes make all the difference:
>>> from timeit import timeit
>>> import random
>>> testdata = [i*random.random() for i in range(1000)]
>>> def slow_moving_average(samples, n=3):
...     return [float(sum(samples[i-j] for j in range(n)))/n for i in range(n-1, len(samples))]
... 
>>> def fast_moving_average(samples, n=3):
...     return [sum(samples[i - n + 1:i + 1], 0.0) / n for i in range(n-1, len(samples))]
... 
>>> def verbose_moving_average(samples, n=3):
...     l =[]
...     for i in range(n-1, len(samples)):
...         x = 0.0
...         for j in range(n): 
...             x+= samples[i-j]
...         l.append(x / n)
...     return l
... 
>>> timeit('f(s)', 'from __main__ import verbose_moving_average as f, testdata as s', number=1000)
0.9375386269966839
>>> timeit('f(s)', 'from __main__ import slow_moving_average as f, testdata as s', number=1000)
1.9631599469939829
>>> timeit('f(s)', 'from __main__ import fast_moving_average as f, testdata as s', number=1000)
0.5647804250038462
",A,13
4245313,2010-11-22 12:19:03.040000+00:00,"To add information into the db from forms, you should be using ModelForms. It's all very well documented here.",A,3
53109969,2018-11-01 21:59:29.950000+00:00,"soup = soup.prettify() returns a string, and because you assigned it back to soup, that makes soup a string when you call soup.find_all().
From the pretty printing section of the BeautifulSoup documentation:

The prettify() method will turn a Beautiful Soup parse tree into a nicely formatted Unicode string.

Don't replace your soup with a prettified string. BeautifulSoup doesn't need prettifying, that's only needed when you want to turn the soup back into a  string for saving to a file or debugging.
soup = BeautifulSoup(text, 'html.parser')
print(soup.find_all('div')) 

works just fine.
You also do not want to use str(html) to decode the bytes object. Normally you'd use html.decode('utf8') or similar; str(html) gives you a value that starts with b' and ends with '
However, BeautifulSoup is perfectly capable of decoding bytes values by itself. It can also read directly from the response:
with urllib.request.urlopen('https://jalopnik.com/search?q=mazda&u=&zo=-07:00') as response:
    soup = BeautifulSoup(response, 'html.parser')
print(soup.find_all('div')) 
",A,1
49262269,2018-03-13 17:24:22.860000+00:00,"A lot of Python 3 code runs in Python 2 because Python was specifically designed to make that true.

print is a bit of a special case—simple prints of a single variable do the same thing in both languages, but more complex prints may not. How does this work?
In Python 3, print(shopping_list) is a call to the print function with a single argument, shopping_list.
In Python 2, print(shopping_list) is a print statement with one printable, the value (shopping_list), which is of course the same value as shopping_list. So you get the same thing.
Of course that isn't true as soon as you print(x, y) (which works, but prints a single 2-tuple value rather than two values in Python 2) or print(x, file=sys.stderr) (which is an error in Python 2).

input is another special case. This one wasn't designed to be compatible, you're just getting lucky here. Or maybe unlucky.
In Python 2, input() does eval(raw_input())—that is, it takes what you type and tries to evaluate it as Python source.
In Python 3, input() does what raw_input() did in Python 2—that is, it just returns a string.
So, the code sort of works in both, although you have to type ""DONE"" instead of just DONE in Python 2—just like you have to type ""DONE"" in your source code. Plus, of course, you can type __import__('os').system('rm -rf /') in Python 2 and make yourself very sad. (This is why Python 2's input doesn't exist in Python 3.)

If you're asking why Python 3 was designed this way:
Once it was decided that some backward incompatibility was required in order to fix two decades of backlogged problems, there was a discussion about whether ""Python 3000"" should be intentionally incompatible to noisily break everything, or as compatible as possible (but no more so) to make migration easier.
The team decided on the latter. That included making some changes to Python 2.6 to make it easier to eventually migrate to 3.x, and some design decisions in Python 3.0 that were only made to make migration from 2.6 easier.
Once Python 3 got out into the wild, that turned out to be the right choice. To almost everyone's surprise, it was actually easier to write ""dual-version"" code (with the help of adapter libraries like six) than to write code that could be auto-transformed via 2to3 and 3to2.
So they went even farther. Python 2.7 added a few features designed just for dual-version code, and they've continued to add things like the u string literal prefix (which does nothing in Python 3, but it's a way of making sure you have Unicode strings whether you run in 2.7 or 3.4) and bytes.__mod__ (which allows Python 2 %-formatting code to continue to work in Python 3 in cases where you've deliberately chosen to keep things in bytes—e.g., parsing HTTP, where all you know is that it's some ASCII-compatible charset until you reach the charset header).
By the way, one of those ""forward-compatibility"" changes to Python 2 was the addition of from __future__ import print_function, which gives you Python 3-style print.",A,4
11596034,2012-07-21 21:37:30.323000+00:00,"Because this is Python, not C++ (or Java). Calling it a misfeature is a fundamental misunderstanding of how Python works.
In Python you don't declare variables, or attributes. There's no such thing as ""a data member of the point class"". Your i is just a class-level variable, but it would be the same wherever you associate that attribute with the class. You can dynamically add attributes to classes, instances, modules, whatever you like. That's what it is to be a dynamically typed language.
In fact, doing this like this is the only way to define instance variables. As I said, your i above is a class attribute, shared by all members of the class. The only way to get an instance-level variable is to ""dynamically"" add it, usually in the __init__ method but you can do it wherever you like.",A,7
39663307,2016-09-23 14:29:28.670000+00:00,"You overrode the fields, so they won't preserve any of the attributes; you need to set them explicitly.
line1 = forms.CharField(label=""Address"", required=False)

Note also that your model field definitions shouldn't have null=True for CharFields, this is bad practice.",A,2
18970869,2013-09-24 00:04:50.097000+00:00,"Python doesn't treat NUL bytes as anything special; they're no different from spaces or commas. So, split() works fine:
>>> my_string = ""Health\x00experience\x00charactername\x00""
>>> my_string.split('\x00')
['Health', 'experience', 'charactername', '']

Note that split is treating \x00 as a separator, not a terminator, so we get an extra empty string at the end. If that's a problem, you can just slice it off:
>>> my_string.split('\x00')[:-1]
['Health', 'experience', 'charactername']
",A,5
1650507,2009-10-30 15:30:56.410000+00:00,"Keys are a hack.
A relational database must use keys because of the restrictions placed on the locations of rows within a table.  It can't use direct references because no such reference can possibly exist in a proper relational database.
An object can contain a direct reference to another object without resorting to keys or key lookup or any such foolishness.",A,0
52046041,2018-08-27 19:43:58.980000+00:00,"You don't pass it to reverse_lazy. Instead of using success_url, you should define the get_success_url method, which allows you to create the URL dynamically using whatever parameters you want.
However there are few other things wrong with your code here. Firstly, you should not be trying to do all that calculation based on the HTTP_REFERER attribute. If your view needs a piece of information, you should pass it in the URL as a keyword parameter, which you can then get in your view by using self.kwargs. In your case it looks like your view already has the pk argument; you can use self.kwargs['pk']. 
Given that, your get_success_url method would look like:
def get_success_url(self):
    return reverse('piece-detail', kwargs={'pk': self.kwargs['pk')})

Secondly, your get_form_kwargs method will always give a KeyError; the super method won't return a dictionary with a ""piece"" key, and even if it did the method must return a dict, not an individual value, including all the relevant items like the actual POST data. Again it's not clear what you are trying to do with this method; since you don't specify a custom form, it doesn't need custom kwargs. You should remove this method altogether.
Finally, you don't need to call form.save() inside your form_valid method, even with commit=False. A CreateView already assigns an instance to the form, so you can just do form.instance.piece = ....",A,1
36907283,2016-04-28 06:50:34.293000+00:00,"Set the trust_env variable on the session to False. If not true (True is the default), proxy information from the environment is ignored altogether:
session = requests.Session()
session.trust_env = False
response = session.get(url)

This does also disable .netrc authentication support. If you need that still, then you have two more options that I can see:

add a NO_PROXY environment variable; set to * means no proxies should be used at all. You could do this by directly setting the key in the os.environ dictionary.
simply delete the proxy keys from os.environ.
Take into account that on OSX and Windows Python will look for proxies in the system configuration too (so the registry on Windows, and SysConf on Mac OS X).

Altering os.environ is safe. It is a regular dictionary, adding or deleting keys  in your program is fine, the parent shell environment won't be altered.",A,3
30549517,2015-05-30 18:01:15.527000+00:00,"You are summing strings, which results in concatenation, while you want to treat the values as numbers instead. Convert the string to a number first.
Use the int() function to convert to integer numbers, for example:
num1 = int(raw_input())
# ...
num2 = int(raw_input())
",A,2
11156021,2012-06-22 12:18:58.423000+00:00,"During import the ZODB has to remap the persistent ids in the dumped pickles, and to do so it has to 'load' all the entries in your export file.
During that remapping process it thus will try and import every referenced class. Your export still holds references to collective.formcriteria classes, and that's why you get the error during import.
There are two things you can do:

Install collective.formcriteria into your buildout just for this import to work. There is no need to run install code or anything, the Zope process just needs to be able to import the class during the import process. You can remove it again afterwards.
Fill in the missing imports with empty stubs. This is a bit more involved, as you'll need to create a dummy module that you'll insert into sys.modules to be loaded for this process.
The dummy module could look like this; I've named it 'placeholder.py':
from persistent import Persistent

class PlaceHolderClass(Peristent):
    def __repr__(self):
        return ""<Persistent placeholder class for now-removed instances>""

And you stub out missing imports with:
import sys
import placeholder

sys.modules['collective.formcriteria.criteria.relativepath'] = placeholder

placeholder.FormRelativePathCriterion = placeholder.PlaceHolderClass

It is quite likely you'll need to stub out more such classes, based on the collective.formcriteria package layout.
Using this technique does not remove the broken references; it'll only let you import your .zexp file into a new instance.
",A,2
33888703,2015-11-24 08:28:20.413000+00:00,"You can't solve this problem with map(). The recursive calls return a list, so for a list in the input you replaced the result with another list. map() always has to produce the same number of elements in the output as it was given in the input, after all.
With reduce() you can append more elements to an existing list:
def flatten(lists):
    return reduce(lambda res, x: res + (flatten(x) if isinstance(x, list) else [x]), lists, [])

This starts with an empty list, and appends elements to that for each element in lists. If that element is a list itself, recursion is used.
This produces the expected output:
>>> def flatten(lists):
...     return reduce(lambda res, x: res + (flatten(x) if isinstance(x, list) else [x]), lists, [])
...
>>> lists = [ 1 , 2 , [ 3 , 4, 5], 6, [7, 8, 9] ]
>>> flatten(lists)
[1, 2, 3, 4, 5, 6, 7, 8, 9]
",A,5
5449242,2011-03-27 12:39:46.857000+00:00,"Unfortunately, python-twitter does not yet support the Twitter Retweet REST call. 
You'll have to make that call directly yourself (using direct calls to api._FetchURL) or apply the patch in issue 130 to add support.
You're better off with using tweepy; read the API documentation, there is a handy retweet(id) method for retweeting.
Quick and dirty example:
import tweepy
auth = tweepy.BasicAuthHandler(""username"", ""password"")
api = tweepy.API(auth)
for status in api.user_timeline('someuser'):
    api.retweet(status.id)

This will retweet the last 20 statuses from someuser. You'll want to do some more coding to prevent it from retweeting those same messages again next time you run the script though.
Edit: Twitter no longer accepts BasicAuth authentication, and you'll have to use the OAuth authentication exchange to get a authorisation token. Changing the example above to use OAuth would detract from the retweet API point I was trying to make, see the Tweepy OAuth tutorial for an extensive tutorial.",A,7
13921765,2012-12-17 20:35:47.757000+00:00,"First, you have to know how to check whether a value is in a range. That's easy:
if n in range(0, 101):

Almost a direct translation from English. (This is only a good solution for Python 3.0 or later, but you're clearly using Python 3.)
Next, if you want to make them keep trying until they enter something valid, just do it in a loop:
for i in range(total):
    while True:
        n = int(input(""Enter a test score >> ""))
        if n in range(0, 101):
            break
    myList.append(n)

Again, almost a direct translation from English.
But it might be much clearer if you break this out into a separate function:
def getTestScore():
    while True:
        n = int(input(""Enter a test score >> ""))
        if n in range(0, 101):
            return n

for i in range(total):
    n = getTestScore()
    myList.append(n)

As f p points out, the program will still ""just end with a error"" if they type something that isn't an integer, such as ""A+"". Handling that is a bit trickier. The int function will raise a ValueError if you give it a string that isn't a valid representation of an integer. So:
def getTestScore():
    while True:
        try:
            n = int(input(""Enter a test score >> ""))
        except ValueError:
            pass
        else:
            if n in range(0, 101):
                return n
",A,2
41299271,2016-12-23 10:05:15.847000+00:00,"You are misunderstanding how >> redirection operator. In Python 2, the print statement does support redirection, but you have to put it before the expression producing the print value, and you need to open the file first:
with open(logfile, 'a') as log:
    print >> log, '###################################    ' + ip + ' logs'

Note that the print statement is not a function, and doesn't take parentheses.
You can use the Python 3 compatible print() function in Python 2, but you need to use the following import statement at the top:
from __future__ import print_function

after which you'd use the file argument:
with open(logfile, 'a') as log:
    print('###################################    ' + ip + ' logs', file=logfile)

You may want to look at using string formatting with str.format() to interpolate data into a string:
with open(logfile, 'a') as log:
    print('###################################    {} logs'.format(ip), file=logfile)

You can also write directly to the file, but do remember to add a newline explicitly:
with open(logfile, 'a') as log:
    log.write('###################################    {} logs\n'.format(ip))

Note the \n at the end of the string template there.",A,0
44101603,2017-05-21 20:44:06.647000+00:00,"A queryset is an array of Book objects. What you're showing there is a list of dictionaries, which is something else entirely. You can achieve that by using values():
Book.objects.filter(author='JK Rowling').values()

Note that by doing this you lose all the functionality associated with returning the actual Book objects themselves, such as the ability to call their methods, modify and save them back to the db, etc. Unless you have a really good reason, you should stick to the actual queryset.",A,1
35891060,2016-03-09 12:02:00.643000+00:00,"As of Python 3.6, you can use the new %G, %u and %V directives. See issue 12006 and the updated documentation:

%G
  ISO 8601 year with century representing the year that contains the greater part of the ISO week (%V).
%u
  ISO 8601 weekday as a decimal number where 1 is Monday.
%V
  ISO 8601 week as a decimal number with Monday as the first day of the week. Week 01 is the week containing Jan 4.

Given a string with year, weeknumber and weekday number, it is easy to parse those out to a date with:
from datetime import datetime

datetime.strptime('2002 01 1', '%G %V %u').date()

or as a function with integer inputs:
from datetime import datetime

def date_from_isoweek(iso_year, iso_weeknumber, iso_weekday):
    return datetime.strptime(
        '{:04d} {:02d} {:d}'.format(iso_year, iso_weeknumber, iso_weekday),
        '%G %V %u').date()
",A,7
44994572,2017-07-09 08:45:01.967000+00:00,"You seem to be confusing the word type with the type() built-in. Here they simply references the first argument passed into super().
What the documentation tells you is that if you pass in two arguments, then the second argument either has to be an instance of the first argument, or it has to be a subclass. In other words, either isinstance(first_argument, second_argument) or issubclass(first_argument, second_argument) must be true. There is no other meaning here.
Just like int() or str() or any of the other built-in types, the type of the object returned by calling super() is that type. There are no separate types returned for different arguments. See the C source code defining the object.
The super() object implements a __getattribute__ hook that implements specific attribute behaviour. The documentation tells you that the rules for attribute lookups are the same as for getattr() (but with the documented MRO skip), but that does not mean that super() returns an ancestor class.
What actually happens is that super().__getattribute__ takes the MRO of the second argument (either type(instance).__mro__ or cls.__mro__, depending on wether isinstance() or issubclass() was true), find the first argument in that sequence and start testing for attributes after that. Because the MRO is scanned for the (type of) the second argument first, it does have to be findable, which is why the constraints are what they are. 
In Pure Python, this is what super() does (simplified to focus on just the two argument behaviour):
def _supercheck(type_, obj):
    try:
        if issubclass(obj, type_):
            return obj
    except TypeError:
        # obj is not a type so issubclass throws a TypeError
        pass
    if isinstance(obj, type_):
        return type(obj)
    raise TypeError(
        ""super(type, obj): obj must be an instance or subtype of type"")


class super_:
    def __init__(self, type_, obj):
        # simplified for the two-argument case
        self.type_ = type_
        self.obj = obj
        self.obj_type = _supercheck(type_, obj)

    def __getattribute__(self, name):
        if name == '__class__':
            # __class__ should always come from this object, not
            # the represented MRO.
            return super().__getattribute__(name)

        # avoid infinite recursion issues
        sd = super().__getattribute__('__dict__')
        starttype = sd['obj_type']
        type_ = sd['type_']
        obj = sd['obj']

        mro = iter(starttype.__mro__)

        # skip past the start type in the MRO
        for tp in mro:
            if tp == type_:
                break

        # Search for the attribute on the remainder of the MRO
        for tp in mro:
            attrs = vars(tp)
            if name in attrs:
                res = attrs[name]
                # if it is a descriptor object, bind it
                descr = getattr(type(res), '__get__', None)
                if descr is not None:
                    res = descr(
                        res,
                        None if obj is starttype else obj,
                        starttype)
                return res

        return super().__getattribute__(name)
",A,4
36953953,2016-04-30 10:52:44.177000+00:00,"The code example says explicitly:
# For this example, assume that
# the text file contains only ASCII characters.

i.e., it is not expected to work if the input contains non-ascii characters. To send email that may contain non-ascii characters, use this answer.",A,0
896973,2009-05-22 08:59:29.277000+00:00,"As you say, you are not using the custom form you have defined. This is because you aren't passing it in anywhere, so Django can't know about it.
The solution is simple - just pass the custom form class into modelformset_factory:
LocFormSet = modelformset_factory(MyModel, form=MyModelForm) 

Edit in response to update 3:
Firstly, you have the redefinition for locid in the wrong place - it needs to be at the class level, not inside the __init__.
Secondly, putting the queryset inside the form won't do anything at all - forms don't know about querysets. You should go back to what you were doing before, passing it in as a parameter when you instantiate the formset. (Alternatively, you could define a custom formset, but that seems like overkill.)
class MyModelForm(ModelForm):
    locid = forms.IntegerField(min_value=1, required=True)

    def __init__(self, *args, **kwargs):
        super(MyModelForm, self).__init__(*args, **kwargs)
        self.fields['locid'].widget.attrs[""type""] = 'visible'
    class Meta:
        model = MyModel
        fields = ('locid', 'name')

LocFormSet = modelformset_factory(MyModel, form = MyModelForm)
pformset = LocFormSet(request.POST, request.FILES,
                      queryset=MyModel.objects.order_by('name')))
",A,1
12477433,2012-09-18 12:52:27.553000+00:00,"Because Plone items are not files on a disk, and because we can use acquisition in Plone, any piece of HTML can be rendered via different URLs. Via acquisition, it could be almost anything for example, but folderish items can be rendered both with and without a trailing slash.
The browser bases relative URLs within a page on the current page URL, unless a <base /> tag is present, in which case any relative URLs on the page are made absolute relative to that URL instead.
Remove the tag at your own peril; there are always more relative URLs to contend with than you could hope to eliminate.",A,4
16033058,2013-04-16 09:13:28.990000+00:00,"You need to provide custom __getitem__(), __setitem__ and __delitem__ hooks.
These are passed a slice object when slicing the list; these have start, stop and step attributes (which could be None):
def __getitem__(self, key):
    if isinstance(key, slice):
        return [self.list[i] for i in xrange(key.start, key.stop, key.step)]
    return self.list[key]

or, for your case:
def __getitem__(self, key):
    return self.list[key]

because a list can take the slice object directly.
In Python 2, list.__getslice__ is called for slices without a stride (so only start and stop indices) if implemented, and the built-in list type implements it so you'd have to override that too; a simple delegation to your __getitem__ method should do fine:
def __getslice__(self, i, j):
    return self.__getitem__(slice(i, j))
",A,31
28387692,2015-02-07 21:38:44.727000+00:00,"Sort your input lists by length, then take the longest list and add it to the output. Create a set from this longest list against which you test other lists. Any subsequent shorter list that intersects with this set is discarded. 
If you find a shorter list that doesn't intersect, add it to the output, and update your base set; shorter lists that now intersect share at least one number with the one or more lists in the output, after all. Continue until all lists have been tested:
def eliminate_shorter(list_of_lists):
    inputlist = sorted(list_of_lists, key=len)
    outputlist = [inputlist.pop()]
    numbers = set(outputlist[0])
    for sublist in reversed(inputlist):
        if not numbers.intersection(sublist):
            numbers.update(sublist)
            outputlist.append(sublist)
    return outputlist

This is an algorithm of O(NlogN) complexity (because of the initial sort).
Demo:
>>> sample = [[0, 3, 7], [0, 3, 7, 9], [0, 2, 3, 4, 7, 8, 9], [2, 4, 8, 9], [2, 4, 7, 8, 9], [5, 6]]
>>> def eliminate_shorter(list_of_lists):
...     inputlist = sorted(list_of_lists, key=len)
...     outputlist = [inputlist.pop()]
...     numbers = set(outputlist[0])
...     for sublist in reversed(inputlist):
...         if not numbers.intersection(sublist):
...             numbers.update(sublist)
...             outputlist.append(sublist)
...     return outputlist
... 
>>> eliminate_shorter(sample)
[[0, 2, 3, 4, 7, 8, 9], [5, 6]]
",A,4
23971088,2014-05-31 14:20:41.510000+00:00,"Yes, dictionary lookups take constant time. Your if k not in Lst may have to scan the whole list to see if the number is not yet in the list, before appending. It is this scan that makes list containment tests take O(n) time, and is what is killing your algorithm. 
A python dictionary on the other hand, uses a hash table to test for membership. Each key is hashed (reduced to a number), with the number then being turned into in index into the table. If the key found at that location is equal to the key you are testing with, a match is found.  Hashing can lead to collisions (two values hashing to the same table index), but the Python dictionary implementation has an algorithm to then look for a next slot in an efficient manner. If an empty slot is found, the containment test has failed, the key is not present.
So, to test if k is in your dictionary, for most tests just 1 calculation is needed. For some, a few more tests could be required. But on average, the lookup time is constant.
If you are curious, and understand C well enough, take a look at the C implementation for all the (well documented) details. You could also watch this Pycon 2010 presentation by Brandon Rhodes about how CPython dict works, or pick up a copy of Beautiful Code, which includes a chapter on the implementation written by Andrew Kuchling.
You way want to look at the set() type; this is, like a dictionary, an unordered collection with O(1) membership tests, but just the values, no keys:
some_set = set()

def do_something():
    some_set.add(k)

while N < 100000:
    if k not in some_set:
        do_something()

Internally, set() objects use a hash table as well.",A,5
21117844,2014-01-14 15:54:08.907000+00:00,"Like any other assignment, it simply creates another name pointing at the self object.
As to why, that's a good question: it doesn't actually do anything useful.",A,3
27569537,2014-12-19 15:56:27.830000+00:00,"When you read a line from a file, you get the whole line, including the newline character, as a string. You should strip that off and then compare with another string:
if d.strip('\n') == ""True"":
",A,3
46161885,2017-09-11 18:08:26.410000+00:00,"self.u is None; you set it to None here:
self.u = self.listWidget_3.itemClicked.connect(self.ClickRW1)

The connect() call returned None.
You then pass self.u to plot_rain() here:
def push4(self):
    plot_rain(self.z, self.u)

which uses that value as mont, so mont*4 fails.
The solution is to not assign the .connect() result to anything, it doesn't need assigning:
self.u = 0
self.listWidget_3.itemClicked.connect(self.ClickRW1)
",A,2
43737206,2017-05-02 11:54:03.053000+00:00,"As you say, you use annotate. But that doesn't imply an aggregation, it's just a calculated field. So:
MyTable.objects.annotate(calculated_field=F('column_a') - F('column_b')).filter(calculated_field__lte=1)
",A,0
7023592,2011-08-11 09:26:33.490000+00:00,"I don't think it's bad practice at all, I do similar things for similar situations.
You could use an attribute annotation, which would help prevent conflicts with other attributes, but that's a style and performance choice more than anything. Attribute annotations are stored in their own ZODB persistent record, so it depends on how often this attribute will change compared to the other attributes on the folder what impact this has.
Last but not least, I would probably encapsulate the behaviour in an adapter, to make the implementation flexible for future uses. You can either register the adapter to the ATFolder interface, or to IAttributeAnnotatable, depending on how much your implementation relies on what the adapted object needs to provide.
Other notes: We've also used plone.app.relations connections between objects in the past (maintained outside the object schema, like your attribute), but found five.intid (the underlying machinery .relations relies on) to be too fragile and would use simple UID attributes with catalog searches in the future.
In reference to Ross's answer, if the information in question doesn't need to be end-user editable, a schemaextender attribute is overkill.",A,6
27643297,2014-12-25 00:22:26.590000+00:00,"The request.authorization attribute is only set when you have a valid Basic Authorization or Digest Authorization header; the Authorization header has specific formats where the first word in the header value sets the type, and the attribute only handles those two specific types (flagged by the Basic or Digest type keywords).
From the AuthorizationMixin.authorization attribute documentation

The Authorization object in parsed form.

following that to the Authorization object docs:

Represents an Authorization header sent by the client. You should not create this kind of object yourself but use it when it’s returned by the parse_authorization_header function.

which is documented as:

Parse an HTTP basic/digest authorization header transmitted by the web browser. The return value is either None if the header was invalid or not given, otherwise an Authorization object.

Bold emphasis mine. 
Your header is not such a valid header; it is doesn't have the Basic or Digest type indicator. If you did have such a header the function returns something other than None:
>>> from werkzeug.http import parse_authorization_header
>>> parse_authorization_header('testkey:')
>>> parse_authorization_header('testkey:') is None
True
>>> parse_authorization_header('Basic QWxhZGRpbjpvcGVuIHNlc2FtZQ==')
{'username': 'Aladdin', 'password': 'open sesame'}
>>> result = _
>>> type(result)
<class 'werkzeug.datastructures.Authorization'>
>>> result.username
'Aladdin'
>>> result.password
'open sesame'

I used a Basic type header there, where the remainder of the header value is a base-64 encoded username and password pair separated by a : colon.
If you want to implement your own authentication scheme, just access the header itself and parse it manually.
auth = request.headers.get('authorization')
",A,11
11053779,2012-06-15 15:30:35.180000+00:00,"Why would you want to do it in JS? If you're redirecting to a different page already, just add #whatever to the redirect URL to go direct to the anchor.",A,3
2636822,2010-04-14 11:15:14.837000+00:00,"Here's generalized solution based on numpy.where():
#!/usr/bin/env python
import string, itertools

def where(selectors, x, y):
    return (xx if s else yy for xx, yy, s in itertools.izip(x, y, selectors))

condition = [True, True, False, True, True, False, True]
print ''.join(where(condition, string.uppercase, itertools.cycle(' ')))
# -> AB DE G

import numpy as np
print ''.join(np.where(condition, list(string.uppercase)[:len(condition)], ' '))
# -> AB DE G
",A,0
33036170,2015-10-09 10:41:18.923000+00:00,"You could probably use the ""data"" uri format, which allows you to pass the image as a base64-encoded string. Of course, you'll need to encode the images first in your getImages function:
for iid in img_ids:
    images.append(base64.b64encode(self.gfs.get(iid).read()))

and in the template you can output the data directly:
{% for image in images %}
    <img src=""data:image/png;base64,{{ img }}"">
{% endfor %}

(obviously, replace png with jpg or whatever as necessary).",A,3
32411951,2015-09-05 10:19:18.407000+00:00,"One way would be to decompose the keys into individual elements, and create a new dict from those:
new_dic = {}
for k, v in dic.items():
    for sub_k in k:
        new_dic[sub_k] = v

Now it's a simple matter of looping through the list:
output = [new_dic[i] for i in L]

and you can de-duplicate with set:
output = list(set(output))
",A,1
48023541,2017-12-29 13:42:55.033000+00:00,"__str__ must return a string value. If you return a unicode object instead, it'll be automatically encoded as ASCII.
Encode explicitly instead:
class Test(object):
    def __str__(self):
        d = {
            'foo': u'中文'
        }
        return json.dumps(d, ensure_ascii=False).encode('utf8')

Personally, I'd not use the __str__ method to provide a JSON encoding. Pick a different method name instead, like tojson().",A,2
13604543,2012-11-28 11:57:40.527000+00:00,"Do not reuse connections across threads. Create a new connection for each thread instead.
From the MySQLdb User Guide:

The MySQL protocol can not handle multiple threads using the same connection at once. Some earlier versions of MySQLdb utilized locking to achieve a threadsafety of 2. While this is not terribly hard to accomplish using the standard Cursor class (which uses mysql_store_result()), it is complicated by SSCursor (which uses mysql_use_result(); with the latter you must ensure all the rows have been read before another query can be executed. It is further complicated by the addition of transactions, since transactions start when a cursor execute a query, but end when COMMIT or ROLLBACK is executed by the Connection object. Two threads simply cannot share a connection while a transaction is in progress, in addition to not being able to share it during query execution. This excessively complicated the code to the point where it just isn't worth it.
The general upshot of this is: Don't share connections between threads. It's really not worth your effort or mine, and in the end, will probably hurt performance, since the MySQL server runs a separate thread for each connection. You can certainly do things like cache connections in a pool, and give those connections to one thread at a time. If you let two threads use a connection simultaneously, the MySQL client library will probably upchuck and die. You have been warned.

Emphasis mine.
Use thread local storage or a dedicated connection pooling library instead.",A,5
51643277,2018-08-01 22:50:29.120000+00:00,"
As Im little confused about the sequence and want to call the send_status_report() define after the file is closed, how do I make it work.

You have a with open…:. That statement will automatically close the file when the block is finished. So, if you want to run something right after the file is closed, just put it right after that statement:
with open(LOG+OUTFILE, 'wb') as log_file:
    def read(fd):
    # ...
    rc = pty.spawn(argv, read)

send_status_report()

As long as your existing code is working (e.g., the file isn't getting closed before you're done with it), and isn't doing anything really weird like closing the file multiple times, this will call send_status_report() right after the file is closed.",A,0
24302248,2014-06-19 08:38:02.903000+00:00,"The django-tables2 documentation you link to show exactly where you're going wrong: the context item that contains the current row is called record, not model_name.
column_name = tables.TemplateColumn('<a href=""{{record.link_id}}"">{{record.name_id}}text_for_example</a>')
",A,1
30273926,2015-05-16 09:20:52.343000+00:00,"You are using Django 1.8. There is no reason to add South: versions of Django since 1.7 have included built-in migrations. In fact, the South docs explicitly state that it is not compatible with 1.7+.",A,1
20790969,2013-12-26 20:41:28.523000+00:00,"That project appears to only define a model field, not a form field.
It does however provide a form widget, s3direct.widgets.S3DirectEditor, which it uses in conjunction with a standard form text field. You can no doubt do the same.",A,0
32802298,2015-09-26 21:40:29.753000+00:00,"To run two subprocesses concurrently:
from subprocess import Popen

# start processes
with open(dosout, ""wb"", 0) as dout, open(jijout, ""wb"", 0) as jout:
    processes = [Popen([""mpirun"", ""-np"", ""8"", ""...WORK/run1"", cmd], stdout=file)
                 for cmd, file in [(dosinp, dout), (jijinp, jout)]]

# wait for them to finish
statuses = [p.wait() for p in processes]

Note: the file are closed on exit from the with-statement, don't call .close() manually.",A,0
33971776,2015-11-28 13:19:09.493000+00:00,Use writerow - without the s - to write a single row.,A,0
35011308,2016-01-26 10:07:24.333000+00:00,"Call the base version via super():
class MyClass(object):
    myattrib = None

    def __setattr__(self, prop, val):
        super().__setattr__('myattrib', val)
        print(""setting myattrib"")

You probably do not want to ignore the prop argument here, it is not necessarily 'myattrib' that is being set.
However, consider using a property instead of intercepting all attribute setting:
class MyClass(object):
    _myattrib = None

    @property:
    def myattrib(self):
        return self._myattrib

    @myattrib.setter
    def myattrib(self, val):
        self._myattrib = val
        print(""setting myattrib"")

I added object as a base-class; this is the default in Python 3, but a requirement for super() and property objects to work in Python 2.",A,4
26220774,2014-10-06 16:38:17.473000+00:00,"Remove the comma:
self.list = [],    
#  this comma ^

In Python, it is the comma that makes something a tuple; parentheses are only needed to disambiguate a tuple from other syntax that might also use commas.",A,5
18002702,2013-08-01 19:18:53.663000+00:00,"First, your problem has nothing to do with SQL. Throw away all the SQL-related code and do this:
var = 'foo'
"" OR tble_tble.var LIKE %'%s'%"" % var

You'll get the same error. It's because you're trying to do %-formatting with a string that has stray % signs in it. So, it's trying to figure out what to do with %', and failing.

You can escape these stray % signs like this:
"" OR tble_tble.var LIKE %%'%s'%%"" % var

However, that probably isn't what you want to do.

First, consider using {}-formatting instead of %-formatting, especially when you're trying to build formatted strings with % characters all over them. It avoids the need for escaping them. So:
"" OR tble_tble.var LIKE %'{}'%"".format(var)


But, more importantly, you shouldn't be doing this formatting at all. Don't format the values into a SQL string, just pass them as SQL parameters. If you're using sqlite3, use ? parameters markers; for MySQL, %s; for a different database, read its docs. So:
"" OR tble_tble.var LIKE %'?'%""

There's nothing that can go wrong here, and nothing that needs to be escaped. When you call execute with the query string, pass [var] as the args.
This is a lot simpler, and often faster, and neatly avoids a lot of silly bugs dealing with edge cases, and, most important of all, it protects against SQL injection attacks.
The sqlite3 docs explain this in more detail:

Usually your SQL operations will need to use values from Python variables. You shouldn’t assemble your query using Python’s string operations… Instead, use the DB-API’s parameter substitution. Put ? as a placeholder wherever you want to use a value, and then provide a tuple of values as the second argument to the cursor’s execute() method. (Other database modules may use a different placeholder, such as %s or :1.) …


Finally, as others have pointed out in comments, with LIKE conditions, you have to put the percent signs inside the quotes, not outside. So, no matter which way you solve this, you're going to have another problem to solve. But that one should be a lot easier. (And if not, you can always come back and ask another question.)",A,7
52136920,2018-09-02 12:16:20.287000+00:00,"You should absolutely definitely completely not be doing any of this. It is unsafe for several reasons.
Firstly, the point of a form is that you call its is_valid() method and then access the data via cleaned_data.
Secondly, never store data between requests like this. It will be visible to all users; so the next user to go to tested will see the data you sent. Normally I would recommend using the session to store data between requests, but that is not the right thing to do either because you should be using Django's built-in authentication framework; doing so would then give you access to the logged-in user via request.user.
Finally, note that your form's clean_data method is pointless and will never be called. But again, within that method you would access self.cleaned_data; there would be no such thing as self.request there.",A,1
43425350,2017-04-15 11:13:39.397000+00:00,"You are trying to loop over in integer; len() returns one.
If you must produce a loop over a sequence of integers, use a range() object:
for i in range(len(str_list)):
    # ...

By passing in the len(str_list) result to range(), you get a sequence from zero to the length of str_list, minus one (as the end value is not included).
Note that now your i value will be the incorrect value to use to calculate an average, because it is one smaller than the actual list length! You want to divide by len(str_list):
return str_sum / len(str_list)

However, there is no need to do this in Python. You loop over the elements of the list itself. That removes the need to create an index first:
for elem in str_list
    str_sum += len(elem)

return str_sum / len(str_list)

All this can be expressed in one line with the sum() function, by the way:
def str_avg(s):
    str_list = s.split()
    return sum(len(w) for w in str_list) / len(str_list)

I replaced the name str with s; better not mask the built-in type name, that could lead to confusing errors later on.",A,5
13131877,2012-10-30 01:08:55.007000+00:00,"There are at least two issues:

you use positional arguments (they do not start with '-', or '--'), but you provide their names at command line
you use nargs='*' that consumes all arguments that it can
",A,0
9622833,2012-03-08 18:28:14.390000+00:00,"The file position is at EOF:
>>> soup = BeautifulSoup("""", 'xml')
>>> soup.prettify()
'<?xml version=""1.0"" encoding=""utf-8"">\n'

Or the content is not valid xml:
>>> soup = BeautifulSoup(""no <root/> element"", 'xml')
>>> soup.prettify()
'<?xml version=""1.0"" encoding=""utf-8"">\n'
",A,4
16405167,2013-05-06 18:47:36.823000+00:00,"I still haven't been able to run your code to get the claimed results, but I think I know what the problem is:
>>> line = '> AY538167 1 1411 1411bp rna Acholeplasma hippikon Acholeplasmataceae'
>>> regex = re.compile(r'(>)\s(\w+).+[rna]\s+([A-Z].+)')
>>> regex.findall(line)
[('>', 'AY538167', 'Acholeplasmataceae')]

The problem is that [rna]\s+ matches any one of the characters r, n, or a at the end of a word. And, because all of the matches are greedy, with no lookahead or anything else to prevent it, this means that it matches the n at the end of hippikon.
The simple solution is to remove the brackets, so it matches the string rna:
>>> regex = re.compile(r'(>)\s(\w+).+rna\s+([A-Z].+)')

That won't work if any of your species or genera can end with that string. Are there any such names? If so, you need to come up with a better way to describe the cutoff between the 1409bp part and the rna part. The simplest may be to just look for rna surrounded by spaces:
>>> regex = re.compile(r'(>)\s(\w+).+\s+rna\s+([A-Z].+)')

Whether this is actually correct or not, I can't say without knowing more about the format, but hopefully you understand what I'm doing well enough to verify that it's correct (or at least to ask smarter questions than I can ask).

It may help debug things to add capture groups. For example, instead of this:
(>)\s(\w+).+[rna]\s+([A-Z].+)

… search for this:
(>)(\s)(\w+)(.+[rna]\s+)([A-Z].+)

Obviously your desired capture groups are now \1\3 \5 instead of \1\2 \3… but the big thing is that you can see what got matched in \4:
[('>', ' ', 'AY538167', ' 1 1411 1411bp Acholeplasma hippikon ', 'Acholeplasmataceae')]

So, now the question is ""Why did .+[rna]\s+ match '1 1411 1411bp Acholeplasma hippikon '? Sometimes the context matters, but in this case, it doesn't. You don't want that group to match that string in any context, and yet it will always match it, so that's the part you have to debug.

Also, a visual regexp explorer often helps a lot. The best ones can color parts of the expression and the matched text, etc., to show you how and why the regexp is doing what it does.
Of course you're limited by those that run on your platform or online, and work with Python syntax. If you're careful and/or only use simple features (as in your example), perl/PCRE syntax is very close to Python, and JavaScript/ActionScript is also pretty close (the one big difference to keep in mind is that replace/sub uses $ instead of \1).
I don't have a good online one to strongly recommend, but from a quick glance Debuggex looks pretty cool.",A,2
38850784,2016-08-09 12:28:48.503000+00:00,"This is a question about Python imports.
When you import a package, you don't automatically get access to all the modules underneath it; you need to import those specifically. So instead of doing import accounting and then trying to access accounting.admin, you need to explicitly do from accounting import admin and then accessing admin.BankInline etc.",A,2
52030746,2018-08-26 22:36:56.733000+00:00,"The first thing to do is to break down the expression into smaller ones:
bits = ['{}{}'.format(k, sum(1 for _ in g)) for k, g in groupby(string)]
x=''.join(bits)

The second one is easy: we have some list of bits, each of which is a string, and we just join them up into one big string.
The first one is a list comprehension. Every list comprehension can be rewritten as a for statement around an append, so let's do that:
bits = []
for k, g in groupby(string):
    bits.append('{}{}'.format(k, sum(1 for _ in g)))


The groupby part may seem tricky if you've never seen groupby before, but if you just call it separately, it should be pretty obvious:
for k, g in groupby(string):
    print(k, list(g))

This gives you:
W ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']
B ['B']
W ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']
B ['B', 'B', 'B']
W ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']
B ['B']

In other words, each group g is a run of equal elements, and k is just the thing they're all equal to.

Now let's break down the inner statement:
bits.append('{}{}'.format(k, sum(1 for _ in g)))

into parts:
count = sum(1 for _ in g)
bit = '{}{}'.format(k, count)
bits.append(bit)

The last two lines are hopefully obvious. So, that just leaves us with the first one.

We're calling sum on a generator expression. Generator expressions are just like list comprehensions but lazy, and we don't care about the laziness here, so we can break it down the same way we did above:
things = []
for _ in g:
    things.append(1)
count = sum(things)

So now it should be obvious what sum(1 for _ in g) does: it's just the number of things in g. In fact, it's just like calling len(g), except that it works for arbitrary iterables, including lazy iterators, not just sequences.
This is the idiomatic way to count possibly-lazy iterables—but we could have replaced it (at the cost of a bit of performance) with:
count = len(list(g))


So, putting it all back together:

Use groupby to convert the string into a bunch of groups, each of which is the same character repeated over and over.
For each one:


Count the length of that group.
Make a string like 'W12' using the key 'W' and the fact that the group of Ws has 12 members.
Append that to a list.

Take that list of ['W12', 'B1', 'W12', 'B3', 'W24', 'B1'] and join it into a string 'W12B1W12B3W24B1'.
",A,2
25975397,2014-09-22 13:30:07.207000+00:00,"You are posting to the wrong URL; your own data shows the form posts to https://web.iress.com.au/html/logon.aspx but you are posting to /LogonForm.aspx instead.
Note that the session object will take care of the cookie for you, do not set the Cookie header yourself. You should avoid setting the Host, Origin and Content-Type headers, and the Cache-Control, Accept* headers and Pragma are not going to have any influence how this works.",A,0
20550329,2013-12-12 17:36:08.520000+00:00,"The simplest way would be to add a keyword argument to your recursive function, and returning the resulting sigma:
def buildDTree(sofar, todo, sigma=None):
    if sigma is None:
        sigma = [[]]
    sal = []
    if not todo:
        return sigma, binaryTree(sofar)
    else:
        _, withelt = buildDTree(sofar + [todo[0]], todo[1:], sigma)
        _, withoutelt = buildDTree(sofar, todo[1:], sigma)
        here = binaryTree(sofar)
        here.setLeftBranch(withelt)
        here.setRightBranch(withoutelt)
        sal += here.getLeftBranch().getValue()
        sigma += [sal]
        return sigma, here

sigma, _ = buildDTree([], [1,2,3])

This at least makes sigma 'local' to the recursive call.
A better approach is to make a class:
class DTreeBuilder(object):
    def __init__(self):
        self.sigma = [[]]

    def buildDTree(self, sofar, todo):
        sal = []
        if len(todo) == 0:
            return binaryTree(sofar)
        else:
            withelt = buildDTree(sofar + [todo[0]], todo[1:])
            withoutelt = buildDTree(sofar, todo[1:])
            here = binaryTree(sofar)
            here.setLeftBranch(withelt)
            here.setRightBranch(withoutelt)
            sal += here.getLeftBranch().getValue()
            self.sigma += [sal]
            return here

and use it like:
builder = DTreeBuilder()
builder.build([], [1,2,3])
print builder.sigma
",A,4
35748861,2016-03-02 13:40:06.480000+00:00,"Like the error says, you need an actual User, not an ID. You could just pass request.user directly. You'll probably get the same error for topic; again, you should just pass the object itself. 
LastVisitedTopic.objects.create(user=request.user, topic=t, lastvisited=lv)
",A,1
53193892,2018-11-07 16:37:16.080000+00:00,"__PYENV_LAUNCHER__ is an implementation detail of the way a Python framework build on Mac works.
In a framework build (a special build that allows you to run GUI apps powered by Python), Apple places strict limits on what you can do with the process. To break out of those limitations, the Python binary is actually a wrapper that then launches the 'real' Python binary as a child process (the Resources/Python.app/Contents/MacOS/Python binary in the same framework package).
To let the child process know what the path was that was used to launch the wrapper binary, the wrapper sets the __PYVENV_LAUNCHER__ environment variable, and the actual Python binary then uses that instead of argv[0] (set by the OS). This is important in case of hardlinked copies of the launcher binary, such as those used in a virtualenv. Hence the name PYVENV in the variable.
The variable really shouldn't leak out beyond the point it has done its job, so it is being removed from the environment once read in future Python releases.",A,2
51577169,2018-07-29 04:15:10.090000+00:00,"If you don't want to put the arrays in an inner list, just don't put them in a list:
>>> np.array([a, *d.values()], dtype=object)
array([12, array([12, 11, 23]), array([1, 2, 3])], dtype=object)

Or, if you're on an older Python and can't unpack into a list display, create the list, but add it to another one to get the same flat list:
>>> np.array([a] + list(d.values()), dtype=object)
array([12, array([12, 11, 23]), array([1, 2, 3])], dtype=object)
",A,2
11264698,2012-06-29 15:32:58.070000+00:00,"You misplaced a closing parenthesis:
url(r'^cribme/(?P<meekme_id>\d+)/$', 'meebapp.views.cribdetail', name='cribdetail'),

Note that the (?P<meekme_id> .. ) group construct should include the \d+ characters you are trying to match. In your incorrect regular expression you define a group that doesn't include any matched characters, thus always the empty string ''.",A,0
38463498,2016-07-19 16:03:38.577000+00:00,"You don't need to pass in divisor as an argument; it is simply available as a closure:
new_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
print ""Get the divisor""
divisor = int(raw_input())  # on Python 2, you want to use raw_input

# divisor is taken from the parent scope here
h = lambda x: x % divisor == 0
ans = filter(h, new_list)
print ans

Note that you need to pass the function object as the first argument to filter(), not the result of calling the function. filter() passes just one argument to that function object, always.",A,0
15317155,2013-03-09 23:32:30.370000+00:00,"No need to use a regular expression:
stack = []
result = []
for elem in inputstring.split('|'):
    if not elem: continue
    stack.append(elem)
    result.append('|' + '|'.join(stack))

which produces:
>>> result
['|item1', '|item1|item2', '|item1|item2|item3', '|item1|item2|item3|item4']

You could do this with a generator too:
def generate_items(inputstring):
    stack = []
    for elem in inputstring.split('|'):
        if not elem: continue
        stack.append(elem)
        yield '|' + '|'.join(stack)

for item in generate_items(inputstring):
    print item
",A,2
23086633,2014-04-15 14:26:13.250000+00:00,"struct does not natively support 24-bit integers.
If these are unsigned integers, on trick you could use is to use the array.array() type to read the data as bytes, then process these per 3:
import array

b = array.array('B', yourdata)
result = [b[i] << 16 | b[i + 1] << 8 | b[i + 2] for i in xrange(0, len(b), 3)]

or you could use array.fromfile() to read the data from a file input as needed:
with open('somefilename', 'rb') as infh:
    b = array.array('B')
    b.fromfile(infh, 920)
    result = [b[i] << 16 | b[i + 1] << 8 | b[i + 2] for i in xrange(0, len(b), 3)]

Adjust as needed for byte order (swap the b[i] and b[i + 2] references).
If these are signed 24-bit integers, you may have to stick to struct, and pad the least significant side with a null-byte, then right-shift the result by 8. That way you don't have to worry about negative vs. positive numbers and how to pad either type on the most-significant side:
[struct.unpack('>i', yourdata[i:i+3] + '\x00')[0] >> 8
 for i in range(0, len(yourdata), 3)]

for big-endian, and 
[struct.unpack('<i', '\x00' + yourdata[i:i+3])[0] >> 8
 for i in range(0, len(yourdata), 3)]

for little-endian signed 24-bit integers.
If you are using Python 3.2 or newer, you can read the data 3 bytes at a time and convert to integers with int.from_bytes(), which gives you more flexibility over endianess and if you are parsing signed or unsigned integers.",A,1
5759405,2011-04-22 19:48:52.950000+00:00,"You could use lxml to work with links:
from lxml import html

for _, attr, link, _ in html.iterlinks('<a href=""/abc/def/ghk/"">test_test</a>'):
    if attr == 'href':
       print link

Output
/abc/def/ghk/
",A,1
25898013,2014-09-17 18:40:02.447000+00:00,"Use sum() with a generator expression, testing for your conditions:
matching = {'car', 'mercedes', 'ferrari'}
sum(int(value) for key, value in var if key in matching)

The generator expression does much the same as your list comprehension does; loop over the list and do something with each element. I chose to use tuple assignment in the loop; the two elements in each tuple are assigned to key and value respectively. We can then filter using an if statement (if the key value is a member of the matching set), and use only the value part in the sum.
A quick demo to show you what happens, including a list comprehension version to show you that only a subset of values are picked:
>>> var = [(""car"", '1'), (""dog"", '1'), (""mercedes"", '1'), (""cat"", '1'), (""ferrari"", '1'), (""bird"", '1')]
>>> matching = {'car', 'mercedes', 'ferrari'}
>>> [value for key, value in var if key in matching]
['1', '1', '1']
>>> sum(int(value) for key, value in var if key in matching)
3

Of course, this gets a little more interesting when you use values other than '1':
>>> var = [(""car"", '8'), (""dog"", '2'), (""mercedes"", '16'), (""cat"", '4'), (""ferrari"", '32'), (""bird"", '64')]
>>> [value for key, value in var if key in matching]
['8', '16', '32']
>>> sum(int(value) for key, value in var if key in matching)
56

As for your attempt to implement my solution, you replaced your original list with a list with only the values. Remove the list comprehension line rebinding regex2 and run just the sum() line:
>>> regex2 = [(""'goal','corner','rightfoot'"", '1'), (""'goal','directfreekick','leftfoot'"", '1'), 
... (""'goal','openplay','leftfoot'"", '1'), (""'goal','openplay','rightfoot'"", '2'), 
... (""'miss','corner','header'"", '3'), (""'miss','directfreekick','leftfoot'"", '1'), 
... (""'miss','directfreekick','rightfoot'"", '1'), (""'miss','openplay','header'"", '3'), 
... (""'miss','openplay','leftfoot'"", '8'), (""'miss','openplay','rightfoot'"", '11')]
>>> matching = {""'goal','openplay','leftfoot'"", ""'goal','openplay','rightfoot'"", ""'goal','corner','leftfoot'"", ""'goal','corner','rightfoot'""}
>>> sum(int(value) for key, value in regex2 if key in matching)
4
",A,3
24450732,2014-06-27 11:28:38.737000+00:00,"Your first premise is flawed. There is no difference in importing one function vs the whole module.
In order to import anything from a module, Python must first read, parse and execute the entire module. The only difference in importing one function vs the whole thing is what name gets allocated in the current namespace: in this case, it would be one name for the module, or one name for the function. In other words, no difference at all.",A,4
49621950,2018-04-03 03:50:16.710000+00:00,"If you look at the multiprocessing/process.py code (you don't seem to be using Python 3.6, so the line numbers are a bit different, but this part of the code is unchanged), you can see pretty clearly that the string about daemon processes isn't relevant; it's just part of an assert that your code managed to get past without any problems, before failing for a completely unrelated reason a few lines later.

The actual problem is on that line 105, and it's explained in the error message:
AttributeError: Can't pickle local object '_make_date_converter.<locals>.converter'

You're trying to pass an object to a child process that can't be pickled. This is explained a bit in the multiprocessing docs, e.g., under Programming Guidelines, although those docs assume you understand what ""pickle"" means, and that you've read quite a bit of the earlier sections of the documentation. You really should read those earlier sections, and look up pickle in the docs as well, but the basic idea is this:
The multiprocessing module uses the pickle module to pass arguments to functions, return values from functions, put values on queues, etc. The pickle module can only handle data types that are designed to be pickled. So, some types can't be passed around with multiprocessing.
In this case, there should be a very easy workaround: Just pass the filename, and let the child process read it. Of course that won't work for more complicated cases, but if it works for yours, keep it simple.
For more complicated cases, the usual solution with Pandas is to replace the standard pickler with a third-party library like dill or cloudpickle that knows more about Pandas and can coerce it into shape to pass over the network. (Or, sometimes, to replace multiprocess itself with a third-party library like dask.) This isn't that hard to learn, but you do need to look over the options, pick one, and read about how to hook it in, which you may not want to do if it's not needed.

If you're still interested in what daemon processes are anyway, see Processes in the reference docs. But the short version is that a daemon process, in this context, is one that doesn't get joined—in other words, you don't wait for it to finish when the main process finishes, as you do with a normal process.",A,2
3160391,2010-07-01 17:58:13.363000+00:00,"Django simply doesn't work without apps. They're the fundamental building block of a Django site. A whole range of things, not just the admin, will fail to work. Why do you want to do this?",A,2
40523126,2016-11-10 08:35:56.063000+00:00,"If you are already downloaded the URL (using a requests GET request without the stream option, you already have both sizes available as the whole response is downloaded and decompressed, and the original length is available in the headers:
from __future__ import division

r = requests.get(url, headers=headers)
compressed_length = int(r.headers['content-length'])
decompressed_length = len(r.content)

ratio = compressed_length / decompressed_length

You could compare a Accept-Encoding: identity HEAD request content-length header with one with setting Accept-Encoding: gzip instead:
no_gzip = {'Accept-Encoding': 'identity'}
no_gzip.update(headers)
uncompressed_length = int(requests.get(url, headers=no_gzip).headers['content-length'])
force_gzip = {'Accept-Encoding': 'gzip'}
force_gzip.update(headers)
compressed_length = int(requests.get(url, headers=force_gzip).headers['content-length'])

However, this may not work for all servers, as dynamically-generated content servers routinely futz the Content-Length header in such cases to avoid having to render the content first.
If you are requesting a chunked transfer encoding resource, there won't be a content-length header, in which case a HEAD request may or may not provide you with the correct information either.
In that case you'd have to stream the whole response and extract the decompressed size from the end of the stream (the GZIP format includes this as a little-endian 4-byte unsigned int at the very end). Use the stream() method on the raw urllib3 response object:
import requests
from collections import deque

if hasattr(int, 'from_bytes'):
    # Python 3.2 and up
    _extract_size = lambda q: int.from_bytes(bytes(q), 'little')
else:
    import struct
    _le_int = struct.Struct('<I').unpack
    _extract_size = lambda q: _le_int(b''.join(q))[0]

def get_content_lengths(url, headers=None, chunk_size=2048):
    """"""Return the compressed and uncompressed lengths for a given URL

    Works for all resources accessible by GET, regardless of transfer-encoding
    and discrepancies between HEAD and GET responses. This does have
    to download the full request (streamed) to determine sizes.

    """"""
    only_gzip = {'Accept-Encoding': 'gzip'}
    only_gzip.update(headers or {})
    # Set `stream=True` to ensure we can access the original stream:
    r = requests.get(url, headers=only_gzip, stream=True)
    r.raise_for_status()
    if r.headers.get('Content-Encoding') != 'gzip':
        raise ValueError('Response not gzip-compressed')
    # we only need the very last 4 bytes of the data stream
    last_data = deque(maxlen=4)
    compressed_length = 0
    # stream directly from the urllib3 response so we can ensure the
    # data is not decompressed as we iterate
    for chunk in r.raw.stream(chunk_size, decode_content=False):
        compressed_length += len(chunk)
        last_data.extend(chunk)
    if compressed_length < 4:
        raise ValueError('Not enough data loaded to determine uncompressed size')
    return compressed_length, _extract_size(last_data)

Demo:
>>> compressed_length, decompressed_length = get_content_lengths('http://httpbin.org/gzip')
>>> compressed_length
179
>>> decompressed_length
226
>>> compressed_length / decompressed_length
0.7920353982300885
",A,3
12876412,2012-10-13 19:51:01.350000+00:00,"Using natural_key() function:
import re

def natural_key(astr):
    """"""See http://www.codinghorror.com/blog/archives/001018.html""""""
    return [int(s) if re.match(r'\d+$', s) else s
            for s in re.split(r'(\d+)', astr)]

sorted_list = sorted(a, key=lambda d: natural_key(d['cavity']))

Result
[{'cavity': '1K', 'ltg_nr': 'Ltgnr1K'},
 {'cavity': '10', 'ltg_nr': 'Ltgnr10'},
 {'cavity': '11A', 'ltg_nr': 'Ltgnr11A'},
 {'cavity': '11A', 'ltg_nr': 'Ltgnr11A'},
 {'cavity': '11A', 'ltg_nr': 'Ltgnr11A'},
 {'cavity': '17B', 'ltg_nr': 'Ltgnr17B'},
 {'cavity': '20', 'ltg_nr': 'Ltgnr20'},
 {'cavity': '21A', 'ltg_nr': 'Ltgnr21A'},
 {'cavity': '21B', 'ltg_nr': 'Ltgnr170'},
 {'cavity': '21C', 'ltg_nr': 'Ltgnr21C'},
 {'cavity': '22', 'ltg_nr': 'Ltgnr22'},
 {'cavity': '22AA', 'ltg_nr': 'Ltgnr170'},
 {'cavity': '100A', 'ltg_nr': 'Ltgnr170'},
 {'cavity': '101', 'ltg_nr': 'Ltgnr170'}]
",A,3
13732733,2012-12-05 21:25:15.767000+00:00,"Here's the problem:
A[0:1,0:1] = B

You want:
A[0:2,0:2] = B

Why? Because Python uses half-open ranges. So the slice [0:1] is the half-open range [0, 1), meaning just the index 0; the slice [0:2] is the half-open range [0, 2), meaning the indices 0 and 1.",A,4
49293568,2018-03-15 07:18:59.923000+00:00,"From the docs on runserver: 

DO NOT USE THIS SERVER IN A PRODUCTION SETTING. It has not gone through security audits or performance tests. (And that’s how it’s gonna stay. We’re in the business of making Web frameworks, not Web servers, so improving this server to be able to handle a production environment is outside the scope of Django.)
",A,1
27387873,2014-12-09 20:08:46.447000+00:00,"repeated needs to return a callable too; compose() can produce that callable for you, but you need to apply it multiple times:
def repeated(f, count):
    callable = f
    for _ in range(count - 1):
        callable = compose(f, callable)
    return callable

So if count is 1, the original f() is returned. For count = 2, f(f()) is returned, etc.",A,0
15553635,2013-03-21 16:59:55.613000+00:00,"You'll have to copy the object state. The easiest way would be to use the pickle module:
import pickle

copy = pickle.loads(pickle.dumps(someobject))

This is not guaranteed to work. All the pickle module does for you in the general case is to pickle the instance attributes, and restore the instance a-new from the class reference and restore the attribute contents on that.
Since this is a C extension object, if the instance state is not exposed to you, and pickling is not explicitly supported by the type, this won't work either. In that case, you have no other options, I'm afraid.",A,1
39807263,2016-10-01 13:34:05.273000+00:00,"No, threading won't speed this up, for three reasons:

The Python GIL prevents Python code from being executed in parallel; threads executing Python code can only be run concurrently. For the same amount of CPU work, the same amount of time or more is required.
To be able to add to the same datastructure from multiple threads, you'd have to add locking, slowing down threading more.
Your code is slow because it is wasting cycles, because you are recreating the set object each iteration and then discarding it again. This is sucking up all the time as proxies continues to grow, so in the end you created sets for each size of proxies, from length 1 all the way up to length 70k, approaching 5 million steps to throw away 70k sets.

You should produce the set once. You can do so in a set comprehension:
with open('proxy.txt') as f:
    proxies = {tuple(line.strip().split(':')) for line in f}
",A,3
46805285,2017-10-18 07:54:31.787000+00:00,"You can't return anything, no, the return value is ignored. You can set class attributes, which are available to all tests:
class TestProg(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # initialize() creates the JSON file
        myProg.initialize()
        cls.f = myProg.initialize_storage()  # set a class attribute

    def test_prog_func(self):
        # self.f here will find the class attribute
        myProg.prog_func(""test_key"", ""test_value"", self.f)

That's because attribute lookups on the instance will also find class attributes (that's how methods are found, after all).
Note that the test runner will create a new instance of your class for each test being run; that ensures that the instance state is clean. The class state is not cleared, so if you alter your class attributes in a test you no longer will have proper test isolation.",A,3
11683303,2012-07-27 07:30:42.277000+00:00,"Plone does not set any requirements beyond lxml itself. So, if you meet the requirements listed on the lxml installation page, you meet the requirements for Plone 4.2.
In summary, lxml requires libxml 2.6.21 or later, and libxslt 1.1.15 or later. In detail, I quote the lxml installation page:


libxml 2.6.21 or later. It can be found here: http://xmlsoft.org/downloads.html

We recommend libxml2 2.7.8 or a later version.  
If you want to use XPath, do not use libxml2 2.6.27.  
If you want to use the feed parser interface, especially when parsing from unicode strings, do not use libxml2 2.7.4 through 2.7.6.  

libxslt 1.1.15 or later. It can be found here: http://xmlsoft.org/XSLT/downloads.html

We recommend libxslt 1.1.26 or later.



Debian and Ubuntu have libxml2 2.7.8 (Ubuntu has had that version since Natty), and libxslt1.1 1.1.26 (all the way back to Lucid); most other Linux distributions will have matching packages.
On Windows, the lxml egg has been statically linked against the correct libxml and libxslt versions, so you don't need to bother with versions there.",A,3
342442,2008-12-04 23:30:12.170000+00:00,"""explicitly tested against App1,2,3 every time there was a new library"" actually isn't that onerous.
Two things.

You need a formal set of API unit tests that the library must pass.  This is just the API, not every nuance of functionality.  If this passes, then your changes are good to go.  If this fails, your changes broke the API.
You also need a set of unit tests for functionality, separate from the API.  This is bigger, and might be classified as ""onerous"".

Once you start unit testing, you get addicted.  Once you have reasonably complete tests, this problem is easy to manage.",A,4
16447683,2013-05-08 18:28:32.707000+00:00,"It is an issue with the fact you installed Python from source.
You need to install the zlib1g-dev package to provide the headers to Python to be able to compile in zlib support:
sudo apt-get install zlib1g-dev

You may be missing other dependencies; here is a list of packages I'd install if I were to compile Python on an Ubuntu machine:
build-essential
libbz2-dev
libncursesw5-dev
libreadline5-dev
libssl-dev
libgdbm-dev
libc6-dev
libsqlite3-dev
tk-dev
",A,2
1226722,2009-08-04 10:20:49.710000+00:00,"
I need a way to configure variables that will be available in Python while running. 
Do what Django does.  Use a simple importable script of Python assignment statements.
I need a way to store state on the server. 
Do what Django does.  Use SQLite3.

Also, read PEP 333 and structure your application components to support WSGI.  This should be a relatively small revision to have the proper WSGI structure to existing code.
When you switch from mod_python to mod_wsgi, you can find numerous components to do these things for you and reduce the amount of code you've written.",A,0
38168365,2016-07-03 09:00:03.360000+00:00,"a.text is a Unicode object, but you are trying to write it to a plain Python 2 file object:
f.write(a.text)

The f.write() method only takes a byte string (type str), triggering an implicit encode to the ASCII codec, triggering your exception if the text can't be encoded as ASCII.
You'll either need to explicitly encode it with a codec that can encode your data, or use a io.open() file object that does the encoding for you.
Encoding explicitly to UTF-8 would work, for example:
f.write(a.text.encode('utf8'))

or use io.open() with an explicit encoding:
import io

# ...

f = io.open('workfile', 'w', encoding='utf8')

after which all calls to f.write() must be Unicode objects; prefix any literal strings with u:
for a in auth:
    f.write(a.text)
    f.write(u',')
f.write(u'\n')
",A,1
3826848,2010-09-30 00:20:49.860000+00:00,"
Good idea / bad idea?
Generally, this kind of thing is a bad idea.  That's what PHP is for.
What's wrong with http://www.phpformclass.com/
http://www.x-code.com/vdaemon_web_form_validation.php 
or other PHP form management tools?
Is this the right kind of syntax?
No.  What's wrong with PHP?  It has good syntax for this kind of thing.
Are there more elegant ways of naming the rules.
Yes.  PHP object classes.  Numerous Other projects.  You're not the first person to validate form input.
What's missing.
Answering the fundamental question: What's wrong with PHP?
A list of related projects that already do this and specific reasons why your project is better than all the other ones already out there.
",A,-2
38238899,2016-07-07 06:36:31.153000+00:00,"
If I do it by ""with"" statement, I will need to pass the instance to each function that I call:

No, you don't. Just use your singleton:
# global
db = Pdb()

# any other context
with db:

All that is required is that the expression produces a context manager. Referencing a singleton object with __enter__ and __exit__ methods would satisfy that requirement. You can even ignore the __enter__ return value, as I did above. The global will still be available to all your functions, the only thing that changes is that __enter__ and __exit__ will be called at the appropriate locations.
Note that even in Python 2, you should not rely on __del__ being called. And in the CPython implementation, outside circular references, the rules for when __del__ are called have not changed between Python 2 and 3.",A,1
43921753,2017-05-11 17:08:58.480000+00:00,"Your two outer loops are creating combinations of lists; use the itertools.combinations() iterator for those. Your innermost double loop produces the carthesian product, so use the itertools.product() iterator.
Don't generate indices with range(), just loop directly over the polygon lists; useenumerate()` to add indices rather than make indices work the other way around.
To pair up sections, the pairwise() recipe from the itertools recipes section; that'll let you get all segments to work with. To circle round to the start again (pairing up the last coordinate with the first), just append a list with the first element to the end.
Once you get rid of nested loops, you can use break to end them rather than use a flag variable.
from itertools import combinations, product

def pairwise(iterable):
    ""s -> (s0,s1), (s1,s2), (s2, s3), ...""
    a, b = tee(iterable)
    next(b, None)
    return zip(a, b)

for (i, a_poly), (j, b_poly) in combinations(enumerate(polygons), 2):
    for a in a_poly:
        if isInside(a.x, a.y, b_poly):
            union(i, j)
    for b in b_poly:
        if isInside(b.x, b.y, a_poly):
            union(j, i)

    # attach the first element at the end so you go 'round'
    a_segments = pairwise(a_poly + a_poly[:1])
    b_segments = pairwise(b_poly + b_poly[:1])
    for a_seg, b_seg in product(a_segments, b_segments):
        if doIntersect(*a_seg, *b_seg):
            union(i,j)
            break

In fact, once you have determined something is a union, you don't have to continue with the rest of the tests. You could use the any() function to stop testing the isInside() and doIntersect functions early:
for (i, a_poly), (j, b_poly) in combinations(enumerate(polygons), 2):
    if any(isInside(a.x, a.y, b_poly) for a in a_poly):
        union(i, j)
        break  # union found, no need to look further

    for any(isInside(b.x, b.y, a_poly) for b in b_poly):
        union(i, j)
        break  # union found, no need to look further

    # attach the first element at the end so you go 'round'
    a_segments = pairwise(a_poly + a_poly[:1])
    b_segments = pairwise(b_poly + b_poly[:1])
    if any(doIntersect(*a_seg, *b_seg) 
           for a_seg, b_seg in product(a_segments, b_segments)):
        union(i,j)

This is not only far more readable now, it should also be more efficient!",A,2
3971553,2010-10-19 18:41:18.560000+00:00,"Here's more versions of the same that produce slightly different results:
glob.iglob()
import glob
for f in glob.iglob(""/mydir/*/*.txt""): # generator, search immediate subdirectories 
    print f

glob.glob1()
print glob.glob1(""/mydir"", ""*.tx?"")  # literal_directory, basename_pattern

fnmatch.filter()
import fnmatch, os
print fnmatch.filter(os.listdir(""/mydir""), ""*.tx?"") # include dot-files
",A,21
15491515,2013-03-19 04:30:17.203000+00:00,"First, you can easily convert any sequence into a sequence of pairs of adjacent items. Just tee it, shift it forward, and zip the unshifted and unshifted copies. The only trick is that you need to start with either (<something>, 1) or (139, <something>), because in this case we want not each pair of elements, but a pair for each element:
def pairify(it):
    it0, it1 = itertools.tee(it, 2)
    first = next(it0)
    return zip(itertools.chain([first, first], it0), it1)

(This isn't the simplest way to write it, but I think this may be the way that's most readable to people who aren't familiar with itertools.)
>>> a = [1, 6, 9, 100, 102, 105, 109, 134, 139]
>>> list(pairify(a))
[(1, 1), (1, 6), (6, 9), (9, 100), (100, 102), (102, 105), (105, 109), (109, 134), (134, 139)]

Then, with a slightly more complicated version of Ned Batchelder's key, you can just use groupby.
However, I think in this case this will end up being more complicated than an explicit generator that does the same thing.
def cluster(sequence, maxgap):
    batch = []
    for prev, val in pairify(sequence):
        if val - prev >= maxgap:
            yield batch
            batch = []
        else:
            batch.append(val)
    if batch:
        yield batch
",A,2
16763473,2013-05-26 20:35:57.210000+00:00,"Use the fileinput module, which handles files correctly when replacing data, with the inplace flag set:
import sys
import fileinput

for line in fileinput.input('my_text_file.txt', inplace=True):
    x = process_result(line)
    if x:
        line = line.replace('something', x)

    sys.stdout.write(line)

When you use the inplace flag, the original file is moved to a backup, and anything your write to sys.stdout is written to the original filename (so, as a new file). Make sure you include all lines, altered or not.
You have to rewrite the complete file whenever your replacement data is not exactly the same number of bytes as the parts that you are replacing.",A,4
43027401,2017-03-26 10:06:51.677000+00:00,"You're iterating too much. d is the set of key-value pairs in the dict; filteritems is one of those keys, not an attribute of the pairs themselves. Remove that inner loop.
{% for c in filterdata.data %}
   {{ c.filtervalue }}
{% endfor %}
",A,1
34315576,2015-12-16 15:13:40.083000+00:00,"In this case, none. But what if A.__init__ did loads of complex logic? You don't want to have to duplicate all that in B.
An enhancement on Option 2 is to use the super() function:
class B(A):
    def __init_(self,a,b,c):
        super(B, self).__init__(a, b)
        self.c = c
",A,4
20702885,2013-12-20 11:42:04.880000+00:00,"You could, but there's no point, since the App Engine production environment is running 1.8.8.  So you might make it work locally, but it wouldn't work when you deploy.
There are various tutorials around which describe how to install Java 7 on Snow Leopard, though, such as in this question.",A,0
22430939,2014-03-15 23:12:19.597000+00:00,"You have a local variable get_player_move inside the function get_player_move(); you cannot then still use the function name (a global).
Rename the get_player_move local variable.
So, instead of:
get_player_move = raw_input("" 1) [R]ock, 2) [P]aper, 3) [S]cissors:"")

use:
move = raw_input("" 1) [R]ock, 2) [P]aper, 3) [S]cissors:"")

perhaps.
To get user input, it's best not to rely on recursion, however. The user could hit 'C' forever and then your program would crash with an RuntimeError: maximum recursion depth exceeded. It's easier to use a loop instead:
while True:
    move = raw_input("" 1) [R]ock, 2) [P]aper, 3) [S]cissors:"")
    if move == ""R"":
        print (""You used Rock!"") 
        return 1              

    # etc.

    else:
        print ""Invalid input, please use capitalized initial (R,P,S)""

Because you return from the function when a correct choice is made, the loop automatically is exited as well. If however you get to the end and Invalid input is printed, the while True loop starts at the top again and the user is asked once more to enter a choice.
Next: although your function returns a choice (an integer), you never store that return value. You must store it where you called the function:
player_move = get_player_move()
computer_move = get_computer_move()
result = compare_moves(player_move, computer_move)

if result == 1:

Note that it's not the function name that holds the return value; it's a separate variable. player_move is assigned whatever the get_player_move() returned, for example.
You can then pass these returned values to compare_moves(); it also returns a result, here stored in result for further comparisons.",A,2
12466054,2012-09-17 19:41:08.107000+00:00,"The only known discrete logarithm solvers are built around knowing the factors. If you don't have the factors, you need to generate them.
The best reasonable-time algorithm for this is Shor's algorithm. The problem is that you need a quantum computer with enough qubits, and nobody's built one large enough for your sample data yet. And it looks like it'll be quite a few years before anyone does; currently people are still excited about factoring numbers like 15 and 21.
If you want to use classical computing, the best known algorithms are nowhere near ""reasonably fast"". I believe someone recently showed that the Bonn results on 2^1039-1 should be reproducible in under 4 months with modern PCs. Another 5 years, and maybe it'll be down to a month.
It shouldn't surprise you that there are no known reasonable fast algorithms, because if there were, most private key encryption would be crackable and therefore worthless. It would be major news if someone gave you the answer you're looking for. (Is there an SO question for ""Is P=NP?"")",A,1
51293748,2018-07-11 20:29:43.760000+00:00,"What you're doing mostly makes sense, but it has one problem.

File objects are context managers that close themselves on __exit__. As the gzip docs make clear, that includes the GzipFile objects returned by gzip.open:

GzipFile supports the io.BufferedIOBase interface, including iteration and the with statement. 

So, if you write with f: on an opened regular file or GzipFile, that guarantees that close will be called after the with statement.
In Python 2.7, the details are slightly different, but it works the same way. In Python 2.6, a GzipFile was not a context manager. But there's a very easy solution (that's worth knowing about for other types, even if you don't care about Python 2.6): you can wrap anything with a close method in closing to get a context manager that calls that close on __exit__. So, you could write:
with contextlib.closing(R1):

… and it would work on R1 whether it's a file object, or some other kind of thing (like a 2.6 GzipFile) that doesn't know how to be a context manager.

However, what happens if R1 opens successfully, but R2 fails? Then you haven't even gotten into the with R1: when the exception is raised, so you never close R1.
You could fix this by doing the with R1 before opening R2:
if zipped:
    R1 = gzip.open(file1, 'r')
else:
    R1 = open(file1, 'r')
with R1:
    if zipped:
        R2 = gzip.open(file2, 'r')
    else:
        R2 = open(file2, 'r')
    with R2:

Or you could use an ExitStack.
But there's a much simpler solution here: Both gzip.open and open are callable objects, so you can store them in a variable, and call it later. Since they have the same signature, and you want to call them with the exact same arguments, using that variable is trivial:
if zipped:
    zopen = gzip.open
else:
    zopen = open
with zopen(file1, 'r') as R1:
    with zopen(file2, 'r') as R2:

And notice that you can make this a lot more concise without making it any less readable:
zopen = gzip.open if zipped else open
with zopen(file1, 'r') as R1, zopen(file2, 'r') as R2:
",A,5
19256631,2013-10-08 19:30:30.747000+00:00,"There is no simple script you can run that will do this in all cases. (That's a large part of the reason pip was invented…)

Apple's pre-installed Python 2.6 sets its default site-packages directory to:
/Library/Python/2.6/site-packages

… and its scripts directory to:
/usr/local/bin

So, sudo python setup.py install will install everything to those two directories.
If you're using a third-party Python, or have changed the configuration, the locations may be elsewhere, so I'll refer to these as ${sitedir} and ${bindir} below.
Exactly what you get installed depends on which version of PIL or pillow you installed. With the last few versions of PIL, it should be:

${sitedir}/PIL
${sitedir}/PIL-x.y.z-py2.6.egg-info
${bindir}/pil*.py (five scripts)

Depending on the way you have setuptools and/or distribute installed, you may not have the egg-info directory. With Pillow, the egg-info file is named Pillow-x.y.z-py2.6.egg-info. With older PIL, you may have only four scripts instead of five.
Anyway, if you have the egg-info directory, look at the installed-files.txt inside of it. All of those files, relative to the egg-info directory itself, are what got installed. However, just passing that file to rm -rf will leave behind any directories automatically created along the way—most notably, the ${sitedir}/PIL package directory. You could write a script that removes each file, then removes any directories that are left empty as a result… but that could be a bad idea if, e.g., you happen to have nothing else in /usr/local/bin. So, it's useful as a guide to drive or double-check manual cleanup, but it won't make everything automatic.",A,3
699459,2009-03-30 23:22:20.477000+00:00,"Bad news.  From a Unix command-line perspective those two invocations of your program are identical.
Unix can't easily distinguish them.  What you're asking for isn't really sensible, and you need to think of another way of using your program.
In the case where it's not in a pipeline, what's it supposed to read if it doesn't read stdin?
Is it supposed to launch a GUI?  If so, you might want to have a ""-i"" (--interactive) option to indicate you want a GUI, not reading of stdin.
You can, sometimes, distinguish pipes from the console because the console device is ""/dev/tty"", but this is not portable.",A,2
26791164,2014-11-06 23:09:37.833000+00:00,"First, if you want to do numerical comparisons against version, use version_info, don't try to parse the string in version.*
This also means you can take advantage of the way tuples are compared and write code like:
if sys.version_info < (3, 3):

… and it won't break if Python gets to 3.10.0 or 4.0 ever come out.
* Unless you need to worry about 1.5 vs. 2.0, in which case you've obviously got a time-machine gatewaying between Usenet and StackOverflow, and surely you can think of better uses for that.

Anyway, it's probably better to just test for shlex.quote existing in the first place. That way, it's obvious to the reader what you're doing: using shlex.quote if possible, falling back to pipes.quote if not.
You'll see that pattern all over the place—even in the stdlib, where code is imported from C accelerator modules if possible, but fallback code is used if not (e.g., if you're using PyPy instead of CPython).

Also, note that pipes.quote is only documented in 2.7. Not 3.0-3.2 (which is what it seems like you care about…), or 2.6, or any other version. You're relying on something that happens to be there in a particular implementation. (Well, practically, in all of them,* but still, why rely on that if you don't have to?)
* As far as I know, there aren't any 3.2 implementations that have pipes and shlex but don't have pipes.quote. And there probably won't be too many new 3.2 implementations created in the future.

Also, while it can occasionally be useful to import in the middle of a function, it's kind of a weird thing to do. If there's a problem with the installation on a machine you deploy this on, would you really expect to be able to import your module successfully, but then later get an ImportError on calling some function? That's the kind of thing people get baffled by and run to SO for help with all the time. :) (It's also obviously a bit of a performance hit to do all this looking in sys.modules and converting strings to ints and so on when you don't need to, but I doubt that's going to matter.)

So, I think the way I'd write it would be:
try:
    from shlex import quote as cmd_quote
except ImportError:
    from pipes import quote as cmd_quote
",A,9
19221229,2013-10-07 09:29:35.843000+00:00,"Just concatenate the lists:
call(['./myProgram'] + myArgs)

The first argument must be a list of strings; simply build that list from two separate lists.",A,3
20619808,2013-12-16 20:00:25.083000+00:00,"
I've been doing it by using other people's script on their webpage, how can i not rely on these random site forwarding script and make my own?

There are two ways to do this:

Create your own spider that amasses a huge collection of websites, and pick from that collection.
Access some pre-existing collection of websites, and pick from that collection. For example, DMOZ/ODP lets you download their entire database;* Google used to have a customized random site URL;** etc.

There is no other way around it (short of randomly generating and testing valid strings of arbitrary characters, which would be a ridiculously bad idea).
Building a web spider for yourself can be a fun project. Link-driven scraping libraries like Scrapy can do a lot of the grunt work for you, leaving you to write the part you care about.

* Note that ODP is a pretty small database compared to something like Google's or Yahoo's, because it's primarily a human-edited collection of significant websites rather than an auto-generated collection of everything anyone has put on the web.
** Google's random site feature was driven by both popularity and your own search history. However, by feeding it an empty search history, you could remove that part of the equation. Anyway, I don't think it exists anymore.",A,7
31686932,2015-07-28 20:57:12.090000+00:00,"Use pytz module to work with timezones in Python. To get the local timezone as pytz tzinfo object, you could use tzlocal module:
from tzlocal import get_localzone # $ pip install tzlocal

naive = _to_datetime('07/27/2015', '06:00 AM')
aware = get_localzone().localize(naive, is_dst=None)
print(aware.strftime('%Y%m%d%H%M%S %z'))
# -> 20150727060000 -0400
",A,3
44265043,2017-05-30 14:28:55.987000+00:00,"json.dumps does the opposite of what you claim; it converts a Python object into a string.
The result of request.json is already a Python datastructure. You don't need to do anything else with it.",A,0
49345794,2018-03-18 08:10:14.640000+00:00,"Your code is almost correct, but you have one problem:
else:
    # if char < aStr[(len(aStr)//2)]:
    return isIn(char, aStr[:(len(aStr)//2)])
    return isIn(char, aStr[(len(aStr)//2):])    

That first return is going to exit the function, returning either True or False, so that second one will never get called at all.
If you trace through it by adding a print(char, aStr) at the start of the function, or using a breakpoint in your debugger (you really should try at least one of these two), you'll see that this is exactly the problem. With s, i, and e, that second isIn(char, aStr[(len(aStr)//2):]) would have returned True, but you never call it.
S, you want to return True if either one returns True, and False if both return False. The easy way to do that is to use or, exactly the same way you would in English:
else:
    # if char < aStr[(len(aStr)//2)]:
    return isIn(char, aStr[:(len(aStr)//2)]) or isIn(char, aStr[(len(aStr)//2):])

With that change, your function now correctly finds every letter in 'frostbite'.",A,1
31845976,2015-08-06 02:42:20.650000+00:00,"Python is not very suitable for shell one-liners. You could play with The Pyed Piper:
$ ls | pyp ""p[0] | pp.sort() | p + ' first letter, sorted!'""
  # it gives sorted list of first letters of every line

It uses standard Python string and list methods as well as custom functions. There is also pyline:
$ ls | pyline -m os 'line and os.path.abspath(line.strip())'
$ ls | pyline -r '\(.*\)' 'rgx and (rgx.group(0), rgx.group(1)) or line'
$ ls | pyline -p 'p and p.abspath() or (""# "".format(line))'

Another alternative is to use ipython as a shell or a browser-based notebook (recommended). Or if you want more BASHwards-looking syntax and tab completion for subprocess commands; try xonsh as your shell:
xonsh$ [i*i for i in range(10)]
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
xonsh$ date -u
Tue Oct  6 04:25:27 UTC 2015

subprocess and its alternatives (plumbum, pexpect, sarge, sh (module), fabric) allow you to create arbitrary complex commands by exploiting the best of python and bash.
There are also several nice python -m one-liners e.g.:
$ python3 -m http.server # serve current directory over http
$ python -m zipfile # work with zipfiles
$ python -m calendar # show calendar
$ python -m telnetlib towel.blinkenlights.nl # Star Wars
",A,5
16843289,2013-05-30 18:20:49.190000+00:00,"I'm assuming you want to unhexlify the data, and store the resulting bytestrings as fixed-length character strings rather than object. (You can't store them as some kind of int128 type, because numpy doesn't have such a type.)
To avoid reading 3.2GB of text into memory, and using roughly the same amount pre-processing it into the desired form, you probably want to use fromiter, so:
with open(myfile) as f:
    iv = binascii.unhexlify(f.readline().strip())
    key = binascii.unhexlify(f.readline().strip())
    count = int(f.readline())
    a = np.fromiter((binascii.unhexlify(line.strip()) for line in f), dtype='|S16')

If you have 10GB of RAM to spare (rough ballpark guess), it might be faster to read the whole thing in as an array of object, then transform it twice… but I doubt it.

As to whether this will help… You might get a little benefit, because AES-ing 16 bytes may be fast enough that the cost of iteration is noticeable. Let's test it and see.
With 64-bit Mac Python 2.7.2, I created an array of 100000 S16s by copying your example repeatedly. Then:
In [514]: %timeit [aes.encrypt(x) for x in a]
10 loops, best of 3: 166 ms per loop
In [515]: %timeit np.vectorize(aes.encrypt)(a)
10 loops, best of 3: 126 ms per loop

So, that's almost a 25% savings. Not bad.
Of course the array takes longer to build than just keeping things in an iterator in the first place—but even taking that into account, there's still a 9% performance gain. And it may well be reasonable to trade 1.6GB for a 9% speedup in your use case.
Keep in mind that I'm only building an array of 100K objects out of a pre-existing 100K list; with 100M objects read off disk, I/O is probably going to become a serious factor, and it's quite possible that iterative processing (which allows you to interleave the CPU costs with disk waits) will do a whole lot better.
In other words, you need to test with your own real data and scenario. But you already knew that.

For a wider variety of implementations, with a simple perf-testing scaffolding, see this pastebin.
You might want to try combining different approaches. For example, you can use the grouper recipe from itertools to batch things into, say, 32K plaintexts at a time, then process each batch with numpy, to get the best of both. And then pool.imap that numpy processing, to get the best of all 3. Or, alternatively, put the one big numpy array into shared memory, and make each multiprocessing task process a slice of that array.",A,4
4603120,2011-01-05 10:39:25.217000+00:00,"You can follow the relationship across two joins:
invoice.client_contract_number.client_number.name

By the way, your field names are confusing. client_contract_number isn't a number, it's a Contract. And client_number isn't a number either, it's a Client. Just call them client_contract and client.
Edit after question update
I'm not sure what you're trying to do here. invoices_list is a queryset of all invoices - obviously it doesn't make sense to ask what the client name is for that list. Presumably what you actually want to do is iterate through - probably in your template - and print out the name for each one:
{% for invoice in invoices_list %}
    Client name: {{ client_contract_number.client_number.name }}
{% endfor %}
",A,2
21514469,2014-02-02 18:28:08.943000+00:00,"The documentation explains this fully. AbstractUser is a full User model, complete with fields, as an abstract class so that you can inherit from it and add your own profile fields and methods. AbstractBaseUser only contains the authentication functionality, but no actual fields: you have to supply them when you subclass.",A,52
17297964,2013-06-25 12:56:37.043000+00:00,"You can make it a class method:
class Molecule(object):
    # ...

    @classmethod
    def fromFile(cls, filename):
        data = # ... parse data from filename ...
        return cls(data)

This has the added advantage that subclasses can simply inherit it, or override it to adjust what the method does (including calling the original parent implementation). It provides an alternative class factory:
import mymodule

mol = mymodule.Molecule.fromFile('toto')

neatly grouping this factory with the class itself.",A,8
25829389,2014-09-14 01:51:00.640000+00:00,"Decorators are just syntactic sugar to replace a function object with a decorated version, where decorating is just calling (passing in the original function object). In other words, the syntax:
@decorator_expression
def function_name():
    # function body

roughly(*) translates to:
def function_name():
    # function body
function_name = decorator_expression(function_name)

In your case, you can apply your decorator manually instead:
from random import randint

randint = decorator(randint)


(*) When using @<decorator> on a function or class, the result of the def or class definition is not bound (assigned to their name in the current namespace) first. The decorator is passed the object directly from the stack, and only the result of the decorator call is then bound.",A,30
2195979,2010-02-03 22:30:02.733000+00:00,"Don't force Django to do something unnatural.

Allow it to create the test schema.  It's a good thing.
From your existing schema, do an unload to create .JSON dump files of the data.  These files are your ""fixtures"".  These fixtures are used by Django to populate the test database.  This is The Greatest Testing Tool Ever.  Once you get your fixtures squared away, this really does work well.
Put your fixture files into fixtures directories within each app package.  
Update your unit tests to name the various fixtures files that are required for that test case.

This -- in effect -- tests with an existing schema.  It rebuilds, reloads and tests in a virgin database so you can be absolutely sure that it works without destroying (or even touching) live data.",A,2
787200,2009-04-24 19:18:15.440000+00:00,"We do it like this.
Most stuff we download is in site-packages.  They come from pypi or Source Forge or some other external source; they are easy to rebuild; they're highly reused; they don't change much.
Must stuff we write is in other locations (usually under /opt, or c:\opt) AND is included in the PYTHONPATH.
There's no great reason for keeping our stuff out of site-packages.  However, our feeble excuse is that our stuff changes a lot.  Pretty much constantly.  To reinstall in site-packages every time we think we have something better is a bit of a pain.
Since we're testing out of our working directories or SVN checkout directories, our test environments make heavy use of PYTHONPATH.  
The development use of PYTHONPATH bled over into production.  We use a setup.py for production installs, but install to an alternate home under /opt and set the PYTHONPATH to include /opt/ourapp-1.1.",A,4
5271692,2011-03-11 10:25:06.547000+00:00,"As @Felix Kling suggested use action='store_true':
>>> from argparse import ArgumentParser
>>> p = ArgumentParser()
>>> _ = p.add_argument('-f', '--foo', action='store_true')
>>> args = p.parse_args()
>>> args.foo
False
>>> args = p.parse_args(['-f'])
>>> args.foo
True
",A,173
452660,2009-01-17 01:52:49.790000+00:00,"listOfLambdas = [lambda i=i: square(i) for i in listOfNumbers]

Or
listOfLambdas = map(lambda i: lambda: square(i), listOfNumbers)
",A,2
38421775,2016-07-17 13:31:06.123000+00:00,"Add an ORDER BY on the COUNT results; give that result an alias:
SELECT
    software_ssh,
    COUNT (software_ssh) as freq
FROM simple_table
GROUP BY software_ssh 
ORDER BY freq

Use ORDER BY freq DESC if you need the rows ordered from most frequent to least.
Demo:
>>> from pprint import pprint
>>> c.execute('SELECT software_ssh, COUNT (software_ssh)  FROM simple_table group by software_ssh')
<sqlite3.Cursor object at 0x102034490>
>>> pprint(list(c))
[(u'program_version_1.1', 2),
 (u'program_version_1.2', 2),
 (u'program_version_1.3', 20),
 (u'program_version_2.1', 7)]
>>> c.execute('''
... SELECT
...     software_ssh,
...     COUNT (software_ssh) as freq
... FROM simple_table
... GROUP BY software_ssh
... ORDER BY freq
... ''')
<sqlite3.Cursor object at 0x102034490>
>>> pprint(list(c))
[(u'program_version_1.1', 2),
 (u'program_version_1.2', 2),
 (u'program_version_2.1', 7),
 (u'program_version_1.3', 20)]
",A,1
28755312,2015-02-27 00:15:05.750000+00:00,"You are not properly escaping your value; the + character in a URL-encoded query value is the encoded value for a space, so really you are printing:
""c  ""

A c with two spaces. Use the encodeURIComponent() function to properly escape the input value, where spaces will be replaced by + and + will be replaced by %2B, so that Python can decode that back to +:
<a onclick=""window.location='escapetest.py?q='+encodeURIComponent(document.getElementById('inputtxt').value);"">Go!</a>
",A,2
291981,2008-11-15 01:51:41.033000+00:00,"Your three types are probably easiest handled as FK associations with the fundamental tree.
The tree can be homogenous -- class MyNode is a direct subclass of treebeard.Node.  Your node can have a flag (Root, Middle, Leaf) and FK's for A or B or C.  This allows you some SQL-like flexibility in querying MyNode instance.
This allows your tree to grow.  A node can start as a type C (leaf) and then morph to a type B (intermediate).  You change the status, and change the FK's.
The alternative is a bit more complex.
class MyA( treebeard.Node ):
    pass

class MyB( treebeard.Node ):
    pass

class MyC( treebeard.Node ):
    pass

In this case, you can't ""morph"" a node.  When a node starts as a MyC, and gets children, you have to remove the original MyC instance, and replace it with a MyB version that has a new node as a child.  This isn't impossible, but it can be painful.",A,3
14230915,2013-01-09 08:19:27.353000+00:00,"Use the collections.namedtuple class factory to create a named tuple class:
mynamedtuple = collections.namedtuple('mynamedtuple', ('val1', 'val2', 'val3', 'box'))

somenamedtuple = mynamedtuple('a1', 'a2', 'a3', ['d1', 'e1', 'f1'])
somenamedtuple.box  # returns ['d1', 'e1', 'f1']

You can convert your existing list using a list comprehension:
a = [mynamedtuple(*el) for el in a]
",A,11
9351292,2012-02-19 17:16:36.100000+00:00,"if 4th and 5th columns are network prefix and subnet mask then to grep input for an ip:
#!/usr/bin/env python
import fileinput, socket, struct, sys

# x.x.x.x string -> integer
ip2int = lambda ipstr: struct.unpack('!I', socket.inet_aton(ipstr))[0]    

ip = ip2int(sys.argv.pop(1)) # get ip from command-line
for line in fileinput.input(): # read from stdin or file(s) given at command-line
    try:
        family, network_prefix, subnet_mask = line.split('|')[2:5]
        if (family == 'ipv4' and
            (ip & (2**32 - int(subnet_mask))) == ip2int(network_prefix)):
            print line,
    except Exception:
        print >>sys.stderr, ""can't parse"", line,

Example
$ python grep-ip.py 192.168.5.193 <<'.'
x|x|ipv4|192.168.5.128|64|x|x
x|x|ipv4|192.168.5.192|64|x|x
x|x|ipv4|192.168.5.0|64|x|x
x|x|ipv4|192.168.5.0|256|x|x
.

Output
x|x|ipv4|192.168.5.192|64|x|x
x|x|ipv4|192.168.5.0|256|x|x
",A,4
31255312,2015-07-06 20:47:14.237000+00:00,"All you need is a custom authentication backend. With that, you can use your existing table, and have all the benefits of the standard Django user model.
However I would urge you very strongly to try and move your user data to have properly hashed passwords. In your case, for example, you could run a data migration to copy the data to a new table and hash the passwords on the way.",A,2
51489194,2018-07-24 01:28:11.350000+00:00,"First, if you're generating pure ASCII, and you need it to be as fast as possible, it probably going to be faster to generate bytes than str. You can always call decode('ascii') on them if needed, but more simply, just write them directly to sys.stdout.buffer or sys.stdout.buffer.raw.
That means you can get rid of that table mapping numbers to chr values and just construct a bytes or bytearray out of your ints. (As long as all of the values are in range(0, 128), you're guaranteed the same results, but with one function call with a C loop inside of it, instead of a function call inside a Python loop.)
Also, instead of constructing a list of N empty strings and then replacing them one by one, you can just call random.choices(range(33, 127), k=N) and then pass the result to the bytes constructor.
While we're at it, as pointed out by Dillon Davis, randint is pretty slow; you can get somewhere between 3-5x faster by doing the same logic manually. Which turns out not to matter much here (we're doing about one randint for a few hundred choices), but still, might as well fix it.
So, putting that all together:
def create_n_bytes(self,total_bytes):
    bytes_created = 0
    """"""Hack at the moment, this condition will fail only after more than n bytes are 
    written """"""
    chars = range(33, 127)
    while bytes_created < total_bytes:
        bytes_to_create = int(random.random() * (high-low+1) + low)
        word = bytes(random.choices(chars, k=bytes_to_create))
        bytes_created = bytes_created+bytes_to_create+1
        sys.stdout.buffer.write(word + b'\n')
    # necessary if you're doing any prints between calls
    sys.stdout.flush() 


Also, try running the same code in PyPy instead of CPython. It may be 5% faster, or it may be 20x as fast.

If you need to squeeze out a bit more performance, all of the usual micro-optimization tricks might be applicable here, like stashing randint and choices and sys.stdout.buffer (or maybe sys.stdout.buffer.write—try it both ways) in local variables.

If it's still nowhere near fast enough, you need to change things to generate a whole lot more bytes at a time.
That means passing a much larger total_bytes value, but it also probably means dragging in NumPy:
buf = np.random.randint(33, 127, size=total_bytes, dtype=np.uint8)

Now, how do you break this up into words of low to high bytes? I can't think of anything really clever, but a dumb loop should still be faster than all of the above code:
i = 0
while i < len(buf) - self.high:
    i += random.randint(self.low, self.high)
    buf[i] = 10 # newline
sys.stdout.buffer.write(buf.data[:i])

This one ends too soon instead of going too far. But no matter what you do, you're obviously going to have to deal with this—the odds that you're going to exactly hit total_bytes no matter how you do things are 1/(high-low), right?
(It's a bit hacky to slice the memoryview returned by data instead of slicing the array and calling to_bytes() on it, but given the way we created the array it's guaranteed to do the same thing, and it might save a few microseconds, so it might be worth doing with a comment.)

If we don't mind wasting memory, we can build an array of random offsets that's guaranteed to be large enough (but will probably be way too large) and then just use that as an index array:
sizes = np.random.randint(self.low, self.high, total_bytes//self.low, dtype=np.int32)
offsets = np.cumsum(sizes)
last = np.searchsorted(offsets, 1000)
offsets = offsets[:last]
buf[offsets] = 10

The speedup here should be a lot bigger for arrays of, say, 1MB than for something small like 1000 bytes (and it may also be bad for absolutely huge arrays, because of the extra allocation), but it's certainly worth testing.

Performance comparisons are going to depend a lot on your platform (including the terminal you're writing to, if you're not redirecting to /dev/null or NUL), and the parameters you choose.
Using your default values of low=4, high=10, and total_bytes=1000 (I used 1010 for NumPy to be fail), running on my laptop (macOS, IPython inside iTerm, not redirecting stdout, CPython 3.7.0 and PyPy 3.5.2/5.10), here are my results:

Original code: 2.7ms
Original code in PyPy: 938µs
My version with randint: 911µs
My version with random: 909µs
My version in PyPy (without random.choices): 799µs
NumPy loop version: 584µs
Just writing a pre-existing 1000-byte buffer: 504µs

If you subtract out that last one to get the real times for the different algorithms:

Original code: 2.2ms
Original code in PyPy: 434µs
My version with randint: 407µs
My version with random: 405µs
My version in PyPy (without random.choices): 295µs
NumPy loop version: 80µs

I'm on a different laptop for the NumPy randint/cumsum version, but testing it against the original NumPy version, with stdout redirected to /dev/null:

NumPy loop version: 122µs
NumPy cumsum version: 41µs

So, that's a 3x speedup over something that was already a 27x speedup, so we're talking somewhere in the ballpark of 80x. But writing to a terminal slows things down enough that the wall-clock speedup is only about 5x. (Probably a lot worse if you're on Windows.) Still, not bad.",A,4
10212758,2012-04-18 15:30:14.277000+00:00,"I don't think your problem has to anything to do with Javascript. The issue is here:
{{ e|add:err }}

This makes no sense at all. It seems like you're trying to build up a list, e, consisting of all the items in form.email.errors. But you can't do that sort of thing in a template - no data manipulation is allowed, by design. The add filter simply performs numeric calculations for display, it doesn't modify objects.
You probably want to serialize the errors to JSON in your view, and pass that JSON object to classEditor.set.",A,1
13424658,2012-11-16 21:27:32.380000+00:00,"You need to pass the contents of your list as separate parameters:
intersect(*d)

What happens otherwise is that the whole list is being used as one set instead. The *d syntax indicates to Python that you want to use d as a sequence of parameters to the function, instead using the whole d list as just one parameter.",A,10
52697652,2018-10-08 07:48:45.087000+00:00,"You need to use two separate cursors to do that. A cursor represents a single result set, the inner c.execute() clears the result set that the outer loop attached to the cursor.
If c is a connection object, you need to explicitly create cursors from that:
outercursor = c.cursor()
for tablename, in outercursor.execute(""SELECT tablename FROM triggers_sql""):
    innercursor = c.cursor()
    for row in innercursor.execute('SELECT * FROM ""{}""'.format(tablename)):
        # ...

If c is a cursor object, just create another one from the connection. You can even do so from an existing cursor with:
innercursor = c.connection.cursor()

Note that there are probably better ways of structuring your database where you don't have to use dynamic table names in the first place. Store everything that you now use separate tables for in a single table with one extra column replacing the tablename. At that point you can use a JOIN and leave it to sqlite to worry about how to produce the looping.",A,3
44097593,2017-05-21 13:41:52.490000+00:00,"Yes, Queue.get() is blocking by default. From the documentation:

If optional args block is true and timeout is None (the default), block if necessary until an item is available.

Bold emphasis mine. Because q.get() blocks, it won't return until the other thread has put something in the queue for it to fetch.
Even so, removing an item from the queue means it won't be there the next time. q.get() doesn't leave the number there to be fetched again and again.
Instead, if you were to use q.get(False) (or used q.get_nowait()) to prevent blocking, an Empty exception is raised instead.",A,1
628066,2009-03-09 21:13:07.780000+00:00,"""It is better to return a polynomial and keep the former value of p intact""
Life is better if you make your numeric types behave like other numbers.
The ""destructive"" operations are a necessary evil when you have to optimize performance by reducing memory allocations.
The tradeoff is that your processing becomes opaque because of the more complex-looking state changes in the variables.  You don't want hidden state change; you want your assignments of values to variables to be a big, obvious first-class, with-an-= kind of thing.",A,3
30824428,2015-06-13 23:05:52.137000+00:00,"The ABCMeta class implements hooks to customize instance and subclass checks; both __instancecheck__() and __subclasscheck__() are provided.
These hooks delegate to the ABCMeta.__subclasshook__() method:

__subclasshook__(subclass)
Check whether subclass is considered a subclass of this ABC. This means that you can customize the behavior of issubclass further without the need to call register() on every class you want to consider a subclass of the ABC.

This hook then can check if a given subclass implements the expected methods; for Callable the implementation simply has to see if there is a __call__ method present:
@classmethod
def __subclasshook__(cls, C):
    if cls is Callable:
        if _hasattr(C, ""__call__""):
            return True
    return NotImplemented

This is restricted to the specific Callable class; subclasses will have to implement their own version as you most likely would add methods.",A,0
50859582,2018-06-14 14:13:30.123000+00:00,"Fedora / CentOS patches various Python module to alter the installation locations, see this ServerFault post on the difference between /usr/lib and /usr/lib64.
Note that the /usr/lib64 path is only used for platform specific code, not for pure-python libraries. So the correct method to use is to set the plat_specific argument to get_python_lib() (the first one) to True:
$ python3 -c ""import distutils.sysconfig as sc; print(sc.get_python_lib(True, prefix=\""$HOME/some_prefix\""))""

However, if your target prefix directory is for a separate Python installation without the Fedora / CentOS patches, then you should really use the Python binary installed in that prefix to handle the installation:
$HOME/some_prefix/bin/python3 setup.py install

It'll come with all the right system configuration, including an unpatched distutils and a sysconfig module with the right local $PREFIX value, and no PYTHONPATH warning will be issued. 
And if you wanted to isolate packages from the system installation, use a virtualenv and install with the environment-specific Python binary:
$ virtualenv name_of_virtualenv_directory
$ name_of_virtualenv_directory/bin/python setup.py install
",A,1
18261428,2013-08-15 20:40:51.913000+00:00,"Use a list comprehension:
[sublist[::-1] for sublist in outerlist]

Demo:
>>> outerlist = [[1, 2], [3, 4], [5, 6]]
>>> [sublist[::-1] for sublist in outerlist]
[[2, 1], [4, 3], [6, 5]]

This produces a new list. You can also reverse sublists in place by calling the list.reverse() method on each one in a loop:
for sublist in outerlist:
    sublist.reverse()
",A,4
27589327,2014-12-21 12:24:24.550000+00:00,"Gunicorn is itself a Python application; the Gunicorn PPA only publishes Python 2 versions.
You'd install Gunicorn into Python 3 instead (using pip, preferably into your virtualenv) and run that version:
/home/www/app-server/venv/bin/gunicorn
",A,7
19646351,2013-10-28 22:36:45.817000+00:00,"list.index() returns the index of a given value in the list:
>>> ['spam', 'ham', 'eggs'].index('ham')
1

but raises a ValueError when the item is not found in the list:
>>> ['spam', 'ham', 'eggs'].index('monty')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: 'monty' is not in list

If you have indices instead, just use indexing on the list:
>>> ['spam', 'ham', 'eggs'][1]
'ham'

If you wanted to pair up elements of two lists, use the zip() function instead:
for kw, definition in zip(keyword, define):
    if kw == definition:
        # the values match at the same index.
",A,1
50902968,2018-06-18 05:08:54.280000+00:00,"Whenever you want a default value that can't easily be specified statically, the idiom is to use a sentinel value.
If None is not a valid value that anyone might ever pass, use that:
def do_a_thing(self, my_arg=None):
    if my_arg is None:
        my_arg = self.bar
    do_something_else(my_arg)

Or, if there's no falsey value that would ever be valid:
def do_a_thing(self, my_arg=None):
    if not my_arg:
        my_arg = self.bar
    do_something_else(my_arg)

Or, if even None could be a value, you need to create a private sentinel that nobody will ever pass:
_sentinel = object()
def do_a_thing(self, my_arg=_sentinel):
    if my_arg is _sentinel:
        my_arg = self.bar
    do_something_else(my_arg)


By the way, calling this a ""default keyword argument"" implies a serious misunderstanding. The fact that the syntax for default values in function definitions looks similar to the syntax for keyword arguments in function calls is a coincidence. In particular, this is not a keyword parameter, and it can be called without using keyword arguments. For example:
my_thing.do_a_thing(20)

If you actually want a keyword-only parameter, it has to come after a * (or after *args, if you have one of those). And, because these are completely unrelated features, you can have keyword-only parameters without default values:
def do_a_thing(self, *, my_arg):
    do_something_else(my_arg)

And ""kwargs"" usually isn't used as shorthand for ""keyword argument"" or ""keyword parameter"", but rather for the **kwargs parameter that gathers all unknown keyword arguments.",A,4
21345487,2014-01-25 01:43:16.373000+00:00,"
There can also sometimes be multiple Rate\Rates tags, so I can't just ask it to give me the 2nd ""Total"" tag.

Why not just iterate over all the Total tags and skip the ones that have no Taxes child?
reservations = message.find_all('HotelReservation')
for reservation in reservations:
    totals = reservation.find_all('Total')
    for total in totals:
        if total.find('Taxes'):
             # do stuff
        else:
             # these aren't the totals you're looking for

If you more generally want to eliminate those that have no child nodes, you could do either of these:
if next(total.children, None):
    # it's a parent of something

if total.contents:
    # it's a parent of something

Or you could use a function instead of a string as your filter:
total = reservation.find(lambda node: node.name == 'Total' and node.contents)

Or you could look at other ways to locate this tag: it's a direct child of RoomStay rather than just a descendant; it's not a descendant of Rate; it's the last Taxes descendant under a RoomStay; etc. All of these can be done just as easily.

That being said, this seems like a perfect job for XPath, which BeautifulSoup doesn't support, but ElementTree and lxml do…",A,1
42009949,2017-02-02 18:43:35.293000+00:00,"A 12-hour clock has no 0 hour; %I will only match 1 through to 12. Your timestamp has an impossible time in it:
0:03 pm

From the strftime() and strptime() Behavior documentation:

%I
  Hour (12-hour clock) as a zero-padded decimal number.
  01, 02, ..., 12

Assuming 0 is really 12, you could repair this by replacing the ' 0:' with '12:' (note the leading space for the zero!):
>>> from datetime import datetime
>>> latest_datetime = 'Feb 1, 2017  0:03 pm'
>>> datetime.strptime(latest_datetime.replace(' 0:', '12:'), ""%b %d, %Y %I:%M %p"")
datetime.datetime(2017, 2, 1, 12, 3)

It doesn't really matter if you have one or two spaces between the year and the hour, the string will be parsed either way.",A,2
30024890,2015-05-04 07:35:49.590000+00:00,"If the website has an API that requires you to logout, requests can't possibly know that. You have to explicitly make the logout API request.
While requests has something called a Session, and many web service frameworks do too, a session isn't actually a persistent connection between two computers like a TCP socket.

A web service framework session is essentially just a way to pass an ID back and forth to the client in cookies and map it to some database record or equivalent. It still only sees one request at a time, and the only way it knows you're done, unless you tell it, is by not seeing your session ID for a while.
A requests session is essentially just a place to store cookies across multiple requests the way a browser would, so web service frameworks' sessions will work the same way they would in a browser.

So, clearing the cookies doesn't do anything that's visible to the server. The server can't see anything you do, except for the requests that you send.",A,4
53994821,2019-01-01 10:44:53.360000+00:00,"The python-barcode project requires Pillow to be installed if you want to render barcodes to an image, otherwise it sets ImageWriter to None. See the barcode.writer source code for details (PIL is the name of the package that Pillow provides).
Run pip install Pillow to remedy this.
You can also tell pip to pull in dependencies by installing the python-barcode images extras:
pip install python-barcode[images]
",A,1
27668273,2014-12-27 14:39:38.907000+00:00,"get_map_iterator() returns a function object (dispatch). You are trying to treat that object as if it was a dictionary.
You want to call it instead:
while it('has_more'):
    it('Next')

Your dispatch() function does not itself return another function object, so you'd not call whatever it() returns.
Your has_more route then fails with:
>>> it('has_more')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 17, in dispatch
  File ""<stdin>"", line 9, in has_more
TypeError: next expected at least 1 arguments, got 0

presumably because you meant to use the Next() function you defined, not the built-in next() function.
However, even fixing that won't get you your output, because slist[0] != slist[1].
It sounds as if you were trying to actually return a dictionary:
dispatch = {
    'Next': lambda: gfunc(Next()),
    'has_more': has_more
}
return dispatch

This return value you would use like you did originally, by looking up the callable via a key, then calling it.",A,1
33890724,2015-11-24 10:07:59.843000+00:00,"Your user is logged in, but because you are strangely rendering the Home and Index templates via template.render(None), there is no request or user context in your template. Instead, use the render shortcut which runs context processors.",A,2
16039920,2013-04-16 14:37:04.083000+00:00,"Your diagnosis is incorrect. The last statement will match if die3 is equal to either die1 or die2:
>>> die1, die2 = 1, 2
>>> die3 = die1
>>> die3 == die1 or die3 == die2
True
>>> die3 = die2
>>> die3 == die1 or die3 == die2
True

However, if die1 is greater than die2 your first elif matches, but your comparison to die3 is flawed:
>>> die1, die2 = 2, 1
>>> die3 = die1
>>> die3 < die2 or die3 > die1
False

and nothing is ever printed. You need to clean up the logic for that case.
Since you only have 3 different outcomes; winning, losing and hitting the post, you can really simplify the whole thing down to:
if die1 < die3 < die2:
    print('\n*** You win! ***')
elif die3 == die1 or die3 == die2:
    print('\n*** You hit the post - You lose double your bet! ***')
else:
    print('\n*** Sorry - You lose! ***')

Note that I used chained comparisons to simplify the first if expression some more. You also made sure that die1 is lower than die2, so testing for die2 < die3 < die1 is always going to be False.
Another note: == should be transitive, so die1 == die2 or die2 == die1 is redundant. You can simplify that to just die1 == die2.
Simplifying your whole program:
import random

die1 = random.randint(1,10)
die2 = random.randint(1,10)

if die1 > die2:
    die1, die2 = die2, die1

print ('Die 1:', die1,'  Die 2:', die2,)

if die1 == die2:
    print('Even-steven!')
else:
    print(""Not the same, let's play!"")
    die3 = random.randint(1,10)

    print ('Die 3:', die3)

    if die1 < die3 < die2:
        print('\n*** You win! ***')
    elif die3 == die1 or die3 == die2:
        print('\n*** You hit the post - You lose double your bet! ***')
    else:
        print('\n*** Sorry - You lose! ***')

print ('Thanks for playing!')
",A,1
34737882,2016-01-12 07:33:35.047000+00:00,"If you want to use .disable_warnings() then you should import module that is used by requests:
from requests.packages import urllib3
urllib3.disable_warnings()

There are other ways to disable warnings. If possible; you should enable certificate verification instead.",A,3
27887135,2015-01-11 12:38:26.573000+00:00,"You always get a cursor, because the datastore doesn't know or care if there are any more results. What you should do is check that the cursor actually returns something, and if not don't show the link for more results.",A,1
7042592,2011-08-12 15:38:18.727000+00:00,"Mutable objects shouldn't really be used as dictionary keys. That said, this works because the base model class defines __hash__ in terms of the model's primary key, which is unlikely to change. But I would prefer to use the pk directly as the key.",A,10
28859147,2015-03-04 16:04:33.197000+00:00,"This is an immensely bad idea. You're running eval on input received from the browser. So if I use the browser dev tools to modify the contents of the hidden field to os.system('rm -rf /'), what do you think would happen?
I can't see any need for this at all. You have two form subclasses; why don't you simply put the validation in a method in those subclasses?
class Form1(AbstractForm):
    def validate_modif(self, act):
        return act.validated < 2

class Form(AbstractForm):
    def validate_modif(self, act):
        return act.validated_attendance == 0

and you can simply call self.validate_modif(act) to perform the validation.
Note also that you should not be overriding is_valid(), but clean(). And your Meta class does nothing because this is a normal Form, not a ModelForm.",A,1
22478111,2014-03-18 11:33:57.563000+00:00,"Your code doesn't work because looping over the Counter() object only returns keys, not tuples of (keyword, count) values.
There is no need to use heapq explicitly here, the Counter() object will do that for you when you call the Counter.most_common() method:
actualKeywords = Counter(possibleKeywords).most_common(2)

This gives you a list with two (keyword, count) tuples. If you want just the keywords, use:
actualKeywords = [k for (k, c) in Counter(possibleKeywords).most_common(2)]
",A,1
5681034,2011-04-15 18:45:26.647000+00:00,"
Or is there any better way to do this?

Depends on the application.  Some applications write their PID to a file.",A,0
40628661,2016-11-16 09:43:26.970000+00:00,"To interpret a byte sequence as a text, you have to know the
corresponding character encoding:
unicode_text = bytestring.decode(character_encoding)

Example:
>>> b'\xc2\xb5'.decode('utf-8')
'µ'

ls command may produce output that can't be interpreted as text. File names
on Unix may be any sequence of bytes except slash b'/' and zero
b'\0':
>>> open(bytes(range(0x100)).translate(None, b'\0/'), 'w').close()

Trying to decode such byte soup using utf-8 encoding raises UnicodeDecodeError.
It can be worse. The decoding may fail silently and produce mojibake
if you use a wrong incompatible encoding:
>>> '—'.encode('utf-8').decode('cp1252')
'â€”'

The data is corrupted but your program remains unaware that a failure
has occurred.
In general, what character encoding to use is not embedded in the byte sequence itself. You have to communicate this info out-of-band. Some outcomes are more likely than others and therefore chardet module exists that can guess the character encoding. A single Python script may use multiple character encodings in different places.

ls output can be converted to a Python string using os.fsdecode()
function that succeeds even for undecodable
filenames (it uses
sys.getfilesystemencoding() and surrogateescape error handler on
Unix):
import os
import subprocess

output = os.fsdecode(subprocess.check_output('ls'))

To get the original bytes, you could use os.fsencode().
If you pass universal_newlines=True parameter then subprocess uses
locale.getpreferredencoding(False) to decode bytes e.g., it can be
cp1252 on Windows.
To decode the byte stream on-the-fly,
io.TextIOWrapper()
could be used: example.
Different commands may use different character encodings for their
output e.g., dir internal command (cmd) may use cp437. To decode its
output, you could pass the encoding explicitly (Python 3.6+):
output = subprocess.check_output('dir', shell=True, encoding='cp437')

The filenames may differ from os.listdir() (which uses Windows
Unicode API) e.g., '\xb6' can be substituted with '\x14'—Python's
cp437 codec maps b'\x14' to control character U+0014 instead of
U+00B6 (¶). To support filenames with arbitrary Unicode characters, see  Decode poweshell output possibly containing non-ascii unicode characters into a python string",A,11
20543789,2013-12-12 12:37:14.750000+00:00,"Well, this is one way of running Python scripts, but not a very good way. There are plenty of better ways of running Python scripts on a webserver, of which the best is a (micro) framework running via WSGI.
To answer your question though, you would use an Alias directive in your Apache configuration to map your URL to your script location. Note also that there's nothing magical about the cgi-bin directory: any directory can be used for executable scripts as long as you set the ExecCGI flag (and an AddHandler) in the Options directive for that directory.",A,0
29297949,2015-03-27 10:01:44.890000+00:00,"The issue has nothing at all to do with your string formatting. The error is coming when you attempt to set items in your dict. That's because you're attempting to set items in a nested dict, without ever creating the inner dictionary: filename[y] does not exist, so you can't do filename[y][i].
You could fix this by creating a dict in the outer loop:
filename = {}
for y in range(0,4):
    filename[y] = {}
    for i in range(0,2):
        filename[y][i] = 'Learning_S%s_P%s.xlsx' % (1+y,1+i)

although I would question whether dicts are the right data structure here: seems like nested lists would be more appropriate:
filename = []
for y in range(0,4):
    inner = []
    for i in range(0,2):
        inner.append('Learning_S%s_P%s.xlsx' % (1+y,1+i))
    filename.append(inner)
",A,3
29673949,2015-04-16 11:55:09.323000+00:00,"If you need to handle duplicates, and it's not acceptable to reshuffle the list every time, there's a simple solution: Instead of just randomly picking a word, randomly pick an index. Like this:
index = random.randrange(len(wordlist))
word = wordlist.pop(index)
with open(""words.txt"", 'w') as f:
    f.write('\n'.join(new_wordlist))

Or, alternatively, use enumerate to pick both at once:
word, index = random.choice(enumerate(wordlist))
del wordlist[index]
with open(""words.txt"", 'w') as f:
    f.write('\n'.join(new_wordlist))
",A,1
15188006,2013-03-03 16:48:14.763000+00:00,"You can't do it that way. chapter_set is a query set, it doesn't have an attribute page_set.
Instead, turn it around:
Page.objects.filter(chapter__book=my_book)
",A,4
14861195,2013-02-13 19:13:49.937000+00:00,"As the docs say:

Operators in the same box group left to right (except for comparisons… and exponentiation, which groups from right to left).

In other words, ** is right-associative, while // (like all other operators except comparisons) is left-associative.
Elsewhere, there's a whole section on The power operator that, after giving a rule (which isn't relevant here) about how power and unary operators interacts, clarifies that:

[I]n an unparenthesized sequence of power and unary operators, the operators are evaluated from right to left…

This is actually the way most programming languages do it.
Exponentiation isn't written with symmetrical operator syntax in mathematics, so there's really no reason it should have the same default associativity. And right-associative exponentiation is much less useful, because (2**3)**4 is exactly the same thing as 2**(3*4), whereas there's nothing obvious that's the same thing as 2**(3**4).",A,3
18241888,2013-08-14 20:41:15.207000+00:00,"You need to retrieve the y, m and d values from the form object first:
y = int(form['year'].value)
m = int(form['month'].value)
d = int(form['day'].value)

assuming that the HTML form that submits to this script has year, month and day fields. Looking at http://www2.cs.sfu.ca/CourseCentral/165/ggbaker/1134-dist/lab/files/lab10-dateform.html, your original form does use exactly those fields. GET and POST provided data are always strings, but the calendar.weekday() function wants you to pass in integer values, so here they are parsed into integers with the int() function. 
Then, you are assigning the output of the calendar.weekday() function to the name weekday, but are not using that at all in the rest of the code. Perhaps you need to re-examine what all those if statements should be testing against.",A,2
28013958,2015-01-18 19:38:57.690000+00:00,"The code you link to is counting on the sequences using integers. The integers can then readily be transformed to indexes into the transformation matrix (1 is translated to index 0, etc.).
The algorithm you linked to also only works for unique elements, the matrix built there is 3 by 3, not 10 by 10.
You'd have to do the same for your input list:
from collections import Counter, defaultdict
from itertools import count

def tmatrix(self, lst):
    # defaultdict that'll produce a unique index for each unique character
    # encountered in lst
    indices = defaultdict(count().next)
    unique_count = len(set(lst))
    b = [[0 for _ in xrange(unique_count)] for _ in xrange(unique_count)]
    for (x, y), c in Counter(zip(lst, lst[1:])).iteritems():
        b[indices[x]][indices[y]] = c
    return b

Here the indices dictionary maps characters back to indices in the input list; an itertools.count() instance provides an auto-incrementing integer value for any character not already in the dictionary.
This produces a 29 by 29 matrix for your input sample:
>>> tmatrix(None, sample)
[[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 2, 5, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0],
 [0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 2, 1, 0, 5, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
 [0, 0, 1, 0, 0, 1, 0, 6, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [1, 3, 0, 3, 6, 4, 0, 2, 4, 2, 2, 1, 1, 2, 3, 0, 0, 3, 4, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1],
 [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 1, 0, 3, 2, 2, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 2, 2, 0, 1, 7, 0, 0, 0, 3, 0, 3, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0],
 [0, 1, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 6, 2, 2, 0, 0, 11, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 3, 0, 0, 0, 4, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],
 [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]

You probably want to return the indices mapping too, so you know what character mapped to what index in that matrix.",A,1
29220378,2015-03-23 21:04:42.410000+00:00,"s is None:
>>> import unicodedata
>>> unicodedata.normalize('NFD', None)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: must be unicode, not None

Check what stt.stt() returns, or handle the None case.
The generator expression parentheses are optional, but when using str.join() it is actually faster to use a list comprehension here (the str.join() code has to convert the input to a list as it'll need to traverse the list twice):
def remove_tildes(s):
   # Remove spanish tildes so there won't be errors with ascii
   return ''.join([
       c for c in unicodedata.normalize('NFD', s or u'')
       if unicodedata.category(c) != 'Mn'])

where s or u'' handles the case where s is None by replacing it with the empty string.",A,0
39973525,2016-10-11 08:46:53.493000+00:00,"The end point in a range() is not included. Since sqrt(10) is 3.1623, your range() loops to 2 and no further, and the multiples of 3 are not removed from your list. Your code works for 100, because it doesn't matter if you test for multiples 10 (those are already covered by 2 and 5).
The same issue applies to your other loops; if you want to include n itself as a candidate prime number you should also include it in the other ranges.
Note that you also want to ignore 0 and 1, those are not primes. You could add A[0] = A[1] = False at the top to make sure your last loop doesn't include those, or start your last loop at 2 rather than 0.
You want to add one to the floored square root to make sure it is tested for:
for i in range(2, int(sqrt(n)) + 1):

I'd use booleans rather than 0 and 1, by the way, just for clarity (there is not much of a performance or memory footprint difference here):
def prime_numbers(n):
    sieve = [True] * (n + 1)  # create a list n elements long
    for i in range(2, int(sqrt(n)) + 1):
        if sieve[i]:
            for j in range(i * 2, n + 1, i):
                sieve[j] = False
    for i in range(2, n + 1):
        if sieve[i]:
            print(i)

I used [..] * (n + 1) to create a list of n items (plus 0); this produces a list with n shallow copies of the contents of the left operand. That's faster than a list comprehension, and the shared references are fine since True is a singleton in Python.
Demo:
>>> prime_numbers(31)
2
3
5
7
11
13
17
19
23
29
31

Note that 31 is included there; your code would have resulted in incorrect output as you'd have left in all the multiples of 5.",A,2
124896,2008-09-24 01:31:09.057000+00:00,"SOAP and XML -- ""all that XML surround so little functionality makes me wonder if it's worth the trouble.""
Totally.  SOAP is heavy-weight, and -- to a large extent -- a workaround to the need for static binding throughout the Java technology stack.
REST, on the other hand, is much lighter weight.  Further, REST with JSON or REST with YAML is very lightweight, and very easy to implement.  It builds right on top of the off-the shelf HTTP protocol.
REST requires you to define resources (named via URI's), and transactions based on the canonical CRUD rules (GET, POST, PUT and DELETE).  Very simple and canonical.",A,0
41197630,2016-12-17 10:31:08.307000+00:00,"
Using @staticmethod is obviously not an option, since no function is defined on class definition/customization, and decorations dont apply to attributes.

No, staticmethod is the option, just call it directly to produce an instance:
class A:
    ff = staticmethod(f)

@decorator syntax is only syntactic sugar to produce the exact same assignment after a function object has been created.
This works fine:
>>> def f(x):
...     return x**2
...
>>> class A:
...     f_unchanged = f
...     f_static = staticmethod(f)
...
>>> A().f_unchanged
<bound method f of <__main__.A object at 0x10cf7b2e8>>
>>> A().f_static
<function f at 0x10cfb6510>
>>> A().f_static(4)
16

It doesn't matter where a function is defined, a def statement produces a function object regardless where it is used. def name is two things: creating the function object and an assignment of that function object no a name. Wether or not this takes place in a class statement or elsewhere doesn't actually matter.
What turns functions into bound methods is accessing them on an instance, as then the descriptor protocol kicks in. For example,  accessing A().ff is turned into A.__dict__['ff'].__get__(A()), and it is the __get__ method on a function that produces the bound method. The bound method is only a proxy for the actual function, passing in the instance as a first argument when called.
A staticmethod defines a different __get__, one that just returns the original function, unbound. You can play with those __get__ methods directly:
>>> f.__get__(A())   # bind f to an instance
<bound method f of <__main__.A object at 0x10cf9f630>>
>>> A.__dict__['f_unchanged']  # bypass the protocol
<function f at 0x10cfb6510>
>>> A.__dict__['f_static']     # bypass the protocol
<staticmethod object at 0x10cf60f28>
>>> A.__dict__['f_static'].__get__(A())  # activate the protocol
<function f at 0x10cfb6510>
",A,1
27875488,2015-01-10 11:04:04.753000+00:00,"Major is a class. The class itself doesn't have that attribute, only the instances do. For some reason, you're getting or creating an instance, ignoring it, then trying to use a non-existent class attribute. Instead you should actually keep and use the instance you got.
major_instance, created = Major.objects.get_or_create(
     major_name=self.cleaned_data.get('other_major')
  )
self.cleaned_data['other_major'] = major_instance.major_name
",A,0
185402,2008-10-08 23:51:06.207000+00:00,Be intimate with the OWASP threats and know precisely how to counter them in your application and framework.  It's hard to believe how many people embrace goofy half-solutions to SQL Injection and cross-site Scripting attacks.,A,0
37391281,2016-05-23 12:40:01.220000+00:00,"I guess the author of the slide didn't actually test the code.
The code you produced is trying to use map() to call s as a function:
map(s,s[3:]+ s[:3])
#   ^ This must be a *callable object*, like a function

If you wanted to create a dictionary mapping ASCII letters to a letter 3 spots along in the alphabet, use the zip() function instead:
d = dict(zip(s, s[3:] + s[:3]))

zip() then pairs up each element in s with each element in s[3:] + s[:3], and those (letter, letter + 3) pairs are then passed to dict() to form key-value pairs:
>>> s = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""
>>> dict(zip(s, s[3:] + s[:3]))
{'A': 'D', 'I': 'L', 'L': 'O', 'X': 'A', 'N': 'Q', 'D': 'G', 'S': 'V', 'B': 'E', 'V': 'Y', 'C': 'F', 'K': 'N', 'W': 'Z', 'R': 'U', 'O': 'R', 'T': 'W', 'P': 'S', 'F': 'I', 'J': 'M', 'M': 'P', 'E': 'H', 'Q': 'T', 'H': 'K', 'G': 'J', 'Z': 'C', 'U': 'X', 'Y': 'B'}

Next, your last line will completely fail to do any encryption, because your map only handles uppercase letters, but you lowercased your input. Either produce a lowercase map, or lowercase your input.
Lowercasing the map could look like this:
def encrypt_caesar(plaintext):
    s = ""abcdefghijklmnopqrstuvwxyz""
    d = dict(zip(s, s[3:] + s[:3]))
    return ''.join(map(lambda l: d.get(l, l), plaintext.lower()))

or you could just use the string.ascii_lowercase constant:
from string import ascii_lowercase

def encrypt_caesar(plaintext):
    d = dict(zip(ascii_lowercase, ascii_lowercase[3:] + ascii_lowercase[:3]))
    return ''.join(map(lambda l: d.get(l,l), plaintext.lower()))

Using this method is rather slow, however. For blazingly-fast 'encryption', use the str.translate() method; the input map for that is best produced with str.maketrans:
from string import ascii_lowercase as alc

def encrypt_caesar(plaintext, shift=3):
    map = str.maketrans(alc, alc[shift:] + alc[:shift])
    return plaintext.lower().translate(map)

I added a shift parameter to define how much of an alphabet shift should be applied.
I'll leave handling both lowercase and uppercase letters as an exercise to the reader!",A,0
14619150,2013-01-31 05:33:47.730000+00:00,"Using numpy:
import numpy as np

a = np.loadtxt(""data.csv"", delimiter="","")
mean = np.mean(a, axis=0)
print(mean)

Using csv module:
import csv
import sys

it = csv.reader(sys.stdin, quoting=csv.QUOTE_NONNUMERIC)
avg = next(it, [])
count = 1
for count, row in enumerate(it, start=2):
     for i, value in enumerate(row):
          avg[i] += value
avg = [a/count for a in avg]
print(avg)

Output
[0.0, 431655407.0, 9492692.6, 2106081.8, 4434137.0, 3128214.6]
",A,1
20302807,2013-11-30 16:43:01.633000+00:00,"Python module names have to be valid identifiers. ""8"" is not a valid identifier. Try ""eight"" or ""file8"" or anything that does not start with a number.",A,5
156851,2008-10-01 09:23:00.990000+00:00,"Here is a alternative solution in Python 3.0 which allows non-string list items:
>>> alist = ['a', 1, (2, 'b')]


a standard way
>>> "", "".join(map(str, alist))
""a, 1, (2, 'b')""

the alternative solution
>>> import io
>>> s = io.StringIO()
>>> print(*alist, file=s, sep=', ', end='')
>>> s.getvalue()
""a, 1, (2, 'b')""


NOTE: The space after comma is intentional.",A,12
24287978,2014-06-18 14:16:58.947000+00:00,"You could parse the dates into datetime objects:
from datetime import datetime

sorted(lst, key=lambda d: datetime.strptime(d, '%b %d'))

The parsed datetime objects are then used to determine the ordering.
Demo:
>>> from datetime import datetime
>>> lst = ['Jan 01', 'Feb 02', 'Mar 05', 'Feb 04', 'Jan 05']
>>> sorted(lst, key=lambda d: datetime.strptime(d, '%b %d'))
['Jan 01', 'Jan 05', 'Feb 02', 'Feb 04', 'Mar 05']
",A,6
15663467,2013-03-27 16:01:40.320000+00:00,"You have supplied a name for that URL, so you should use that in the call to reverse:
url = reverse('FilterDisplay', kwargs={'continent': 'America',})
",A,2
20078815,2013-11-19 18:08:58.010000+00:00,"That's because instances of B are not a simple type. Because you gave B a __repr__ method, the instance is printed as it's JSON representation, but it is not itself a supported JSON type.
Remove the __repr__ method and the traceback is much less confusing:
>>> class A(object):
...     def __init__(self):
...         self.b_list = []
... 
>>> class B(object):
...     def __init__(self):
...         self.x = 'X'
...         self.y = 'Y'
... 
>>> a = A()
>>> a.b_list.append(B())
>>> 
>>> print dumps(a.__dict__)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python2.7/json/__init__.py"", line 243, in dumps
    return _default_encoder.encode(obj)
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/Users/mj/Development/Library/buildout.python/parts/opt/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <__main__.B object at 0x10a753e10> is not JSON serializable

Use the default keyword argument to encode custom objects:
def encode_b(obj):
    if isinstance(obj, B):
        return obj.__dict__
    return obj

json.dumps(a, default=encode_b)

Demo:
>>> def encode_b(obj):
...     if isinstance(obj, B):
...         return obj.__dict__
...     return obj
... 
>>> dumps(a.__dict__, default=encode_b)
'{""b_list"": [{""y"": ""Y"", ""x"": ""X""}]}'
",A,6
15124774,2013-02-27 23:42:04.770000+00:00,"Since your question is very vague, here's a somewhat vague answer:
First, think about whether you really need to do this at all. Why can't the first script just import the second script as a module and call some function on it?
But let's assume you've got a good answer for that, and you really do need to ""close"" and run the other script, where by ""close"" you mean ""make your GUI invisible"".
def handle_button_click(button):
    button.parent_window().hide()
    subprocess.call([sys.executable, '/path/to/other/script.py'])
    button.parent_window().show()

This will hide the window, run the other script, then show the window again when the other script is finished. It's generally a very bad idea to do something slow and blocking in the middle of an event handler, but in this case, because we're hiding our whole UI anyway, you can get away with it.
A smarter solution would involve some kind of signal that either the second script sends, or that a watcher thread sends. For example:
def run_other_script_with_gui_hidden(window):
    gui_library.do_on_main_thread(window.hide)
    subprocess.call([sys.executable, '/path/to/other/script.py'])
    gui_library.do_on_main_thread(window.show)

def handle_button_click(button):
    t = threading.Thread(target=run_other_script_with_gui_hidden)
    t.daemon = True
    t.start()

Obviously you have to replace things like button.window(), window.hide(), gui_library.do_on_main_thread, etc. with the appropriate code for your chosen window library.
If you'd prefer to have the first script actually exit, and the second script re-launch it, you can do that, but it's tricky. You don't want to launch the second script as a child process, but as a sibling. Ideally, you want it to just take over your own process. Except that you need to shut down your GUI before doing that, unless your OS will do that automatically (basically, Windows will, Unix will not). Look at the os.exec family, but you'll really need to understand how these things work in Unix to do it right. Unless you want the two scripts to be tightly coupled together, you probably want to pass the second script, on the command line, the exact right arguments to re-launch the first one (basically, pass it your whole sys.argv after any other parameters).
As an alternative, you can use execfile to run the second script within your existing interpreter instance, and then have the second script execfile you back. This has similar, but not identical, issues to the exec solution.",A,3
11925595,2012-08-12 20:22:10.657000+00:00,"Did you find the Celery periodic tasks documentation?

You'll have to use the identifier of your method to register a scheduler entry. If your execute_command task lives in a module named foobar, the task value of the CELERYBEAT_SCHEDULE structure should be foobar.execute_command.
Celery will import the task for you, provided that import foobar.execute_command would work.
Check the celery.schedule.crontab API; the following should execute on weekdays at 07:30 am:
crontab(minute=30, hour=7, day_of_week=’mon-fri’)

Remember that this task is going to be performed asynchronously. You cannot query for a database object when you schedule this task and expect it to be there still when the task is called.
Thus, you should only pass in python values that remain constant, and have your task connect to the database and look things up based on the arguments you pass in.
If this task is only ever going to execute tasks for one specific user, then by all means pass in the identifier for that user (user id, email, whatever you can use to look up the user from the database).
",A,4
19610167,2013-10-26 18:13:17.550000+00:00,"The csv module gives you as string; simply split than column on the dash with str.split(), and map to int():
val1, val2 = map(int, row[2].split('-', 1))

Now you have two integer values to calculate with:
>>> example = '8-10'
>>> map(int, example.split('-', 1))
[8, 10]
",A,1
25879027,2014-09-16 21:53:10.047000+00:00,"This is a bit of a strange thing, but it looks like you want to handle any Python integer literal format as if it were an int. The best way to do that is probably to call ast.literal_eval to parse it, then see if it parsed to an int. Like this:
def intify(x):
    if isinstance(x, int):
        return x
    parsed = ast.literal_eval(x)
    if isinstance(parsed, int):
        return parsed
    raise ValueError('{} is not an int literal'.format(x))

Note that you might want to instead test isinstance(x, numbers.Integral) and if so return int(x), or even just return int(x) whenever it succeeds, or something different, depending on what you want to do with things that are int-like and int-convertible but not actually int, for some appropriate definition of int-like.
Now, you just do this:
try:
    return tuple(intify(x) for x in iterable)
except ValueError:
    return None

And if you want to do that for a whole iterable of iterables, you need a nested loop, like this:
try:
    return [tuple(intify(x) for x in subiterable) for subiterable in iterable]
except ValueError:
    return None


If you want to do this lazily, yielding the elements without ever building a list, there's no way to ""bail out early"" and return None—once you've already yielded some values, you can't unyield them. So, what you want isn't directly possible. But if you could explain why you want it, something similarly useful might be actually doable. For example:
yield from (tuple(intify(x) for x in subiterable) for subiterable in iterable)

Or, if you don't have Python 3.3:
for subiterable in iterable:
    yield tuple(intify(x) for x in subiterable)

This will raise an exception on the first bad value. So, if you're just using the iterator to pass to a call to list, the partially-built list will be abandoned to handle the exception.
Then again, if the only thing you're doing with this function is building a list, there's really no reason to use a generator in the first place; just return a list.",A,4
24118751,2014-06-09 10:52:58.950000+00:00,"You have a UTF-8 encoded U+200E LEFT-TO-RIGHT MARK in your docstring:
'\n    A company manages owns one of more stores.\xe2\x80\x8e\n    '

Either remove that codepoint (and try to use a code editor, not a word processor) from your code, or just put the PEP-263 encoding comment at the top of the file:
# encoding=utf8

Python 3 uses UTF-8 by default, Python 2 defaults to ASCII for source code unless you add that comment.",A,9
23246526,2014-04-23 13:54:26.037000+00:00,"You have a class attribute, so NewType.__dict__ would work.
Alternative routes would be:
type(n).__dict__
vars(NewType)
vars(type(n))

Demo:
>>> NewType=type('nt',(object,),{'x':'hello'})
>>> n=NewType()
>>> n.x
'hello'
>>> NewType.__dict__
dict_proxy({'__dict__': <attribute '__dict__' of 'nt' objects>, 'x': 'hello', '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'nt' objects>, '__doc__': None})
>>> type(n).__dict__
dict_proxy({'__dict__': <attribute '__dict__' of 'nt' objects>, 'x': 'hello', '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'nt' objects>, '__doc__': None})
>>> vars(NewType)
dict_proxy({'__dict__': <attribute '__dict__' of 'nt' objects>, 'x': 'hello', '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'nt' objects>, '__doc__': None})
>>> vars(type(n))
dict_proxy({'__dict__': <attribute '__dict__' of 'nt' objects>, 'x': 'hello', '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'nt' objects>, '__doc__': None})

The class dictionary has a few more attributes in it (including __dict__ itself).",A,5
6903909,2011-08-01 20:14:16.270000+00:00,"
I don't see how pyramid helps me accomplish what I can do in PHP, and if it does do this, why so complicated?


A web framework like Pyramid is not a web framework like PHP.  They're different.
Complexity is a matter of ""experience"".  If you're experienced with PHP, Python seems complex.  If your experienced with RoR, PHP seems complex.  Everything that's new seems complex.
Python has a dozen or so web frameworks of varying capabilities.  None of them will look like PHP.  Zero.  They'll all be different (and appear complex).
Consequently, if you don't like one.  Move on.  There are a lot of choices.  Keep trying different ones.  
Ask specific questions.  ""I don't see how pyramid helps me accomplish what I can do in PHP"" is too vague to discuss further.  If there's a specific thing you want to know about, search for that specific question (it's probably already been asked).   If you don't find anything, ask the specific question.  Code samples help.
",A,5
35101311,2016-01-30 11:39:34.643000+00:00,"If sys.stdout.isatty() is false (the output is redirected to a file/pipe) then configure PYTHONIOENCODING envvar outside your script.
Always print Unicode, don't hardcode the character encoding of your environment inside your script:
$ PYTHONIOENCODING=utf-8 python simple.py | grep pattern
",A,4
51244535,2018-07-09 11:30:45.187000+00:00,"The @property decorator on a function creates a new property object and sets the decorated function as the getter. So within the same class, the @p.getter decorator is not very useful, no.
However, the decorator is very useful if you wanted to override the getter in a subclass, or a reuse just the setter of the property on, but on a different class altogether:
class Foo(object):
    @property
    def p(self):
        return self._p

    @p.setter
    def p(self, value):
        self._p = value

class Bar(Foo):
    @Foo.p.getter
    def p(self):
        return self._p + 10

or by copying the property object across to a different class:
class NotASubclass(object):
    @Foo.p.getter
    def p(self):
        return self._p * 10

Now Bar()and NotASubclass have a different getter for their property p, but both reuse the setter defined on the property for Foo().
In both cases the @p.getter decorator creates a new property instance to store as an attribute of the class object, but their fset attributes all point to the same, single function object that acts as the setter.  This is why it is enough to just reference @Foo.p.getter in either case; you only need the single property instance in the class namespace.
Also see How does the @property decorator work? and Python overriding getter without setter",A,3
22023756,2014-02-25 19:04:57.810000+00:00,"You never call the function. You do need to run the function to get the output:
print word_identify(words)
",A,5
13942715,2012-12-18 22:44:19.220000+00:00,"Using the bisect module of course:
>>> import bisect
>>> lst = [207, 357, 470, 497, 537]
>>> bisect.bisect_left(lst, 0)
0
>>> bisect.bisect_left(lst, 360)
2

The module uses binary search, which requires a sorted sequence. With such a sequence you can divide the sequence in half by picking an index mid-way between the first and last, to see if the value you need is in either half. You then continue dividing the selected half until you found a matching insertion point. That lets you find the insertion point in O(log N) time for a sequence of length N, i.e. very fast.",A,18
35426253,2016-02-16 07:24:01.330000+00:00,"You are sending perfectly formed JSON. Python's json module will, by default, use JSON \uhhhh escapes for non-ASCII codes to avoid any transfer issues even:
>>> print json.dumps({'message': u'data:text/html,\xe9\xe9\xe9<br><br>\xe0\xe0\xe0<br>'})
{""message"": ""data:text/html,\u00e9\u00e9\u00e9<br><br>\u00e0\u00e0\u00e0<br>""}

It is the receiving end that is the problem here, you have what is called a Mojibake, an incorrect re-coding of the data. I can reproduce that by encoding to UTF-8, then decoding as Latin-1 or Windows codepage 1252, for example:
>>> print u'data:text/html,\xe9\xe9\xe9<br><br>\xe0\xe0\xe0<br>'.encode('utf8').decode('latin1')
data:text/html,Ã©Ã©Ã©<br><br>Ã Ã Ã <br>

There is nothing to be done on the Python end, fix the receiving end instead.",A,2
38996764,2016-08-17 12:32:04.360000+00:00,"_com_interfaces_ is an optional attribute a Server Policy looks for:

_com_interfaces_
Optional list of IIDs exposed by this object. If this attribute is missing, IID_IDispatch is assumed (ie, if not supplied, the COM object will be created as a normal Automation object.

The list is used to answer QueryInterface enqueries, see the win32com.server.policy module to see how this is being used, specifically the BasicPolicy._wrap() and  BasicPolicy._QueryInterface_ methods.",A,2
31205356,2015-07-03 11:07:54.360000+00:00,"You are updating the wrong globals. You are updating the globals of your zen_basic module, not the __main__ module (the namespace for your script or the interactive interpreter). The globals() function always returns the globals of the module in which the code was defined, not the module that called your function.
You'd have to retrieve the globals of the calling frame. Note that it rarely is advisable to modify the globals of the calling frame, but if you must then you can retrieve the calling frame by using the sys._getframe() function:
import sys

def from_module_import_alias(module_name, var_name, alias):
    """""" equal to from module import a as b """"""
    agent = importlib.import_module(module_name)
    calling_globals = sys._getframe(1).f_globals
    calling_globals[alias] = vars(agent)[var_name]

In your experiments, python -i zen_basic.py runs zen_basic as the main script entry, and thus globals() references the __main__ module.",A,5
33175349,2015-10-16 16:31:45.917000+00:00,"To get bytes as a list of hex strings:
>>> ['%02x' % d for d in bytearray(b'\x1c\xe7\x00\x00\x27\xd4')]
['1c', 'e7', '00', '00', '27', 'd4']

Here's how to read bytes from a file:
with open(filename, 'rb') as file:
    data = file.read()
    hex_list = ['%02x' % d for d in bytearray(data)]

On Python 3, the bytearray() call is optional.",A,1
29966162,2015-04-30 11:09:50.180000+00:00,"First, I think you're on 2.7, so I'll do most of this with 2.7. But it's worth noting that if you're really interested in optimizing your code, the 3.x branch continues to get performance improvements, and the 2.x branch never will. And why are you using CPython instead of PyPy?

Anyway, some further micro-optimizations to try (in addition to the ones in jonrsharpe's answer:

Caching attribute and/or global lookups in local variables (it's called LOAD_FAST for a reason). For example:
def way1a(theList, theDict):
    resultsList = []
    rlappend = resultsList.append
    for listItem in theList:
        if listItem in theDict:
            rlappend(theDict[listItem])
    return resultsList

In [10]: %timeit way1(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 13.2 ms per loop
In [11]: %timeit way1a(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 12.4 ms per loop

But for some operator special methods, like __contains__ and __getitem__, that may not be worth doing. Of course you won't know until you try:
def way1b(theList, theDict):
    resultsList = []
    rlappend = resultsList.append
    tdin = theDict.__contains__
    tdgi = theDict.__getitem__
    for listItem in theList:
        if tdin(listItem):
            rlappend(tdgi(listItem))
    return resultsList

In [14]: %timeit way1b(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 12.8 ms per loop

Meanwhile, Jon's way6 answer already optimizes out the resultList.append entirely by using a listcomp, and we just saw that optimizing out the lookups he does have probably won't help. Especially in 3.x, where the comprehension is going to be compiled into a function of its own, but even in 2.7 I wouldn't expect any benefit, for the same reasons as in the explicit loop. But let's try just to be sure:
def way6(theList, theDict):
    return [theDict[item] for item in theList if item in theDict]
def way6a(theList, theDict):
    tdin = theDict.__contains__
    tdgi = theDict.__getitem__
    return [tdgi(item) for item in theList if tdin(item)]

In [31]: %timeit way6(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 14.7 ms per loop
In [32]: %timeit way6a(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 13.9 ms per loop

Surprisingly (at least to me), this time it actually helped. Not sure why.
But what I was really setting up for was this: another advantage of turning both the filter expression and the value expression into function calls is that we can use filter and map:
def way6b(theList, theDict):
    tdin = theDict.__contains__
    tdgi = theDict.__getitem__
    return map(tdgi, filter(tdin, theList))
def way6c(theList, theDict):
    tdin = theDict.__contains__
    tdgi = theDict.__getitem__
    return map(tdgi, ifilter(tdin, theList))

In [34]: %timeit way6b(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 10.7 ms per loop
In [35]: %timeit way6c(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 13 ms per loop

But that gain is largely 2.x-specific; 3.x has faster comprehensions, while its list(map(filter(…))) is slower than 2.x's map(filter(…)) or map(ifilter(…)).

You don't need to convert both sides of a set intersection to a set, just the left side; the right side can be any iterable, and a dict is already an iterable of its keys.
But, even better, a dict's key view (dict.keys in 3.x, dict.keyview in 2.7) is already a set-like object, and one backed by the dict's hash table, so you don't need to transform anything. (It doesn't have quite the same interface—it has no intersection method but its & operator takes iterables, unlike set, which has an intersection method that takes iterables but its & only takes sets. That's annoying, but we only care about performance here, right?)
def way3(theList,theDict):
    return list(set(theList).intersection(set(theDict.keys())))
def way3a(theList,theDict):
    return list(set(theList).intersection(theDict))
def way3b(theList,theDict):
    return list(theDict.viewkeys() & theList)

In [20]: %timeit way3(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 23.7 ms per loop
In [20]: %timeit way3a(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 15.5 ms per loop
In [20]: %timeit way3b(listOfRandomInts, dictionaryOfRandomInts)
100 loops, best of 3: 15.7 ms per loop

That last one didn't help (although using Python 3.4 instead of 2.7, it was 10% faster…), but the first one definitely did.
In real life, you may also want to compare the sizes of the two collections to decide which one gets setified, but here that information is static, so there's no point writing the code to test it.

Anyway, my fastest result was the map(filter(…)) on 2.7, by a pretty good margin. On 3.4 (which I didn't show here), Jon's listcomp was fastest (even fixed to return the values rather than the keys), and faster than any of the 2.7 methods. Also, 3.4's fastest set operation (using the key view as a set and the list as an iterable) were a lot closer to the iterative methods than in 2.7.",A,5
26013968,2014-09-24 09:57:49.297000+00:00,"No, this is not related to memcache or cursors, but with eventual consistency. A non-ancestor query is explicitly not guaranteed to return updated data.
See the documentation on structuring data for strong consistency.",A,4
310252,2008-11-21 21:27:20.083000+00:00,"Downloaded both myself.  Like Komodo better.  
Komodo Pros:  Like it better.  Does more.  Looks like an IDE.  Edits Django templates
Notepad++ Cons: Don't like it as much.  Does less.  Looks less like and IDE.",A,-4
9885191,2012-03-27 07:35:38.997000+00:00,"I might be missing something, but why not just call the cleanup function on the final result of the recursive function? eg
result = cleanup(recursive())
",A,0
37030880,2016-05-04 14:34:45.070000+00:00,"You need another table. This is a many-to-many relationship, and as such needs a through table with foreign keys to both Movies and Categories. Then you can insert one row for each pair of (movie, category): so:
Dog, 1
Cat, 1
Cat, 2
Ant, 0
Ant, 2
etc.

(Also, it's good practice to have a primary key for each table: for movies, you might want to use an autoincrement integer field rather than the title.)",A,1
14489370,2013-01-23 21:05:28.223000+00:00,"You can use the same principle, by using the first three elements as a key, and using int as the default value factory for the defaultdict (so you get 0 as the initial value):
from collections import defaultdict

a_list = [['apple', 50, 60, 7],
          ['orange', 70, 50, 8],
          ['apple', 50, 60, 12]]

d = defaultdict(int)
for sub_list in a_list:
    key = tuple(sub_list[:3])
    d[key] += sub_list[-1]

new_data = [list(k) + [v] for k, v in d.iteritems()]

If you are using Python 3, you can simplify this to:
d = defaultdict(int)
for *key, v in a_list:
    d[tuple(key)] += v

new_data = [list(k) + [v] for k, v in d.items()]

because you can use a starred target to take all 'remaining' values from a list, so each sublist is assigned mostly to key and the last value is assigned to v, making the loop just that little simpler (and there is no .iteritems() method on a dict in Python 3, because .items() is an iterator already).
So, we use a defaultdict that uses 0 as the default value, then for each key generated from the first 3 values (as a tuple so you can use it as a dictionary key) sum the last value.

So for the first item ['apple', 50, 60, 7] we create a key ('apple', 50, 60), look that up in d (where it doesn't exist, but defaultdict will then use int() to create a new value of 0), and add the 7 from that first item.
Do the same for the ('orange', 70, 50) key and value 8.
for the 3rd item we get the ('apple', 50, 60) key again and add 12 to the pre-existing 7 in d[('apple', 50, 60)]. for a total of 19.

Then we turn the (key, value) pairs back into lists and you are done. This results in:
>>> new_data
[['apple', 50, 60, 19], ['orange', 70, 50, 8]]

An alternative implementation that requires sorting the data uses itertools.groupby:
from itertools import groupby
from operator import itemgetter

a_list = [['apple', 50, 60, 7],
          ['orange', 70, 50, 8],
          ['apple', 50, 60, 12]]

newlist = [list(key) + [sum(i[-1] for i in sublists)] 
    for key, sublists in groupby(sorted(a_list), key=itemgetter(0, 1, 2))]

for the same output. This is going to be slower if your data isn't sorted, but it's good to know of different approaches.",A,3
13922368,2012-12-17 21:19:55.047000+00:00,"It looks like you're decoding the entire thing, including the begin-base64 644 data.xml.gz part, so you're getting a bunch of garbage at the start:
b1 = '''begin-base64 644 data.xml.gz\nH4sIAAAAAAAAA y9a4 lx3Hn
d6fguB7JzNuGZkNigNfdrAGbMAYaXeNfbPolXplYiRSIFu'''

b2 = '''\nH4sIAAAAAAAAA y9a4 lx3Hn
d6fguB7JzNuGZkNigNfdrAGbMAYaXeNfbPolXplYiRSIFu'''

If you run your algorithm on b2, you get something starting with this:
m\xe8""\x9d\xb6\xac{\xae

(I don't know how you lost the m in copying and pasting, but either way, it's not valid.)
If you run it on b2, you get something starting with this:
\x1f\x8b\x08\x00\x00\x00

That's what you want.
Of course taking off the '\n' has the same effect, since base64 ignores whitespace. So most likely, it's being used as a delimiter. If that's actually a '\\n' (aka r'\n') rathern than a '\n', you have to remove it to get the right answer.
Also, you seem to be doing a lot of extra work for no good reason. Most likely the data is actually correctly padded, but that part may be worthwhile. But the whole translate(dict(zip(map(ord, u'-_'), u'+/'))) does the same thing as passing an altchars argument to b64decode, but less efficiently and harder to read (if it's correct). (By the way, if you were doing translate as an optimization against the cost of calling replace twice, the conversion to and from Unicode is almost certain to overwhelm the savings. Even if you had profiled and determined that it made a difference, you'd probably want to generate the translate map above—both for efficiency, so you don't do it once per string, and, more importantly, for readability.)
Putting it together:
data = '''begin-base64 644 data.xml.gz\nH4sIAAAAAAAAA y9a4 lx3Hn
d6fguB7JzNuGZkNigNfdrAGbMAYaXeNfbPolXplYiRSIFu'''
_, data = data.split('\n', 1)
padding_factor = (4 - len(data) % 4) % 4
data += ""=""*padding_factor
data_decoded = base64.b64decode(data, '-_')

Again, if you've got a '\\n' rather than a '\n', change the split line accordingly.",A,0
18923501,2013-09-20 18:29:34.260000+00:00,"You stumbled on a bug in Django 1.5: #19954 Storing of Binary fields leads to Exceptions.
The committed fix was to force decode the last executed query with 'replace':
return force_text(cursor._last_executed, errors='replace')

You can either edit your Django source code, or apply a monkeypatch:
from django.utils.encoding import force_text
from django.db.backends.mysql.base import DatabaseOperations

def fixed_last_executed_query(self, cursor, sql, params):
    return force_text(cursor._last_executed, errors='replace')

DatabaseOperations.last_executed_query = fixed_last_executed_query

Do so as early as possible, at the top of your settings module or package, for example.
I don't think the patch was backported; it so far only made it into the 1.6 development line, possibly because 1.6 added a new Binary field type.",A,5
2223772,2010-02-08 18:10:09.183000+00:00,"Part of this ""disconnect"" is because OWL describes an open world ontology.  An Ontology has little or nothing to do with a program, other than a program can manipulate an ontology.  
Trying to relate OWL concepts to programming languages is like trying to relate A Pianist and A Piano Sonata.  
The sonata doesn't really have a concrete manifestion until someone is playing it -- ideally a Pianist, but not necessarily.  Until it's being played, it's just potential relationships among notes manifested as sounds.  When it's being played, some of actual relationships will be relevant to you, the listener.  Some won't be relevant to the listener.",A,2
4149610,2010-11-10 22:09:24.920000+00:00,"You can add your views directly to the AdminSite object, rather than to any particular ModelAdmin subclass which you then register. 
The default AdminSite is accessed via django.contrib.admin.site, which is what you call register and autodiscover on. Instead of using this, you could create your own subclass and add your own views to it, and then register your models against that rather than the default one.",A,5
6190013,2011-05-31 15:36:31.507000+00:00,"Best practice is to use jQuery.data(), as this follows the HTML5 standard for such information.",A,1
12809947,2012-10-09 23:38:46.553000+00:00,"You could use csv to store the replacements in a human-editable form in a file:
import csv

with open('replacements.csv', 'rb') as f:
     replacements = list(csv.reader(f))

for old, new in replacements:
    your_string = your_string.replace(old, new)

where replacements.csv:
ten,10s
twy,20s
fy,40s

It avoids unnecessary markup such as "", [] in the json format and allows a delimiter (,) to be present in a string itself unlike the plain text format from @WoLpH's answer.
(live example)",A,0
29957805,2015-04-30 02:05:41.620000+00:00,"It's showing as ""not responding"" because it's not responding. An application on OS X (as opposed to just a plain ""Unix executable""/script, agent, or daemon) has to respond to messages from the operating system.
Normally, you do this by using a Cocoa run loop. PyObjC offers some high-level helpers that make it even simpler, or just lets you access the same Cocoa methods that the Apple docs describe from Python.
Another option is to use a script-wrapper that just runs your script while maintaining a run loop (with or without a GUI) for you.
Finally, do you actually need to be an application in the first place?",A,3
27602658,2014-12-22 12:21:41.490000+00:00,"You are using a comma:
FilteredData = CassandraData[int(startOffsetAsPerPage),int(endOffsetAsPerPage)]
#                                                     ^

A comma there makes it a tuple, not a slice.
Use a colon instead:
FilteredData = CassandraData[int(startOffsetAsPerPage):int(endOffsetAsPerPage)]
#                                                     ^

Note that Python doesn't need to use semicolons at the ends of lines.",A,1
1999998,2010-01-04 14:53:08.677000+00:00,"Use the Django debug toolbar - gives you this on the Templates tab, along a whole range of other useful debugging information.",A,8
46156166,2017-09-11 12:46:54.530000+00:00,"A row in the CSV file is always a list, even for a file with just one column. You don't really need to use a CSV reader when you have just one value per line; just strip the whitespace from each line to create a flat list, and use the standard universal newline support:
with open('Data') as inputfile:
    # create a list of non-empty lines, with whitespace removed
    stripped = (line.strip() for line in inputfile)
    listofEAN = [line for line in stripped if line]

Now it is trivial to make that into groups of a fixed size:
per_ten = [listofEAN[i:i + 10] for i in range(0, len(listofEAN), 10)]
",A,1
42750615,2017-03-12 17:20:34.127000+00:00,"& is not the boolean operator, use and instead. The & operator is a  bitwise operator and has a different precedence from the and operator.
As a result, your test is interpreted like this:
my_age < (18 & my_age) >= 13

which is a very different test indeed.
Use
my_age < 18 and my_age >= 13

or use a chained comparison:
13 <= my_age < 18

Next, when the age is lower than 13, you don't set girls_age to anything. You only print, so there is nothing set to that local and you get your error. Either raise the error or return a dummy value:
def dating_age (my_age):
    if 13 <= my_age < 18:
        girls_age = my_age / 2 + 5
    elif my_age < 13:
        print (""You are ineligible to date"")
        return
    else:
        girls_age = my_age / 2 + 9
    return girls_age

You may as well return the values directly, and exit early when my_age is too low; in this example I'm using an exception and used a conditional expression to simplify the calculation:
def dating_age (my_age):
    if my_age < 13:
        raise ValueError(""You are ineligible to date"")
    return my_age / 2 + (5 if my_age < 18 else 9)

my_age_input = int(input(""Enter your age: ""))
try:
    dating_limit = dating_age(my_age_input)
except ValueError as ex:
    print(ex.args[0])
else:
    print(""I can date girls of {} and higher"".format(dating_limit))
",A,0
13748530,2012-12-06 16:52:16.840000+00:00,"An import is really just an assignment: it sets a name in your current namespace. So, in the third case, you set the name datetime as equal to the datetime class, then immediately reassign it to the datetime module.",A,1
7713976,2011-10-10 14:06:22.560000+00:00,"That's more or less the point of the new TemplateResponse class, which is used by TemplateView  - it allows you to modify items after calling render_to_response and have those modifications show up in the output.
To explicitly disable that, you can just call render() on the result of the render_to_response. 
response = super(FrontpageView, self).render_to_response(content, **response_kwargs)
reponse.render()
if ...

return response

(Also note, you should use super rather than specifying the class name directly.)",A,2
4607368,2011-01-05 17:48:43.637000+00:00,"cron will absolutely  run the script again.  You need to think this through a little more carefully than just ""sleep"" and ""email every 10 minutes.""
You need to write out your use cases.

System sends message and user does something.
System sends message and user does nothing.  Why email the user again?  What does 2 emails do that 1 email didn't do?  Perhaps you should SMS or email someone else.  

How does the user register that something was done?  How will they cancel or stop this cycle of messages?
What if something is found in the log, an email is sent and then (before the sleep finishes) the thing is found again in the log.  Is that a second email?  It is two incidents.  Or is that one email with two incidents?",A,4
21104754,2014-01-14 01:49:54.377000+00:00,"Well, as far as I can tell, futures isn't documented to work on top of threading.Condition, and gevent isn't documented to be able to patch futures safely. So, in theory, someone could write a Python implementation that would break gevent.
But in practice? It's hard to imagine what such an implementation would look like. You obviously need some kind of sync objects to make a Future work. Sure, you could use an Event, Lock, and Rlock instead of a Condition, but that won't cause a problem for gevent. The only way an implementation could plausibly break things would be to go directly to the pthreads/Win32/Java/.NET/whatever sync objects instead of using the wrappers in threading.
How would you deal with that if it happened? Well, futures is implemented in pure Python, and it's pretty simple Python, and there's a fully functional backport that works with 2.5+/3.2+. So, you'd just have to grab that backport and swap out concurrent.futures for futures.
So, if you're doing something wacky like deploying a server that's going to run for 5 years unattended and may have its Python repeatedly upgraded underneath it, maybe I'd install the backport now and use that instead.
Otherwise, I'd just document the assumption (and the workaround in case it's ever broken) in the appropriate place, and then just use the stdlib module.",A,4
46533056,2017-10-02 20:15:52.250000+00:00,"The dataType parameter in the ajax call tells jQuery what type to expect back from the server. You've specified json, but you are not returning JSON from the Flask handler so jQuery throws an error.
Either actually return JSON, or remove that parameter.",A,2
15949631,2013-04-11 13:08:29.150000+00:00,"Remove the print statement, just call MoveTower(n) without printing it's return value:
MoveTower(n)
print str(2**n-1), 'moves total'

All python functions return some value. If you don't return anything explicitly, the default is value is None. Calling print MoveTower(n) prints the default None returned by MoveTower().",A,5
20735336,2013-12-22 23:49:02.280000+00:00,"+ in a query parameter is the escape code for a space. You receive ' 1907727500', with the space.
Use %2B instead, or better still, have JavaScript quote your values properly
var l_sParams = 'number=' + encodeURIComponent(customer_number.value);
",A,7
28714437,2015-02-25 08:34:10.477000+00:00,That is already a Python dict. It is not JSON and there is no need to load it.,A,0
8188617,2011-11-18 20:21:06.770000+00:00,"You seem to have got confused between fields and widgets. You inherit from ModelMultipleChoiceField, which (as the name implies) is a field, not a widget. But render and render_options are methods on widgets, not fields. And you've used your class in the widgets dictionary.
I suspect you do mean to create a widget. You should inherit from a widget class, probably forms.CheckboxSelectMultiple.",A,16
21184655,2014-01-17 11:18:48.503000+00:00,"The first option would almost certainly be easier. Just map a URL to the view that calls the matplotlib code, and return the image data there.
For the second option, you could embed image data in HTML by using a data uri as the source of your image in the template, but this doesn't really seem to be necessary.",A,2
25837646,2014-09-14 20:21:14.470000+00:00,"You're looking in the wrong place for this. Django can't tell which form is submitted unless you give it some information in the POST. The best place to do this is in the submit button, which you can give a name and value to like any other form element.
<form action=""."" method=""POST"">
    {{ login_form.as_p }}
    <input type=""submit"" name=""login"" value=""login"">
</form>

<form action=""."" method=""POST"">
    {{ signup_form.as_p }}
    <input type=""submit"" name=""signup"" value=""signup"">
</form>

And in the view, all you need to do is check which input is in the POST:
if 'login' in request.POST:
    # do something with login_form
else:
    # do something with signup_form
",A,1
34647309,2016-01-07 04:19:26.920000+00:00,"datetime.date represents a day in Gregorian calendar. It is immutable and therefore all values must be known at the instant it is created. You can't omit the day if you use the constructor explicitly.

I have a dataset with not full specified dates

datetime.strptime() provides the default values if necessary:
>>> from datetime import datetime
>>> datetime.strptime('2016-02', '%Y-%m').date()
datetime.date(2016, 2, 1)
",A,1
51220695,2018-07-07 06:46:41.763000+00:00,"This exception means that you're trying to construct a dict from an iterable, and that iterable's first element is not a sequence. As the docs explain, you can construct a dict two ways:

From a mapping, or
From an iterable of key-value pairs

So, if you try to construct it from, say, a set of numbers:
>>> dict({1, 2, 3})
TypeError: cannot convert dictionary update sequence element #0 to a sequence

… it's trying to use the first element as a key-value pair—that is, a sequence of 2 values—but there's no way to interpret the number 1 as a key-value pair, so it raises a TypeError.

Meanwhile, I know absolutely nothing about Plotly streaming but what's on this page, but this code is clearly wrong:
stream1.write({dateTime,tempt,humty})

I can't imagine why you'd want to stream a set.
Plus, the examples all have either a dict, or a string that's a JSON-encoding of a dict.
So, obviously, that API is expecting you to pass it either a dict or something you can feed to the dict constructor. But you're passing it a set. So, it feeds that set to the dict constructor, and gets this exception.
Since I have no idea what you're actually trying to do here, I have no idea what dict you should be sending here. But you definitely should be passing a dict.

Also, even if you fix this, based on the sign_in call, it looks like you're using Plotly Cloud. But, as the same page says:

Streaming is no longer supported in Plotly Cloud.

So, if you're trying to use streaming with Plotly Cloud, then, even if you fix your code to make sense, it's probably still going to fail, just with an error from Plotly rather than a TypeError about passing nonsense.",A,1
36100090,2016-03-19 09:33:02.687000+00:00,"You do have the object, in result.object. You've used a with statement to assign that to post, but then you've ignored that variable completely.",A,0
34467220,2015-12-25 23:10:51.173000+00:00,"Writing, by default, always takes place at the end of the file. Calling file.readline() doesn't change this behaviour, especially since readline() calls can use a buffer to read in larger blocks.
You could override by using file.seek() explicitly to go to the end of a line; you just read the line, you know the length, seek to that point:
x = file.readline()
file.seek(len(x), 0)
file.write(text) #write the text

Note that you cannot insert lines, or easily replace lines. A file is a stream of individual bytes, not lines, so if you write in a line of 10 characters (including the newline) you can only replace that line with 10 other characters. Longer or  shorter lines won't work here; you are just going to replace fewer or more characters in the file and either partially replace a line or overwrite (part of) the next line.",A,5
146862,2008-09-28 22:07:43.983000+00:00,Why not use Mako?  Seriously.  What feature do you require that Mako doesn't have?  Perhaps you can adapt or extend something that already works.,A,1
760709,2009-04-17 14:51:34.357000+00:00,"In your debugging, print the full path.  I think you're walking the thumbs subdirectory after you walk the . directory.
Also. 
class ThumbnailGenerator( object ):

Usually works out better in the long run.
Please do NOT use __ in front of your method names (generate_image_list and create_thumbnail_dir).
Do not use ""%s%s%s"" % (self.image_path, os.sep, thumb_path) to make path names, use os.path.join.",A,3
31732073,2015-07-30 19:06:02.140000+00:00,"The error message shows you what is wrong. You have defined urls under /polls/, so that is the address you should go to.",A,2
1203289,2009-07-29 21:53:40.967000+00:00,"If that's the straight copy and paste from your model, you've misspelled unicode on the Tarifas model.",A,2
26485354,2014-10-21 11:15:34.287000+00:00,"To synchronize the execution of your Python script with the child C process, you could use communication e.g., write a byte to stdout putchar('\0') in C program in the place where you want it to stop and wait while Python does something and try to read back a char getchar() to continue execution and in reverse in the Python script: read a byte, start doing something, and write byte to signal C program to continue:
from subprocess import Popen, PIPE

# start child process, do not block Python
p = Popen(""./prog"", stdin=PIPE, stdout=PIPE, bufsize=0)
while p.poll() is None: # while C program is running
    # block here until C program writes a byte and flushes its stdout buffer
    if not p.stdout.read(1): # EOF
        break
    # .. do something while C program waits
    p.stdin.write(b'\0') # signal C program to continue

To unbuffer stdout in C, call at the very beginning of the C program: 
setvbuf(stdout, NULL,_IONBF,0);

Otherwise Python program won't see any output until you call fflush() explicitly or the corresponding buffer overflows.",A,0
258329,2008-11-03 11:00:13.417000+00:00,"I'd recommend modifying your design to include some status field on A.
The issue is one of ""why?""  Why does A have < 2 B's and why does A have >= 2 B's.  Is it because user's didn't enter something?  Or is because they tried and their input had errors.  Or is it because the < 2 rule doesn't apply in this case.
Using presence or absence of a Foreign Key limits the meaning to -- well -- present or absent.  You don't have any way to represent ""why?""
Also, you have the following option
[ a for a in A.objects.all() if a.b_set.count() < 2 ]

This can be pricey because it does fetch all the A's rather than force the database to do the work.

Edit: From the comment ""would require me to watch for user join / user leaving the pool events"".
You don't ""watch"" anything -- you provide an API which does what you need.  That's the central benefit of the Django model.  Here's one way, with explict methods in the A class.
class A( models.Model ):
    ....
    def addB( self, b ):
        self.b_set.add( b )
        self.changeFlags()
    def removeB( self, b ):
        self.b_set.remove( b )
        self.changeFlags()
    def changeFlags( self ):
        if self.b_set.count() < 2: self.show= NotYet
        else: self.show= ShowNow

You can also define a special Manager for this, and replace the default b_set Manager with your manager that counts references and updates A.",A,3
26569496,2014-10-26 03:25:55.497000+00:00,"If you are expecting just the one row, then use cursor.fetchone() instead of fetchall() and simply index into the one row that that method returns:
cursor.execute(sql)
row = cursor.fetchone()
newID = row[0] + 1

Rather than use an ORDER BY, you can ask the database directly for the maximum value:
sql = """"""SELECT MAX(id) FROM profiles""""""
",A,1
48104949,2018-01-04 23:13:53.030000+00:00,"Just use a queue or stack to track which neighbours to process next; the following function does exactly the same work as your recursive function, iteratively:
from collections import deque

def clusterise(cell, cluster, tile_index, image, n, windows, image_clusters):
    to_process = deque([(cell, tile_index)])
    while to_process:
        cell, tile_index = to_process.pop()
        neighbouring_windows, neighbouring_indices = get_neighbouring_windows(tile_index[0], tile_index[1], n, windows)
        neighbours = get_and_remove_all_neighbours(cell, image, tile_index, neighbouring_windows, neighbouring_indices)
        if neighbours:
            for neighbour, (n_i, n_j) in neighbours:
                add_to_current_cluster(cluster, image_clusters, neighbour, (n_j, n_j))
                to_process.append((neighbour, (n_i, n_j))

So instead of using the Python stack to track what still needs to be processed, we move the varying arguments (cell and tile_index) to a deque stack managed by the function instead, which isn't bound like the Python stack is. You can also use it as a queue (pop from the beginning instead of the end with to_process.popleft()) for a breadth-first processing order. Note that your recursive solution process cells depth-first.
As a side note: yes, you can use a regular Python list as a stack too, but due to the nature of how a list object is grown and shrunk dynamically, for a stack the deque linked-list implementation is more efficient. And it's easier to switch between a stack and a queue this way. See Raymond Hettinger's remarks on deque performance.",A,4
21480693,2014-01-31 13:00:24.480000+00:00,"A filter can certainly return a boolean to be used in an if, and that is by far the best solution here. Extending the User class wouldn't help because you can't pass a parameter to a method in a template anyway.
The filter could be as simple as:
@register.filter
def is_in_group(user, group_name):
    return user.groups.filter(name=group_name).exists()

and you would use it like this:
{% if user|is_in_group:""foo"" %}
",A,5
16844741,2013-05-30 19:50:53.087000+00:00,"The line:
 ""AskClass"": ""A fine name %s. What is your class? "" % self.herodict['Name'],

is executed when you create the class, not when you later print it. When you create the class, self.herodict['Name'] is set to 'Jimmy'.
You'll have to do the interpolation later on, when you actually have a name. Perhaps you need to use callables instead, like lambda objects:
self.herotext = {
    ""Welcome"": lambda self: ""Greetings, hero. What is thine name? "",
    ""AskClass"": lambda self: ""A fine name %s. What is your class? "" % self.herodict['Name'],
}

then call them passing in self later on:
n = raw_input(self.herotext[textkey](self))
",A,1
1189336,2009-07-27 16:54:11.877000+00:00,"Templates are text files in a directory on your server.
The menu is a list of directories.
The specific template to be loaded is named in your render_to_response calls in each view function.
It's wonderfully quite simple and elegant.
You never need to do a template ""swap"".  You can simply have all the templates you ever want in a  template search path.  All can be available to all users at all times.  You just provide directory_name/template_name.  All handled for you.
Don't mess with explicit cookies.  Django already does this for you.  See chapter 12 of the Django Book.
Use the Profiles extension and put the selected template directory name in the user's profile.",A,6
2551963,2010-03-31 10:09:26.803000+00:00,"request.session is a dictionary like any other, so you just use the normal template mechanism for attributes and members:
{{ request.session.name }}

Don't forget to pass the request into the template context, or even better ensure you are using RequestContext and have the request context processor enabled. See the documentation.",A,16
712373,2009-04-03 02:39:00.520000+00:00,No.  http://en.wikipedia.org/wiki/List_of_U.S._states_by_time_zone,A,0
21952826,2014-02-22 10:13:12.653000+00:00,"A regex to get to get the two floats:
re.compile(r'GOOGLE_MAPS_LONGITUDE:\s*(\d+\.\d+),\s*GOOGLE_MAPS_LATITUDE:\s*(\d+\.\d+)')

Demo:
>>> import re
>>> inputtext = '''\
... <script type=""text/javascript"">
...     window.DETAILS_PAGE_MAP_GLOBALS = {
...         GOOGLE_MAPS_LONGITUDE: 55.4159330979,
...         GOOGLE_MAPS_LATITUDE: 25.1549968578
...     };
... </script>
... '''
>>> pattern = re.compile(r'GOOGLE_MAPS_LONGITUDE:\s*(\d+\.\d+),\s*GOOGLE_MAPS_LATITUDE:\s*(\d+\.\d+)')
>>> pattern.search(inputtext).groups()
('55.4159330979', '25.1549968578')

And used in the context of Scrapy selectors:
>>> import re
>>> from scrapy.selector import Selector
>>> inputtext = """"""<script type=""text/javascript"">
...     window.DETAILS_PAGE_MAP_GLOBALS = {
...         GOOGLE_MAPS_LONGITUDE: 55.4159330979,
...         GOOGLE_MAPS_LATITUDE: 25.1549968578
...     };
... </script>""""""
>>> selector = Selector(text=inputtext, type=""html"")
>>> regexp = re.compile(r'GOOGLE_MAPS_LONGITUDE:\s*(\d+\.\d+),\s*GOOGLE_MAPS_LATITUDE:\s*(\d+\.\d+)')
>>> selector.xpath('.//script[contains(text(), ""window.DETAILS_PAGE_MAP_GLOBALS"")]/text()').re(regexp)
[u'55.4159330979', u'25.1549968578']
",A,2
1600975,2009-10-21 13:52:53.823000+00:00,"Think about this situation.
Person wants to exit your program.  They touch the trackpad.  Your program moves the cursor somewhere they didn't expect.  They're upset and confused.
They continue to touch the trackpad, your program continues to do something they didn't expect.  They find they're unable to control the cursor.  Now what?
Generally, having your program move the cursor is a recipe for disaster.  

The cursor is hard enough to spot on the screen.  X-windows applications which do ""cursor warping"" to dialog boxes have an option to disable this because it  is confusing.
Removing control of the cursor from the user makes the computer (already very hard to use) much harder to use because there's this ""mode"" thing.  When your program is running, one thing happens.  When your program is not running, something different happens.

Look at http://java.sun.com/j2se/1.4.2/docs/api/java/awt/Cursor.html
There do not appear to be any methods to change the cursor's position.  It tracks the mouse.
However, look at http://java.sun.com/j2se/1.5.0/docs/api/java/awt/Robot.html 
This has the ability to synthesize mouse events.  Feel free to play with it.",A,4
24247200,2014-06-16 15:28:51.717000+00:00,"pickle will make sure it can re-import a class, as only the data on the instance itself is pickled, plus the import location of the class. As such, pickle looks for the __module__ attribute on the class to determine where it came from.
It appears the Django interactive environment does not set this __module__ attribute; as a result TestObj.__module__ is inherited from the object base class instead, and that's __builtin__. Perhaps no __name__ global is set. As a result, the pickle module ends up looking in the wrong place for your class. There is no __builtin__.TestObj after all.
From the comments, I gather that you are trying to store mocked objects in the Django cache. That won't work, as mock objects are not pickleable. That makes sense, as on unpickling (which could be in an entirely new Python process), how would pickle know what original class was being mocked?",A,1
27938516,2015-01-14 08:22:42.673000+00:00,"There is no way for you to specify what version pip should install, no. You can specify Python compatibility in the Trove classifiers, but pip will not differentiate on those, let alone pick a different version based on that.
You are stuck with writing a version that works on both Python 2 and 3. I recommend you read through the Porting Python 2 Code to Python 3 guide and use a compatibility layer support library like six. You can use the Modernize tool to help you rewrite your code here. ",A,1
28444573,2015-02-11 00:36:39.057000+00:00,"You could get random bytes using os.urandom():
>>> import binascii, os
>>> random_bytes = os.urandom(10)
>>> random_bytes
b'\xe4\x19\x9e\xbb\r\xe6C\xaa\x1e\x1f'
>>> binascii.hexlify(random_bytes)
b'e4199ebb0de643aa1e1f'

If you want to get an exception if the PRNG has not been seeded with enough data; you could use ssl.RAND_bytes():
>>> import ssl
>>> ssl.RAND_bytes(10)
b'\xbdH\xec\xc2+\x03\x1f\x07\xd0R'

To get random bytes from openssl subprocess:
>>> import binascii
>>> import subprocess
>>> hex_data = subprocess.check_output('openssl rand -hex 10'.split()).strip()
>>> hex_data
b'd310f3378f3e93e1f5ca'
>>> random_bytes = binascii.unhexlify(hex_data)
>>> random_bytes
b'\xd3\x10\xf37\x8f>\x93\xe1\xf5\xca'
",A,0
41401860,2016-12-30 19:43:02.953000+00:00,"The google packages take care to register themselves as a namespace package. With a properly set up sys.path there is no conflict here.
You need to set up your library environment correctly. Add a appengine_config.py file in the root of your project with:
from google.appengine.ext import vendor

# Add any libraries installed in the ""lib"" folder.
vendor.add('lib')

This adds the lib subdirectory in the right location of sys.path. See the Installing a third-party library section in the Developing Python Apps on App Engine How-To.
From here on out imports of google.cloud just work:
$ ls -1d lib *.py *.yaml
app.yaml
appengine_config.py
lib
main.py
$ pip install -t lib google-cloud
# installing into the lib subdirectory
$ cat main.py
import google
from google.cloud import datastore
from google.appengine.api import memcache
import os.path

here = os.path.dirname(os.path.abspath(__file__))

def app(*args, **kwargs):
    return '''
google: {}<br />
google.cloud.datastore: {}<br />
google.appengine.api.memcache: {}'''.format(
        os.path.relpath(google.__file__, here),
        os.path.relpath(datastore.__file__, here),
        os.path.relpath(memcache.__file__, here))

and in the browser I am served:
google: ../google-cloud-sdk/platform/google_appengine/google/__init__.py
google.cloud.datastore: lib/google/cloud/datastore/__init__.pyc
google.appengine.api.memcache: ../google-cloud-sdk/platform/google_appengine/google/appengine/api/memcache/__init__.pyc
",A,7
16787426,2013-05-28 08:32:47.713000+00:00,"Just break it down into small pieces, each of which is easy.
In English: For each line in the file, you want the first two values, and you want to split the first value after the colon, and you want to treat the second value as a number. Then, you want to keep track of the top 10 pairs, ordered by that second value.
In Python:
with open('large_file.txt') as f:
    pairs = (line.split()[:2] for line in f)
    processed_pairs = ((pair[0].split(':')[1], float(pair[1])) for pair in pairs)
    top_10_pairs = heapq.nlargest(10, processed_pairs, key=operator.itemgetter(1))

Now you've got a list of name, score pairs, which is easy to print out:
for name, score in top_10_pairs:
    print('{} {}'.format(name, score))

No matter how big the file is, this won't keep more than 10 processed pairs (plus a read buffer and some other basic stuff) in memory at a time, because we're just transforming an iterator full of files (a file) step by step into other iterators, and feeding that into heapq.nlargest, which only keeps the top n around.",A,6
4353994,2010-12-04 14:33:28.577000+00:00,"Find 'http-equiv' using BeautifulSoup
import urllib2
from BeautifulSoup import BeautifulSoup

f  = urllib2.urlopen(""http://example.com"")
soup = BeautifulSoup(f) # trust BeautifulSoup to parse the encoding
for meta in soup.findAll('meta', attrs={
    'http-equiv': lambda x: x and x.lower() == 'content-type'}):
    print(""content-type: %r"" % meta['content'])
    break
else:
    print('no content-type found')

#NOTE: strings in the soup are Unicode, but we can ask about charset
#      declared in the html 
print(""encoding: %s"" % (soup.declaredHTMLEncoding,))
",A,1
16447342,2013-05-08 18:05:06.140000+00:00,"The type is defined in the descrobject.c file.
You can locate Python types like these by first looking for the function name in bltinmodule.c; in this case the following line defines the property() function:
SETBUILTIN(""property"",              &PyProperty_Type);

then grep for the PyProperty_Type definition in the Objects subdirectory.",A,9
20058445,2013-11-18 21:37:28.307000+00:00,"In your code, the if prefix != words[0] part is happening outside the loop, after the loop has finished running. So, for a non-empty file, words will be the split of the last line of the file. And for an empty file, words will never have been set, causing exactly the error you posted.
As a side note, that for lines in f: is looping over some global object f, not the file you just opened, which is called funf. So, I suspect that f is some kind of empty iterable, and you're seeing this function even when the file you wanted to look at is not empty. If you want to loop over funf, you have to tell Python funf, not f.
And you already know this isn't correct, as in this comment:

word is the split of line. I can't do it outside the for loop

If you want to run it inside the loop, you will need to indent it to match the code inside the loop. In Python, block structure is based on indentation level:
def lines(path, prefix):
    funf = open(path, 'r')
    dictionary = {}
    for lines in f:
        word = lines.split()
        a_word = (word[0])
        dictionary[a_word] = dictionary.get(a_word, 0) + 1
        if prefix != word[0]:
            return 0
        else:
            return dictionary[prefix]
    funf.close()

That means you'll no longer get an error; words will always be defined when you use it.
There are other problems with this code: you're returning after each line, meaning you'll never get to the second line; you're returning before you close the file, meaning the file never gets closed; it's very misleading to use plural variables names for individual things and singular variable names for lists of things; it's confusing to use a local variable with the same name as the function; etc. But one thing at a time…

After half an hour of pulling teeth, you finally explained what you're trying to do:

I'm trying to count the number of lines whose first word matches the prefix

There is no way to do that with this structure. Whether you do the if inside the loop or out, it doesn't make any sense.
The simplest way to fix it is to remove the if entirely. You're building up a dictionary of counts of each first word, right? So, just look up the value for the given prefix at the end:
def lines(path, prefix):
    funf = open(path, 'r')
    dictionary = {}
    for lines in funf:
        word = lines.split()
        a_word = (word[0])
        dictionary[a_word] = dictionary.get(a_word, 0) + 1
    funf.close()
    return dictionary.get(prefix, 0)

This will work, but it's incredibly wasteful to build up this whole dictionary just to get a single value out of it, and makes your code much more complicated as well… the whole thing could be written as:
def lines(path, prefix):
    with open(path) as f:
        return sum(1 for line in f if line.startswith(prefix))

Here's my filetext1.txt:
This is a test.
But this isn't.
But this is.
And this isn't.

The output should obviously be 2, right?
And both versions of my code—the ""simplest fix"" and the two-liner—both print ut this:
2

This works in both Python 3.3 and 2.7. If it's not working for you, either you failed at copying and pasting the code, or your input file doesn't have any lines starting with ""But "".",A,2
35459581,2016-02-17 14:33:53.090000+00:00,"The first dirname literally gets the dir name that the file is in, not its parent.
You probably should do:
DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, '..', 'data', 'mydbtester.db'))
",A,2
20751388,2013-12-23 21:23:46.813000+00:00,"If splitting the data and joining the results can be done reasonably, this is almost always going to be more efficient—and a lot simpler—than having them all fight over shared data.

There are cases where there is no reasonable way to do this (it's either very complicated, or very slow, to join the results back up). However, even in that case there can be a reasonable alternative: return ""mutation commands"" of some form. The parent process can then, e.g., iterate over the output queue and apply each result to the single big array.

If even that isn't feasible, then you need sharing. There are two parts to sharing: making the data itself sharable, and locking it.
Whatever your graph type is, it probably isn't inherently shareable; it's probably got internal pointers and so on. This means you will need to construct some kind of representation in terms of multiprocessing.Array, or multiprocessing.sharedctypes around Structures, or the like, or in terms of bytes in a file that each process can mmap, or by using whatever custom multiprocessing support may exist in modules like NumPy that you may be using. Then, all of your tasks can mutate the Array (or whatever), and at the end, if you need an extra step to turn that back into a useful graph object, it should be pretty quick.
Next, for locking, the really simple thing to do is create a single multiprocessing.Lock, and have each task grab the lock when it needs to mutate the shared data. In some cases, it can be more efficient to have multiple locks, protecting different parts of the shared data. And in some cases it can be more efficient to grab the lock for each mutation instead of grabbing it once for a whole ""transaction"" worth of sequences (but of course it may not be correct). Without knowing your actual code, there's no way to make a judgment on these tradeoffs; in fact, a large part of the art of shared-data threading is knowing how to work this stuff out. (And a large part of the reason shared-nothing threading is easier and usually more efficient is that you don't need to work this stuff out.)

Meanwhile, I'm not sure why you need an explicit JoinableQueue here in the first place. It sounds like everything you want can be done with a Pool. To take a simpler but concrete example:
a = [[0,1,2], [3,4,5], [6,7,8], [9,10,11]]
with multiprocessing.Pool() as pool:
    b = pool.map(reversed, a, chunksize=1)
    c = [list(i) for i in b]

This is a pretty stupid example, but it illustrates that each task operates on one of the rows of a and returns something, which I can then combine in some custom way (by calling list on each one) to get the result I want.",A,1
19597116,2013-10-25 18:34:58.783000+00:00,"If you want Lines to be able to use ndarray methods, but not have its public namespace cluttered with those methods, use delegation instead of inheritance. In other words, instead of this:
class Lines(np.ndarray):
    def __init__(self, whatever):
        super().__init__(stuff)
    def dostuff(self, thingies):
        np.do_thingy(self.stuff(spam))
        return self.spam(eggs)

… do this:
class Lines(object):
    def __init__(self, whatever):
        self.lines = np.array(stuff)
    def dostuff(self, thingies):
        np.do_thingy(self.lines.stuff(spam))
        return self.lines.spam(eggs)


Meanwhile, it sounds like you have a set of Line methods that you want ufuncify in Lines, and you're sick of repeating yourself. So do that dynamically. Here's a trivial example to give you the idea:
for name in 'bam', 'biff', 'pow', 'kazaam':
    func = getattr(Line, name)
    ufunc = np.frompyfunc(func, 1, 1)
    setattr(Lines, name, ufunc)
",A,3
24871450,2014-07-21 17:47:34.370000+00:00,"You don't need to build a string here at all; retain your dictionary as is and use the **kwargs call syntax instead:
conn = DBAPI.connect(**dbsettings)

This passes each key-value pair in dbsettings as a keyword argument to the DBAPI.connect() function.",A,6
21145969,2014-01-15 18:59:01.593000+00:00,"b is the argument to the lambda function:
lambda b: ....

The first part before the : defines the arguments for the anonymous function, this one defines just b just like a function signature would:
def binary_to_int(b):
    # ...

It could have been given another name too:
lambda anothername: int(anothername, 2)

It is bound to the argument you pass to this lambda:
binary_to_int = lambda b: int(b, 2)
result = binary_to_int('010101')  # so b is bound to '010101'

The function returns the result of the function int(b, 2), which interprets a string first argument as an integer in base 2 (binary):

If x is not a number or if base is given, then x must be a string or Unicode object representing an integer literal in radix base. Optionally, the literal can be preceded by + or - (with no space in between) and surrounded by whitespace. A base-n literal consists of the digits 0 to n-1, with a to z (or A to Z) having values 10 to 35. The default base is 10. The allowed values are 0 and 2-36. Base-2, -8, and -16 literals can be optionally prefixed with 0b/0B, 0o/0O/0, or 0x/0X, as with integer literals in code. Base 0 means to interpret the string exactly as an integer literal, so that the actual base is 2, 8, 10, or 16.
",A,2
23800452,2014-05-22 07:31:38.490000+00:00,"The zipfile.ZipFile() class has a comment attribute you can set.
Open the file in append mode, alter the comment, and it'll be written out when closed:
from zipfile import ZipFile

with ZipFile('test.zip', 'a') as testzip:
    testzip.comment = 'short'

When you use a ZipFile as a context manager, like a regular file it'll automatically be closed when the with block is exited.",A,2
541704,2009-02-12 15:12:04.607000+00:00,"Yes, there is a way. 
Allocate memory for the buffers in a such way that they are adjacent in memory.
Example:
char* a = malloc(a_size + b_size);
char* b = a + a_size;
",A,5
47472763,2017-11-24 11:52:52.293000+00:00,"random.shuffle() only works on mutable sequences, which is usually a list object. range() produces an immutable sequence object, random.shuffle() can't move the values in a range around.
Convert the range to a list first:
ids = list(range(data_size))

In Python 2, range() used to produce a list of integers (vs. xrange(), producing the immutable sequence), so you can still find code online that uses range() without list() before shuffling. Take this into account when trying to adapt online code examples to Python 3. Also see NameError: global name 'xrange' is not defined in Python 3",A,2
21993349,2014-02-24 16:29:08.140000+00:00,"User.objects.exclude(teacher=None)
",A,1
50310425,2018-05-12 20:28:55.523000+00:00,"You need to use & to separate elements in a querystring:
https://[store-username].myshopify.com/admin/collects.json?limit=250&page=1
Note, you really should use the Shopify python client, though.",A,3
30024069,2015-05-04 06:45:20.777000+00:00,"You have at least three problems here:

You try to use the global root before defining it, so your program is just going to raise a NameError.
You're assigning something to self.mAsk when you don't have anything named self, so that's also going to raise a NameError. (Do you not understand what classes are, and why self appears in methods of classes in many tkinter examples?)
You're not calling pack, grid, or place to actually place mAsk on the parent window. See the chapters on the three different Geometry Managers in the Tkinter book if you have no idea what this means.

If you fix all three, then it works:
from tkinter import *

root=Tk()
mAsk = Scale(root, orient=""horizontal"", from_=1, to=16, label = ""Mines"", resolution = 1, sliderlength=25)
mAsk.pack()
root.mainloop()
",A,1
21269017,2014-01-21 21:01:36.573000+00:00,"Your code doesn't actually do what you say it does. In fact, it doesn't even run. But here's a simple example that does demonstrate the problem you're seeing:
def s_sort(numbers):
    alist=[]
    alist.append(numbers)
    numbers.sort()
    alist.append(numbers)
    return alist

The problem is that alist is not a list of two different lists, it's a list of the same list twice in a row. So, when you modify that one list, of course that one list is modified everywhere it appears—in numbers, and in alist[0], and in alist[1].
The solution is to not add the same list multiple times; instead, add a new one. For example:
def s_sort(numbers):
    alist=[]
    alist.append(numbers[:])
    alist.append(sorted(numbers))
    return alist

Now you've created two brand-new lists—one an exact copy of the original, one a sorted copy—and returned a list of them.
So, instead of returning [[3, 5, 7], [3, 5, 7]] (and also changing numbers to be [3, 5, 7]), it returns [[5, 7, 3], [3, 5, 7]] (and leaves numbers alone).
I have no idea why you expected [3, 7, 5] for the first element, but maybe you're doing some other work to the first element of alist which you didn't show us. In which case, as long as you do that work in a copying rather than mutating way (ala sorted(n) vs. n.sort()) or do it to a copy, everything will be fine.",A,2
32444559,2015-09-07 18:52:39.270000+00:00,"The time format is similar to the date/time format used in emails. You could use email module to parse it:
>>> from email.utils import parsedate_tz, mktime_tz
>>> ts1 = mktime_tz(parsedate_tz('Fri 11 Feb 2078 00:05:21 +0400'))
>>> ts2 = mktime_tz(parsedate_tz('Mon 29 Dec 2064 03:33:48 -1100'))
>>> ts1 - ts2
413962293
",A,0
21583817,2014-02-05 17:09:23.387000+00:00,"To check if a float value is a whole number, use the float.is_integer() method:
>>> (1.0).is_integer()
True
>>> (1.555).is_integer()
False

The method was added to the float type in Python 2.6.
Take into account that in Python 2, 1/3 is 0 (floor division for integer operands!), and that floating point arithmetic can be imprecise (a float is an approximation using binary fractions, not a precise real number). But adjusting your loop a little this gives:
>>> for n in range(12000, -1, -1):
...     if (n ** (1.0/3)).is_integer():
...         print n
... 
27
8
1
0

which means that anything over 3 cubed, (including 10648) was missed out due to the aforementioned imprecision:
>>> (4**3) ** (1.0/3)
3.9999999999999996
>>> 10648 ** (1.0/3)
21.999999999999996

You'd have to check for numbers close to the whole number instead, or not use float() to find your number. Like rounding down the cube root of 12000:
>>> int(12000 ** (1.0/3))
22
>>> 22 ** 3
10648

If you are using Python 3.5 or newer, you can use the math.isclose() function to see if a floating point value is within a configurable margin:
>>> from math import isclose
>>> isclose((4**3) ** (1.0/3), 4)
True
>>> isclose(10648 ** (1.0/3), 22)
True

For older versions, the naive implementation of that function (skipping error checking and ignoring infinity and NaN) as mentioned in PEP485:
def isclose(a, b, rel_tol=1e-9, abs_tol=0.0):
    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
",A,269
27333312,2014-12-06 15:43:24.903000+00:00,"You are creating a new list with just one element every time, then indexing that:
[c][1]

With only one element, that indexing will fail. If c is an index into the usa_univs list, you do need to reference the list:
usa_univs[c][1]

However, rather than manually manage indexes into that list, use a for loop to access each element directly:
a = 0
b = 0

for element in usa_univs:
    a = a + element[1]
    b = b + (element[1] * element[2])

where I also set b to 0 (you forgot that part), and c is no longer needed at all.
Python lets you unpack sequences into their constituent parts; you can do so even in the for loop:
for name, a_value, b_value in usa_univs:
    a = a + a_value
    b = b + (a_value * b_value)

and you can use augmented assignment to make adding to a and b a little more compact:
for name, a_value, b_value in usa_univs:
    a += a_value
    b += a_value * b_value

In the end your function should use return rather than printing. Leave printing to the caller; you already do so but your function now returns None. Your whole code, with all these improvements, then looks like:
usa_univs = [ ['California Institute of Technology',2175,37704],
          ['Harvard',19627,39849],
          ['Massachusetts Institute of Technology',10566,40732],
          ['Princeton',7802,37000],
          ['Rice',5879,35551],
          ['Stanford',19535,40569],
          ['Yale',11701,40500]  ]

def total_enrollment(usa_univs):
    a = 0
    b = 0
    for name, a_value, b_value in usa_univs:
        a += a_value
        b += a_value * b_value
    return a, b

print total_enrollment(usa_univs)
",A,0
29842363,2015-04-24 08:28:42.017000+00:00,"raise interrupting the next steps immediately is exactly what it's supposed to do. In fact, that's the whole point of exceptions.
But then return also interrupts the next steps immediately, because returning early is also the whole point of return.
If you want to save an error until later, continue doing some other work, and then raise it at the end, you have to do that explicitly. For example:
def spam():
    error = None
    try:
        do_some_stuff()
    except IOError as e:
        print 'WARNING: cant find file %s' % e
        error = Exception('NoFile')
    try:
        do_some_more_stuff()
    except OtherError as e:
        print 'WARNING: cant frob the glotz %s' % e
        error = Exception('NoGlotz')
    # etc.
    if error:
        raise error

Now, as long as there's no unexpected exception that you forgot to handle, whatever failed last will be in error, and it'll be raised at the end.

As a side note, instead of raising Exception('NoFile'), then using == to test the exception string later, you probably want to create a NoFileException subclass; then you don't need to test it, you can just handle it with except NoFileException:. And that means you can carry some other useful information (the actual exception, the filename, etc.) in your exception without it getting in the way, too. If this sounds scary to implement, it's not. It's literally a one-liner:
class NoFileException(Exception): pass
",A,1
17853370,2013-07-25 08:56:06.030000+00:00,"Python does not have a multidimensional array type. It only has lists.
numpy (a 3rd-party Python extension) does have array types, and these serve a specialized function within that library, namely fast C-based mathematical operations on homogenous sequences.
With the standard Python list type, putting one inside the other creates a nested structure that can be used to model a multidimensional structure. You nest the [index] item access, [1][42] first retrieves the second element of the outer list, then the 43rd element of that second element.
numpy arrays are specialist structures that explicitly model multiple dimensions as part of the main type, rather than nesting arrays inside arrays, and that means they can support addressing of multiple dimensions in the [index] syntax, where index comes in the form of a tuple, technically.
Python does have a single dimensional array type, that, like numpy arrays, models homogenous C-type sequences.",A,7
19056942,2013-09-27 17:42:40.360000+00:00,"You have UTF-8 encoded data (there is no such thing as UNICODE encoded data).
Encode the unicode value to Latin-1, then decode from UTF8:
encoded_id.encode('latin1').decode('utf8')

Latin 1 maps the first 255 unicode points one-on-one to bytes.
Demo:
>>> encoded_id = u'abcd\xc3\x9f'
>>> encoded_id.encode('latin1').decode('utf8')
u'abcd\xdf'
>>> print encoded_id.encode('latin1').decode('utf8')
abcdß
",A,4
19692698,2013-10-30 19:58:40.213000+00:00,Seems like for whatever reason the Books schema in your postgres db doesn't match the models - it has a location_id column. You should drop the table and rerun syncdb.,A,0
16471185,2013-05-09 21:11:18.577000+00:00,"You need to use zip() to combine the two lists:
for n, m in zip(results, settings):
    if (m-.1) <= n <= (m+.1):
        print 'ok'
    else:
        print 'fail'

zip() creates a new list made by combining each nth element from each input sequence:
>>> a = range(5)
>>> b = 'abcde'
>>> zip(a, b)
[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e')]

You can use all() to short-circuit testing; all() returns False as soon as possible. We use itertools.izip() here instead to avoid creating a whole new list where perhaps only the first few pairs might be tested:
from itertools import izip

if all((m-.1) <= n <= (m+.1) for n, m in izip(results, settings)):
    print 'All are ok'
else:
    print 'At least one failed'
",A,1
11618280,2012-07-23 18:27:02.500000+00:00,"Sure. URLs are processed in order, and two includes can have the same prefix - if one doesn't succeed in matching, processing will just move on to the next one.
urlpatterns = patterns('',
    url(r'^user/', include('registration.urls')),
    url(r'^user/', include('profile.urls')),
)
",A,34
5684292,2011-04-16 03:31:33.033000+00:00,"Here's an iterative approach to a similar problem:
def copyfile(path, dstdir, verbose=True, dryrun=False):
    """"""Copy `path` file to `dstdir` directory incrementing name if necessary.""""""
    filename = os.path.basename(path)
    basename, ext = os.path.splitext(filename)

    for i in itertools.count(2):
        destpath = os.path.join(dstdir, filename)
        if not os.path.exists(destpath):
            if verbose:
                print(path, '->', destpath)
            if not dryrun:
                shutil.copyfile(path, destpath)
            return
        # increment filename
        filename = ""%s_%02d%s"" % (basename, i, ext)
",A,1
19507121,2013-10-22 00:35:56.057000+00:00,"The easiest way to do this is in two passes: First get the lines and line numbers of the non-matching lines, and then get the lines of the matching lines.
d_rows = [line.strip() for line in open(dfile)]
good_rows = [(i, row) for i, row in enumerate(d_rows) if is_good_row(row)]
bad_rows = [(i, row) for i, row in enumerate(d_rows) if not is_good_row(row)]

This does mean making two passes over the list, but who cares? If the list is small enough to read the whole thing into memory as you're already doing, the extra cost is probably negligible.
Alternatively, if you need to avoid the cost of building two lists in two passes, you probably also need to avoid reading the whole file at once in the first place, so you'll have to do things a little more cleverly:
d_rows = (line.strip() for line in open(dfile)) # notice genexp, not list comp
good_rows, bad_rows = [], []
for i, row in enumerate(d_rows):
    if is_good_row(row):
        good_rows.append((i, row))
    else:
        bad_rows.append((i, row))

If you can push things even farther back to the point where you don't even need explicit good_rows and bad_rows lists, you can keep everything in an iterator all the way through, and waste no memory or up-front reading time at all:
d_rows = (line.strip() for line in open(dfile)) # notice genexp, not list comp
with open(outfile, 'w') as f:
    for i, row in enumerate(d_rows):
        if is_good_row(row):
            f.write(row + '\n')
        else:
            whatever_you_wanted_to_do_with(i, row)
",A,0
37925004,2016-06-20 14:24:03.957000+00:00,"You can't do things like that in Python, as assignment is always a statement. The usual pattern is:
while True:
    name_of_file = move_first_matching_file()
    if not name_of_file:
        break
    ...
",A,2
7661941,2011-10-05 13:23:10.420000+00:00,"Either one, depending on how often your data changes and how big it is. It's all about how the ZODB stores this information.
Storing data directly on the object using setattr means that data is stored in the persistent record for that object. If that's a big object that means there is a big transaction going to take place for that write.
Storing data in a zope.annotations annotation means you get a separate persistent record for each annotation entry, so any changes to your data will result in a smaller transaction. But if you want to access this data often, that extra persistent record will need to be loaded, on top of all the other persistent records. It'll take a slot in your ZODB cache, your ZEO server or RelStorage server will need to serve it, etc.
plone.uuid uses setattr because it is generally generated only once for a given object, usually at a time it is being created already. It is also a piece of data that is accessed often and quite small. So, by putting it directly on the object itself it'll be loaded as soon as you load that object, no extra trips to the ZODB required, and it'll only be changed once in it's lifetime.
Note: the above presumes that the annotations are stored with the AttributeAnnotations adapter, which is the most common method and the default for Plone content.",A,6
26585986,2014-10-27 10:55:45.643000+00:00,"To test that your mock was not called, just test the called attribute is False:
self.assertFalse(sendEmails_mock.called)
",A,3
28628704,2015-02-20 12:11:51.697000+00:00,"You could use .count() method to find the depth:
import os

def files(rootdir='.', mindepth=0, maxdepth=float('inf')):
    root_depth = rootdir.rstrip(os.path.sep).count(os.path.sep) - 1
    for dirpath, dirs, files in os.walk(rootdir):
        depth = dirpath.count(os.path.sep) - root_depth
        if mindepth <= depth <= maxdepth:
            for filename in files:
                yield os.path.join(dirpath, filename)
        elif depth > maxdepth:
            del dirs[:] # too deep, don't recurse

Example:
 print('\n'.join(files(mindepth=3)))

The answer to the related question uses the same technique.",A,5
22505820,2014-03-19 12:26:49.267000+00:00,"You could use the ifchanged template tag:
{% for question in Question_group1 %}
    {% ifchanged question.question_tag %}
      <div class='question_header'>
         <p class='question_tag'>.{{question.question_tag.tag_name}}</p>
         <p class='question_tag_descript'>({{question.question_tag.tag_description}})</p>
      </div>
    {% endifchanged %}
  <label class=""question_quest"">{{question.question_box}}</label>
  <textarea name=""answer"" placeholder=""Some text about the question!"" rows=""3""></textarea>
{% endfor %}

Now the contents of the ifchanged block will only be output when the question_tag changes.",A,0
11915539,2012-08-11 14:30:48.010000+00:00,"To store a list in a db you could use a new table:
class Word(Base):
    __tablename__ = ""words""

    id = Column(Integer, primary_key=True)
    eng = Column(String(32), unique=True)
    chinese = relationship(""Chinese"", backref=""eng"")

    def __init__(self, eng, chinese):
        self.eng = eng
        self.chinese = map(Chinese, chinese)

class Chinese(Base):
    __tablename__ = ""chinese_words""

    word = Column(String(128), primary_key=True)
    eng_id = Column(Integer, ForeignKey('words.id'), primary_key=True)

    def __init__(self, word):
        self.word = word

See full example.
Don't use str()/eval() if you want to store chinese as a blob you could use json.dumps()/json.loads(). Using suggested by @thebjorn TypeDecorator:
class Json(TypeDecorator):

    impl = String

    def process_bind_param(self, value, dialect):
        return json.dumps(value)

    def process_result_value(self, value, dialect):
        return json.loads(value)

class Word(Base):
    __tablename__ = ""words""

    eng = Column(String(32), primary_key=True)
    chinese = Column(Json(128))

See full example.",A,7
35369274,2016-02-12 18:00:12.843000+00:00,"* is a size placeholder. It tells the formatting operation to take the next value from the right-hand-side tuple and use that as the precision instead.
In your example, the 'next' value is 5 for the first slot, so you could read this as %.5g, which is used to format 2.23523523. The second slot uses 3 for the width, so becomes %.3g to format 12.353262.
See the String Formatting Operations documenation:

A conversion specifier contains two or more characters and has the following components, which must occur in this order:
(...)

Minimum field width (optional). If specified as an '*' (asterisk), the actual width is read from the next element of the tuple in values, and the object to convert comes after the minimum field width and optional precision.
Precision (optional), given as a '.' (dot) followed by the precision. If specified as '*' (an asterisk), the actual width is read from the next element of the tuple in values, and the value to convert comes after the precision.


So both the minimum width and the precision can be made variable with *, and the documenation explicitly states that the value to convert comes after the width and precision.",A,5
22454602,2014-03-17 12:43:02.300000+00:00,"The original fieldsets dict is a class-level attribute. By inserting your field, you're modifying the class-level dictionary, which affects all instances derived from it.
You might want to do a deepcopy of the fieldset before inserting:
import copy
...
fieldsets = copy.deepcopy(super(FeatureAdmin, self).get_fieldsets(request, obj))
",A,13
20895418,2014-01-03 02:14:57.607000+00:00,"
Because strings are immutable, don't a and b point to the same string?

No. In Java, they don't because you're explicitly asking it to create a new String object out of the literal, and then to create another new String out of the same literal, which guarantees that you get two different objects. (And in your Python test, you get the exact same results, although that isn't guaranteed by the language—see below for details.)


I'm assuming that Java's == is the same as Python's 'is', and Java's .equals is the same as Python's ==

Close enough to true for this question.

in which case the two blocks of code contradict each other.

No they don't. You translated two tests to Python, and got the exact same results as in the two corresponding Java tests. You didn't translate the third test at all, so there's nothing there to contradict with anything.
Let's repeat your Java:
String a = new String(""Wow"");
String b = new String(""Wow"");
String sameA = a;

boolean r1 = a == b;      // This is false, since a and b are not the same object
boolean r2 = a.equals(b); // This is true, since a and b are logically equals
boolean r3 = a == sameA;  // This is true, since a and sameA are really the same object

… and translate it more closely to Python:
a = ""Wow""[:]
b = ""Wow""[:]
sameA = a

r1 = a is b               # This is False, since a and b are not the same object
r2 = a == b               # This is True, since a and b are logically equals
r3 = a is sameA           # This is True, since a and sameA are really the same object

The [:] are there to match the new in the original code. You're explicitly asking Java to create a new String object out of the literal ""Wow"". In Python, there's no way to explicitly ask for a new object, but you can always ask for a copy, which is close enough (as long as you don't care about the fact that you might be creating a couple extra strings as garbage to be immediately collection).

However, if you remove the [:], you're often going to get the same results anyway—as you saw in your own tests. You got True for a == b and False for a is b too. Repeating the tests that way:
a = ""Wow""
b = ""Wow""
sameA = a

r1 = a is b               # This is False, since a and b are not the same object
r2 = a == b               # This is True, since a and b are logically equals
r3 = a is sameA           # This is True, since a and sameA are really the same object


In your comment, you say that a is b was actually True, not False.
As explained above, that's also perfectly legal. While Java requires that new String create a new String object, nothing in Python requires that evaluating the same string literal twice has to create two separate string objects, or even that copying a string object has to create a new string object. See below for more.


I also have a suspicion that this might have to do w/ the fact that Java doesn't treat Strings as primitives?

Indirectly, it's sort of to do with the fact that Java doesn't treat Strings as primitives, and also to do with the fact that Python doesn't treat string literals the same way as some other kinds of literals.
In Java, because String is not a primitive, you have a choice to explicitly create a new String. And you do that. So, it's not going to be the same as any previous instance. The fact that they're both immutable instances is irrelevant; Java is not allowed to collapse separate objects into a single object if there's any way that it would be visible.
In Python, ""Wow"" is a literal, and you're not asking it to create a new string. And strings are immutable, and Python is allowed to collapse separate immutable built-in objects in ways that Java is not. So it can combine the two literals into one. Even with the [:], it's allowed to collapse the new copy into the original. And with small integers, it will generally do so—try the same test with 0 (use copy.copy(0) to test explicit copying) rather than ""Wow"" and see what happens. But the major Python implementations often happen to not do this for strings, so you end up with separate objects, so you can get the same result as in Java.

What this means in practice is that, with code like yours, either with or without the explicit copy, a is b and a is not b are both perfectly legal and reasonable things to happen, so your code should never rely on either one being true. Fortunately, there's very little reason to do so. (If you were thinking performance might be a good reason, try calling timeit on a is b vs. a == b, and you'll find something like 55.3ns vs. 61.0ns.)

One last thing to keep in mind: Python and Java are very different languages, so it shouldn't be all that surprising that similar-looking code sometimes acts very differently (or that very different-looking code sometimes acts similarly). The fact that English rules for when to use this vs. that are not the same as Japanese rules for kore vs. sore vs. are is not surprising, it's just something you have to learn when you learn Japanese (or when you learn English).",A,4
44571027,2017-06-15 15:08:39.817000+00:00,"You need to remove the space after the colon:
{{ post.date|date:""Y-m-d"" }}
",A,5
51845451,2018-08-14 15:57:20.430000+00:00,"It isn’t clear from your question, but I think your problem is that you’re already using None to mean “this is a leaf, nothing to print”, so you can’t also use None as a default value in the same parameter to mean “use whatever your default value is”.
The way around this is to define a “sentinel” object that isn’t None and also isn’t the same as any possible actual value. For example:
_sentinel = object()
def in_order_traversal(self, node=_sentinel):
    if node is self._sentinel:
        node = self.root
    if node != None:
        print(node, end="" "")
        self.in_order_traversal(node.left)
        self.in_order_traversal(node.right)
",A,0
22957426,2014-04-09 08:54:40.697000+00:00,"Don't use list.index(); that'll only match the first occurrence of a letter; for 'o' it'll never find the second 'o'; you'll only repeatedly add the same characters to the sets.
Use enumerate() to add an index to your loop, instead:
def function(s : str) -> {str:{str}}:
    listt=list(s)
    dictt= {}
    for next_index, char in enumerate(listt[:-1], 1):
        if char not in dictt:
            dictt[char] = set()
        dictt[char].update(listt[next_index])
    return dictt

I started enumerate() at 1 instead of the default 0 so it always represents the next position.
Demo:
>>> def function(s : str) -> {str:{str}}:
...     listt=list(s)
...     dictt= {}
...     for next_index, char in enumerate(listt[:-1], 1):
...         if char not in dictt:
...             dictt[char] = set()
...         dictt[char].update(listt[next_index])
...     return dictt
... 
>>> print(function('bookeeper'))
{'p': {'e'}, 'o': {'o', 'k'}, 'e': {'p', 'r', 'e'}, 'b': {'o'}, 'k': {'e'}}

Now that it is working, lets simplify this a little; use dict.setdefault() to add the set to the dictionary when the key is missing, for example. Strings are already sequences, no need to cast them to a list either:
def function(s : str) -> {str:{str}}:
    dictt = {}
    for next_index, char in enumerate(s[:-1], 1):
        dictt.setdefault(char, set()).update(s[next_index])
    return dictt

Instead of enumerate(), we could also use zip() to pair up the letters of the word:
def function(s : str) -> {str:{str}}:
    dictt = {}
    for char, next_char in zip(s, s[1:]):
        dictt.setdefault(char, set()).update(next_char)
    return dictt
",A,3
16322630,2013-05-01 16:52:09.133000+00:00,"Use the SubElement factory to add new elements, it's much easier to use:
from xml.etree import ElementTree as ET

# drugs is a reference to your <Drugs> element

for row in csvreader:
    drug = ET.SubElement(drugs, 'Drug')
    ET.SubElement(drug, 'DrugID').text = row[0]
    ET.SubElement(drug, 'Dose').text = row[1]
    ET.SubElement(drug, 'Unit').text = row[2]

where I assume that the columns 1 - 3 are the drug id, dose and unit, adjust as required for your CSV file.
Calling SubElement() creates the element, adds it to the parent and returns the newly created element for further processing.",A,1
30310883,2015-05-18 18:56:15.477000+00:00,"You are being rate limited, but the server does so in a way that violates the HTTP specification. Their response headers promise a Chunked transfer encoding, then do not send such a response.
If you look at the URL with curl in verbose mode, you get the following output:
$ curl -v https://market.yandex.ru/catalog/90555/list
* Hostname was NOT found in DNS cache
*   Trying 213.180.204.22...
* Connected to market.yandex.ru (213.180.204.22) port 443 (#0)
* TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
* Server certificate: market.yandex.ru
* Server certificate: Certum Level IV CA
* Server certificate: Certum CA
> GET /catalog/90555/list HTTP/1.1
> User-Agent: curl/7.37.1
> Host: market.yandex.ru
> Accept: */*
> 
< HTTP/1.1 302 Found
* Server nginx is not blacklisted
< Server: nginx
< Date: Mon, 18 May 2015 18:53:15 GMT
< Content-Type: text/html; charset=UTF-8
< Transfer-Encoding: chunked
< Connection: keep-alive
< Keep-Alive: timeout=120
< X-Forwardtouser-Y: 1
< Set-Cookie: spravka=dD0xNDAwNDM5MTk1O2k9ODQuOTIuOTguMTcwO3U9MTQwMDQzOTE5NTUxNjUwOTExMjtoPWNkMzVlMzBlMjgxMTg4YWM0YjYyZDg3OTg4ZjUyNWFj; domain=.yandex.ru; path=/; expires=Wed, 17-Jun-2015 18:53:15 GMT
< Location: http://market.yandex.ru/showcaptcha?cc=1&retpath=http%3A//market.yandex.ru/catalog/90555/list%3F_bfd13d35fbf1551a835f050d3775fc4b&t=0/1431975195/029660aeb063916c78e30ebd9444fd4b&s=4dd645e7048b399008278208fa776ba9
< Set-Cookie: uid=CniLolVaNRthdR2JDtV0Ag==; path=/
< 
* transfer closed with outstanding read data remaining
* Closing connection 0
curl: (18) transfer closed with outstanding read data remaining

The are sending you a redirect, but the Transfer-Encoding: chunked header in the response means that the client side has to load chunks, which are not there.
The redirect leads to a captcha:
http://market.yandex.ru/showcaptcha?cc=1&retpath=http%3A//market.yandex.ru/catalog/90555/list%3F_bfd13d35fbf1551a835f050d3775fc4b&t=0/1431975195/029660aeb063916c78e30ebd9444fd4b&s=4dd645e7048b399008278208fa776ba9
#                       ^^^^^^^^^^^
",A,1
28190542,2015-01-28 11:00:33.223000+00:00,"The long_hash() function indeed loops and depends on the size of the integer, yes:
/* The following loop produces a C unsigned long x such that x is
   congruent to the absolute value of v modulo ULONG_MAX.  The
   resulting x is nonzero if and only if v is. */
while (--i >= 0) {
    /* Force a native long #-bits (32 or 64) circular shift */
    x = (x >> (8*SIZEOF_LONG-PyLong_SHIFT)) | (x << PyLong_SHIFT);
    x += v->ob_digit[i];
    /* If the addition above overflowed we compensate by
       incrementing.  This preserves the value modulo
       ULONG_MAX. */
    if (x < v->ob_digit[i])
        x++;
}

where i is the 'object size', e.g. the number of digits used to represent the number, where the size of a digit depends on your platform.",A,5
3610732,2010-08-31 15:45:58.713000+00:00,"This is ordinary relational database design.  Don't over-optimize it with OO and inheritance techniques.  
You have a Product Category table with (probably) just names.
You have a Product Type table with not much information with FK's to category.
You have a Product table with an FK relationship to Product Type.
You have a Product Feature table with your data points and an FK to product.
To work with a Product, you query the product.  If you need the features, you're just working with the ""feature_set"".  Django fetches them for you.
This works remarkably well and is a well-established feature of a relational database and an ORM layer.  The DB caching and the ORM caching make these queries go very quickly.",A,1
41431463,2017-01-02 18:11:28.633000+00:00,"You are making things way too complicated. You are calling os.listdir() repeatedly, while moving files out. So your result list gets shorter and shorter each time.
You start out with 12 names, but once you moved one you now get only 11 names in the directory, then 10, and by the time your x value hits 6, there are only 6 names left with indexes 0 through to 5, so os.listdir()[x] fails with an IndexError exception.
You don't need a length, and you don't need to use range(). Call os.listdir() once, and loop over the results:
import os.path
import shutil

src = '/var/www/html/uploads/'
dst = '/media/pi/external/Photos/'

for picture in os.listdir(src):
    print(picture)
    shutil.move(os.path.join(src, picture), os.path.join(dst, picture))

Now you have one complete list of all the names at the start, and the for loop assigns those names one by one to the picture variable. That those names are moved out no longer matters, as we don't call os.listdir() again.",A,7
17606195,2013-07-12 01:32:00.283000+00:00,"If you really want to define a function by building it up iteratively, you can. I can't think of  any situation where this would be the best answer, or even a reasonable one, but it's what you asked for, so:
def my_func(components):
    f = lambda x: -1
    for component in components:
        def wrap(f):
            return lambda x: component * x[0] + f(x[1:])
        f = wrap(f)
    return f

Now:
>>> f = my_func([1, 2, 3])
>>> f([4,5,6])
44

Of course this will be no fun to debug. For example, look at the traceback from calling f([4,5]).",A,0
25877141,2014-09-16 19:42:48.440000+00:00,"
I'm guessing you need to replace ""getLines()"" in the for-loop with an actual list. The function must return the list ""inputs"".

Exactly.


But inside the function's while loop, ""if line == -1:"" never returns True. How come?

Try this on the interactive interpreter:
>>> '-1' == -1
False

Except in a very limited set of special cases (e.g., int and float), values of different types are never equal.
Now look up what raw_input returns:

… The function then reads a line from input, converts it to a string (stripping a trailing newline), and returns that.

So, when you type -1, that raw_input will return the string '-1'.
You could fix this by checking against the string '-1' instead of the number -1. But is that what you want? If someone types, say, -1 with a space, should that count as quitting? If so, you might want to call line.strip().

But there's actually a third error here… and the fact that they only claim 2 errors makes me think they want you to solve the two together.
inputs is going to be a list of strings. You can add those all together, but all that's going to do is concatenate them into one big string, not add them as numbers. So really, you probably need to convert each string into an integer, e.g.:
try:
    number = int(line)
except ValueError:
    print 'I said a positive integer, does %s look like an integer?' % (line,)
    continue

And if you do that, the previous problem magically goes away, because number == -1 will be true.
Of course you also need to figure out how to deal with the user typing 0 or -32 or something else that is a valid int but neither a positive integer nor -1. Don't you just love edge cases?",A,2
39971603,2016-10-11 06:34:30.390000+00:00,"You never return the result of the recursive call:
else:
    for i in range(0,n+1):
        formula=((x1)**(n-i))*((x2)**(i))
        list.append(formula)

    mapfeature_binominal_array(x1,x2,n-1,list)

The function just ends there without a return so None is returned instead.
Add return there:
else:
    for i in range(0,n+1):
        formula=((x1)**(n-i))*((x2)**(i))
        list.append(formula)

    return mapfeature_binominal_array(x1,x2,n-1,list)

A recursive function call is just like any other function call; you still need to do something with the result of that function call; the value it returns does not automatically propagate to be the return value of the outer call.
You are also falling into the default mutable argument value trap; you set list=[] in the function signature and append to that. The values in that list persist across calls, so future calls to the function will still have those values from previous calls, and you'll get wrong results.
Use a sentinel like None instead, and try to avoid using the names of built-in types like list:
def mapfeature_binominal_array(x1, x2, n=6, lst=None):
    if lst is None:
        lst = [x1, x2]

You can immediately set that initial list to [x1, x2].
The corrected code then runs and produces your results:
>>> import numpy as np
>>> def mapfeature_binominal_array(x1, x2, n=6, lst=None):
...     if lst is None:
...         lst = [x1, x2]
...     if n == 1:
...         return np.array(lst)
...     else:
...         for i in range(n + 1):
...             formula = (x1 ** (n - i)) * (x2 ** i)
...             lst.append(formula)
...         return mapfeature_binominal_array(x1, x2, n - 1, lst)
...
>>> mapfeature_binominal_array(-0.25, 1.5)
array([ -2.50000000e-01,   1.50000000e+00,   2.44140625e-04,
        -1.46484375e-03,   8.78906250e-03,  -5.27343750e-02,
         3.16406250e-01,  -1.89843750e+00,   1.13906250e+01,
        -9.76562500e-04,   5.85937500e-03,  -3.51562500e-02,
         2.10937500e-01,  -1.26562500e+00,   7.59375000e+00,
         3.90625000e-03,  -2.34375000e-02,   1.40625000e-01,
        -8.43750000e-01,   5.06250000e+00,  -1.56250000e-02,
         9.37500000e-02,  -5.62500000e-01,   3.37500000e+00,
         6.25000000e-02,  -3.75000000e-01,   2.25000000e+00])
",A,0
7699555,2011-10-08 20:24:09.287000+00:00,"As @Chris mentions in the comment you could define a function:
def words_fromfile(filename):
    with open(filename) as f:
         return f.read().splitlines()

dictionary_words = words_fromfile('dictionary.txt')
substrings = words_fromfile('substrings.txt')

You could always change the function definition later if you don't like it for some reason without touching the rest of the code.",A,1
6164481,2011-05-28 21:36:16.180000+00:00,"Python gives you loads of options to deal with such a situation. If you have example code we could narrow that down for you.
One option you could look at is the all operator:
>>> all([1,2,3,4])
True
>>> all([1,2,3,False])
False

You could also check for the length of the filtered list:
>>> input = [1,2,3,4]
>>> tested = [i for i in input if i > 2]
>>> len(tested) == len(input)
False

If you are using a for construct you can exit the loop early if you come across negative test:
>>> def test(input):
...     for i in input:
...         if not i > 2:
...             return False
...         do_something_with_i(i)
...     return True

The test function above will return False on the first value that's 2 or lower, for example, while it'll return True only if all values were larger than 2.",A,10
28799871,2015-03-01 22:21:02.593000+00:00,"You could use ps -C ""$process"" to find a process using its executable name (comm, not args).",A,0
22761938,2014-03-31 12:47:27.467000+00:00,"def update_file(path, entry):
    with open(path) as infile:
        data = json.load(infile)

    current_players = [member['account_id'] for member in entry.players]
    data[""matches""].append({
        ""match"": entry.match_id,
        ""winner"": entry.radiant_win, 
        ""players"": current_players
    })

    with open(path, ""w"") as outfile:
        json.dump(data, outfile)
",A,0
47001646,2017-10-29 14:57:54.847000+00:00,"This problem actually has nothing to do with template tags.
You've defined get_item_with_topic_id as an instance method, which takes self as its first argument. But you're calling it directly on the Comment class, so there is no instance involved and self is not passed - so Python thinks that you are passing the value of topic_id in the place of self and not passing topic_id at all.
You could fix this by declaring this as a classmethod and putting cls instead of self, but you shouldn't do that; custom query methods like this belong on the Manager, not the model. So:
class CommentManager(models.Manager):
  def get_item_with_topic_id(self, topic_id):
    comments = self.get(id=topic_id)
    return comments

class Comment(models.Model): 
  ...
  objects = CommentManager()

and now your template tag can call Comment.objects.get_item_with_topic_id.
(As an aside, are you sure that query is right? Should it not be .get(topic_id=topic_id)?)",A,0
5770156,2011-04-24 11:05:11.893000+00:00,"If you are using buildout (I do hope you are) the easiest way to get all the zodbconvert dependencies properly included in the python path is to have buildout create the script for you:
[buildout]
...
parts =
     ...
     zodbconvert

[zodbconvert]
recipe = zc.recipe.egg
eggs = ${buildout:eggs}
scripts = zodbconvert

Buildout then will create a new bin/zodbconvert script for you that includes all the buildout eggs in sys.path.
Alternatively, you can create a generic python script runner that includes all eggs in your buildout and can run arbitrary scripts; you can use this instead of the bare-bones python interpreter to run arbitrary python scripts with all the buildout eggs in sys.path:
[buildout]
...
parts =
    ...
    zopepy

[zopepy]
recipe = zc.recipe.egg
eggs = ${buildout:eggs}
interpreter = zopepy
scripts = zopepy

The bin/zopepy script can then be use to run arbitrary python scripts with all your buildout eggs already in sys.path, so bin/zopepy zodbconvert.py fstodb.conf should work.
Note that the Plone unified installer already comes with the zopepy part included, and my choice of partname for this script was deliberately using the same name.
If you are not using buildout (and with Plone 4, that's not a good idea on the whole), you can also list the required packages (ZODB3, zope.interface, RelStorage, psycopg2) in your PYTHONPATH environment variable.",A,5
24593951,2014-07-06 08:12:59.880000+00:00,"That is the correct way. The upload URL is handled by AppEngine itself: it will accept the file upload, then call your own handler directly.",A,2
12253341,2012-09-03 19:54:43.030000+00:00,"''.join() is a python function, not anything BS specific. It let's you join a sequence with the string as a joining value:
>>> '-'.join(map(str, range(3)))
'0-1-2'
>>> ' and '.join(('bangers', 'mash'))
'bangers and mash'

'' is simply the empty string, and makes joining a whole set of strings together into one large one easier:
>>> ''.join(('5', '4', 'apple', 'pie'))
'54applepie'

In the specific case of your example, the statement finds all text contained in the <td> element, including any contained HTML elements such as <b> or <i> or <a href=""""> and puts them all together into one long string. So td.find(text=True) finds a sequence of python strings, and ''.join() then joins those together into one long string.",A,5
24038993,2014-06-04 13:37:21.057000+00:00,"I really can't understand what you are trying to achieve here, so I can't tell you how to solve your problem. Identifying the problem, though, is simple: _basic_ingredients is a class-level variable, so will be shared by all members of the Article model, and will persist across requests until the process is killed or restarted.",A,0
2413248,2010-03-09 22:55:15.697000+00:00,"If this is for use within the Admin app, as you say in your answer to Jake, then you shouldn't override the model save method at all. Instead, you should override the save_model method of the ModelAdmin class. 
See the original code in django.contrib.admin.options - you'll see that it's already passed the request object. So all you need to do is to assign the publisher there:
def save_model(self, request, obj, form, change):
    obj.slug = slugify('%s' % (obj.question))
    if not obj.id:
        obj.publish_date = datetime.datetime.now()
        obj.publisher = request.user
    obj.modification_date = datetime.datetime.now()
    obj.modifier = request.user
    obj.save()
",A,2
49505060,2018-03-27 05:34:21.400000+00:00,"According to the docs:
col_values(colx, start_rowx=0, end_rowx=None)


Returns a slice of the values of the cells in the given column.

So, you don't pass it an iterable like i for i in range(29) (which, by the way, is just a more verbose and slower way to get the same values as just range(29)), you pass it a column number, start row, and end row.
If you want to get a list of column 0, column 1, column 2, etc., you can do that like this:
[file.sheet_by_name(names).col_values(i) for i in range(29)]

If you're trying to get a list of column 0, column 1, etc. just for a specific row, you can instead use the row_values function, passing the row number and then the start and end column.",A,1
49009675,2018-02-27 13:20:04.747000+00:00,"I don't see why you want to use range here at all.
if n in (0x0985,0x0986,0x0987,0x0988,0x0989):
",A,2
7241475,2011-08-30 09:56:33.680000+00:00,"
What is the alternative?

Do nothing.  Until you create millions of circular references like this -- and can prove that this (and only this) is breaking your program -- it doesn't actually matter.",A,2
9884862,2012-03-27 07:09:35.557000+00:00,The (populated) form is one of the elements passed in the kwargs to get_context_data. You can get it from there and call is_valid() on it.,A,1
12990639,2012-10-20 16:54:20.470000+00:00,"To add current process/thread name to a log message you could specify in the format string: 
%(processName)s %(threadName)s

To get them as strings:
process_name = multiprocessing.current_process().name
thread_name = threading.current_thread().name
",A,21
12249962,2012-09-03 14:40:56.437000+00:00,"You need to specify the whole path to the directory to be removed. Only the last part of the path will be deleted, the /home/dir/dir/ part will be untouched.
The deletion refers to any sub-directories contained within the named path, so if there is a /home/dir/dir/dirtoberemoved/foo sub-directory it'll be removed together with it's parent.",A,4
25627266,2014-09-02 15:40:09.073000+00:00,"You can uppercase just every second letter with an extended slice, picking every second letter:
>>> sample = 'Sunshine makes me happy, on a cloudy day'
>>> sample[::2].upper()
'SNHN AE EHPY NACOD A'
>>> sample[1::2].lower()
'usiemksm ap,o  luydy'

Now all you need to do is put those together again:
from itertools import izip_longest

result = ''.join([l 
    for pair in izip_longest(sample[::2].upper(), sample[1::2].lower(), fillvalue='') 
    for l in pair])

izip_longest() pairs up the uppercased and lowercased strings again, making sure that if there is an odd number of characters to pad out the series with an empty string.
Demo:
>>> from itertools import izip_longest
>>> ''.join([l 
...     for pair in izip_longest(sample[::2].upper(), sample[1::2].lower(), fillvalue='') 
...     for l in pair])
'SuNsHiNe mAkEs mE HaPpY, oN A ClOuDy dAy'

Note that whitespace isn't ignored here; the m of make is lowercased even though the e at the end of Sunshine is too.
If you need to vary the letters more precisely, you can make use of iteration still:
from itertools import cycle
from operator import methodcaller

methods = cycle((methodcaller('upper'), methodcaller('lower')))
result = ''.join([next(methods)(c) if c.isalpha() else c for c in sample])

Here itertools.cycle() lets us alternate between two operator.methodcaller() objects, which either upper or lowercase the argument passed in. We only advance to the next one (using next()) when the character is a letter. 
Demo:
>>> from itertools import cycle
>>> from operator import methodcaller
>>> methods = cycle((methodcaller('upper'), methodcaller('lower')))
>>> ''.join([next(methods)(c) if c.isalpha() else c for c in sample])
'SuNsHiNe MaKeS mE hApPy, On A cLoUdY dAy'
",A,4
35092094,2016-01-29 19:01:56.370000+00:00,"You are almost there. You only have one placeholder slot on your string, so you pass in just one argument. Your range() should start at 1 (the first argument):
def displayFractions(n):
    for i in range(1, n + 1):
        print('{:.3f}'.format(1/i), end=', ')

This does print one extra comma. You could ignore the last iteration in the loop and just add one extra print() call outside the loop to print the last fraction:
def displayFractions(n):
    for i in range(1, n):
        print('{:.3f}'.format(1/i), end=', ')
    print('{:.3f}'.format(1/n))
",A,0
32141496,2015-08-21 13:25:31.050000+00:00,"You're not ever putting the file contents into the response, so naturally it is 0 bytes. X-Sendfile is for a completely different purpose - when you're redirecting to a static server - and needs a URL, not a file path, anyway.
file_path = os.path.join(os.path.dirname(os.path.realpath(__name__)), 'EvalofSelf1.xlsx')
response = HttpResponse(open(file_path, 'rb').read())
response['Content-Type'] = 'mimetype/submimetype'
response['Content-Disposition'] = 'attachment; filename=DownloadedEval.xlsx'
",A,4
32860477,2015-09-30 07:38:40.820000+00:00,"If you are running main.py as the main entry script from the command line, then the whole namespace of the module is stored in the __main__ module, not the main module. This is also why the if __name__ == '__main__' test succeeds.
However, your frontend/helpers.py module imports from main, which as far as Python is concerned, a different module. You now created two distinct Flask instances, each living in a different module namespace.
In other words, __main__.app is being used to run your Flask server, but your before_request hook is registered with a different, separate main.app object.
Don't use the same module to run as a script and to create the Flask WSGI object. Use a separate module for this. Don't try to use from __main__ import app as that'll break when you deploy as to a WSGI container like gunicorn or mod_wsgi.",A,3
24069999,2014-06-05 20:48:42.543000+00:00,"You are ignoring the result of the recursive call; fileSize is not shared between calls.
Instead of passing in fileSize to recursive calls, sum the returned sizes:
def myFunction(myPath): 
    fileSize = 0

    for myfile in glob.glob(myPath): 
        if os.path.isdir(myFile):
            fileSize += myFunction(myFile)

        if fnmatch.fnmatch(myFile, '*.jpg'):
            fileSize += os.path.getsize(myFile)


    return fileSize

then print the final total returned by the outermost call:
print myFunction('C:/trial/')
",A,1
23691365,2014-05-16 01:43:27.453000+00:00,"As @Wooble mentioned in the comment, it might be python.el issue: C-c C-c runs
python-shell-send-buffer function:

python-shell-send-buffer is an interactive compiled Lisp function in
  `python.el'.
(python-shell-send-buffer &optional ARG)
Send the entire buffer to inferior Python process. With prefix ARG
  allow execution of code inside blocks delimited by ""if __name__=='__main__':""

i.e., to print ""indeed"", add prefix C-u C-c C-c.

Q: I have tried to dig through python.el, and I am still not sure how and where it does this. Can you explain, so I can modify the default behaviour?

To find out what C-c C-c does in your case open a python file and type M-x describe-key RET followed by C-c C-c (actually press the keys). By default it runs python-shell-send-buffer function in python.el. You could redefine the keys to call the function with an argument so that C-c C-c would behave like C-u C-c C-c that enables running ""if __name__=='__main__':"" part:
;; Make C-c C-c behave like C-u C-c C-c in Python mode
(require 'python)
(define-key python-mode-map (kbd ""C-c C-c"")
  (lambda () (interactive) (python-shell-send-buffer t)))
",A,4
22559153,2014-03-21 12:44:50.673000+00:00,"Your understanding is, unfortunately, completely wrong. Python does not copy the value, nor does it allocate space for a new one. It passes a value which is itself a reference to the object. If you modify that object (rather than rebinding its name), then the original will be modified.
Edit
I wish you would stop worrying about memory allocation: Python is not C++, almost all of the time you don't need to think about memory. 
It's easier to demonstrate rebinding via the use of something like a list:
def my_func(foo):
    foo.append(3)  # now the source list also has the number 3
    foo = [3]      # we've re-bound 'foo' to something else, severing the relationship
    foo.append(4)  # the source list is unaffected
    return foo


original = [1, 2]
new = my_func(original)

print original     # [1, 2, 3]
print new          # [3, 4]

It might help if you think in terms of names rather than variables: inside the function, the name ""foo"" starts off being a reference to the original list, but then we change that name to point to a new, different list.",A,7
12000891,2012-08-17 06:37:17.300000+00:00,"I think you need to look into the ZODB.
The ZODB is an object database that uses pickle to serialize data, is very adept at handling hierarchies of objects, and if your objects use the included persistent.Persistent base-class, will detect and only save the objects that changed when you commit; e.g. there is no need to write out the whole hierarchy on every little change.
Included in the ZODB project is a package called BTrees, which are ZODB aware and make storing thousands of objects in one place efficient. Use these for your Region object to store the Columns. We use BTrees to store millions of datapoints at times.",A,0
14696121,2013-02-04 21:38:26.113000+00:00,"You need to define the constants at module level, not in your socket class:
AF_INET = 2
SOCK_DGRAM = 2

class socket:
    def __init__(.....
",A,4
21210725,2014-01-18 22:30:34.273000+00:00,"Simple:
def both_decorators(func):
    return decorator1(decorator2(func))

because that's all decorators do, really.",A,5
37225073,2016-05-14 09:39:17.243000+00:00,"This problem does not apply solely to files, but to any case where you want the user to add an arbitrary number of any piece of data. And the solution is the same: define a separate table with a ForeignKey to the user.
class UserUpload(models.Model):
    file = models.FileField(...)
    description = models.CharField(...)
    user = models.ForeignKey(User)

Now you can get all a user's uploads with user.userupload_set.all().",A,1
25158732,2014-08-06 11:04:41.373000+00:00,"You can still access that identifier with getattr():
getattr(Cls, 'invalid identifier')

or directly on the class __dict__ mapping:
Cls.__dict__['invalid identifier']

You just cannot use direct attribute access as it is indeed not a valid identifier.",A,2
36726376,2016-04-19 18:17:00.700000+00:00,"No, there is no difference in how zip() operates with 2 or more lists. Instead, your list comprehension assigned to the name sm in the loop, while at the same time you used the name sm to reference the statsmodels module.
Your simpler two-list version doesn't do this, so the name sm isn't rebound, and you don't run into the issue.
In Python 2, names used in the list comprehension are part of the local scope:
>>> foo
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'foo' is not defined
>>> [foo for foo in 'bar']
['b', 'a', 'r']
>>> foo
'r'

Here the name foo was set in the for loop of the list comprehension, and the name is still available after the loop.
Either rename your import, or rename your loop variables.",A,1
27885587,2015-01-11 09:21:42+00:00,"An alternative solution is to keep a running total:
balance = 0
for month in range(1, month):
    balance += income
    balance -= spend
    print...
",A,0
33010488,2015-10-08 08:29:17.267000+00:00,"In Python 3.5+, there is namereplace error handler. You could use it to convert several emoticons at once:
>>> import re
>>> my_text =""\U0001F601, \U0001F602, ♥ and all of this \U0001F605""
>>> re.sub('[\U0001F601-\U0001F64F]+',
...        lambda m: m.group().encode('ascii', 'namereplace').decode(), my_text)
'\\N{GRINNING FACE WITH SMILING EYES}, \\N{FACE WITH TEARS OF JOY}, ♥ and all of this \\N{SMILING FACE WITH OPEN MOUTH AND COLD SWEAT}'

There are more Unicode characters that are emoji than the regex pattern indicates e.g., ♥ (U+2665 BLACK HEART SUIT).",A,0
44702060,2017-06-22 14:17:57.937000+00:00,"Use the itertools.groupby() object to both group the consecutive None values, and to count the group length:
[sum(1 for _ in g) for k, g in groupby(input) if k is None]

Since g is a iterator of unknown length, I used sum() to avoid materialising that into a list; that could be quite memory intensive otherwise. If you are 100% certain that your groups of consecutive None objects is small (a few hundred at most) it may be faster to just use len(list(g)) instead.
The default behaviour for groupby() is to produces groups of identical objects, so consecutive objects for which an is test is true. This is ideally suited for None values. For each group, a k and a dependent iterator are produced, with k set to the common group object; if k is None you know you have a series of consecutive None objects on hand.",A,2
1420994,2009-09-14 11:26:40.050000+00:00,"Don't do this.
Use inheritance.
import some_module

class MyVersionOfAClass( some_module.AClass ):
    def someMethod( self, *args, **kwargs ):
        # do your ""decoration"" here.
        super( MyVersionOfAClass, self ). someMethod( *args, **kwargs )
        # you can also do ""decoration"" here.

Now, fix you main program to use MyVersionOfAClass instead of some_module.AClass.",A,8
375775,2008-12-17 19:43:06.040000+00:00,"""how to convince IT that F/OSS software isn't (automatically) less trustworthy than any other software just because it's free/oss.""
""How can I promote the acceptance of OSS at my company?""
You can't.
All you can do is the following.

Find the  F/OSS they currently use.  This can be hard.  In some cases, it's trivial because many folks use Apache and Java without thinking about it.
Ask how is what you're going to use different than what they're already using?

That will make the case for exactly one new piece of F/OSS.  Or, they'll go crazy and banish stuff they've been using.
You can't make a general understanding happen.  You can only make the case one specific detailed case at a time until someone else starts to piece the big picture together on their own.",A,1
34207195,2015-12-10 16:40:11.730000+00:00,"You essentially have word-counter pairs. Using collections.Counter() lets you handle those in a natural, Pythonic way:
from collections import Counter

c = (Counter(dict(a)) + Counter(dict(b))).items()

Also see Is there any pythonic way to combine two dicts (adding values for keys that appear in both)?
Demo:
>>> from collections import Counter
>>> a = [('apple', 10), ('of', 10)]
>>> b = [('orange', 10), ('of', 7)]
>>> Counter(dict(a)) + Counter(dict(b))
Counter({'of': 17, 'orange': 10, 'apple': 10})
>>> (Counter(dict(a)) + Counter(dict(b))).items()
[('orange', 10), ('of', 17), ('apple', 10)]

You could just drop the .items() call and keep using a Counter() here.
You may want to avoid building (word, count) tuples to begin with and work with Counter() objects from the start.",A,3
21001204,2014-01-08 16:28:36.590000+00:00,"The len() function will cast anything the __len__ method returns to int, using the PyInt_AsSsize_t() API function:

Will first attempt to cast the object to a PyIntObject or PyLongObject, if it is not already one, and then return its value as Py_ssize_t.

where Py_ssize_t is your architecture pointer size (64 or 32 bit usually). See the source for the type.slot_sq_length slot.
Because you subclassed int but did not provide a __new__ method, it has the the default value 0 when cast back to int().
You cannot hope to get your custom type through the len() function as a return value; len() always returns an int(); the best you can hope to achieve is to return sys.maxsize.
If your object is meant to be unsized, you probably should not implement a __len__ at all. Make your object an iterator instead.",A,1
20403350,2013-12-05 15:02:24.137000+00:00,"You can call the super-class implementation in func1:
class B(A):
    def func1(self):
        super(B, self).func1()
        op3()

Here super(B, self).func1 will search the class hierarchy in MRO (method resolution order), from class B onwards for the next func1() method, bind it to self and let you call it. 
This will invoke the original func1() first, executing op1() and op2(), and your overridden version can then add new operations to follow.
You could also call op3() first, then invoke the base implementation.
You can also look up the parent method directly on class A, but that method will then be unbound. Pass in self explicitly:
class B(A):
    def func1(self):
        A.func1(self)
        op3()

This bypasses searching the class inheritance hierarchy making your class less flexible.
See Raymond Hettinger's blog post on super() for some excellent practical advice on when to use super().",A,7
26899603,2014-11-13 01:01:37.587000+00:00,"Without seeing any documentation or source code, this is a wild guess, but…
I'll bet the playSongNumber function just sends the robot the play command, without waiting for it to finish, and when you send it a play command while it's already playing something, it ignores you.
If I'm right, then hopefully the API provides some way to wait until it's done. If it doesn't provide that, but it does provide a way to poll the current state, you can do that in a loop (sleeping briefly between each check) until it's done. If it doesn't even provide that, then you have to work out how long the song will take by counting the number of notes (or adding the durations, if one of the numbers in each of those pairs is a duration) and sleep that long.",A,0
22731426,2014-03-29 13:02:03.150000+00:00,"In general, self.model.optimize(self.mycallback) should work (note: no parens after mycallback).
It may fail if the code serializes the callable e.g., to be send via pipe/socket to another process (even on different machine):
from multiprocessing import Pool

class C:
    def method(self, i):
        return ""called"", i

if __name__==""__main__"":
    print(Pool().map(C().method, range(10)))

It works on recent Python versions where methods are pickable.
Or it may fail if model.optimize() has a bug and check for the exact function type instead of accepting any callable.",A,1
16769208,2013-05-27 08:31:53.310000+00:00,"Filter the list afterwards; either turn the filename portion to an int or create a range of strings that are to be included:
included = {'d{:03d}'.format(i) for i in range(15, 26)}  # a set

binary = sorted(f for f in glob.glob('C:/Users/Desktop/bin/*.bin') if f[21:25] in included) 

The above code generates the strings 'd015'  through to 'd025' as a set of strings for fast membership testing, then tests the first 4 characters of each file against that set; because glob() returns whole filenames we slice off the path for that to work.
For variable paths, I'd store the slice offset, for speed, based on the path:
pattern = 'C:/Users/Desktop/bin/*.bin'
included = {'d{:03d}'.format(i) for i in range(15, 26)}  # a set
offset = len(os.path.dirname(pattern)) + 1

binary = sorted(f for f in glob.glob(pattern) if f[offset:offset + 4] in included) 

Demo of the latter:
$ mkdir test
$ touch test/d014-recomb.bin
$ touch test/d015-recomb.bin
$ touch test/d017-recomb.bin
$ touch test/d018-recomb.bin
$ fg
bin/python2.7
>>> import os, glob
>>> pattern = '/tmp/stackoverflow/test/*.bin'
>>> included = {'d{:03d}'.format(i) for i in range(15, 26)}  # a set
>>> offset = len(os.path.dirname(pattern)) + 1
>>> sorted(f for f in glob.glob(pattern) if f[offset:offset + 4] in included)
['/tmp/stackoverflow/test/d015-recomb.bin', '/tmp/stackoverflow/test/d017-recomb.bin', '/tmp/stackoverflow/test/d018-recomb.bin']
",A,0
747028,2009-04-14 10:35:10.273000+00:00,"Usually, we use the setattr function directly.
setattr( entity_two, 'some_variable', some_value )
",A,3
26940420,2014-11-14 23:18:12.100000+00:00,"The print ""magic comma"" always inserts spaces, so you can't do things this way.
You have three choices:

Join the words up into a string first, then print that string: print ''.join(word[:2].lower() for word in namelist).
Write directly to stdout instead of using print: sys.stdout.write(word[:2].lower())
Use Python 3-style print, which can do things this way. First, from __future__ import print_function at the top of your code. Then, print(word[:2].lower(), end='').
",A,2
20937365,2014-01-05 18:22:23.587000+00:00,"You need to import all global names mentioned in the test:
print(timeit.timeit(""test(f0)"", setup=""from __main__ import test, f0""))

The line test(f0) needs to look up f0 too, not just test().",A,1
29154773,2015-03-19 20:51:50.850000+00:00,"You assigned None to Add_user; ttk.Button.grid() returns None:
Add_user = ttk.Button(...).grid(row=1, column=0)

You should not use the same name for the button reference and the function; Python will use the local variable in this case, not the global function.
Use a different name, and call .grid() separately:
add_user_button = ttk.Button(
    frame_27, text=""Add User"", 
    command=lambda: Add_user(frame_27, data_dictionary, row_num))
add_user_button.grid(row=1, column=0)

The same applies to the other buttons.
If, however, you are not using the add_user_button reference anywhere else, you can make it one line, but you don't have to bother about assigning the result:
ttk.Button(
    frame_27, text=""Add User"", 
    command=lambda: Add_user(frame_27, data_dictionary, row_num)
).grid(row=1, column=0)
",A,3
16016463,2013-04-15 13:30:29.333000+00:00,"Use type=float:
parser.add_argument(""-e"", nargs=6, metavar=('a', 'b', 'c', 'd', 'e', 'f'),
                        help=""my help message"", type=float,
                        default=None)

Demo:
>>> import argparse
>>> parser = argparse.ArgumentParser(description='Process some floats.')
>>> parser.add_argument(""-e"", nargs=6, metavar=('a', 'b', 'c', 'd', 'e', 'f'),
...                         help=""my help message"", type=float,
...                         default=None)
_StoreAction(option_strings=['-e'], dest='e', nargs=6, const=None, default=None, type=<type 'float'>, choices=None, help='my help message', metavar=('a', 'b', 'c', 'd', 'e', 'f'))
>>> try:
...     parser.parse_args('-e 1.0 2.0 3.33 4.45 5.15 6.0'.split())
... except SystemExit:
...     pass
... 
Namespace(e=[1.0, 2.0, 3.33, 4.45, 5.15, 6.0])
>>> try:
...     parser.parse_args('-e foo bar baz spam ham eggs'.split())
... except SystemExit:
...     pass
... 
usage: [-h] [-e a b c d e f]
: error: argument -e: invalid float value: 'foo'
",A,5
16170462,2013-04-23 13:20:34.437000+00:00,"You can detect if something is a property, by looking at the same attribute on the class:
class_attribute = getattr(type(instance), some_key, None)
if isinstance(class_attribute, property):
    # this is a property
    if class_attribute.fset is None:
        print ""Read-only""

You can also test .fget and .fdel to test if the property has a getter and a deleter, respectively.
However, you can always catch the AttributeError exception to deal with the setter missing:
>>> class Foo(object):
...     @property
...     def bar(self):
...         return 'spam'
... 
>>> f = Foo()
>>> class_attribute = getattr(type(f), 'bar', None)
>>> isinstance(class_attribute, property)
True
>>> class_attribute.fget
<function bar at 0x10aa8c668>
>>> class_attribute.fset is None
True
>>> try:
...     f.bar = 'baz'
... except AttributeError:
...     print 'Read-only'
... 
Read-only
",A,2
14823745,2013-02-12 00:52:58.913000+00:00,"It looks like your actual problem was just an intermittent download failure from Sourceforge:
curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume. Error: Download failed: downloads.sf.net/project/machomebrew/Bottles/…

Homebrew should just recover automatically from this error if you try brew install gfortran again. So, that's the first thing to try.
If that doesn't work, see if brew doctor finds any problems, then brew install -d gfortran to see where it's storing the partially-downloaded file so you can delete it manually and try again.
If all else fails, you can force it to not use the bottle by using --build-from-source. Of course building from source takes a lot longer than installing a binary bottle, but it should give the same result.",A,3
11696093,2012-07-27 21:49:45.950000+00:00,"data is just a nested dictionary. That is, it's just the same as any other element of the dictionary, except that its value is itself a dictionary.
If you were building up the object line by line (which you would almost certainly never do) you might do something like this:
obj = {}

obj[""collapse_key""] = ""Food-Promo""
obj[""time_to_live""] = 3600,
obj[""delay_while_idle"" : ""true"",

data = {}
data[""Category""] = ""FOOD""
data[""Type""] = ""VEG""

obj[""data""] = data

obj[""registration_ids""] = [""APA4lj5jl54l2h...""]
",A,0
51992318,2018-08-23 18:40:30.723000+00:00,"The problem is that Line isn't a name for a type at all within point.py, and Point isn't a name for a type within line.py. Putting it in quotes doesn't help; that just delays when the string is resolved. It still has to eventually resolve to a type, so you're just delaying things to a point where they resolve to a NameError, which doesn't help thing.
If you do an import line in Point and import point in Line, then point.Point and line.Line become types, which solves that problem. But of course it creates a new problem: a circular import.

In some cases, as explained in PEP 484, you can resolve that just by doing a conditional ""static-typing-only"" import, like this:
import typing
if typing.TYPE_CHECKING:
    import line

… and then using 'line.Line' in the type annotation.
See the docs on TYPE_CHECKING for more details. (In particular, if you need compatibility with Python 3.5 before 3.5.2, this will give you a NameError instead of False at runtime, which is a pain… but not many people need to run on 3.5.1.)

If that doesn't solve the problem, you need some scheme to avoid the circular import, the same way you would for a runtime circular import.
For example, you can go with the traditional ""interface"" solution where the first type depends on the second, but the second doesn't depend on the first, it depends only on a superclass for the first.
Typically, this superclass would be an ABC, to indicate that it's serving only as an interface to some real class defined elsewhere. (And that way, you know, and Python and your static checker can enforce, that anything that types as the ABC must be an instance of one of the concrete subclasses of that ABC—of which there's only one.)
# pointbase.py
import abc
class PointBase(abc.ABC):
    @abc.abstractmethod
    def method_that_does_not_need_line(self):
        pass

# point.py
import pointbase
import line
class Point(pointbase.PointBase):
    def method_that_does_not_need_line(self):
        do_stuff()
    def method_that_does_need_line(self, line: line.Line):
        do_stuff(line)

# line.py
import pointbase
class Line:
    def method_that_needs_point(self, point: pointbase.PointBase):
        do_stuff(point)
",A,1
11774931,2012-08-02 09:45:50.063000+00:00,"There should never be any need in Python to use a method call just to get an attribute. The people who have written this are probably ex-Java programmers, where that is idiomatic. 
In Python, it's considered proper to access the attribute directly.
If it turns out that you need some code to run when accessing the attribute, for instance to calculate it dynamically, you should use the @property decorator.",A,11
9011487,2012-01-25 22:41:12.477000+00:00,"To process groups of files in sequence while working on files within a group in parallel:
#!/usr/bin/env python
from multiprocessing import Pool

def work_on(args):
    """"""Process a single file.""""""
    i, filename = args
    print(""working on %s"" % (filename,))
    return i

def files():
    """"""Generate input filenames to work on.""""""
    #NOTE: you could read the file list from a file, get it using glob.glob, etc
    yield ""inputfile1""
    yield ""inputfile2""

def process_files(pool, filenames):
    """"""Process filenames using pool of processes.

    Wait for results.
    """"""
    for result in pool.imap_unordered(work_on, enumerate(filenames)):
        #NOTE: in general the files won't be processed in the original order
        print(result) 

def main():
   p = Pool()

   # to do ""successive"" multiprocessing
   for filenames in [files(), ['other', 'bunch', 'of', 'files']]:
       process_files(p, filenames)

if __name__==""__main__"":
   main()

Each process_file() is called in sequence after the previous one has been complete i.e., the files from different calls to process_files() are not processed in parallel.",A,1
45688177,2017-08-15 07:23:08.153000+00:00,"Because it's not a field on User. You need to do the same as you have for birth_date - declare it separately in the form class, and save it explicitly to the profile in the view.",A,0
15636737,2013-03-26 12:08:52.910000+00:00,"You need to tell print to use a space instead of a newline after each printed line:
if t2ht in proovitud_t2hed:
    print(t2ht, end=' ')
else:
    print(""_"", end=' ')
print()

with an extra print() at the end to print a final newline.
Alternatively, pass a joined string to print():
print(' '.join(t2ht if t2ht in proovitud_t2hed else '_' for t2ht in s6na))
",A,3
606073,2009-03-03 11:47:24.647000+00:00,"The data warehousing folks have several algorithms for ""slowly-changing dimensions"".
The more sophisticated algorithms provide data ranges around a dimension value to indicate when it's valid.
Depending on your versioning requirements you could do one of these things, cribbed from Kimball's The Data Warehousing Toolkit.

Assign a version number to rows of the structure table.  This means you have to do some reasoning to collect a a complete structure.  It includes the selected version number unioned with rows that are unchanged in an earlier version.
Assign a date range or version range to rows of the structure table.  This means that some rows have start dates and end dates; some rows will have end dates at some epoch in the impossible future.  Or, if you use version numbers, you'll have a start-end pair or a start-infinity pair that indicates this row is still current.  You can then trivially query the rows that are valid ""today"" or apply to the requested version.
Clone the structure for each version.  This unpleasant because the clone operation is costly.  The queries however, are trivial because the entire structure is available with a single, consistent version number.
",A,6
30138567,2015-05-09 10:02:54.297000+00:00,"Migrations are a cumulative record. The initial migration describes the state of your database when you initially created it. Subsequent changes are described by further migration files, each of which takes your database to the state at the time that migration was created.
So when you run makemigrations, a migration 0002 will be created which will include the code to remove your field.",A,1
12721741,2012-10-04 06:48:35.820000+00:00,"Python used to lack booleans, we just used integers, 0 for False and any other integer for True. So when booleans were added to the language, the values False and True, can be treated as the integer values 0 and 1 still by the interpreter, to help backwards compatibility. Internally, bool is a sub-class of int.
In other words, the following equations are True:
>>> False == 0
True
>>> True == 1
True
>>> isinstance(True, int)
True
>>> issubclass(bool, int)
True

and as you found out:
>>> True * 3
3

This doesn't extend to strings however.",A,3
11350284,2012-07-05 18:24:25.903000+00:00,"houbysoft's answer above is correct, but the OP needs a little more detail.
The basic idea is that you need the ""Command Line Tools"" environment installed. But the name for that environment, and how you install it, has changed multiple times over the years. So, houbysoft's steps work for 4.3.3, but not for 3.2.6.
In the 3.2 era, the tools were called ""UNIX Development"". And, rather than being downloaded and installed from within Xcode, they came as part of the Xcode mpkg and were installed as part of the initial Xcode install. In most versions, there was a checkbox named ""UNIX Development"", usually checked by default, but in some versions it was a separate step. Looking at the ""Xcode 3.2.6 and IOS SDK 4.3"" disk image currently available from Apple, it's a checkbox. Anyway, that's the only officially-supported way to get them, but lots of other ways work… Here are your options:

Throw away Xcode 3.2.6 (just trash the whole /Developer) and install 4.3.3, then follow houbysoft's steps to get the Command Line Tools. Unless there's a good reason you can't use 4.3, this is almost certainly the best answer.
Throw away Xcode 3.2.6 and reinstall it, and this time make sure the ""UNIX Development"" checkbox is on.
Just re-run the 3.2.6 installer, and make sure the ""UNIX Development"" checkbox is on.
Log into developer.apple.com and look through the downloads for a package named ""UNIX Development Tools"", ""Developer Tools CLI"", ""CLI Developer Environment"", ""Command Line Tools"", or similar that corresponds to 3.2.6. There is such a package for most, but not all, versions of Xcode, and can be used to set up a command-line build environment either with or without Xcode.
Look at the invisible packages on the Xcode disk image. I don't remember whether you need just DeveloperToolsCLI.pkg, or that plus a few others, but a bit of trial and error or googling should get you there.
",A,3
29252919,2015-03-25 10:19:46.563000+00:00,"None of !, && and || are valid Python operators; eval() can only handle valid Python expressions.
You'd have to replace those expressions with valid Python versions; presumably ! is not, && is and, and || is or, so you could just replace those with the Python versions:
eval(string.replace('&&', 'and').replace('||', 'or').replace('!', 'not '))

Note the space after not because Python requires this.
The better approach would be to not use the wrong spelling for those operators (they look like Java or JavaScript or C).",A,2
11393723,2012-07-09 10:58:54.437000+00:00,"To replace parts of a python list, you can use slice assignment:
>>> array1 = ['A', 'B', 'C', 'D', 'E', 'F']
>>> array2 = ['G', 'H', 'I']
>>> array1[-1:] = array2
>>> array1
['A', 'B', 'C', 'D', 'E', 'G', 'H', 'I']

You can use slice assignment to replace any part of a list, including insertion of lists where you do not replace existing elements:
>>> array1[0:0] = ['1st', '2nd']
>>> array1
['1st', '2nd', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'I']

Here the [0:0] slice selects an empty part of array1 and ""replaces"" it with the new elements.",A,16
14608663,2013-01-30 16:19:16.520000+00:00,"I'm not entirely sure I understand what you're trying to do, but you would probably just pass an integer index. You can't pass a reference to the the as the two are both exactly the same.",A,0
23490068,2014-05-06 08:51:34.800000+00:00,"Switch to itertools.dropwhile() to skip everything under 3 digits, then take the next value after that with next():
print next(itertools.dropwhile(lambda x: x < 100, fib_function()))

I changed the digits test to test for 100 instead; this saves you 2 function calls.
Demo:
>>> print next(itertools.dropwhile(lambda x: x < 100, fib_function()))
144
",A,4
22179410,2014-03-04 17:56:08.163000+00:00,"You could use str.center():
for i in range(1, base + 1, 2):
    print ('*' * i).center(base)

but do use a step size of 2, and adjust your range. The first line starts with 1 star always, and range() doesn't include the last value.
For 7, that means you want to print 1, 3, 5 and 7 stars, incrementing by 2 each iteration.",A,2
12339023,2012-09-09 12:22:53.560000+00:00,"Each episode element has child-elements, including a title element. Your code, however, is looking for attributes instead.
To get text out of a minidom element, you need a helper function:
def getText(nodelist):
    rc = []
    for node in nodelist:
        if node.nodeType == node.TEXT_NODE:
            rc.append(node.data)
    return ''.join(rc)

And then you can more easily print all the titles:
for episode in xml.getElementsByTagName('episode'):
    for title in episode.getElementsByTagName('title'):
        print getText(title)
",A,1
3815608,2010-09-28 17:52:47.083000+00:00,"Firstly, defining extra fields in a linking table is what the through functionality of ManyToManyField is for. So, keep your ItemListOwnership table with its FKs, but add a userlist=ManyToMany('User', through='ItemListOwnership') to UserList.
Once you've done this, you can easily count the number of items for each user by using annotate:
from django.db.models import Count
User.objects.all().annotate(item_count=Count('userlist__item'))

Now each user has an item_count attribute which is the number of items they have.",A,3
410284,2009-01-04 02:03:01.520000+00:00,Examine https://docs.python.org/library/queue.html to see if it meets your needs.,A,7
29957970,2015-04-30 02:24:14.700000+00:00,"You have a list, not an array. If you actually need to slice the list and build a new list, that's going to take linear time, no matter how you do it. The answer by thefourtheye is probably the best you're going to do:
small_primes = primes[:bisect.bisect(primes, n)]

If you have NumPy, it knows how to create views that look like slices, but actually reference rather than copying the data. In fact, if primes were an ndarray, you could use exactly the same code as thefourtheye's answer and it would be O(log N).
small_primes = primes[:bisect.bisect(primes, n)]

If you only need to iterate the ""array"" once rather than use it as a list, you can use a lazy iterator:
small_primes = itertools.takewhile(lambda p: p<n, primes)

Now the up-front time cost is 0; but there's a comparison attached to each value at the time you consume it. Of course it's more space-efficient than anything else.
Realistically, ""most efficient"" here is unlikely to matter, and if it does matter you'll need to measure it, and if you're not using NumPy or running your code under PyPy you almost certainly want to do one of those before any micro-optimizations…",A,5
24594366,2014-07-06 09:19:33.653000+00:00,"You cannot match all possible top-level domains with a regex, and the list of what is a TLD changes from time to time.
Use a library to extract the TLD instead, like tldextract or publicsuffix.
Demo:
>>> import tldextract
>>> tldextract.extract('domainname.net.pl')
ExtractResult(subdomain='', domain='domainname', suffix='net.pl')
>>> tldextract.extract('www.domainname.net.pl')
ExtractResult(subdomain='www', domain='domainname', suffix='net.pl')
>>> from publicsuffix import PublicSuffixList
>>> psl = PublicSuffixList()
>>> psl.get_public_suffix('domainname.net.pl')
'domainname.net.pl'
>>> psl.get_public_suffix('www.domainname.net.pl')
'domainname.net.pl'

tldextract gives you a parsed result, while publicsuffix merely gives you the domain name that still is considered one entity.",A,5
39491156,2016-09-14 13:02:14.703000+00:00,"Strings are immutable. replace does not work in-place, it returns a new string. You need to reallocate that new string to the original name.
if x in z:
    z = z.replace(x,""Firstname"")

(Also, please use more than one space indentation.)",A,2
14793196,2013-02-10 00:19:17.430000+00:00,"Your my_hmac variable was a class variable; one shared by all instances of the class.
This means that for every call to .get_signed_url() the self.my_hmac.update() call would add data to the digest, across instances, globally to the application.
Since you only want to calculate the digest for one string only, (the ""GET ..."" string) and not for all strings cumulatively, you must create a new hmac object for each new digest you want to calculate.
Note the documentation for the .update()` method:

Update the hash object with the string arg. Repeated calls are equivalent to a single call with the concatenation of all the arguments: m.update(a); m.update(b) is equivalent to m.update(a+b).
",A,4
46283680,2017-09-18 16:00:37.273000+00:00,"self.fields is a normal dict, you can just iterate over its values:
for field in self.fields.values():
    field.widget.attrs['class'] = 'class1'
    field.required = False
",A,3
263068,2008-11-04 19:27:27.463000+00:00,"No process can change its parent process (or any other existing process' environment).
You can, however, create a new environment by creating a new interactive shell with the modified environment.
You have to spawn a new copy of the shell that uses the upgraded environment and has access to the existing stdin, stdout and stderr, and does its reinitialization dance.
You need to do something like use subprocess.Popen to run /bin/bash -i.
So the original shell runs Python, which runs a new shell.  Yes, you have a lot of processes running.  No it's not too bad because the original shell and Python aren't really doing anything except waiting for the subshell to finish so they can exit cleanly, also.",A,17
26856125,2014-11-11 01:41:09.287000+00:00,"
I'm trying to replace 'RNA' but my dictionary has 'rna'. Is there a way I can ignore the case.

Sure. Just call casefold on each key while creating the dictionary, and again while looking up values:
mydict = {rows[0].casefold(): rows[1] for rows in reader}

# ...

da = ''.join( mydict.get(word.casefold(), word) for word in re.split( '(\W+)', s ) )

If you're using an older version of Python that doesn't have casefold (IIRC, it was added in 2.7 and 3.2, but it may have been later than that…), use lower instead. It won't always do the right thing for non-English characters (e.g., 'ß'.casefold() is 'ss', while 'ß'.lower() is 'ß'), but it seems like that's OK for your application. (If it's not, you have to either write something more complicated with unicodedata, or find a third-party library.)


Also, I don't want it to replace 'corna' (I know such a word doesn't exist, but I want to make sure it doesn't happen) with 'coribonucleic acid'.

Well, you're already doing that with your re.split, which splits on any ""non-word"" characters; you then look up each resulting word separtely. Since corna won't be in the dict, it won't be replaced. (Although note that re's notion of ""word"" characters may not actually be what you want—it includes underscores and digits as part of a word, so rna2dna won't match, while a chunk of binary data like s1$_2(rNa/ might.)

You've also got another serious problem in your code:
ss = open (""trial.csv"", ""r"").readlines()
s = str(ss)

Calling readlines means that ss is going to be a list of lines. Calling str on that list means that s is going to be a big string with [, then the repr of each line (with quotes around it, backslash escapes within it, etc.) separated by commas, then ]. You almost certainly don't want that. Just use read() if you want to read the whole file into a string as-is.

And you appear to have a problem in your data, too:
rna,ibonucleic acid

If you replace rna with ibonucleic acid, and so forth, you're going to have some hard-to-read output. If this is really your dictionary format, and the dictionary's user is supposed to infer some logic, e.g., that the first letter gets copied from the abbreviation, you have to write that logic. For example:
def lookup(word):
    try:
        return word[0] + mydict[word.casefold()]
    except KeyError:
        return word
da = ''.join(lookup(word) for word in re.split('(\W+), s))


Finally, it's a bad idea to use unescaped backslashes in a string literal. In this case, you get away with it, because Python happens to not have a meaning for \W, but that's not always going to be true. The best way around this is to use raw string literals, like r'(\W+)'.",A,1
1196815,2009-07-28 21:22:31.460000+00:00,"""I appended to sys.path ...""
Please don't.
Set the PYTHONPATH environment variable from outside your application.",A,-2
19839621,2013-11-07 15:18:14.150000+00:00,"You are using conditional expressions without an else part; that's a syntax error.
The correct translation is:
[(val,col) if val in span[k] else (val,row) for val in G[k] if val in span[k] or S[val][k] != 'col']

e.g. filter out anything that doesn't match your two conditions first, and select between the two branches for values that do result in something added to the output.
I simplified the conditions; there was some redundant testing in the second expression (val in G[k] is always true for a loop over G[k] and if val in span[k] is not True then the inverse val not in span[k] is certainly True as well and doesn't need to be tested for again.
Personally, I find the explicit for loop more readable, but you can at least simplify it in the same manner:
for val in G[k]:
    if val in span[k]:
        result.append((val,col))
    elif S[val][k] != 'col':
        result.append((val,row))
",A,2
41448953,2017-01-03 17:21:25.073000+00:00,"You are reading text with line separators. Your lines don't look like this:
tom@gmail.com,Password1

They actually look like this:
tom@gmail.com,Password1\n

When splitting that on the comma, you get this:
line = ['tom@gmail.com', 'Password1\n']

and the test 'Password1' in line fails, but 'Password1\n' in line would succeed.
The \n is a newline character. You'd need to remove that first; you could use str.strip() to handily remove all whitespace from the start and end:
for line in file:
    line = line.strip().split("","")
    if email in line:
        print(""Correct email"")
    if password in line:
        print(""Correct Password"")

Rather than manually split, you can use the csv module to read your file:
import csv

with open(""logindata.txt"", ""r"") as file:
    reader = csv.reader(file)
    for user, pw in reader:
        if email == user:
            print(""Correct email"")
        if password == pw:
            print(""Correct Password"")
",A,2
19434203,2013-10-17 18:15:29.653000+00:00,"You're never actually closing fp_numpy, so the script can be empty or incomplete at the time you try to run it.
It isn't guaranteed to be empty, but it's very likely. When I try this on two different *nix computers with 7 different versions of Python, it's empty every time… (The fact that, after your script finishes, the file gets closed, and therefore flushed, makes the problem harder to debug.)
The best way to fix this is to use a with statement, so it's impossible to forget to close the file:
with open(""numpy_temp.py"", ""w"") as fp_numpy:
    fp_numpy.write(numpy_temp)


But beyond that, you've got another problem. If the generated script raises an exception, it will print nothing to stdout, and will dump a traceback to stderr, which you will read and ignore. It's very hard to debug problems when you're ignoring the errors… I don't know what you're passing for x and y, but if you're, say, passing a numpy.array instead of a string that evaluates to one, you could easily get an exception and never see it. Either send stderr to stdout, or print ""err"", err at the end.
And finally, you really shouldn't be using a command string and shell=True here, because you end up with an extra level of indirection for no good reason, which can also make things harder to debug. Just do this:
cmd = [""/remote/Python-2.7.2/bin/python"", ""numpy_temp.py""]
proc = subprocess.Popen(cmd, stdout = subprocess.PIPE,
                        stderr = subprocess.PIPE)
",A,2
42212227,2017-02-13 19:41:32.607000+00:00,"Unittest doesn't have this functionality. You need to use the third-party pytest module, which has extensive support for parameterised testing.",A,1
18667977,2013-09-06 23:20:33.237000+00:00,"This is explained in the FAQ, under How do I create a multidimensional list?
The problem is in this part of the code:
board = []
for i in range(7):
    board.append(line)

You're creating a list with 7 references to the same list, line. So, when you modify one, the others all change, because they're the same list.
The solution is to create 7 separate lists, like this:
def createBoard(self):
    board = []
    for i in range(7):
        line = []
        for i in range(7):
            line.append(' ')
        board.append(line)
    return board

Or, more simply, make separate copies of the original list:
def createBoard(self):
    line = []
    for i in range(7):
        line.append(' ')
    board = []
    for i in range(7):
        board.append(line[:])
    return board


While we're at it, you could simplify this tremendously by using list comprehensions:
def createBoard(self):
    return [[' ' for j in range(7)] for i in range(7)]


Or, as the FAQ suggests, you might do better to use a smarter multidimensional array object, like the ones numpy or pandas provide:
def createBoard(self):
    return np.tile(' ', (7, 7))

The disadvantage is that you will need to install numpy, because there's nothing in the standard library that works like this. But the advantage is that you have powerful tools for dealing with arrays—a[1, 1] isn't much simpler than a[1][1], but a[:,1] to access the second column is a lot simpler than [row[1] for row in a].",A,6
47669710,2017-12-06 08:17:33.780000+00:00,"sum() has a starting value, from which it starts the sum. This also provides a default if there are no values to sum in the first argument. That starting value is 0, an integer.
From the sum() function documentation:

sum(iterable[, start])
Sums start and the items of an iterable from left to right and returns the total. start defaults to 0.

When summing Counter objects, give it an empty Counter() to start off with:
sum((Counter(composition[aa]) for aa in each), Counter())

If you then assign the result to a key in a dictionary assigned to out you get your expected result as Counter instances:
>>> out = {}
>>> for each in string_input:
...     out[each] = sum((Counter(composition[aa]) for aa in each), Counter())
...
>>> out
{'ACDE': Counter({'H': 22, 'C': 15, 'O': 8, 'N': 4, 'S': 1}), 'CCCDA': Counter({'H': 25, 'C': 16, 'O': 7, 'N': 5, 'S': 3})}
",A,5
18905400,2013-09-19 21:42:15.983000+00:00,"You are missing a closing parenthesis in your else: block:
output = open(""logfile.txt"", ""a"")
localtime = time.asctime(time.localtime(time.time())
                                  #-----------------^

This causes Python to report a syntax error on the next line. All you need to do is add that one missing ); you have 3 opening parens, but only 2 closing.",A,7
19168972,2013-10-03 20:48:57.220000+00:00,"Your test method is ignored because it takes an argument. Test methods never take an argument. This is quite beside the fact that Python won't ever magically pass a local variable name into a function as an argument; you'd have to explicitly pass it in.
Integrate your url loading into the test itself instead:
class TestConnector(unittest.TestCase):

    def setUp(self):
        [...]

    def test_connections(self):

        with open('URL_list.txt') as f:
            for url in f:
                conn = Connector(self.user)
                self.assertNotEqual(conn.read(url.strip()), None)

if __name__ == '__main__':
    unittest.main()

or create test methods dynamically:
class TestConnector(unittest.TestCase):
    def setUp(self):
        [...]

def generate_test(url):
    def test(self):
        conn = Connector(self.user)
        self.assertNotEqual(conn.read(url), None)

if __name__ == '__main__':
    with open('URL_list.txt') as f:
        for i, url in enumerate(f):
            test_name = 'test_{}'.format(i)
            setattr(TestConnector, test_name, generate_test(url.strip()))

    unittest.main()
",A,3
27639910,2014-12-24 16:46:17.297000+00:00,"To wait for the response no more than a second:
from subprocess import Popen, PIPE

p = Popen(['command', 'the first argument', 'the second one', '3rd'],
          stdin=PIPE, stdout=PIPE, stderr=PIPE,
          universal_newlines=True)
out, err = p.communicate('\n', timeout=1) # write newline

The timeout feature is available on Python 2.x via the http://pypi.python.org/pypi/subprocess32/ backport of the 3.2+ subprocess module. See subprocess with timeout.
For solutions that use threads, signal.alarm, select, iocp, twisted, or just a temporary file, see the links to the related posts under your question.",A,1
3699819,2010-09-13 11:05:43.500000+00:00,"As rebus says in his comment, it's nonsense to call your project 'myapp.settings'. It's neither an app (which is a component of a site), nor is it a settings file. Call it something sensible - if you really can't think of anything, call it 'mysite'.
Finally, however, you should not be using mod_python. Use mod_wsgi instead.",A,2
2119468,2010-01-22 18:19:39.737000+00:00,"Step 1.  Get a book on data warehousing -- since that's what you're doing.
Step 2.  Partition your data into facts (measurable things like $'s, weights, etc.) and dimensions (non-measurable attributes like Product Name, Order Number, User Names, etc.)
Step 3.  Build a fact table (e.g., order items) surrounded by dimensions of that fact.  The order item's product, the order item's customer, the order item's order number, the order item's date, etc., etc.  This will be one fact table and several dimension tables in a single database.  Each ""origin"" or ""source"" is just a dimension of the basic fact.
Step 4.  Use very simple ""SELECT SUM() GROUP BY"" queries to summarize and analyze your data.
This is the highest performance, most scalable way to do business.  Buy Ralph Kimball's Data Warehouse Toolkit books for more details.
Do not build N databases with identical structure.  Build one for TEST, and one for PRODUCTION, but don't build N.
Do not build N tables with identical structure.  That's what keys are for.",A,5
14838171,2013-02-12 17:22:59.817000+00:00,"Globals are per module, and functions look up globals in the module they are defined in.
So a class Foo defined in a module named bar, that needs access to a function named spam will look up that function in it's own namespace, so in module bar.
If functions were to look up globals in the module they were imported into, you'd have to repeatedly import all the dependencies of any function you ever wanted to use. This would not be practical.",A,2
29844780,2015-04-24 10:12:43.713000+00:00,"If you want some variable or variables to be shared between functions, there are a few ways to do it:*

Pass each variable's value into every function as an argument, and return it from every function as part of a return tuple.
Wrap all of the values up in some structure, like a dict that you can look each thing up in by name, so you only have one value to pass and return.
Make the values attributes of an object, and turn the functions into methods of that object's type.
Use a closure, which I won't bother to explain because I'm sure you haven't learned about closures yet.
Use globals.

The Pythonic way to do this is definitely #3, but you haven't learned about classes yet. In that case, I'd just use #5, as you're already doing.
And when you learn about classes, coming back and modifying this script to use a class instead will be a great exercise.

* In fact, under the covers, options 3-5 are all pretty much syntactic sugar for option 2… but don't worry about that.",A,4
18125821,2013-08-08 12:17:23.577000+00:00,"From the os.path.join() documentation:

Join one or more path components intelligently. If any component is an absolute path, all previous components (on Windows, including the previous drive letter, if there was one) are thrown away, and joining continues.

A / at the start makes /tmp an absolute path.
If you wanted to join multiple path elements that perhaps contain a leading path separator, then strip those first:
os.path.join(*(elem.lstrip(os.sep) for elem in elements))

Special-casing absolute paths makes it possible for you to specify either a relative path (from a default parent directory) or an absolute path and not have to detect if you have an absolute or relative path when constructing your final value.",A,16
43768726,2017-05-03 19:46:11.503000+00:00,"You need to do this in two migrations. The first one would add the field, with a one-off default of 0. The second one would copy over the values: this could be done with a RunPython function that does something like this:
from django.db.models import F
Number.objects.update(number=F('id'))
",A,1
25803429,2014-09-12 07:55:46.393000+00:00,"That is the wrong way to produce custom HTML for your form. You should still use the Django form fields - {{ form.first_name }} etc - rather than creating HTML versions yourself and populating them.
However, the issue is simpler than that: you have forgotten to add the {{ form.myfield.errors }} for each field, and the {{ form.non_field_errors }} at the top of the form.
Also note there's no reason to do all that extra work in save. All of those fields are already being set by the superclass: the only thing you need to take care of manually is user.set_password.",A,0
897209,2009-05-22 10:15:39.543000+00:00,You want to use select to monitor the pipes from your subprocesses.  It's better than polling.,A,0
34764467,2016-01-13 10:39:57.317000+00:00,"sudo doesn't inherit your environment. Specify the full path to the Project's python executable.
Look at sys.executable with and without sudo.",A,1
18232247,2013-08-14 12:50:47.700000+00:00,"itertools.chain() returns a iterable, not a sequence. You cannot index or slice an iterable.
Use itertools.islice() to define a subset; when looping over the islice() result, the underlying iterable will be advanced to the starting index, then will yield items until the end index:
objects = islice(self.objects, page0 * self.per_page, 1 + page * self.per_page)

This iterates over the chained sequence, so you cannot then access the items before the start index.",A,6
5435102,2011-03-25 16:15:06.863000+00:00,"The fact that date-based queries work one day but not the next is suggestive. I suspect - although you haven't shown enough code to be sure - that you have inadvertently set some defaults somewhere, which are being persisted across requests.
For example, if you do this:
class MyModel(models.Model):
    my_date_field = model.DateField(default=datetime.date.today())

the value of the field default is evaluated when the server process starts, and remains the same as long as the process is running - which can be days or weeks.
The same is true if you do this in a view:
def my_view(request, date=datetime.date.today()):
    ... do something with date ...

because view arguments, again, are evaluated when the view is defined, not when it is executed, and therefore the value will remain the same as long as the process exists.",A,0
11383245,2012-07-08 12:42:29.190000+00:00,"Use the datetime module instead; it's .strftime can easily meet your needs and more.
>>> from datetime import datetime
>>> datetime.now().strftime('%d.%m.%Y %H:%M:%S')
'08.07.2012 14:43:58'
>>> datetime(2010, 3, 2, 13, 32, 20).strftime('%d.%m.%Y %H:%M:%S')
'02.03.2010 13:32:20'
",A,7
52882375,2018-10-18 20:51:20.943000+00:00,"Jenkins expects you to send the username and password using the Basic authentication feature of HTTP. requests can handle this natively. See the Jenkins Remove access API documentation:

When your Jenkins is secured, you can use HTTP BASIC authentication to authenticate remote API requests.

Just put the username and password in a tuple passed in as the auth argument:
start_build_url = 'http://{0}/jenkins/job/{1}/job/{2}/buildWithParameters?token={3}&{4}'.format(
    jenkins_uri, folder_name, job_name, build_token, build_param)
r = requests.post(
    start_build_url, auth=(username, password),
)

Neither the username nor the password values need to be escaped, leave that to requests to worry about for you.",A,3
21737311,2014-02-12 19:06:41.727000+00:00,"You have too many colons in your URL:
git+git:://github.com/pymc-devs/pymc@2.3
#  ----^

Just one colon is required:
pip install git+git://github.com/pymc-devs/pymc@2.3

or, in editable mode:
pip install -e git+git://github.com/pymc-devs/pymc@2.3

Provided you have a Fortran compiler installed that Just Works.
The #egg=<projectname> part is optional and only used to test for dependencies before downloading. It lets pip test if the package is already installed without needing to clone the whole repository.",A,2
15173005,2013-03-02 09:41:29.553000+00:00,"Importing the module with a simple import statement does not copy the names from that module into your own global namespace.
Either refer to the main name through attribute access:
print(lib.config.main)

or use the from ... import ... syntax:
from lib.config import main

instead.
You can learn more about how importing works in the Modules section of the Python tutorial.",A,2
53803537,2018-12-16 15:16:41.337000+00:00,"I don't understand why you have set post and author to CharFields, when the model has foreign keys. Especially as you are setting then directly in the model anyway.
You should remove those definitions, and then exclude the fields from the form altogether:
class CommentsForm(forms.ModelForm):

    class Meta:
        model = PostComments
        exclude = (""post"", ""author"")
",A,0
45833088,2017-08-23 07:19:02.370000+00:00,"Both snippets of code send a correct POST request. The only difference between them will be what headers are sent. Apart from the headers that are exactly the same, urllib.request will send:
Accept-Encoding: identity
User-Agent: Python-urllib/3.6

while requests will send:
Accept: */*
Accept-Encoding: gzip, deflate
User-Agent: python-requests/2.18.1

You'd have to experiment with adding and altering headers to your urllib.request code to see if those matter.
You can always use http://httpbin.org/post as the URL to see what information you posted; that service echoes back what it receives as a JSON object. See http://httpbin.org for more information.",A,1
18865785,2013-09-18 07:01:09.043000+00:00,"Use the public API instead of internals and leave worrying about content length and reading to the library:
import requests

s = requests.Session()
s.verify = False
s.auth = (token01, token02)
resp = s.get(url, params={'name': token01}, stream=True)
content = resp.content

or, since stream=True, you can use the resp.raw file object:
for line in resp.iter_lines():
    # process a line

or
for chunk in resp.iter_content():
    # process a chunk

If you must have a file-like object, then resp.raw can be used (provided stream=True is set on the request, like done above), but then just use .read() calls without a length to read to EOF.
If you are however, not querying a resource that requires you to stream (anything but a large file request, a requirement to test headers first, or a web service that is explicitly documented as a streaming service), just leave off the stream=True and use resp.content or resp.text for byte or unicode response data.
In the end, however, it appears your server is sending chunked responses that are malformed or incomplete; a chunked transfer encoding includes length information for each chunk and the server appears to be lying about a chunk length or sending too little data for a given chunk. The decode error is merely the result of incomplete data having been sent.",A,4
17390229,2013-06-30 12:11:06.930000+00:00,"You do not have a list. The print() function returns None, not whatever it just printed to your terminal or IDE.
Store the random values, then print:
list1 = [random.randint(1,6) for _ in range(100)]
print(list1)

Now you can just sort the list:
list1 = [random.randint(1,6) for _ in range(100)]
list1.sort()
print(list1)
",A,3
40536317,2016-11-10 20:19:45.850000+00:00,"You can't share objects directly across processes. You need to use one of the classes especially designed for communicating values, Queue and Pipe; see the documentation.",A,1
1221588,2009-08-03 10:21:30.357000+00:00,"""your unit test would fail but it probably wouldn't break your application""
This is -- actually -- really important to know.   It may seem annoying and trivial, but when someone else starts maintaining your code, they may have made a really bad change to Save and (improbably) broken the application.
The trick is to prioritize.
Test the important stuff first.  When things are slow, add tests for trivial stuff.",A,1
39966400,2016-10-10 20:39:18.407000+00:00,"Your actual code is discarding lists. You only ever process the last entry.
Your code works fine otherwise. Just do that in the loop and then append the result to some final list:
results = []

for k in range(len(seq_list)):
    column_list = [[] for i in range(len(seq_list[k][0]))]
    for seq in seq_list[k]:
        for i, nuc in enumerate(seq):
            column_list[i].append(nuc)
    # process `column_list` here, in the loop (no need to assign to ddd)
    tt = ["""".join(y for y in x if y in {'A','G','T','C'}) for x in column_list]

    results.append(tt)

Note that you could use the zip() function instead of your transposition list:
results = []
for sequence in seq_list:
    for column_list in zip(*sequence):
        tt = [''.join([y for y in x if y in 'AGTC']) for x in column_list]
        results.append(tt)
",A,3
22432129,2014-03-16 02:04:13.417000+00:00,"I'd parse the whole input string into a dictionary; a regular expression would help here:
import re
from collections import defaultdict

molecule = re.compile(r'([A-Z][a-z]?)(\d*)')

def parse_formula(f):
    counts = defaultdict(int)
    for name, count in molecule.findall(f):
        counts[name] += int(count or 1)
    return counts

This will count molecules without a digit after the symbol as 1; 'H3O' thus would still be counted correctly.
Now you can simply look up your elements:
counts = parse_formula('Ba4H2Ba5Li3')
print counts['Ba']
print counts['H']

Demo:
>>> counts = parse_formula('Ba4H2Ba5Li3')
>>> counts
defaultdict(<type 'int'>, {'H': 2, 'Ba': 9, 'Li': 3})
>>> counts['H']
2
>>> counts['Ba']
9
>>> parse_formula('H3O')
defaultdict(<type 'int'>, {'H': 3, 'O': 1})
",A,2
33352919,2015-10-26 18:21:57.650000+00:00,"The --data-binary switch means: post the command line argument as the whole POST body, without wrapping in multipart/form-data or application/x-www-form-encoding containers. @ tells curl to load the data from a filename; new_config.xml in this case.
You'll need to open the file object to send the contents as the data argument:
url     = ""http://host:8080/job/myproject/config.xml""
auth    = ('user','pass')
headers = {""Content-Type"" : ""application/xml""}
with open('new_config.xml', 'rb') as payload:
    r = requests.post(url, auth=auth, data=payload, headers=headers)

Note that I pass the file object directly into requests; the data will then be read and pushed to the HTTP socket, streaming the data efficiently.",A,2
23766408,2014-05-20 17:33:57.650000+00:00,"file.write() only takes one argument, a string. You've given it five instead:
file.write(""temperature is "", temperature, "" wet is "", humidity, ""%\n"")

Make that one string:
file.write(""temperature is "" + str(temperature) + "" wet is "" + str(humidity) + ""%\n"")

or use string formatting:
file.write(""temperature is {} wet is {}%\n"".format(temperature, humidity))

file.write() does not act like a print statement.",A,8
13664222,2012-12-01 22:07:59.423000+00:00,"You don't need to use an OrderedDict for this. Much easier with a list comprehension:
options = [o.title() for o in sorted(diff_dict, key=lambda k: diff_dict[k], reverse=True)]
print('\nHere are the 3 possible choices: ', ' - '.join(options))

The [expr for var in iterable] list comprehension applies the .title() method to each string we get from sorting the keys of diff_dict. This way, you can format each word separately, instead of applying .title() to the already joined whole.
This prints:

Here are the 3 possible choices:  Easy - Medium - Hard

If you want to see why this works, here are some of the constituent parts:
>>> sorted(diff_dict, key=lambda k: diff_dict[k], reverse=True)
['easy', 'medium', 'hard']
>>> [o.title() for o in sorted(diff_dict, key=lambda k: diff_dict[k], reverse=True)]
['Easy', 'Medium', 'Hard']
>>> ' - '.join([o.title() for o in sorted(diff_dict, key=lambda k: diff_dict[k], reverse=True)])
'Easy - Medium - Hard'

Hopefully that gives you some ideas on how to manipulate each option string with other methods.",A,1
54275344,2019-01-20 10:03:42.340000+00:00,"You can get all the posts in one query:
all_posts = Posts.objects.filter(user_profile__in=followings).order_by('trending_ratio')

This will order them correctly, as well as being much more efficient.",A,1
26187018,2014-10-03 21:24:57.910000+00:00,"You can search for the cell containing the text, then move up and over to the next cell:
humidity = soup.find(text='Average Humidity')
next_cell = humidity.find_parent('td').find_next_sibling('td')
humidity_value = next_cell.string

I'm using BeautifulSoup version 4 here, not 3; you really want to upgrade as version 3 has been mothballed 2 years ago now.
BeautifulSoup 3 can do this specific trick too; use findParent() and findNextSibling() instead there though.
Demo:
>>> import requests
>>> from bs4 import BeautifulSoup
>>> response = requests.get('http://www.wunderground.com/history/airport/LAX/2002/1/1/DailyHistory.html')
>>> soup = BeautifulSoup(response.content)
>>> humidity = soup.find(text='Average Humidity')
>>> next_cell = humidity.find_parent('td').find_next_sibling('td')
>>> next_cell.string
u'88'
",A,1
29637634,2015-04-14 21:21:56.640000+00:00,"You can catch the exception, clear the sensitive information, then re-raise:
class MyClass(pxssh.pxssh):
    def __init__(self, credentials):
        # ...
        try:
            self.connect(credentials)
        except Exception as e:
            credentials.clear()
            raise

This empties the credentials dictionary altogether; any other references to it will also show it is now empty. Any traceback formatter that includes values of locals will show an empty dictionary instead.
However, note that this cannot protect you from disclosure, as any malicious code can simply install a sys.settrace() hook and access credentials any time it exists before the exception occurs.
Or the attacker could simply replace MyClass.__init__ with a wrapper that captures and stores the credentials value before calling the original version. The same applies to the MyClass.connect() method.
Python's dynamic nature makes it entirely unsuitable to try and protect your values from an attacker that is already part of your process.",A,2
26300591,2014-10-10 13:27:35.020000+00:00,"The name is set from the key in your files dictionary. Just alter that key to set the name field to the desired value:
>>> import requests
>>> files = {'foobar': ('foo.txt', 'foo\ncontents\n','text/plain')}
>>> req = requests.Request('POST', 'http://httpbin.org/post', files=files).prepare()
>>> print req.body
--49773910d9514216894b697cb70e9f21
Content-Disposition: form-data; name=""foobar""; filename=""foo.txt""
Content-Type: text/plain

foo
contents

--49773910d9514216894b697cb70e9f21--

Note the name=""foobar"" element of the per-part Content-Disposition header.",A,2
5492809,2011-03-30 21:41:53.607000+00:00,"Don't try and share servers and databases while you're developing. You'll only get confused.
Each developer should have a local copy of MySQL on their own machine, and run the development server as mipadi recommends. Manage db schema changes via South, and keep data in fixtures or use South's data migrations. Each developer should commit to their local version of the git repository, but only push changes to Github when the particular task is complete and working (or, better, use a remote branch so that changes are synched, but only merge back to master when complete).
A good idea is to have a continuous integration server like Hudson/Jenkins that, after each commit to master, runs the tests and - if they pass - builds an up-to-date version of the site, which you can use for functional/user testing.
Edit to add There's no relationship between the development server and any particular database backend. As I recommend above, it is quite simple to install MySQL or PostgreSQL on your local machine, and use the development server against that. I have been working this way for several years, after initially encountering some of the issues you were worried about when switching between sqlite3 and the production MySQL db.",A,8
20221916,2013-11-26 15:58:03.320000+00:00,"Yes, __new__ can return something else than the new instance:
class A(object):
    def __new__(cls, *args, **kw):
        instance = super(A, cls).__new__(cls, *args, **kw)
        return [instance]

Demo:
>>> class A(object):
...     def __new__(cls, *args, **kw):
...         instance = super(A, cls).__new__(cls, *args, **kw)
...         return [instance]
... 
>>> A()
[<__main__.A object at 0x10f33f390>]

Note that __init__ is not called if the returned object is not the same type (so isinstance(returned_value, A) is False); that means that a custom A.__init__() is never going to be called here! Call it explicitly:
class A(object):
    def __new__(cls, *args, **kw):
        instance = super(A, cls).__new__(cls, *args, **kw)
        if hasattr(cls, '__init__'):
            instance.__init__(*args, **kw)            
        return [instance]
",A,5
44089059,2017-05-20 17:44:52.617000+00:00,"You want to apply str.join() to the whole list, not to each item:
final = [''.join(mylist)]

I've put the result back into a list with one item.
The other question was trying to join some of the elements in the input list, which is why str.join() was applied to x[3:6] there.",A,1
11959628,2012-08-14 19:42:34.400000+00:00,"You're confusing multiple different things.
If you want to switch from Xcode to vim, you can just… do it. If you've installed the Xcode command-line tools, you've got gcc, make, etc. all there on your path, and they'll work fine, and you never have to think about Xcode at all. In fact, you can even install the command-line tools without the rest of Xcode.
If you want to switch from the Xcode toolchain to a different toolchain, or use a hybrid, you can just do that to. You've installed gcc 4.7.1. OK, now use gcc_select, or export CC=/usr/local/bin/gcc-4.7.1, or hard-code the path in your makefiles, or whatever you want. It's not different than having multiple toolchains on any other POSIX platform.
If you want to build Xcode projects without running Xcode, you can use xcodebuild from the command line.
If you want convert existing projects from Xcode to Makefiles (or cmake or whatever), you'll have to do that manually. Usually it's not all that hard, unless the project is very complex or does a lot of Xcode-specific things. You can see all the actual commands Xcode is running from the build output.
If you want to learn how to create Makefiles, there are lots of good tutorials out there. But the first question is: why? Would it be acceptable to learn something easier, like cmake, or is the whole goal here to learn about make?
If you want to do SDK-based development, pass -arch, and use other Apple-specific extensions to gcc with gcc 4.7.1… well, you can't. Those extensions to gcc haven't been ported to any newer versions. If you want to do the work to port them yourself, I'm sure there are people who'd love it, but that's a lot of work.
And if you want to build other people's code with gcc 4.7.1, there's a good chance that isn't going to work out of the box either. Most Mac-specific open source projects depend on the Apple gcc extensions, and most cross-platform open source projects have autotools setups that will detect that you're on a Mac and have Xcode command-line tools and set themselves up to depend on those extensions too.
So, which of these are you having a problem with?

One more thing: 
The macosx-version-min setting is a completely different, and complementary, thing to the SDK. 
To set the SDK to, say, 10.5, you have to pass -isystem $PATH_TO_10_5_SDK/usr/include, and possibly additional -isystem flags for other directories under /usr (e.g., for C++ stdlib), and sometimes -L flags to the linker, and sometimes -F flags to the compiler and/or linker.
You also have to have the SDK. The only way to get a 10.5 SDK is with Xcode 3.2.6 or older—which I don't think you can even install on Mountain Lion. Also, the 10.5 SDK from Xcode 3.2.6 doesn't know how to work on build systems later than 10.6, so you have to hack it up manually (mainly creating a bunch of symlinks that cause it to treat 10.7 and 10.8 the same as 10.6). And if you're using a non-Apple gcc, it will also get confused trying to find various things like the compiler-specific headers (especially for C++), so you need to hack it up even more.
Why is this so hard? Because you almost never need to do it, and Apple specifically recommends that you don't. It's left over from the way they used to recommend doing things back in the 10.4 days—but just because it's there doesn't mean you should use it.
The right way to build code that runs on 10.5 is the -macosx-version-min=10.5. You can, and in fact should, use it with the SDK that matches your OS (or possible even a newer SDK, if you want to be able to use features in the next OS version). Again, this flag may only be supported with clang and Apple-extended gcc. But it makes everything in the SDK magically work in a backward-compatible mode. Except, of course, for new features that were added in 10.6 or later—if you try to go to fullscreen mode on a 10.5 machine, you'll just get an exception or crash. Apple has pretty good documentation on which APIs were added with which version, and on the right way to check for each kind of API (Cocoa, CoreFoundation, and POSIX). And of course you'll need a real 10.5 machine (or VM) to test on. Also, there are a few bugs with the C++ stdlib (10.5 only), and a handful of libs like OpenSSL where Apple no longer recommends anyone build against the system copy for distribution. But it's still much easier than trying to build with the 10.5 SDK. And it's what Apple explicitly recommends.",A,3
35961169,2016-03-12 17:45:22.677000+00:00,"Your form wouldn't display errors when the name or message fields are empty, for the same reason as it's not displaying the invalid email error: because you haven't put anything in the template to display those errors. You need to add {{ form.email.errors }} etc to make it do so.",A,3
28788389,2015-02-28 23:51:38.593000+00:00,"Your format is easily translated to the str.format() formatting syntax:
print(""{:>3s} {:20s} {:>12s}"".format(""Id"", ""State"", ""Population""))
print(""{id:3d} {name:20s} {population:12d}"".format(**state))

Note that left-alignment is achieved by prefixing the width with <, not -, and default alignment for strings is to left-align, so a > is needed for the header strings and the < can be omitted, but otherwise the formats are closely related.
This extracts the values directly from the state dictionary by using the keys in the format itself.
You may as well just use the actual output result of the first format directly:
print("" Id State                  Population"")

Demo:
>>> state = {'id': 15, 'name': 'New York', 'population': 19750000}
>>> print(""{:>3s} {:20s} {:>12s}"".format(""Id"", ""State"", ""Population""))
 Id State                  Population
>>> print(""{id:3d} {name:20s} {population:12d}"".format(**state))
 15 New York                 19750000
",A,1
5129129,2011-02-26 19:11:08.327000+00:00,"The python built-in hash function is not intended for secure, cryptographic hashing. It's intention is to facilitate storing Python objects into dictionaries efficiently.
The internal hash implementations are too predictable (too many collisions) for secure uses. For example, the following assertions are all true:
hash('a') < hash('b')
hash('b') < hash('c')
hash('c') < hash('d')

This sequential nature makes for great dictionary storage behaviour, for which it was designed.
To create a secure hash, use the hashlib library instead.",A,1
53467277,2018-11-25 12:08:22.873000+00:00,"Define form_valid in the view and set it there.
def form_valid(self, form):
    form.instance.app_id = self.kwargs[""app_id""]
    return super().form_valid(form)
",A,0
25982329,2014-09-22 20:06:11.647000+00:00,"t.join() always returns None. That's because the return value of a thread target is ignored.
You'll have to collect your results by some other means, like a Queue object:
from Queue import Queue

results = Queue()

def upload_to_s3(filepath, unique_id):
    # do something
    print s3_url # <-- Confirming that this `s3_url` variable is not None
    results.put(s3_url)


threads = []
for num, list_of_paths in enumerate(chunked_paths_as_list):
    for filepath in list_of_paths:
        t = threading.Thread(target=upload_to_s3, args=(filepath, self.unique_id))
        t.start()
        threads.append(t)
for t in threads:
    t.join()

while not results.empty():
    print results.get()

Alternatively, use multiprocessing.dummy module to get the multiprocessing.Pool behaviour but with threads, which can do what you want; collect return values from the async function calls.",A,6
44263675,2017-05-30 13:29:19.387000+00:00,"You are generating a new hash, one with a totally different salt:
password = hashpw(password, gensalt())

You can't look up users by password; the whole point of using a salted hash is to make the impossible for hackers to use a rainbow table (common passwords hashed, but without a salt) to match hashes. That same protection makes it impossible for you to know, up-front, what salt was used for a given user, and you are almost guaranteed to be generating a different hash from your new salt.
You need to use the username to look up the right now, retrieve the salted password, then ask the bcrypt.checkpw() function to securely test the password against the stored hash. The hash value includes the salt, bcrypt will extract the salt and use it to verify that the same hash is generated. The comparison needs to protect against timing attacks, don't re-implement this check yourself.
Your form needs to send along the username, not just the password:
from bcrypt import checkpw


username = request.form['username']
password = request.form['password']
con = sql.connect(""testDB.db"")
cur = con.cursor()
cur.execute(""SELECT password FROM mytable where username = ?"", (username,))
hashed = cur.fetchone()
if hashed is None or not checkpw(password, hashed[0]):
    # no such user, *or* the password did not match
    return ""user not found""

# password matched the hashed value from the database
return ""welcome guest""
",A,1
20058526,2013-11-18 21:41:29.203000+00:00,"You have a list comprehension with nested loops. It can be translated to:
self.cards = []
for i in range(0, len(SUITS)):
    for j in range(0, len(RANKS)):
        self.cards.append(Card(SUITS[i], RANKS[j]))

It could be simplified further though, by looping over SUITS and RANKS directly:
self.cards = [Card(suit, rank) for suit in SUITS for rank in RANKS]

or you can use itertools.product() to replace the nested loops:
from itertools import product

self.cards = [Card(suit, rank) for suit, rank in product(SUITS, RANKS)]
",A,2
49606823,2018-04-02 07:07:02.427000+00:00,"Python has a builtins module where ""truly global"" things—normally just the standard builtin functions and types—are stored. In Python 2, it was named __builtin__, but worked mostly the same.
This module can be imported just like any other module—but it also magically supplies the builtin names for every other module (that doesn't hide them).

If you're wondering how that works, the builtins docs say:

As an implementation detail, most modules have the name __builtins__ made available as part of their globals. The value of __builtins__ is normally either this module or the value of this module’s __dict__ attribute. Since this is an implementation detail, it may not be used by alternate implementations of Python.

And exec says:

If the globals dictionary does not contain a value for the key __builtins__, a reference to the dictionary of the built-in module builtins is inserted under that key. That way you can control what builtins are available to the executed code by inserting your own __builtins__ dictionary into globals before passing it to exec().

So, at least in CPython, when you evaluate abs, it's looked up in globals()['abs'], not found there, and then looked up in globals()['__builtins__'].__dict__['abs'].
And whenever Python (or at least CPython) creates a new module object, its code is executed against a globals with an empty __builtins__, which means the default builtins module value gets filled in, so that works. And this globals is the one that gets copied for very function and class defined in the module (and anything you do explicitly with globals without explicitly replacing __builtins__), so it works inside functions and classes as well.",A,1
5090536,2011-02-23 11:55:23.310000+00:00,"
What I want, is a way to yield a filename, work on it, and then yield the next one, without reading them all into memory.

No method will reveal a filename which ""changed"".  It's not even clear what you mean by this ""filenames change, new files are added, and files are deleted""?  What is your use case?
Let's say you have three files: a.a, b.b, c.c.
Your magical ""iterator"" starts with a.a.   You process it.
The magical ""iterator"" moves to b.b.  You're processing it.
Meanwhile a.a is copied to a1.a1, a.a is deleted.  What now?  What does your magical iterator do with these?  It's already passed a.a.  Since a1.a1 is before b.b, it will never see it.  What's supposed to happen for ""filenames change, new files are added, and files are deleted""?
The magical ""iterator"" moves to c.c.  What was supposed to happen to the other files?  And how were you supposed to find out about the deletion?


Process A is continuously writing files to a storage location. Process B (the one I'm writing), will be iterating over these files, doing some processing based on the filename, and moving the files to another location.

Don't use the naked file system for coordination.  
Use a queue.
Process A writes files and enqueues the add/change/delete memento onto a queue.
Process B reads the memento from queue and then does the follow-on processing on the file named in the memento.",A,6
14941151,2013-02-18 16:44:21.710000+00:00,"dict1.update((k, dict2[k]) for k in set(dict2).intersection(dict1))

is how I'd do it in Python 2.6 or below (see further on how to do this in later versions). 
Next to another mapping, dict.update() can also take an iterable of (key, value) tuples, which we generate based on the set intersection of the two dictionaries (so all keys they have in common).
Demo:
>>> dict1 = {'foo':'bar', 'ham': 'eggs'}
>>> dict2 = {'ham': 'spam', 'bar': 'baz'}
>>> dict1.update((k, dict2[k]) for k in set(dict2).intersection(dict1))
>>> dict1
{'foo': 'bar', 'ham': 'spam'}

In python 2.7 you can use the new Dict views to achieve the same without casting to sets:
dict1.update((k, dict2[k]) for k in dict1.viewkeys() & dict2.viewkeys())

In Python 3, dict views are the default, so you can instead spell this as:
dict1.update((k, dict2[k]) for k in dict1.keys() & dict2.keys())
",A,10
20567649,2013-12-13 13:25:01.193000+00:00,"Python directly supports what you want to do:
from __future__ import print_function

Any module with that line at the top will treat the print statement as a function instead, making the code compatible with both Python 2 and 3.
This applies just to the print statement; you cannot override other statements.
This does mean you then have to use print() as a function everywhere in that module, but you can then also provide your own implementation if you so desire:
from __future__ import print_function
import __builtin__

def print(*args, **kwargs):
    __builtin__.print('Prefixed:', *args, **kwargs)

print('Hello world!')

Another option is to use a context manager to capture printed statements, directing the output away from sys.stdout into a in-memory file object of your choosing:
from contextlib import contextmanager
import sys
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO


@contextmanager
def capture_sys_output():
    caputure_out = StringIO()
    current_out = sys.stdout
    try:
        sys.stdout = caputure_out
        yield caputure_out
    finally:
        sys.stdout = current_out

and wrap any blocks that you want to capture print output for with the context manager. Here is an example prefixing printed lines:
with capture_sys_output as output:
    print 'Hello world!'

output = output.get_value()
for line in output.splitlines():
    print 'Prefixed:', line

or even provide a wrapper:
from contextlib import contextmanager
import sys

class Prefixer(object):
    def __init__(self, prefix, orig):
        self.prefix = prefix
        self.orig = orig
    def write(self, text):
        self.orig.write(self.prefix + text)
    def __getattr__(self, attr):
        return getattr(self.orig, attr)     

@contextmanager
def prefix_stdout(prefix):
    current_out = sys.stdout
    try:
        sys.stdout = Prefixer(prefix, current_out)
        yield
    finally:
        sys.stdout = current_out

and use as:
with prefix_stdout('Prefixed: '):
    print 'Hello world!'

but take into account that print statements usually write data to stdout in separate chunks; the newline at the end is a separate write.",A,14
14631741,2013-01-31 17:35:08.740000+00:00,"You want to select all descendant text, not just child text:
//div[a[contains(., ""Add to cart"")]]/p//text()

Note the double slash between p and text() there.
This potentially will also include a lot of inter-tag whitespace though, you you'll need to clean that up. Example using lxml:
>>> import lxml.etree as ET
>>> tree = ET.fromstring('''<div>
... <div>
...     <p>
...     <span class=""abc"">Monitor</span> <b>$300</b>
...     </p>
...     <a href=""/add"">Add to cart</a>
... </div>
... <div>
...     <p>
...     <span class=""abc"">Keyboard</span> $20 
...     </p>
...     <a href=""/add"">Add to cart</a>
... </div>
... </div>''')
>>> tree.xpath('//div[a[contains(., ""Add to cart"")]]/p//text()')
['\n    ', 'Monitor', ' ', '$300', '\n    ', '\n    ', 'Keyboard', ' $20 \n    ']
>>> res = _
>>> [txt for txt in (txt.strip() for txt in res) if txt]
['Monitor', '$300', 'Keyboard', '$20']
",A,24
21809043,2014-02-16 08:43:23.213000+00:00,"Parse out the string into a datetime object using datetime.datetime.strptime(), but since your date string has no year attached to it, do attach the current year to it:
import datetime

today = datetime.date.today()
yourdate = datetime.datetime.strptime(inputstring, '%d %b')
yourdate = yourdate.date().replace(year=today.year)

if yourdate >= today:
    # date not before today, attach *last* year
    yourdate = yourdate.replace(year=today.year - 1)

I converted the datetime object to a date object here too since we only need to talk about dates.
This does assume your date strings always are of the form day-of-the-month month-abbreviated in English.
Demo:
>>> import datetime
>>> inputstring = '23 Mar'
>>> today = datetime.date.today()
>>> yourdate = datetime.datetime.strptime(inputstring, '%d %b')
>>> yourdate = yourdate.date().replace(year=today.year)
>>> yourdate
datetime.date(2014, 3, 23)
>>> yourdate >= today
True
",A,3
34637187,2016-01-06 15:49:41.293000+00:00,"The code are trying to call the result of the print() function:
result = print (keyword_encrypt)
result(shift_key, phrase)

That won't work, because print() always returns None.
You probably meant to call keyword_encrypt() function instead and print the return value of that call:
print(keyword_encrypt(shift_key, phrase))

The code is written a little funkily, with the parentheses and spaces, but if you actually ran it with Python 2 (as the first #! line of the file tries to do), the code as posted would actually work. That's because print is a statement in Python 2 and the parentheses around keyword_encrypt are effectively ignored.",A,1
32141423,2015-08-21 13:22:01.037000+00:00,"author is a many-to-many, so you need to use ModelMultipleChoiceField.
However I don't know why you've redefined those fields in the first place. ModelChoiceField is already the default for a foreign key field. Remove those two definitions, and add author and publisher into the fields list.",A,2
21224589,2014-01-20 00:44:10.210000+00:00,"According to the documentation:

The only character set supported by the clipboard is Unicode in its UTF-8 encoding.

What you're calling ""Unicode"" is UTF-16-LE. Which is not UTF-8. If you try to interpret UTF-16-LE as UTF-8, it's going to look like it starts with ""V\0"", and then most code will just treat that \0 as the end of the string and stop reading.
You need to encode it to UTF-8—which is stored as char, not wchar_t—and paste that.",A,0
34819497,2016-01-15 20:40:25.480000+00:00,"requests.get() returns a Response object; it is that object that has the .text attribute; it is not the 'source code' of the URL, it is an object that lets you access the source code (the body) of the response, as well as other information. The Response.text attribute gives you the body of the response, decoded to unicode.
See the Response Content section of the Quickstart documentation:

When you make a request, Requests makes educated guesses about the encoding of the response based on the HTTP headers. The text encoding guessed by Requests is used when you access r.text.

Further information can be found in the API documentation, see the Response.text entry:

Content of the response, in unicode.
If Response.encoding is None, encoding will be guessed using chardet.
The encoding of the response content is determined based solely on HTTP headers, following RFC 2616 to the letter. If you can take advantage of non-HTTP knowledge to make a better guess at the encoding, you should set r.encoding appropriately before accessing this property.

You can also use Response.content to access the response body undecoded, as raw bytes.",A,2
33974369,2015-11-28 17:39:57.213000+00:00,"You need to nest your try...except statements here:
def date_format(date):  
    try:  
        datetime.datetime.strptime(date, '%Y-%m')  
        return 1  
    except ValueError:  
        try:
            datetime.datetime.strptime(date, '%Y-%m-%d')  
            return 2  
        except ValueError:  
            return 'Wrong date format you dweezle! Must be YYYY-MM or YYYY-MM-DD'

although you could just put them in series, since the first will return out of the question if successful; have the first exception handler pass:
def date_format(date):  
    try:  
        datetime.datetime.strptime(date, '%Y-%m')  
        return 1  
    except ValueError:
        pass

    try:
        datetime.datetime.strptime(date, '%Y-%m-%d')  
        return 2  
    except ValueError:  
        return 'Wrong date format you dweezle! Must be YYYY-MM or YYYY-MM-DD'
",A,2
37227332,2016-05-14 13:35:45.897000+00:00,"IDLE supports the full built-in help() function out of the box. In the interactive interpreter, just use help(object) to print out the documentation for that object.
You can use this on modules, classes, and methods:
",A,0
22564692,2014-03-21 16:49:15.397000+00:00,"Parse out the format using datetime.datetime.strptime():
import datetime

format = '%Y%m%d%H%M%S%f'
dt1 = datetime.datetime.strptime(str(t1), format)
dt2 = datetime.datetime.strptime(str(t2), format)

difference = (dt2 - dt1).total_seconds()
",A,6
19488479,2013-10-21 07:27:28.203000+00:00,"You need to set your locale correctly for Apache; without a correct locale, Python falls back to using ASCII for filenames.
Set the LANG environment variable to LANG=en_US.UTF-8 to the Apache envvars file. On Debian and Ubuntu systems, you find that in /etc/apache2/envvars, on RedHat, in /etc/sysconfig/httpd. 
Add the line:
EXPORT LANG=en_US.utf8

If you have trouble locating the envvars file, see this question on ServerFault for some more hints about how to find that file.",A,2
2546388,2010-03-30 15:19:30.287000+00:00,"
I could copy a random object to every experiment and do than a jumpahead of 50.000 * expid. 

Approximately correct.  Each thread gets their own Random instance.
Seed all of them to the same seed value.  Use a constant to test, use /dev/random when you ""run for the record"".
Edit.  Outside Python and in older implementations, use jumpahead( 50000 * expid ) to avoid the situation where two generators wind up with a parallel sequences of values.  In any reasonably current (post 2.3) Python, jumpahead is no longer linear and using expid is sufficient to scramble the state.
You can't simply do jumpahead(1) in each thread, since that will assure they are synchronized.  Use jumpahead( expid ) to assure each thread is distinctly scrambled.

The documentation suggests that jumpahead(1) already scrambles the state, but is that really true?

Yes, jumpahead does indeed ""scramble"" the state.  Recall that for a given seed you get one -- long -- but fixed sequence of pseudo-random numbers.  You're jumping ahead in this sequence.  To pass randomness tests, you must get all your values from this one sequence.
Edit.  Once upon a time, jumpahead(1) was limited.  Now jumpahead(1) really does a larger scrambling.  The scrambling, however, is deterministic.  You can't simply do jumpahead(1) in each thread.
If you have multiple generators with different seeds, you violate the ""one sequence from one seed"" assumption and your numbers aren't going to be as random as if you get them from a single sequence.
If you only jumphead 1, you'll may be getting parallel sequences which will might be similar.  [This similarity might not be detectable; theoretically, there's a similarity.]
When you jumpahead 50,000, you assure that you follow the 1-sequence-1-seed premise.  You also assure that you won't have adjacent sequences of numbers in two experiments.
Finally, you also have repeatability.  For a given seed, you get consistent results.
Same jumpahead: Not Good.
>>> y=random.Random( 1 )
>>> z=random.Random( 1 )
>>> y.jumpahead(1)
>>> z.jumpahead(1)
>>> [ y.random() for i in range(5) ]
[0.99510321786951772, 0.92436920169905545, 0.21932404923057958, 0.20867489035315723, 0.91525579001682567]
>>> [ z.random() for i in range(5) ]
[0.99510321786951772, 0.92436920169905545, 0.21932404923057958, 0.20867489035315723, 0.91525579001682567]
",A,5
20440859,2013-12-07 11:33:57.253000+00:00,"Your view is called login, so when you try to call the login function, it calls your view instead of the built-in one. Call your view something else.",A,2
11320969,2012-07-04 00:26:38.593000+00:00,"To get content generated by JavaScript you could also try Selenium, example.",A,0
21767860,2014-02-13 23:29:14.460000+00:00,"I don't think your problem actually exists:

Ok, now self.mapping[example][0] = u'\xe0'. So yeah, that's the character that I need to replace...but the string that I need to call the replace_UTF8() function on looks like u'\u00e0'.

Those are just different representations of the exact same string. You can test it yourself:
>>> u'\xe0' == u'\u00e0'
True


The actual problem is that you're not doing any replacing. In this code:
def replace_UTF8(self, string):
    for old, new in self.mapping:
        print new
        string.replace(old, new)
    return string

You're just calling string.replace over and over, which returns a new string, but does nothing to string itself. (It can't do anything to string itself; strings are immutable.) What you want is:
def replace_UTF8(self, string):
    for old, new in self.mapping:
        print new
        string = string.replace(old, new)
    return string


However, if string really is a UTF-8-encoded str, as the function name implies, this still won't work. When you UTF-8-encode u'\u00e0', what you get is '\xce\xa0'. There is no \u00e0 in there to be replaced. So, what you really need to do is decode it, do the replaces, then re-encode. Like this:
def replace_UTF8(self, string):
    u = string.decode('utf-8')
    for old, new in self.mapping:
        print new
        u = u.replace(old, new)
    return u.encode('utf-8')

Or, even better, keep things as unicode instead of encoded str throughout your program except at the very edges, so you don't have to worry about this stuff.

Finally, this is a very slow and complicated way to do the replacing, when strings (both str and unicode) have a built-in translate method to do exactly what you want.
Instead of building your table as a list of pairs of Unicode strings, build it as a dict mapping ordinals to ordinals:
mapping = {}
for row in reader:
    mapping[ord(row[0].decode(""unicode_escape""))] = ord(row[1])

And now, the whole thing is a one-liner, even with your encoding mess:
def replace_UTF8(self, string):
    return string.decode('utf-8').translate(self.mapping).encode('utf-8')
",A,1
36311278,2016-03-30 14:06:21.863000+00:00,"You are posting to the new_person route but haven't configured the route to accept a POST request. The default is to only allow GET and HEAD. Set the methods argument to the route() decorator:
@app.route(""/api/new_person"", methods=['GET', 'POST'])
def new_person():

See the HTTP Methods section of the Flask Quickstart.
Next, you'll need to commit the transaction if you want your changes to persist (like adding a new Person); add the new person object to your session and commit:
db.session.add(person)
db.session.commit()

See the Inserting Records section of the Flask-SQLAlchemy Select, Insert, Delete chapter.
Alternatively, set the SQLAlchemy session to auto-commit all changes:
db = flask.ext.sqlalchemy.SQLAlchemy(app, session_options={'autocommit': True})

You'll still need to add the new object to a session, however.",A,1
23893972,2014-05-27 16:01:14.290000+00:00,"The argparser object was added on 29 March 2013, after google-api-python-client version 1.1 was released.
It appears your easy_install.py --upgrade google-api-python-client call failed to upgrade your local installation, you are still trying to import from version 1.1. The output of import oauth2client.tools; print oauth2client.tools.__file__ shows this:
C:\Python27\lib\site-packages\google_api_python_client-1.1-py2.7.egg\oauth2clie‌​nt\tools.pyc

You can see the version number in the egg path.",A,1
29192768,2015-03-22 09:25:23.710000+00:00,"You cannot do what you want with the generator; generators are specialised iterators, and iterators can only be iterated over once. Once they are exhausted (StopIterator has been raised), they cannot yield more items. 
Your code on the other hand expects StopIteration to be raised (the first loop ends), and then a second loop continues to iterate again.
Instead of handling this in the generator, simply stop iterating. Create the generator once, then use it in both loops:
def run():
    iterator = read()
    for i in iterator:
        print i
        if i == 2:
            break   # stop iterating

    print 'flip'

    # resume iterating
    for i in iterator:
        print i
",A,1
25940746,2014-09-19 19:02:56.473000+00:00,"Data read from a file is always going to be a string. You'll need to try and convert those lines, not test their type:
try:
    alpha = int(alpha)
    if alpha == 1:
        totalGold = totalGold + 1
        print(""gold medal won"")
    elif alpha == 2:
        totalSilver = totalSilver + 1
        print(""silver medal won"")
    elif alpha == 3:
        totalBronze = totalBronze + 1
        print(""bronze medal won"")
except ValueError:
    print('is a string')

int() will raise a ValueError when alpha cannot be interpreted as an integer number. The exception, if raised, causes Python to jump to the except ValueError: block instead of executing the rest of the try: suite.",A,2
2193788,2010-02-03 17:10:38.037000+00:00,"Try this as a benchmark:

Use the built-in SQL tools to do a bulk extract of the entire table.  All rows.  All columns.
Drop (or rename) the table.
Use a simple flat-file read/write to create a new file with the updates applied.
Use the bulk-load utility that comes with your database to rebuild the entire table from the extracted file.
Add indexes after the reload.

You may find that this is faster than any SQL solution.  We stopped using UPDATES for a data warehouse because extract -> flat file process -> load was much faster than SQL.",A,2
42600997,2017-03-04 20:25:40.610000+00:00,"In your template, posts - as the name implies - is a queryset, ie a list of BlogPost objects. That queryset doesn't have an id attribute; only the individual posts within that list do.
If you want to link to a specific post, you need to loop through that list, and use the id of each post in the loop:
{% for post in posts %}
<p>
<a href=""{% url 'blogs:edit_posts' post.id %}"">edit post</a>
</p>
{% endfor %}
",A,0
23219437,2014-04-22 12:22:13.920000+00:00,"You can use list.extend() to add elements individually:
for i in a:
    b.extend(i.split(','))

To split text on commas not inside parenthesis you could use the following regular expression:
re.split(r',(?=[^()]*(?:\(|$))', i)

This splits on a comma only if it is not followed by text containing any parentheses (opening or closing) until the next opening parenthesis. This expression falls down when you have nested parenthesis, but works for your simple case:
>>> import re
>>> i = 'john(is,great),paul,school'
>>> re.split(r',+(?=[^()]*(?:\(|$))', i)
['john(is,great)', 'paul', 'school']

For more complex splits, use a full-blown parser.",A,6
45299339,2017-07-25 09:34:20.017000+00:00,"Yes, that's correct. Most of the time, just think of the value as the object itself.
You could also use the word 'state' to describe the object value; for mutable objects the value can change, but in general the object, it's type and identity, do not change.
Some examples:

2048 is an int with the integer value 2048. int is an immutable type, so the value will never change. You generally create a new object of the same type with a different value; 2048 + 1 produces a new int object with value 2049, with a new identity.
[42] is a list with a single reference to another object. You can change the contents, changing the list value. But the identity and type would not change.
For instances of a Python class, __class__ is writable, letting you change the type dynamically. This is rarely needed, but the option exists.
",A,3
15665502,2013-03-27 17:30:48.637000+00:00,"Here's an implementation of @Oz123's solution which is based on os.walk():
import os

def create_empty_dirtree(srcdir, dstdir, onerror=None):
    srcdir = os.path.abspath(srcdir)
    srcdir_prefix = len(srcdir) + len(os.path.sep)
    os.makedirs(dstdir)
    for root, dirs, files in os.walk(srcdir, onerror=onerror):
        for dirname in dirs:
            dirpath = os.path.join(dstdir, root[srcdir_prefix:], dirname)
            try:
                os.mkdir(dirpath)
            except OSError as e:
                if onerror is not None:
                    onerror(e)
",A,0
14892402,2013-02-15 10:13:37.513000+00:00,"Don't use os.system(); subprocess is definitely the way to go.
Your problem though is that you expect Python to understand that you want to interpolate args.fileread into a string. As great as Python is, it is not able to read your mind like that!
Use string formatting instead:
os.system(""rtl2gds -rtl={args.fileread} -rtl_top={args.module_name} -syn"".format(args=args)

If you want to pass a filename to another command, you should not use the FileType type option! You want a filename, not an open file object:
parser.add_argument('fileread', help='Enter the file path')

But do use subprocess.call() instead of os.system():
import subprocess

subprocess.call(['rtl2gds', '-rtl=' + args.fileread, '-rtl_top=' + args.module_name, '-syn'])

If rtl2gds implements command line parsing properly, the = is optional and you can use the following call instead, avoiding string concatenation altogether:
subprocess.call(['rtl2gds', '-rtl', args.fileread, '-rtl_top', args.module_name, '-syn'])
",A,10
19233810,2013-10-07 20:11:11.060000+00:00,"book_set gives you the Book record, not the BookRating one. You can get that via the standard reverse relationship:
request.user.bookratings_set.all()

That will have the rating field, plus the book ForeignKey to get the Book.",A,2
28827752,2015-03-03 09:04:52.953000+00:00,"
I've looked at the timeit module, but it seems it's only for small snippets of code. I want to time the whole program.

$ python -mtimeit -n1 -r1 -t -s ""from your_module import main"" ""main()""

It runs your_module.main() function one time and print the elapsed time using time.time() function as a timer.
To emulate /usr/bin/time in Python see Python subprocess with /usr/bin/time: how to capture timing info but ignore all other output?.
To measure CPU time (e.g., don't include time during time.sleep()) for each function, you could use profile module (cProfile on Python 2):
$ python3 -mprofile your_module.py

You could pass -p to timeit command above if you want to use the same timer as profile module uses.
See How can you profile a Python script?",A,7
3125012,2010-06-26 18:20:22.227000+00:00,"If you can connect to the database remotely, then you can simply specify its host/port in settings.py exactly as you would a local one.",A,1
30801802,2015-06-12 11:21:03.543000+00:00,"Rather than use a huge long XPath as generated by Chrome, you can just search for a table with the yfnc_tabledata1 class; there is just the one:
>>> tree.xpath(""//table[@class='yfnc_tabledata1']"")
[<Element table at 0x10445e788>]

Get to your <td> from there:
>>> tree.xpath(""//table[@class='yfnc_tabledata1']//td[1]"")[0].text_content()
'Period EndingDec 31, 2014Dec 31, 2013Dec 31, 2012\n                            \n                        Total Revenue\n                            \n                        \n                                \n                            31,821,000\xa0\xa0\n                                \n                            \n                                \n                            30,871,000\xa0\xa0\n                                \n                            \n                                \n                            29,904,000\xa0\xa0\n                                \n                            Cost of Revenue16,447,000\xa0\xa016,106,000\xa0\xa015,685,000\xa0\xa0\n                            \n                        Gross Profit\n                            \n                        \n                                \n                            15,374,000\xa0\xa0\n                                \n                            \n                                \n                            14,765,000\xa0\xa0\n                                \n                            \n                                \n                            14,219,000\xa0\xa0\n                                \n                            \n                    \n                Operating Expenses\n                    \n                Research Development1,770,000\xa0\xa01,715,000\xa0\xa01,634,000\xa0\xa0\n                    \n                Selling General and Administrative6,469,000\xa0\xa06,384,000\xa0\xa06,102,000\xa0\xa0\n                    \n                Non Recurring\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                    \n                Others\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                    \n                \n                    \n                Total Operating Expenses\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                            \n                        Operating Income or Loss\n                            \n                        \n                                \n                            7,135,000\xa0\xa0\n                                \n                            \n                                \n                            6,666,000\xa0\xa0\n                                \n                            \n                                \n                            6,483,000\xa0\xa0\n                                \n                            \n                    \n                Income from Continuing Operations\n                    \n                Total Other Income/Expenses Net33,000\xa0\xa041,000\xa0\xa039,000\xa0\xa0\n                    \n                Earnings Before Interest And Taxes7,168,000\xa0\xa06,707,000\xa0\xa06,522,000\xa0\xa0\n                    \n                Interest Expense142,000\xa0\xa0145,000\xa0\xa0171,000\xa0\xa0\n                    \n                Income Before Tax7,026,000\xa0\xa06,562,000\xa0\xa06,351,000\xa0\xa0\n                    \n                Income Tax Expense2,028,000\xa0\xa01,841,000\xa0\xa01,840,000\xa0\xa0\n                    \n                Minority Interest(42,000)(62,000)(67,000)\n                    \n                \n                    \n                Net Income From Continuing Ops4,956,000\xa0\xa04,659,000\xa0\xa04,444,000\xa0\xa0\n                    \n                Non-recurring Events\n                    \n                Discontinued Operations\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                    \n                Extraordinary Items\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                    \n                Effect Of Accounting Changes\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                    \n                Other Items\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                            \n                        Net Income\n                            \n                        \n                                \n                            4,956,000\xa0\xa0\n                                \n                            \n                                \n                            4,659,000\xa0\xa0\n                                \n                            \n                                \n                            4,444,000\xa0\xa0\n                                \n                            Preferred Stock And Other Adjustments\n            -\n            \xa0\n            -\n            \xa0\n            -\n            \xa0\n                            \n                        Net Income Applicable To Common Shares\n                            \n                        \n                                \n                            4,956,000\xa0\xa0\n                                \n                            \n                                \n                            4,659,000\xa0\xa0\n                                \n                            \n                                \n                            4,444,000\xa0\xa0\n                                \n                            '
>>> print(tree.xpath(""//table[@class='yfnc_tabledata1']//td[1]"")[0].text_content())
Period EndingDec 31, 2014Dec 31, 2013Dec 31, 2012

                        Total Revenue



                            31,821,000  



                            30,871,000  



                            29,904,000  

                            Cost of Revenue16,447,000  16,106,000  15,685,000  

                        Gross Profit



                            15,374,000  



                            14,765,000  



                            14,219,000  



                Operating Expenses

                Research Development1,770,000  1,715,000  1,634,000  

                Selling General and Administrative6,469,000  6,384,000  6,102,000  

                Non Recurring
            -
             
            -
             
            -
             

                Others
            -
             
            -
             
            -
             



                Total Operating Expenses
            -
             
            -
             
            -
             

                        Operating Income or Loss



                            7,135,000  



                            6,666,000  



                            6,483,000  



                Income from Continuing Operations

                Total Other Income/Expenses Net33,000  41,000  39,000  

                Earnings Before Interest And Taxes7,168,000  6,707,000  6,522,000  

                Interest Expense142,000  145,000  171,000  

                Income Before Tax7,026,000  6,562,000  6,351,000  

                Income Tax Expense2,028,000  1,841,000  1,840,000  

                Minority Interest(42,000)(62,000)(67,000)



                Net Income From Continuing Ops4,956,000  4,659,000  4,444,000  

                Non-recurring Events

                Discontinued Operations
            -
             
            -
             
            -
             

                Extraordinary Items
            -
             
            -
             
            -
             

                Effect Of Accounting Changes
            -
             
            -
             
            -
             

                Other Items
            -
             
            -
             
            -
             

                        Net Income



                            4,956,000  



                            4,659,000  



                            4,444,000  

                            Preferred Stock And Other Adjustments
            -
             
            -
             
            -
             

                        Net Income Applicable To Common Shares



                            4,956,000  



                            4,659,000  



                            4,444,000  
",A,1
19668429,2013-10-29 20:28:01.657000+00:00,"Python 2.6 and before (as well as Python 3.0) require that you number the placeholders:
'{0} {1}\n'.format(numb, foo)

The numbering, if omitted in Python 2.7 and Python 3.1 and up, is implicit, see the documentation:

Changed in version 2.7: The positional argument specifiers can be omitted, so '{} {}' is equivalent to '{0} {1}'.

The implicit numbering is popular; a lot of examples here on Stack Overflow use it as it is easier to whip up a quick format string that way. I have forgotten to include them more than once when working on projects that must support 2.6 still.",A,25
18005695,2013-08-01 22:26:17.367000+00:00,"Quoting the documentation:

When an 'r' or 'R' prefix is present, a character following a backslash is included in the string without change, and all backslashes are left in the string. For example, the string literal r""\n"" consists of two characters: a backslash and a lowercase 'n'. String quotes can be escaped with a backslash, but the backslash remains in the string; for example, r""\"""" is a valid string literal consisting of two characters: a backslash and a double quote; r""\"" is not a valid string literal (even a raw string cannot end in an odd number of backslashes). Specifically, a raw string cannot end in a single backslash (since the backslash would escape the following quote character).

Added emphasis mine.
Raw strings thus do attach some meaning to a backslash, but only where quotes are concerned.",A,4
18782307,2013-09-13 09:02:03.513000+00:00,"You need to verify either your network or the URL you are trying to retrieve; Python will read from the result socket until EOF is reached so if you got less data than expected, you either were sent an incomplete response, the network cut the response short (proxy server, firewall, what-have-you), or your assumption about what the server sends for that URL is wrong. 
You also want to use shutil.copyfileobj() to do the reading for you:
from shutil import copyfileobj

im = urllib.urlopen('http://gifs.gifbin.com/082013/tn_1378920172_decapitated_snake_bites_itself.gif')
with open('00000001.gif', 'wb') as out:
    copyfileobj(im, out)

This will not only handle reading the data in chunks, but will avoid filling memory with large blobs of binary data to boot. The with statement handles closing the file object for you.",A,0
975709,2009-06-10 13:45:51.247000+00:00,"Facebook can't redirect the response to 'localhost', as that's obviously local to your machine, hence the name. Your app needs to be somewhere Facebook's servers can actually see it - ie on a public host somewhere. 
In other words, you can't develop and test a Facebook app completely on your local machine, as you would with a normal Django app. You'll need to upload it to your host at regular intervals to see any changes.",A,1
18002483,2013-08-01 19:07:40.757000+00:00,"Your problem is that you're using the wrong environment variables.
LD_LIBRARY_PATH is used by the link-loaders for linux/glibc, FreeBSD, and some other *nix platforms, but not for the Mac OS X link-loader. See the manpage for dyld for what does exist.
The obvious replacement is DYLD_LIBRARY_PATH. However, this is probably not what you want (for really the same basic reasons LD_LIBRARY_PATH usually what you want on linux, but the details are different). Instead, you probably want DYLD_FALLBACK_LIBRARY_PATH. Briefly, what this does is change the fallback search paths (normally ~/lib:/usr/local/lib:/lib:/usr/lib) used for libraries not found in the primary search paths, instead of changing the primary search paths, so you can provide missing libraries, without blocking libraries that actually exist. (Try running, e.g., any X11 app each way for a very simple demonstration of the difference.) The manpage explains the details.
(By the way, if you're wondering what's so special about OS X's link-loader that they felt the need to use different environment variables, it's mainly about frameworks and versioning, although the custom magic paths you can embed are also relevant. Someone at NeXT or Apple presumably felt it would be more misleading than helpful to use the same names to mean something 80% the same but 20% different…)",A,1
39280419,2016-09-01 20:32:18.407000+00:00,"Python has already correctly decoded the UTF-8 encoded JSON data to Python (Unicode) strings, so there is no need to handle UTF-8 here.
You'd have to encode to UTF-16, take the length of the encoded data, and divide by two. I'd encode to either utf-16-le or utf-16-be to prevent a BOM from being added:
>>> len(text.encode('utf-16-le')) // 2
32

To use the entity offsets, you can encode to UTF-16, slice on doubled offsets, then decode again:
text_utf16 = text.encode('utf-16-le')
for entity in entities:
    start = entity['offset']
    end = start + entity['length']
    entity_text = text_utf16[start * 2:end * 2].decode('utf-16-le')
    print('Url: ', entity_text)
",A,4
13865058,2012-12-13 17:18:10.383000+00:00,"You can use the itertools.groupby() function:
from itertools import groupby
from operator import itemgetter

for gear, group in groupby(list_sorted, key=itemgetter('Gear')):
    # group is now an iterator, loop over it to get all items with the same value for Gear.
    # gear is the value of this group's ""Gear"" key.
",A,2
43483976,2017-04-18 23:45:03.797000+00:00,"/ true division always produces a floating point result, and you can't accurately model your number with floats:
>>> huge = 13144131834269512219260941993714669605006625743172006030529504645527800951523697620149903055663251854220067020503783524785523675819158836547734770656069476
>>> huge / 2
6.572065917134756e+153
>>> type(huge / 2)
<class 'float'>

That's 6 times 10^153, but float can only carry 53 binary digits of precision in the mantissa:
>>> import sys
>>> sys.float_info.mant_dig
53

Floating point uses binary fractions to model the decimal portion, which means that for the majority of possible decimal values, this is only an approximation anyway.
Converting that value to int() is not going to bring back the precision that was lost.
// floor division on the other hand, produces an integer for integer inputs, and integers have arbitrary precision, so nothing is lost:
>>> type(huge // 2)
<class 'int'>
",A,5
23844924,2014-05-24 12:17:28.023000+00:00,"Use a list comprehension with a filter to rebuild the list:
persons_list = [person for person in persons_list if person.sexe != 'Male']

This picks out all objects that are not male.",A,1
32724622,2015-09-22 18:48:06.843000+00:00,"Focus on finding the <h6> elements, using a CSS selector, then find the accompanying info from there:
for header in soup.select('tr td a h6'):
    name = header.get_text(strip=True)
    practice = header.parent.find_next_sibling(text=True).strip()
    address = header.find_parent('td').find_next_sibling('td').get_text(' ', strip=True)
    print name, practice, address

So this finds all h6 elements contained in a <tr><td><a> wrapper. From there, we can go back up to the parent (the <a> link) and find the next piece of text, and also find the parent <td> element to find the next <td> containing the remaining text.
Given your sample input in a variable named soup, that produces:
>>> for header in soup.select('tr td a h6'):
...     name = header.get_text(strip=True)
...     practice = header.parent.find_next_sibling(text=True).strip()
...     address = header.find_parent('td').find_next_sibling('td').get_text(' ', strip=True)
...     print name, practice, address
... 
Jane Allopathic & Osteopathic Physicians/Internal Medicine NY Medical Ctr New York City, NY
Greg Allopathic & Osteopathic Physicians/Family Medicine 57950 NYC New York City, NY
",A,1
27676906,2014-12-28 13:36:31.713000+00:00,"Use a Bunch class. There are various recipes around, pydanny's looks good.",A,1
16473335,2013-05-10 00:29:24.047000+00:00,"The error ('URL has an invalid label.') is probably a bug in requests library: it applies idna encoding (for internationalized domain names) on hostname with userinfo attached, source:
netloc = netloc.encode('idna').decode('utf-8')

that might raise 'label empty or too long' error for the long username:password. You can try to report it on the requests' issue tracker.
a:b@example.com form is deprecated otherwise
requests.get('https://a:b@example.com') should be equivalent to requests.get('https://example.com', auth=('a', 'b')) if all characters in username:password are from [-A-Za-z0-9._~!$&'()*+,;=] set.
curl and requests also differ then there are percent-encoded characters in userinfo e.g., https://a:%C3%80@example.com leads to curl generating the following http header:
Authorization: Basic YTrDgA==

but requests produces:
Authorization: Basic YTolQzMlODA=

i.e.:
>>> import base64
>>> base64.b64decode('YTrDgA==')
'a:\xc3\x80'
>>> print _
a:À
>>> base64.b64decode('YTolQzMlODA=')
'a:%C3%80'
",A,2
50013980,2018-04-25 04:16:25.997000+00:00,"There are actually multiple problems here, but let's get to the root of it:
The winning_combo function resets turn = 0 at the start of each call. Sure, you update it to 1 with that turn = (turn + 1) % len(players), but you're never going to use the value again; you just return from the function, and then call it again, and then set turn = 0 again.
The simplest fix here would be to use a global variable for turn, so it can be persistent across calls:
def winning_combo(player1, player2):
    global turn
    players = [player1, player2]
    player = players[turn]
    turn = (turn + 1) % len(players)
    while tiles_available:
        # the rest of the code is the same

def game():
    global turn
    turn = 0
    playing = True
    # the rest of the code is the same

You may have heard that global variables are bad. Can you avoid one here? Sure. While you can't use a local variable in winning_combo, because that function keeps exiting and starting over again, you can use a local variable in game, and pass it in as a parameter to winning_combo:
def winning_combo(player1, player2, turn):
    players = [player1, player2]
    player = players[turn]
    while tiles_available:
        move = int(input('Where would you like to move? '))
        # the rest of the code is the same

def game():
    turn = 0
    playing = True
    player1, player2 = player_input()
    while playing:
        draw_board()
        winning_combo(player1, player2, turn)
        turn = (turn + 1) % 2
        win_combo()

There are many other ways you could improve your design and simplify your code, and bugs to fix (e.g., what happens when the game is a tie?), but this should get you past your current bug, and hopefully give you a little insight into how local variables work.",A,0
43192288,2017-04-03 18:58:52.273000+00:00,"This is the opposite of the frequent question on how to have ""variable variables"". But the answer is exactly the same: don't do that, use a dict.
Store this data as a single dict with those values as the keys, then you can use the .keys() method to give you the result you want.",A,1
22355073,2014-03-12 14:45:25.963000+00:00,"You are looking at the repr() representation of a Python object that doesn't have a literal representation. Such objects follow the default <type ... at 0xid> pattern most Python objects follow.
It's purpose is for you to be able to distinguish different function objects from one another, even if their names are the same; if you have two foo functions you'd want to be able to see if they are the same object or not; the id() value in the representation allows you to do so.
Quoting from the __repr__ method documenation:

Called by the repr() built-in function and by string conversions (reverse quotes) to compute the “official” string representation of an object. If at all possible, this should look like a valid Python expression that could be used to recreate an object with the same value (given an appropriate environment). If this is not possible, a string of the form <...some useful description...> should be returned. The return value must be a string object. If a class defines __repr__() but not __str__(), then __repr__() is also used when an “informal” string representation of instances of that class is required.
This is typically used for debugging, so it is important that the representation is information-rich and unambiguous.

The contents of a function are a) potentially much larger, and b) require a source file to be available. The function object itself only references bytecode, the compiled code object. Even if the source files are available printing a container object full of functions would result in overly verbose output.
If you wanted to see the source code for a given function object, use inspect.getsource() on the object. This can raise an IOError exception if no source is available.",A,3
22713574,2014-03-28 12:59:27.263000+00:00,"To get all output whether the subprocess prints on stdout or stderr, you could use stderr=STDOUT and subprocess.check_output:
from subprocess import check_output, STDOUT

all_output = check_output(['tshark', '-D'], stderr=STDOUT)

tshark manual says:

-D  Print a list of the interfaces on which TShark can capture, and exit.

It doesn't specify that the output must be on stderr. Though on my system it prints the diagnostic information only to stderr. Stdout might be reserved for printing captured data.",A,2
13218066,2012-11-04 11:18:25.573000+00:00,"You need to store a reference to your entry instance on you App:
class App:    
    def __init__(self, master):
        self.e1 = Entry(master)
        self.e1.grid(row=1, column=0, sticky=N)
        Button(master, text=""Start"", command=self.OnButtonClick).grid(row=4, column=0)

Now you can access self.e1 from OnButtonClick and call its.get()` method to get the current entry text:
    def OnButtonClick(self):
        value = self.e1.get()
",A,1
42746999,2017-03-12 11:46:40.350000+00:00,"No, that's not really what Varnish is for; it's more for caching complete pages.
A better fit here would be memcached, which is perfect for storing arbitrary data against a key. Redis could be another alternative.",A,0
4931347,2011-02-08 09:25:46.170000+00:00,"It depends on what you mean by the 'value' of the field.
If you're trying to change what is shown when the form is displayed initially, you need to pass an initial dictionary with a key corresponding to the field name:
self.initial['choice'] = 'myvalue'

If you're trying to change what gets passed into validation and then to cleaned_data, you need to change the data parameter:
self.data['choice'] = 'myvalue'
",A,3
21683230,2014-02-10 16:54:02.697000+00:00,"datetime.date objects are immutable, but they do have a .replace() method to do what you want:
somedate = somedate.replace(day=1)

The method returns a new object with the desired values swapped out.
For completeness sake, there are also datetime.datetime.replace() and  datetime.time.replace() methods.",A,6
20726022,2013-12-22 03:42:45.693000+00:00,"Use a regular expression:
import re

if re.match('^[-0-9^+x]*$', text):
    # Valid input

The re module comes with Python 2.5, and is your fastest option.
Demo:
>>> re.match('^[-0-9^+x]*$', '1x2^4-2')
<_sre.SRE_Match object at 0x10f0b6780>
",A,10
35831161,2016-03-06 19:01:46.993000+00:00,"Iterate over the itertools.permutations() results and use str.join() to create one string from each:
for combo in itertools.permutations([""an"", ""de"", ""si"", ""ta""], 4):
    print(''.join(combo))

Demo:
>>> import itertools
>>> for combo in itertools.permutations([""an"", ""de"", ""si"", ""ta""], 4):
...     print(''.join(combo))
...
andesita
andetasi
ansideta
ansitade
antadesi
antaside
deansita
deantasi
desianta
desitaan
detaansi
detasian
siandeta
siantade
sideanta
sidetaan
sitaande
sitadean
taandesi
taanside
tadeansi
tadesian
tasiande
tasidean
",A,0
5668913,2011-04-14 19:56:08.983000+00:00,"Python
 #!/usr/bin/env python
 import sys
 column = 1 # the column to search
 value = ""the data you're looking for""
 with open(""your file"",""r"") as source:
    for line in source:
        fields = line.strip().split(';')
        if fields[column] == value:
             print line
",A,1
50515071,2018-05-24 17:30:25.137000+00:00,"If nested_list is one of your dictionary values, then you are applying '\t'.join() to the individual words. You'd want to join the whole list:
file.write('\t'.join(nested_list) + '\n')

or, if you were to loop over the values of the dictionary:
file.writelines(
    '\t'.join(nested_list) + '\n'
    for nested_list in dict_with_lists.values())

The above uses the file.writelines() method correctly; passing in an iterable of strings to write. If you were to pass in a single string, then you are only causing Python extra work as it loops over all the individual characters of that string to write those separately, after which the underlying buffer has to assemble those back into bigger strings again.
However, there is no need to re-invent the character-separated-values writing wheel here. Use the csv module, setting the delimiter to '\t':
import csv

with open('fname', 'w', newline='') as file:
    writer = csv.writer(file, delimiter='\t')
    writer.writerows(dict_with_lists.values())

The above writes all lists in the dict_with_lists dictionary to a file. The csv.writer() object doesn't mind if your lists are of differing lengths.",A,0
14553532,2013-01-27 23:29:30.830000+00:00,"On Windows, you want to use time.clock() instead; time.time() only has 1/60th-second granularity, while the former gives you microsecond granularity instead.
Or, to keep it cross-platform, use timeit.default_timer() instead, which will use the correct time function for your platform:
import timeit

start = timeit.default_timer()
",A,6
15155259,2013-03-01 10:17:50.003000+00:00,"You'd have to do some kind of count. Using the collections.Counter() class would make that easy:
from collections import Counter
counts = Counter(main_list)

duplicate_list, unique_list = [], []
for entry in main_list:
    if counts[entry] > 1:
        duplicate_list.append(entry)
    else:
        unique_list.append(entry)

counts is a multi-set or bag; a mapping of entry to it's count in main_list. The above example preserves the ordering of main_list.",A,6
30626141,2015-06-03 16:51:00.183000+00:00,"This is a bug in the library; the User.__repr__ method returns bytes on Python 3:
def __repr__(self):
    return '<User {0!r} {1!r}>'.format(self.id, self.username).encode('utf-8')

You already filed a bug report with the project, which is great!
You can avoid the issue you see in IPython or any other interactive Python console by assigning the result of discogs.identity() to a variable:
user = discogs.identity()

Try to avoid echoing the result.
You can patch the method on the fly with:
import six
from discogs_client import models

orig_repr = models.User.__repr__

def fixed_repr(self):
    r = orig_repr(self)
    if six.PY3 and isinstance(r, bytes):
        r = r.decode('utf8')
    return r

models.User.__repr__ = fixed_repr

You probably have to do this for other models as well; I see more __repr__ implementations with .encode('utf8') calls in the models module.",A,2
22267495,2014-03-08 09:23:33.123000+00:00,"Use the enumerate() function to produce a running index:
for index, i in enumerate(row):
    if i == 5:
        print(index)
    if i == 2:
        print(index)

or simpler still:
for index, i in enumerate(row):
    if i in (2, 5):
        print(index)
",A,7
15145134,2013-02-28 20:34:21.963000+00:00,"Python's standard library doesn't come with any powerful-enough image-manipulation code, but there are a few alternatives that are easy to install and use. I'll show how to do this with PIL.
from PIL import Image

def randomStars(small, large):
    import random
    filename = pickAFile()
    pic = Image.open(filename)
    max_x, max_y = pic.size
    pixels = im.load()
    x = random.randrange(max_x)
    y = random.randrange(max_y)
    for i in range(max_x):
        for j in range(max_y):
            if random.random() < 0.25:
                red = random.randint(256)
                green = random.randint(256)
                blue = random.randint(256) 
                pixels[i, j] = (red, green, blue, 1)
    im.show()

The show function doesn't display the image in your app (for that, you'd need some kind of GUI with an event loop, like tkinter or PySide); it saves a file to a temporary directory and runs a platform-specific program like Preview or xv to display it.
I assume you're also going to want to save the file. That's easy too:
    name, ext = os.path.splitext(filename)
    outfilename = '{}-with-stars.{}'.format(name, ext)
    im.save(outfilename)

This will save it back to a .jpg with default JPEG settings, relying on PIL guessing what you want from the filename. (Which means, yes, you can save it as a PNG just by using '{}-with-stars.png'.format(name).) If you want more control, PIL can do that too, specifying an explicit format, and format-specific options.

So far, this is just how to turn your existing code into something that works, that you can play with and start debugging; it doesn't actually answer the original problem.

I want to write a function that will create a random number (between m and n, inclusive) of stars in the sky of this picture

So first, you need this as your loop, instead of a loop over all pixels:
for _ in random.randint(m, n):

Now:

I want the stars should be randomly composed of either a single white pixel, or a square of 4 adjacent white pixels.

    x, y = random.randrange(max_x), random.randrange(max_y)
    if random.random() < .5:
        # draw white pixel at [x, y]
        pixels[x, y] = (1, 1, 1, 1)
    else:
        # draw square at [x, y], making sure to handle edges


I also do not want to place a 'star' (of 1 pixel) over the tree branches, the moon or the bird though

You need to define how you know what's part of a tree branch, the moon, or the bird. Can you define that in terms of pixel colors?
From a quick glance, it looks like you might be able to. The moon's pixels are all brighter, more saturated, more red-biased, etc. than anything else (except the AP logo in the corner, which is even brighter). The bird and the branches are darker than anything else. In fact, they're so distinct that you probably don't even have to worry about doing correct colorspace math; it may be as simple as something like this:
r, g, b, a = pixels[x, y]
fake_brightness = r+g+b+a
if fake_brightness < 0.2:
    # Tree or bird, pick a new random position
elif 1.2 < fake_brightness < 2.8:
    # Moon, pick a new random position
else:
    # Sky or API logo, scribble away

(Those numbers are obviously just pulled out of thin air, but a bit of trial and error should give you usable values.)
Of course if you're doing this as a learning exercise, you probably want to learn the correct colorspace math, and maybe even write an edge-detection algorithm, rather than relying on this image being so simply-parseable.",A,2
27324319,2014-12-05 20:40:25.930000+00:00,"You are not creating a datetime instance here:
d = datetime

That is just a new reference to the datetime type. The attributes d.year, d.month, etc. are descriptors there, not values you can interpolate:
>>> from datetime import datetime
>>> datetime.year
<attribute 'year' of 'datetime.date' objects>
>>> type(datetime.year)
<type 'getset_descriptor'>
>>> '%04d' % datetime.year
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: %d format: a number is required, not getset_descriptor

If you wanted to current timestamp, you'll need to call datetime.now():
d = datetime.now()

There are better ways to use the datetime value in a string. You could use the datetime.strftime() method to produce a string format for your date:
formatted = d.strftime('%Y%m%d%H%M')
folderToSave = ""/home/timelapse/timelapse_"" + formatted

or you can use the same formatting codes in a str.format() interpolation:
folderToSave = ""/home/timelapse/timelapse_{:%Y%m%d%H%M}"".format(d)
",A,1
41763082,2017-01-20 11:53:12.717000+00:00,"(v>0) produces a boolean object, either True or False. Boolean is a subclass of integer, with True having the integer value 1, False the integer value 0:
>>> issubclass(bool, int)
True
>>> int(True)
1
>>> int(False)
0

When you use a boolean object with an arithmetic operator, it is treated as an integer.
So if v is smaller than or equal to 0, (v>0) produces False, and if used in multiplication the integer value of 0 is used:
>>> v = 42
>>> v > 0
True
>>> v * True
42
>>> v = -81
>>> v > 0
False
>>> v * False
0

Using a boolean test like this is pure obfuscation. Don't ever use it in production code, it'll only serve to confuse the reader.
It is slower than the conditional expression approach too:
>>> from timeit import timeit
>>> from random import randint
>>> values = [randint(-10**6, 10**6) for _ in range(1000)]
>>> timeit('[0 if v < 0 else v for v in values]', 'from __main__ import values', number=10000)
0.4128753509976377
>>> timeit('[v*(v>0) for v in values]', 'from __main__ import values', number=10000)
0.5792852119993768

Selecting between two references (v or 0) based on a boolean test is faster than multiplication. And did I mention that using a boolean like that creates an unreadable mess and should never be used?",A,3
31278287,2015-07-07 20:05:13.693000+00:00,"If you know that your input is valid then you could use a more efficient code (if you need it):
In [1]: import calendar

In [2]: from datetime import datetime

In [3]: dates = ['2/23/2015', '3/2/2015', '3/9/2015', '3/16/2015', '3/23/2015', '3/30/2015', '4/6/2015', '4/13/2015']

In [4]: %timeit [datetime.strptime(s, ""%m/%d/%Y"").strftime('%B') for s in dates]                          
The slowest run took 1373.48 times longer than the fastest. This could mean that an intermediate result is being cached 
10000 loops, best of 3: 128 µs per loop

In [5]: %timeit [calendar.month_name[int(s.partition('/')[0], 10)] for s in dates]
10000 loops, best of 3: 32.3 µs per loop

calendar.month_name[int(s.partition('/')[0], 10)] is noticeably faster here.",A,0
7483908,2011-09-20 10:46:48.727000+00:00,"Nothing is wrong. You have done the correct thing. If you want to use non-ASCII characters in your code, you need to tell Python what encoding you are using.
This has nothing to do with Django, or Unicode. ",A,6
36286766,2016-03-29 14:07:34.813000+00:00,"vuln_cat is only set after the list comprehension completes, you can't access the list being built from inside the comprehension.
Use a generator expression inside the next() function instead, this'll only pick the first matching result:
def vuln_category(vuln_title):
    vuln_title = vuln_title.lower()
    return next(
        (categories[k] for k in categories if k in vuln_title),
        'Other')

The second argument to the next() function is a default, used if the generator expression did not yield any results.
Demo:
>>> categories = {
...     'rhsa': 'Red Hat',
...     'red hat': 'Red Hat',
...     '.net': 'Microsoft .Net',
...     'sql server': 'MS SQL',
...     'microsoft': 'Microsoft',
... }
>>> def vuln_category(vuln_title):
...     vuln_title = vuln_title.lower()
...     return next(
...         (categories[k] for k in categories if k in vuln_title),
...         'Other')
...
>>> vuln_category('Microsoft SQL Server on .NET')
'MS SQL'
>>> vuln_category('Red Hat RHSA')
'Red Hat'
>>> vuln_category('Mac OS X')
'Other'
",A,2
19825550,2013-11-07 00:11:19.143000+00:00,"You really, really, really don't want to try to use repr and eval as a serialization format.
If you just used, say, pickle, you wouldn't have this problem at all:
with open(""task_list2.dat"", ""wb"") as outFile:
    pickle.dump(task_list, outFile)

with open(""task_list2.dat"", ""rb"") as file:
    task_list = pickle.load(file)

Much simpler, yes?

But if you want to know how to solve the immediate problem instead of making it irrelevant: You've got multiple problems in your __repr__ method, all of which need to be fixed if you want it to be round-trippable.

You generate a string representation… and then call repr on it. You want to return the string representation, not a string representation of the string representation. Just leave out the repr.
You should always delegate to the repr of sub-objects, not the str. If you're using %-formatting, that means using %r rather than %s.
Don't try to add quotes around things. That may happen to work if the object itself has no quotes, backslashes, invisible characters, etc. in it, but why rely on that? If you think you need quotes, it's pretty much always a sign that you broke the previous rule, and you should fix that instead.

Here's how you can write a round-trippable repr for this class:
def __repr__(self):
    return 'Task(%r, %r)' % (self.name, self.timespent))

And you can verify that it does what you want:
>>> t = Task('task name', 23.4)
>>> t
Task('task name', 23.4)
>>> eval(repr(t))
Task('task name', 23.4)


Of course in your particular example, just fixing the first problem (removing the spurious call to repr) would have gotten rid of the single quotes and made that particular example work. You could also hack around that on the read side by calling eval twice. Or, for this particular example, even by calling eval(s[1:-1]) or eval(s.strip(""'"")). But any ""fix"" like that is just going to make it harder to debug the general problems you're going to run into once you have, e.g., a name that isn't as simple as a single all-ASCII-letter word.",A,5
17231974,2013-06-21 09:35:42.860000+00:00,"b() will only be evaluated if a(my_var) is True, yes. The and operator short-circuits if a(my_var) is falsey.
From the boolean operators documentation:

The expression x and y first evaluates x; if x is false, its value is returned; otherwise, y is evaluated and the resulting value is returned.

You can test this yourself with a function that prints something when called:
>>> def noisy(retval):
...     print ""Called, returning {!r}"".format(retval)
...     return retval
... 
>>> noisy(True) and noisy('whatever')
Called, returning True
Called, returning 'whatever'
'whatever'
>>> noisy(False) and noisy('whatever')
Called, returning False
False

Python consideres empty containers and numeric 0 values as false:
>>> noisy(0) and noisy('whatever')
Called, returning 0
0
>>> noisy('') and noisy('whatever')
Called, returning ''
''
>>> noisy({}) and noisy('whatever')
Called, returning {}
{}

Custom classes can implement a __nonzero__ hook to return a boolean flag for the same test, or implement a __len__ hook if they are a container type instead; returning 0 means the container is empty and is to be considered false.
On a closely related note, the or operator does the same thing, but in reverse. If the first expression evaluates to true the second expression will not be evaluated:
>>> noisy('Non-empty string is true') or noisy('whatever')
Called, returning 'Non-empty string is true'
'Non-empty string is true'
>>> noisy('') or noisy('But an empty string is false')
Called, returning ''
Called, returning 'But an empty string is false'
'But an empty string is false'
",A,7
17346495,2013-06-27 14:53:37.863000+00:00,"You will have to loop through the list. The good news is is that you can use a generator expression with next() to do that looping:
yourdict = next(d for d in somedict['items'] if d['id'] == 1)

This can raise a StopIteration exception if there is no such matching dictionary.
Use
yourdict = next((d for d in somedict['items'] if d['id'] == 1), None)

to return a default instead for that edge-case (here None is used, but pick what you need).",A,3
18716613,2013-09-10 10:34:26.027000+00:00,"You are trying to assign to literal integer values. 1, 2, etc. are not valid names; they are only valid integers:
>>> 1
1
>>> 1 = 'something'
  File ""<stdin>"", line 1
SyntaxError: can't assign to literal

You probably want to use a list or dictionary instead:
names = []
for i in range(1, 6):
    name = input(""Please enter name {}:"".format(i))
    names.append(name)

Using a list makes it much easier to pick a random value too:
winner = random.choice(names)
print('Well done {}. You are the winner!'.format(winner))
",A,6
27170315,2014-11-27 12:02:02.643000+00:00,"Rather than use a large number of if tests, just split off the extension and use a dictionary lookup:
import os.path

extension = os.path.splitext(self.path)[-1]
mime_types = {
    'html': 'text/html', 
    'jpg':'image/jpg',
    'gif': 'image/gif',
    'js': 'application/javascript',
    '.css': 'text/css',
}
mimetype = mime_types.get(extension)
sendReply = mimetype is not None
",A,2
48050501,2018-01-01 14:31:55.800000+00:00,"Enums support this usecase directly. The documentation for the library covers this in the Planet example in the examples section:

If __new__() or __init__() is defined the value of the enum member will be passed to those methods:
>>> class Planet(Enum):
...     MERCURY = (3.303e+23, 2.4397e6)
...     # ...
...
...     def __init__(self, mass, radius):
...         self.mass = mass       # in kilograms
...         self.radius = radius   # in meters
...     @property
...     def surface_gravity(self):
...         # universal gravitational constant  (m3 kg-1 s-2)
...         G = 6.67300E-11
...         return G * self.mass / (self.radius * self.radius)

[...]
>>> Planet.EARTH.value
(5.976e+24, 6378140.0)
>>> Planet.EARTH.surface_gravity
9.802652743337129


So for your specific example, just define a __init__ method:
from enum import Enum

class Color(Enum):
    BLACK = ('black', '#000')
    WHITE = ('white', '#fff')

    def __init__(self, color_name, hex):
        self.color_name = color_name
        self.hex = hex 

print(Color.BLACK.color_name + ' ' + Color.BLACK.hex)

I did not use name as an attribute because that is a reserved attribute (used to reflect the enum value name, here BLACK and WHITE):
>>> Color.BLACK
<Color.BLACK: ('black', '#000')>
>>> Color.BLACK.name
'BLACK'
>>> Color.BLACK.color_name
'black'
>>> Color.BLACK.hex
'#000'

You can still override the name attribute with a @property but I’d not deviate from the standard here. 
I used this technique to define the virus states in my Advent of Code day 22 solution, defining both the next state name and the direction change for each entry. ",A,9
48110976,2018-01-05 09:49:32.590000+00:00,"Looks like you have a typo in the code of your decorator, which you're not showing - the traceback says that chat/utils.py line 14 has
return func(message, args, **kwargs)

when it should be
return func(message, *args, **kwargs)

Presumably the second argument is an empty tuple.",A,2
22116466,2014-03-01 15:44:11.603000+00:00,"You need to tell your mocks what to return. The __addon__ value is the result of a xbmcaddon.Addon() call, so you can get access to that mock object with:
addon = self.xbmcaddon.Addon.return_value

because .return_value gives you the actual Mock object that calling Addon() would return.
Now you can tell that Mock object what to return when the getSetting() method is called; there are two values to provide here, so you could use the side_effect to set a sequence of values to return:
addon.getSetting.side_effect = ['some_api_key', '42']

where the first call to __addon__.getSetting() will produce the first value 'some_api_key', the second cal will produce '42'.",A,1
20202063,2013-11-25 19:56:32.730000+00:00,"By using sqrt() you end up with floating point values, but the method you are calling wants integers only. Call int() on the values before passing them to pic.addLine():
pic.addLine(black, int(a1), int(b1), int(a2), int(b2))
pic.addLine(black, int(c1), int(d1), int(c2), int(d2))
",A,1
12591702,2012-09-25 21:58:51.837000+00:00,"The default implementation of float.__str__ limits the output to 12 digits only.
Thus, the least significant digits are dropped and what is left is the value 0.2.
To print more digits (if available), use string formatting:
print '%f' % result  # prints 0.200000

That defaults to 6 digits, but you can specify more precision:
print '%.16f' % result  # prints 0.2000000000000002

Alternatively, python offers a newer string formatting method too:
print '{0:.16f}'.format(result)  # prints 0.2000000000000002

Why python produces the 'imprecise' result in the first place has everything to do with the imprecise nature of floating point arithmetic. Use the decimal module instead if you need more predictable precision:
>>> from decimal import *
>>> getcontext().prec = 1
>>> Decimal(5.2) - Decimal(5.0)
Decimal('0.2')
",A,1
15276352,2013-03-07 16:23:28.623000+00:00,"You could use xpath expressions for that:
for name in namelist:
    firstname = library.find('.//firstname[@name=""{}""]'.format(name))
",A,1
8032769,2011-11-07 04:34:16.527000+00:00,"This line (that you say works) shows that wordsofar is a sequence of one element:
c.execute(""SELECT word FROM wordlist WHERE word LIKE ?"", wordsofar)

In this case the second line should be:
c.execute(""SELECT word FROM wordlist WHERE word LIKE ? AND word NOT LIKE ?"", 
          wordsofar + (noletter,))

If noletter is a string and wordsofar is a tuple (as you say in your comment).
execute() docs say that the second argument is always parameters. If you use '?' then number of parameters (len(parameters)) is equal to number of '?' in the sql statement.",A,1
32464486,2015-09-08 18:09:31.250000+00:00,"The coroutines work as is. Just use the returned value from loop.run_until_complete() and call asyncio.gather() to collect multiple results:
#!/usr/bin/env python3
import asyncio

@asyncio.coroutine
def func_normal():
    print('A')
    yield from asyncio.sleep(5)
    print('B')
    return 'saad'

@asyncio.coroutine
def func_infinite():
    for i in range(10):
        print(""--%d"" % i)
    return 'saad2'

loop = asyncio.get_event_loop()
tasks = func_normal(), func_infinite()
a, b = loop.run_until_complete(asyncio.gather(*tasks))
print(""func_normal()={a}, func_infinite()={b}"".format(**vars()))
loop.close()

Output
--0
--1
--2
--3
--4
--5
--6
--7
--8
--9
A
B
func_normal()=saad, func_infinite()=saad2
",A,29
49540873,2018-03-28 17:37:03.727000+00:00,"What you're asking for can be done, but only by changing from an array of numbers to an array of object—that is, general-anything-at-all values:
>>> x = np.array([[0,1,1,2], [0, 5, 0, 0], [2, 0,3,3]], dtype=object)
>>> x[x==0] = 'x'
>>> x
array([['x', 1, 1, 2],
       ['x', 5, 'x', 'x'],
       [2, 'x', 3, 3]], dtype=object)

But this is probably not what you want. Most of the speed and space savings of numpy come from the fact that arrays have a specific data type; if you use the generic object, they're no smaller, and not much faster, than just using a list of lists. If you're only using numpy for syntactic convenience rather than space or speed benefits, that may be fine, but it's definitely something to think about before you do it.
Also, an array of objects follows the usual Python rules for any arithmetic on those objects. For example:
>>> x+2
TypeError: must be str, not int

That doesn't seem very useful.

If you're looking for a special ""marker value"" that prevents you from accidentally thinking some value is meaningful when it actually wasn't, you can do that with floats, although not with ints, by using nan:
>>> x = np.array([[0,1,1,2], [0, 5, 0, 0], [2, 0,3,3]], dtype=np.float64)
>>> x[x==0] = np.nan
>>> x
array([[nan,  1.,  1.,  2.],
       [nan,  5., nan, nan],
       [ 2., nan,  3.,  3.]])

A nan can be stored in a float64 slot, so you still have all the space and speed benefits of a fixed-type array. And (by default), operations on nan don't raise an exception, they just return nan. So:
>>> x+2
array([[nan,  3.,  3.,  4.],
       [nan,  7., nan, nan],
       [ 4., nan,  5.,  5.]])


In some cases, it may be even better to just leave the array alone and operate on the array with a mask:
>>> x = np.array([[0,1,1,2], [0, 5, 0, 0], [2, 0,3,3]], dtype=np.int64)
>>> x[x!=0] += 2
>>> x
array([[0, 3, 3, 4],
       [0, 7, 0, 0],
       [4, 0, 5, 5]])

Or, if your indices don't matter, only your values, you can even do this (the simplest version, ignoring even axes):
>>> x = np.array([[0,1,1,2], [0, 5, 0, 0], [2, 0,3,3]], dtype=np.int64)
>>> y = x[x!=0]
>>> y+2
array([3, 3, 4, 7, 4, 5, 5])
",A,3
32954749,2015-10-05 17:48:34.927000+00:00,"Counter.update() takes a sequence of things to count. If you need to count a tuple, put that value into a sequence before passing it to the Counter.update() method:
a.update([str_tuple])

or use:
a[str_tuple] += 1

to increment the count for that one tuple by one.
Demo:
>>> from collections import Counter
>>> str_tuple = ""abcd"",
>>> a = Counter()  
>>> a.update([str_tuple])
>>> a
Counter({('abcd',): 1})
>>> a = Counter()  
>>> a[str_tuple] += 1
>>> a
Counter({('abcd',): 1})
",A,1
11705135,2012-07-28 22:24:07.397000+00:00,"You should use with to do the expensive query once and store it the context.
{% with item.listing as item_listing %}
   {% if item_listing %} ... {% endif %} ... etc ...
{% endwith %}
",A,2
1890242,2009-12-11 19:11:26.737000+00:00,"I set my PYTHONPATH to point to the latest-and-greatest version.  No editing.
export PYTHONPATH=.:/the/new/version
",A,1
21150794,2014-01-15 23:54:22.283000+00:00,"If you're trying to launch a new terminal window to run a new Python in (which isn't what your question asks for, but from a comment it sounds like it's what you actually want):
You can't. At least not in a general-purpose, cross-platform way.
Python is just a command-line program that runs with whatever stdin/stdout/stderr it's given. If those happen to be from a terminal, then it's running in a terminal. It doesn't know anything about the terminal beyond that.
If you need to do this for some specific platform and some specific terminal program—e.g., Terminal.app on OS X, iTerm on OS X, the ""DOS prompt"" on Windows, gnome-terminal on any X11 system, etc.—that's generally doable, but the way to do it is by launching or scripting the terminal program and telling it to open a new window and run Python in that window. And, needless to say, they all have completely different ways of doing that.
And even then, it's not going to be possible in all cases. For example, if you ssh in to a remote machine and run Python on that machine, there is no way it can reach back to your machine and open a new terminal window.
On most platforms that have multiple possible terminals, you can write some heuristic code that figures out which terminal you're currently running under by just walking os.getppid() until you find something that looks like a terminal you know how to deal with (and if you get to init/launchd/etc. without finding one, then you weren't running in a terminal).",A,2
3585381,2010-08-27 14:51:53.677000+00:00,"Don't use lambdas.  They are late binding.  Perhaps you want a partial from functools, but even that seems too complex.
Your data coming in has field names, numbers and values, right?  
Your command-line parameters use field numbers and values, right?
You want a dictionary keyed by field number.  In that case, you don't need any complex lookups.  You just want something like this.
def match( packet_dict, criteria_list ):
    t = [ packet_dict[f] == v for f,v in criteria_list ]
    return any( t )

Something like that should handle everything for you.",A,2
12579790,2012-09-25 09:15:33.983000+00:00,"You cannot access _dict while defining it. Python first evaluates the {...} dict literal before assigning it to _dict. In other words, while evaluating the {...} dict literal statement, _dict is not yet defined and thus cannot be accessed.
Do this instead:
_dict = {""foo"" : 1234}
_dict[""bar""] = _dict[""foo""] + 1
",A,13
43441568,2017-04-16 20:02:44.377000+00:00,"The split includes all capturing groups. Use (?:...) to create a non-capturing group around the \w. sub-pattern instead:
split_p = r'((?:\w\.){2,})'

Demo:
>>> import re
>>> split_p = r'((?:\w\.){2,})'
>>> string = '(N.B.--I'
>>> re.split(split_p, string)
['(', 'N.B.', '--I']

Next, if you want to split on repeating dashes, just add an alternative pattern with |:
split_p = r'((?:\w\.){2,}|-+)'

Demo:
>>> split_p = r'((?:\w\.){2,}|-+)'
>>> re.split(split_p, string)
['(', 'N.B.', '', '--', 'I']

This gives an empty string in-between because there are 0 characters between the N.B. split point and the -- point; you'd have to filter those out again.",A,0
25780798,2014-09-11 06:43:22.800000+00:00,"The obvious thing to do is shifting and masking.
It would be easier if you wanted the lowest 16 bytes, then the next lowest, etc., with the top part incomplete, so let's do that first:
bits = []
while plain_text:
    bits.append(plain_text & ((1 << 128) - 1))
    plain_text >>= 128
bits.reverse()

Instead, it looks like you want the highest 16 bytes, etc., leaving the smallest part incomplete instead of the largest. But that's not a huge deal. The only tricky part is knowing what mask to start with, and you can get that with plain_text.bit_length() (or, if you're on an older version of Python, by doing the same thing manually by using the equivalent code shown in the docs). Also, it looks like you want to split on the nibble rather than the byte, but that's not too hard either:
bits = []
shift = (math.ceil(plain_text.bit_length() / 4) % 32) * 4
while plain_text:
    bits.append(plain_text & (1 << shift) - 1)
    plain_text >>= shift
    shift = 128
bits.reverse()

Does it work?
>>> part1, part2, part3 = bits
>>> hex(part1), hex(part2), hex(part3)
('0x3243f6a8885a308d313198a2e0370734',
 '0x3243f6a8885a308d313198a2e0370734',
 '0x54a2e')

As a side note, I'm a little concerned that you want to store this in variables named part1, part2, and part3 instead of just a list called parts (or bits). What are you going to do if plain_text is a little bigger and won't fit into 3 parts?",A,2
26230047,2014-10-07 06:42:54.587000+00:00,"You might consider a different approach. Rather than scanning through models to find the relevant ones, you could get the models themselves to register themselves with your system, via the use of a metaclass. Marty Alchin has a great example of a simple way to do that.",A,2
1419098,2009-09-13 23:14:16.883000+00:00,"There are two ways to assign argument values to function parameters, both are used.

By Position.  Positional arguments do not have keywords and are assigned first.
By Keyword.  Keyword arguments have keywords and are assigned second, after positional arguments.

Note that you have the option to use positional arguments.
If you don't use positional arguments, then -- yes -- everything you wrote turns out to be a keyword argument.
When you call a function you make a decision to use position or keyword or a mixture.  You can choose to do all keywords if you want.  Some of us do not make this choice and use positional arguments.",A,23
23763477,2014-05-20 15:03:34.630000+00:00,"This has nothing to do with the Django template, but how you define the variable in the first place.
Basckslashes are only ""interpreted"" when you specify them as literals in your Python code. So given your Python code above, you can either use the double backslash, or use a raw string.
If you were loading the string ""fred\xbf"" from your database and outputting it in a template, it would not be ""escaped"".",A,2
325742,2008-11-28 13:06:02.450000+00:00,"Algorithms are independent of language, true.  But any medium you use is a language, period.  Using Excel means you're using the excel ""language"" (rows, columns, cells, etc.) to express some of your algorithm.  Maybe not the complete, finished product, but you've expressed it in Excel.
UML diagrams are a language for expressing an algorithm.  A sketch on paper is a language for expression.  It's hard to separate the platonic ideal algorithm from all concrete representations.  No matter what you do, you're expressing it in some language.
The trick is to separate the algorithm from specific language features and limitations.
Any ""informal"" notation will help do this.  English (or other natural language), mathematics, diagrams, etc., are all candidates for expressing an algorithm in a language that's free from implementation quirks and problems.
I start with an overview in English.  Plain text, not even MS-Word or something where formatting is a distractor.
For really complex things, a little supplemental mathematics helps put formal assertions around the state of the program.
Also, UML diagrams help.  I use Argo UML -- cheap and effective.
For more on this topic, you want to read about formal verification systems.",A,3
1187761,2009-07-27 11:55:41.380000+00:00,"It's really hard to predict the future.
The units (minutes, hours, days, weeks, fortnights) don't matter.
Pick a unit that makes your manager happy.
Just be clear that an estimate of 30 minutes, .5 hour or .0625 days is only a guess, not a fact.
An estimate of 0.0625 days or 30 minutes looks really precise because it has a lot of decimal places.  However, any ambiguity about the requirements, the architecture, the language, the libraries, the unit tests, or anything else will make this number incorrect.
The very best you can hope for is that the average of all your estimates is reasonably close to the actual facts as they unfold.  This means that half your estimates will be too low and half will be too high.  It also means that some fraction of your estimates will be really, really far from your manager's hoped-for accuracy.",A,2
27930100,2015-01-13 19:33:50.847000+00:00,"Functions are added to the current namespace like any other name would be added. That means you can use the global keyword inside a function or method:
def create_global_function():
    global foo
    def foo(): return 'bar'

The same applies to a class body or method:
class ClassWithGlobalFunction:
    global spam
    def spam(): return 'eggs'

    def method(self):
        global monty
        def monty(): return 'python'

with the difference that spam will be defined immediately as top-level class bodies are executed on import.
Like all uses of global you probably want to rethink the problem and find another way to solve it. You could return the function so created instead, for example.
Demo:
>>> def create_global_function():
...     global foo
...     def foo(): return 'bar'
... 
>>> foo
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'foo' is not defined
>>> create_global_function()
>>> foo
<function foo at 0x102a0c7d0>
>>> foo()
'bar'
>>> class ClassWithGlobalFunction:
...     global spam
...     def spam(): return 'eggs'
...     def method(self):
...         global monty
...         def monty(): return 'python'
... 
>>> spam
<function spam at 0x102a0cb18>
>>> spam()
'eggs'
>>> monty
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'monty' is not defined
>>> ClassWithGlobalFunction().method()
>>> monty()
'python'
",A,21
41139091,2016-12-14 09:31:57.857000+00:00,"You're going about this completely the wrong way. As I said, you can't define a property that takes a parameter; the whole point of defining a method is that you reference it directly without the calling parentheses, so there is literally no way to pass it a parameter. And using annotate makes no sense here either; that defines a database operation, so it can't possibly refer to a method defined in Python.
Luckily your actual desired result is very simple, and does not require either a property or annotate. You just need to use the standard query syntax:
MyModel.objects.filter(othermodel__user=user).order_by(othermodel__field2)
",A,0
2872896,2010-05-20 10:33:14.577000+00:00,"Don't modify elements of the model. There's all sorts of metaclass stuff going on in model definitions that will break things, as you have discovered.
Instead, define a custom form and change the field label in the __init__ method there.",A,4
15397996,2013-03-13 22:46:05.867000+00:00,"The file contains one line, so there is no need to loop over the file, really. Just loop over the split list:
def name_gen():
    with open('C:/python/namessample.txt') as nameFile:
        for name in nameFile.read().split(','):
            yield name.strip('""')

or just return the whole list; you've read it into memory already anyway:
def name_gen():
    with open('C:/python/namessample.txt') as nameFile:
        return [name.strip('""') for name in nameFile.read().split(',')]

If you did have multiple lines with each line containing multiple names, you could nest loops:
def name_gen():
    with open('C:/python/namessample.txt') as nameFile:
        for line in nameFile:
            for name in line.rstrip('\n').split(','):
                yield name.strip('""')
",A,5
33396446,2015-10-28 16:28:08.427000+00:00,"Anything under 192.168 is an internal network address only, and is not visible from elsewhere. 
You need to find the external address of your router. You'll also need to configure your router to forward port 80 to the machine running your server.
Really, though, you should use a proper web host; something like Heroku will probably be free at the level you want.",A,1
21506352,2014-02-02 02:32:06.953000+00:00,"Instead of pre-processing your HTML, trust in BeautifulSoup and use regular expression searches:
soup.find_all('td', text=re.compile(','))

finds all <td> elements with the direct text in the tag containing a comma.",A,0
23033261,2014-04-12 16:42:11.730000+00:00,"hello and hb define bytestrings in a more readable fashion.
The h2bin(x) function does all the work:
def h2bin(x):
    return x.replace(' ', '').replace('\n', '').decode('hex')

so the string of hex digits has all whitespace removed, then is decoded from hex to bytes:
>>> '16 03 02 00 dc'.replace(' ', '')
'16030200dc'
>>> '16 03 02 00 '.replace(' ', '').decode('hex')
'\x16\x03\x02\x00\xdc'

It's just a compact way to specify a series of bytes using hexadecimal notation and extra whitespace.
The hex data itself is just a normal heartbeat protocol message, in raw bytes. The hello string contains a TLS 1.1 record message, identified by the first byte (16 hex, 22 decimal) as a handshake record, sending a client_hello (sixth byte is 01). This is just setting up a TLS session, telling the server what kind of ciphers the client supports. It doesn't really matter what's contained in this, other than that it tells the server the client supports the Heartbeat extension (a 00 0f byte pair at the end of the message).
It is the hb message that is interesting one, really:
hb = h2bin(''' 
18 03 02 00 03
01 40 00
''')

18 is the heartbeat content type record, 03 02 identifies the TLS 1.1 protocol version. The 00 03 denotes how large the payload of the message is; 3 bytes, or all of the second line.
The 3 bytes of the message itself consists of the heartbeat type (01, or 'request'), and the message length (40 00, 16384 bytes), followed by no actual message. This causes a broken SSL server to send back a heartbeat response containing 16kb of memory; the non-existing 0-length request message is echoed plus the memory to make up the request length.",A,12
20096972,2013-11-20 13:12:18.773000+00:00,"A namedtuple() is nothing but a class generated with a factory function:
SomeRowResult = namedtuple('SomeRowResult', 'var1 var2 var3')

Here SomeRowResult is a class object (a subclass of tuple), and calling it will create instances of the class:
for result in results:
    result = SomeRowResult(table1.col1, table2.col1, table3.col1)

If you wanted to have a list of these results, you need to explicitly build that list:
all_results = []
for result in results:
    result = SomeRowResult(table1.col1, table2.col1, table3.col1)
    all_results.append(result)
",A,1
16560975,2013-05-15 09:08:00.683000+00:00,"Use dictionary views to achieve this; the dict.viewkeys() result acts like a set and let you do intersections and symmetrical differences:
def merge(A, B, f):
    # Start with symmetric difference; keys either in A or B, but not both
    merged = {k: A.get(k, B.get(k)) for k in A.viewkeys() ^ B.viewkeys()}
    # Update with `f()` applied to the intersection
    merged.update({k: f(A[k], B[k]) for k in A.viewkeys() & B.viewkeys()})
    return merged

In Python 3, the .viewkeys() method has been renamed to .keys(), replacing the old .keys() functionality (which in Python 2 returs a list).
The above merge() method is the generic solution which works for any given f().
Demo:
>>> def f(x, y):
...     return x * y
... 
>>> A = {1:1, 2:3}
>>> B = {7:3, 2:2}
>>> merge(A, B, f)
{1: 1, 2: 6, 7: 3}
>>> merge(A, B, lambda a, b: '{} merged with {}'.format(a, b))
{1: 1, 2: '3 merged with 2', 7: 3}
",A,6
39169745,2016-08-26 15:26:55.293000+00:00,"The names __class__ and __name__ are special. Both are data descriptors. __name__ is defined on the type object, __class__ is defined on object (a base-class of all new-style classes):
>>> type.__dict__['__name__']
<attribute '__name__' of 'type' objects>
>>> type.__dict__['__name__'].__get__
<method-wrapper '__get__' of getset_descriptor object at 0x1059ea870>
>>> type.__dict__['__name__'].__set__
<method-wrapper '__set__' of getset_descriptor object at 0x1059ea870>
>>> object.__dict__['__class__']
<attribute '__class__' of 'object' objects>
>>> object.__dict__['__class__'].__get__
<method-wrapper '__get__' of getset_descriptor object at 0x1059ea2d0>
>>> object.__dict__['__class__'].__set__
<method-wrapper '__set__' of getset_descriptor object at 0x1059ea2d0>

Because they are data descriptors, the type.__getattribute__ method (used for attribute access on a class) will ignore any attributes set in the class __dict__ and only use the descriptors themselves:
>>> type.__getattribute__(Foo, '__class__')
<class 'type'>
>>> type.__getattribute__(Foo, '__name__')
'Foo'

Fun fact: type derives from object (everything in Python is an object) which is why __class__ is found on type when checking for data descriptors:
>>> type.__mro__
(<class 'type'>, <class 'object'>)

(type.__getattribute__(D, ...) is used directly as an unbound method, not D.__getattribute__(), because all special method access goes to the type).
See the Descriptor Howto an what constitutes a data descriptor and why that matters:

If an object defines both __get__() and __set__(), it is considered a data descriptor. Descriptors that only define __get__() are called non-data descriptors (they are typically used for methods but other uses are possible).
Data and non-data descriptors differ in how overrides are calculated with respect to entries in an instance’s dictionary. If an instance’s dictionary has an entry with the same name as a data descriptor, the data descriptor takes precedence. If an instance’s dictionary has an entry with the same name as a non-data descriptor, the dictionary entry takes precedence.

For data descriptors on type, a class is just another instance.
So when looking up the __class__ or __name__ attributes, it doesn't matter what is defined in the D.__dict__ namespace, because for either a data descriptor is found in the namespace formed by type and it's MRO.
These descriptors are defined in the typeobject.c C code:
static PyGetSetDef type_getsets[] = {
    {""__name__"", (getter)type_name, (setter)type_set_name, NULL},
    /* ... several more ... */
}

/* ... */

PyTypeObject PyType_Type = {
    PyVarObject_HEAD_INIT(&PyType_Type, 0)
    ""type"",                                     /* tp_name */
    /* ... many type definition entries ... */
    type_getsets,                               /* tp_getset */
    /* ... many type definition entries ... */
}

/* ... */

static PyGetSetDef object_getsets[] = {
    {""__class__"", object_get_class, object_set_class,
     PyDoc_STR(""the object's class"")},
    {0}
};

PyTypeObject PyBaseObject_Type = {
    PyVarObject_HEAD_INIT(&PyType_Type, 0)
    ""object"",                                   /* tp_name */
    /* ... many type definition entries ... */
    object_getsets,                             /* tp_getset */
    /* ... many type definition entries ... */
}

On instances, object.__getattribute__ is used, and it'll find the __name__ and __class__ entries in the D.__dict__ mapping before it'll find the data descriptors on object or type.
If you omit either, however, then looking up the names on D() will only __class__ as a data descriptor in the MRO of D (so, on object). __name__ is not found as the metatypes are not considered when resolving instance attributes.
As such you can set __name__ on an instance, but not __class__:
>>> class E: pass
...
>>> e = E()
>>> e.__class__
<class '__main__.E'>
>>> e.__name__
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'E' object has no attribute '__name__'
>>> e.__dict__['__class__'] = 'ignored'
>>> e.__class__
<class '__main__.E'>
>>> e.__name__ = 'this just works'
>>> e.__name__
'this just works'
",A,9
51374333,2018-07-17 06:11:10.137000+00:00,"
My question is, how can I approximate the values ​​as follows?

You have to write the code explicitly.


'name'= DD8EFC (Always deleting the first three lines)

Fetch the string, then slice it:
name = plane.get('name')[3:]
print(f""'name' = {name}'"")

However, the fact that you're using get rather than [] implies that you're expecting to handle the possibility that name doesn't exist in plane.
If that isn't a possibility, you should just use []:
name = plane['name'][3:]

If it is, you'll need to provide a default that can be sliced:
name = plane.get('name', '')[3:]



'speed'= 136.  (Approximate whole)

It looks like you want to round to 0 fractional digits, but keep it a float? Call round with 0 digits on it. And again, either you don't need get, or you need a different default:
speed = round(plane['speed'], 0)

… or:
speed = round(plane.get('speed', 0.0), 0)

As for printing it: Python doesn't like to print a . after a float without also printing any fractional values. You can monkey with format fields, but it's probably simpler to just put the . in manually:
print(f""'speed': {speed}."")
",A,3
19282603,2013-10-09 20:58:45.443000+00:00,"Custom python classes can be parsed using the O format:

O (object) [PyObject *]
  Store a Python object (without any conversion) in a C object pointer. The C program thus receives the actual object that was passed. The object’s reference count is not increased. The pointer stored is not NULL.
",A,1
2968475,2010-06-03 17:48:20.917000+00:00,"This is what ""Data Warehousing"" is for.  Consider buying a good book on warehousing.
Collect the raw data in some ""current activity"" schema.
Periodically, move it into a ""warehouse"" (or ""datamart"") star schema that's (a) separate from the current activity schema and (b) optimized for count/sum/group-by queries. 
Move, BTW, means insert into warehouse schema and delete from current activity schema.
Separate your ongoing transactional processing from your query/analytical processing.",A,0
42230134,2017-02-14 15:32:55.710000+00:00,"The simplest thing is just to pass the user to the class initialiser.
def __init__(self, holiday, user):
    super(HolidayCalendar, self).__init__()
    self.user = user
    self.holiday = self.holiday_days(holiday)

def formatday(self, day, weekday):
    ...
    if holiday_object.person_id == self.user:

and in your view:
cal = HolidayCalendar(my_holidays, request.user).formatmonth(year, month)
",A,1
20960208,2014-01-06 22:07:58.150000+00:00,"From a comment on arshajii's answer:

but that's removing the negative numbers. i need the negative signs removed but still keep the number in the list.

Removing the negative numbers is exactly what your code is clearly trying to do, and it's also the only way to get the desired result:

THe result is suppose to be [1, 2, 3, 6, 3, 1]

But if you really want to ""remove the negative signs"" from the numbers, that's even easier. For example, to remove the negative sign from -3, you just negate it and get 3, right? You can do this in-place, as in your existing code:
for index, item in enumerate(num_list): 
    if item < 0: 
       num_list[index] = -item

… or in a list comprehension, as in arshajii's:
num_list = [-item if item < 0 else item for item in num_list]

And it's even easier with the abs function, which does exactly that—negates negative numbers, leaves positive and zero alone:
num_list = [abs(item) for item in num_list]

Either way, of course, this will give you [1, 2, 3, 3, 6, 1, 3, 1], which is the wrong answer… but if your comment is correct, it's the answer you asked for.",A,1
17431751,2013-07-02 17:12:21.623000+00:00,"You are creating your dictionary incorrectly. Use:
for row in r:
    summary = dict(zip(columns, row))
    results.append(summary)

instead.
Your code sets the whole row sequence as the value for each key in Summary, instead of the individual column value, then appending that same dictionary to the results list for each column key..",A,2
51792186,2018-08-10 18:26:26.073000+00:00,"In Python, it's relatively easy to protect values from accidental mutation, but next to impossible to protect them from intentional mutation.

If you just want to make it so that obj.name = 'NewName' will raise a TypeError, this is easy:
class MyObject:
    def __setattr__(self, name, value):
        if self._frozen:
            raise TypeError(f""frozen '{type(self).__name__}' objects do not support assignment"")

Of course you then need to go around the normal self.name = value mechanism to assign the values in the first place. Normally you either do this by calling super().__setattr__ or by going straight to self.__dict__[name], and often you want to do it in a __new__ method rather than an __init__ method. But if you're careful, you can also just do it before self._frozen = True.
If you need more flexibility, to freeze just some specific attributes while leaving others assignable, you can use a descriptor—the stdlib's @property without a setter works fine for this. If you also want to prevent creation of new attributes, you still need __setattr__—or you might want to consider using __slots__.

So, to construct an object, you might do something like this:
class Freezable:
    def __new__(cls, *args, frozen=False, **kwargs):
        obj = super().__new__(cls, *args, **kwargs)
        super().__setattr__(obj, '_frozen', frozen)
        return obj
    def __setattr__(self, name, value):
        if self._frozen:
            raise TypeError(f""frozen '{type(self).__name__}' objects do not support assignment"")
        super().__setattr__(name, value)

And now, you can just make all of your classes into subclasses of Freezable. They'll need to handle the frozen construction parameter by ignoring it in __init__ (and explicitly passing it to super in __new__, if any of them need a custom __new__, but they probably won't).
Then, your get_readonly_obj_by_id just does something like:
return Whatever(user_id, obj_id, frozen=True)


But this won't stop someone from getting around the restriction if they want to. Whatever you do in your __setattr__, they can do directly. Or, if you're using a @property to store the value elsewhere, they can just assign to that elsewhere. Not to mention that they can just set obj,_frozen = False. (You can make them jump through the same hoops again, but that doesn't make it any more secure.)
If you have some other storage that is protected, you can always change your objects into some kind of proxy to that storage, instead of holding the values directly, but that's pretty complicated—and it relies on you having already built some kind of protected store; it isn't a way to build one in the first place.

Your database presumably is just such a protected store. But you don't want to proxy every attribute lookup and assignment through the database—in fact, you don't want anything written there at all until save.
But that means a hybrid approach is probably fine.
Use the ""consenting adults"" protection of __setattr__ and _frozen to protect people from accidentally mutating an object they won't be allowed to store.
If they go behind your back and find a way to mutate the object anyway, when they call save, you can block the update (or the database can do it automatically for you) and give them an exception.",A,2
20720922,2013-12-21 16:10:46.060000+00:00,"Split the line into words; the simplest is to use str.split():
for line in sentences:
    if any(q.lower() == word.lower() for word in line.split()):
        outfile.write(line + '\n')

You can add a .strip('?!.""()') to remove most common punctuation as well, perhaps.
Note that Python files, opened in text mode, will already use \r\n on Windows if you write out a \n.  The code above also directly writes the matched lines to the output file.
Alternatively, use a regular expression to find matches:
import re

def finding(q, sentences, outfile):
    pattern = re.compile(r'\b{}\b'.format(re.escape(q), flags=re.IGNORE)
    for line in sentences:
        if pattern.match(line)
            outfile.write(line + '\n')

re.IGNORE makes the match ignore case, \b adds word boundaries and re.escape() removes any expression metacharacters from the input query.",A,2
34560546,2016-01-01 22:11:25.557000+00:00,"Big-O complexity doesn't work like that. k (the length of the vowels) is a constant, it doesn't change depending on the length of the input. So we discount it in calculating the complexity.
Your function is just O(n), ie linear complexity.",A,7
14657208,2013-02-02 00:16:43.683000+00:00,"You have to actually know what the data means before you can decide how to parse it. Just randomly hacking at it until it works for one example isn't going to help.
So, you're getting the string out of a URL, and it looks like this:

http:///a=maven&v=1.1.0&classifier=''&ttype=pom

Normally, when given a URL, the right thing to do is call urlparse.urlparse and then call urlparse.parse_qs on the query. But that won't actually help here, because this is not actually a valid URL.
Well, it is a valid URL, but it's one with a path <someurl>/a=maven&v=1.1.0&classifier=''&ttype=pom, not one with a path <someurl>/ and a query a=maven&v=1.1.0&classifier=''&ttype=pom. You need a ? to set off the query.
And, on top of that, the query is clearly not generated correctly. You don't quote empty strings in a query. You don't quote anything (you entity-escape ampersands and percent-escape any other special characters). So, unless the URL literally means that the classifier is '' rather than the empty string, it's wrong.
And, if it weren't wrong, you wouldn't be asking these questions.
If you have any control over how these URLs are getting generated, obviously you want to get that fixed. If you can't control it, but at least know how they're being generated, you can write code to reverse that to get the original values. But if you don't even know that, you have to guess.
You ideally need more than one example to guess. Are they quoting just empty strings, or are they also, e.g., quoting strings with "" characters or spaces or ampersands in them? If it's the latter, you can probably just strip(""'""), but if it's the former, that will be incorrect in any cases where the original data actually has quotes.",A,1
46502899,2017-09-30 12:54:03.540000+00:00,"You should not pass in cls to super().__call__(); super() takes care of binding for you and so cls is already passed in automatically.
You may have gotten confused by the super().__new__(cls, ...) call in __new__; that's because __new__ is the exception here, see the object.__new__ documentation:

Called to create a new instance of class cls. __new__() is a static method (special-cased so you need not declare it as such) that takes the class of which an instance was requested as its first argument.

Removing cls from the super().__call__(...) expression works:
class ListMetaclass(type):
    def __new__(cls, name, bases, attrs):
        attrs['add'] = lambda self, value: self.append(value)
        new_cls = type.__new__(cls, name, bases, attrs)
        return new_cls;
    def __call__(cls, *args, **kw):
        return super().__call__(*args, **kw)

By passing in cls, you are effectively executing list(cls), telling list() to convert cls to values in the new list; that required cls to be an iterable.
When you remove the __call__ method on your metaclass, the default type.__call__ method is used when you call MyClass(), which just receives the normal arguments (none in your example) and returns a new instance.",A,4
35242633,2016-02-06 15:19:26.483000+00:00,"models.py is just a convention. You are not required to put your models in any specific module, you could put everything in one file if you wanted to.
If your contact form doesn't store anything in your database, you don't need any models either. You could do everything with just a form, then email the information entered elsewhere, or write it to disk by other means.
Even if you did want to put the information into a database, you could still do that without creating a model. However, creating a model just makes this task far easier and convenient, because Django can then generate a form from that, do validation, provide helpful feedback to your users when they make a mistake, handle transactions, etc.",A,2
23780545,2014-05-21 10:28:11.933000+00:00,"You do it by configuring your actual server - Apache, or whatever - to serve files from wherever MEDIA_ROOT is, on whatever MEDIA_URL is. Django should have nothing to do with it.",A,1
19148273,2013-10-02 23:09:44.857000+00:00,"There are two different things you could mean here.

First, you could be dragging a file onto the program's executable file. For example, I open my home directory in one window, and /usr/local/bin in another, and drag spam.txt from the left window and drop it on myprogram in the right.
In theory, what happens here is entirely up to the file manager. However, most file managers—including Windows Explorer and OS X Finder and the default things that come with most of the popular linux distros—will do the exact same thing: just run the program, as if you had typed this at a shell:
""/usr/local/bin/myprogram"" ""/home/kevinmills/spam.txt""

So, from inside myprogram, you just need to look at sys.argv[1:] for the list of files, and it will work the same way whether they were supplied on the command line or by drag and drop.
If you drag a file to a symlink, Windows shortcut, or Mac alias to the program's executable file stored somewhere else, like on your desktop, effectively the same thing happens, and you don't have to worry about the differences.
If you've built an application bundle for OS X instead of just a flat script, things are more complicated. However, all of the simpler wrappers that would have let you build such a bundle without knowing what you're doing should automatically take care of the magic that makes it look the same as a flat script, so you shouldn't have to worry about that.

Alternatively, you could be asking about dropping a file on some kind of representation of your already-running program on a dock or taskbar or the like.
That's more complicated. And it's different for each platform. But fortunately, that's not what you're doing.",A,0
20608561,2013-12-16 10:30:12.003000+00:00,"
As the embedded C is sending quite a lot of numbers and I want to cut down on the transfer time,

To improve time performance you could send and/or receive more than one number at a time. In my tests, array.fromfile() is 10 - 100 times faster than struct.unpack(). It comes at the cost of calling array.byteswap() sometimes to take into account endianness explicitly. 

If the C program and Python script were running on the same machine (same size, same endianess); then you could use fwrite to write short ints as platform values on C side and array.fromfile() on Python side to read them back in a native format.
For example, print short ints as binary:
#include <stdio.h>
#include <stdlib.h>

int main(void) {
  short a[] = {31415, 9265, 3589, 793};
  size_t n = sizeof a / sizeof *a;
  if (fwrite(&n, sizeof n, 1, stdout) != 1) exit(EXIT_FAILURE); /* send size */
  return (fwrite(a, sizeof *a, n, stdout) < n) ? EXIT_FAILURE : EXIT_SUCCESS;
}

Read it in Python:
#!/usr/bin/env python3
import sys
import array
import struct

# make stdin binary
file = sys.stdin.detach()

# read size
size_format = 'N' # size_t
n, = struct.unpack(size_format, file.read(struct.calcsize(size_format)))
print(n)

a = array.array('h') # native short int
a.fromfile(file, n)
print(a.tolist()) # -> [31415, 9265, 3589, 793]

array.fromfile should be efficient both time and memory-wise. If you don't know the size then call a.fromfile until EOFError is raised.

If C program and Python script are on different machines then you could send integers in the network byte order:
#include <stdio.h>
#include <stdlib.h>

#include <netinet/in.h> /* htons() */

int main(void) {
  short a[] = {31415, 9265, 3589, 793};
  /* print one integer at a time */
  short *p = a, *end = &a[sizeof a / sizeof *a];
  for ( ; p != end; ++p) {
    uint16_t s = htons(*p); /* convert from native to network byte order */
    if (fwrite(&s, sizeof s, 1, stdout) != 1)  exit(EXIT_FAILURE);
  }
  return 0;
}

And swap byte order if necessary on the Python side:
#!/usr/bin/env python
import array
import sys

a = array.array('h') # short int in native byte order, byte swap might be needed
for i in range(15, 128): 
    try: # double size to avoid O(n**2) behaviour
        a.fromfile(sys.stdin, 2 << i)
    except EOFError:
        break
if sys.byteorder != 'big': # if not network order
    a.byteswap()  # swap byte order
print(a.tolist()) # -> [31415, 9265, 3589, 793]

To avoid converting to network order, you could send a magic number instead. It allows to send numbers in a native byte order on C side (as in the 1st code example) and check it in Python to swap bytes if necessary:
MAGIC = 1
if a[0] != MAGIC:
   a.byteswap()
   if a[0] != MAGIC:
      raise ValueError(""Unexpected %d"" % a[0])
",A,0
634725,2009-03-11 14:24:58.477000+00:00,"I agree with @S.Lott you should reconsider your grammar.
Recursive definitions can be introduced using Forward():
from pyparsing import (Literal, Word, OneOrMore, Forward, nums, alphas)

def BNF():
    """"""
    element      :: id
    elements     :: '{' element [ ',' element ]+ '}' 
                  | 'cross' '(' elements ',' '{' element '}' ')'
    """"""
    lcb, rcb, lb, rb, comma = [Literal(c).suppress() for c in '{}(),']
    element  = Word(alphas, alphas+nums+""_"") # id
    elements = Forward()
    elements << ((lcb + element + OneOrMore(comma + element) + rcb) 
                 | (Literal('cross') + lb + elements + comma
                    + lcb + element + rcb + rb))
    return elements

print BNF().parseString(""cross(cross({a,b},{c1}),{c2})"")

Output:
['cross', 'cross', 'a', 'b', 'c1', 'c2']
",A,3
476633,2009-01-24 20:49:28.050000+00:00,"To answer your question: no.
Feel free to research it further.  
Note, however, that you've conflated design patterns (which are just ideas) with code (which is an implementation.)
Good code often reflects a number of interlocking design patterns.  There's no easy way for formalize this.  The best you can do is a nice picture, well-written docstrings, and method names that reflect the various design patterns.
Also note that a meta-class is a class.  That's a loop.  There's no higher level of abstractions.  At that point, it's just intent.  The idea of meta-meta-class doesn't mean much -- it's a meta-class for meta-classes, which is silly but technically possible.  It's all just a class, however.

Edit
""Are classes that create metaclasses really so silly? How does their utility suddenly run out?""
A class that creates a class is fine.  That's pretty much it.  The fact that the target class is a meta class or an abstract superclass or a concrete class doesn't matter.  Metaclasses make classes.  They might make other metaclasses, which is weird, but they're still just metaclasses making classes.
The utility ""suddenly"" runs out because there's no actual thing you need (or can even write) in a metaclass that makes another metaclass.  It isn't that it ""suddenly"" becomes silly.  It's that there's nothing useful there.
As I seed, feel free to research it.  For example, actually write a metaclass that builds another metaclass.  Have fun.  There might be something useful there.
The point of OO is to write class definitions that model real-world entities.  As such, a metaclass is sometimes handy to define cross-cutting aspects of several related classes.  (It's a way to do some Aspect-Oriented Programming.)  That's all a metaclass can really do; it's a place to hold a few functions, like __new__(), that aren't proper parts of the class itself.",A,7
8570014,2011-12-20 02:13:39.990000+00:00,"A more concise way using BeautifulSoup:
>>> soup('div', text='John')
[u'John']
>>> import re
>>> soup('div', text=re.compile('Jo'))
[u'John', u'Joe']

soup() is equivalent to soup.findAll(). You could use string, regular expression, arbitrary function to select what you need.
stdlib's ElementTree is enough in your case:
from xml.etree import cElementTree as etree

xml = """"""
    <div>Bill</div>
    <div>John</div>
    <div>Joe</div>
""""""
root = etree.fromstring(""<root>%s</root>"" % xml)
for div in root.getiterator('div'):
    if ""John"" in div.text:
       print(etree.tostring(div))
",A,3
17201216,2013-06-19 20:59:12.230000+00:00,"Use a list comprehension:
lines = ['~$ ' + line for line in command.split('\n')]

If you have to use a for loop, you'd use enumerate() to include an index so you can replace the individual items in the list:
for i, line in enumerate(lines):
    lines[i] = '~$ ' + line
",A,8
20983637,2014-01-07 22:51:04.633000+00:00,"There are some problems with your code… but not the one you're asking asking about.
Since you didn't provide enough to run anything, I added this extra stuff:
class Genome(object):
    i = 0
    def __init__(self, newi = None):
        if newi is None:
            newi = Genome.i
            Genome.i += 1
        self.i = newi
    def __repr__(self):
        return 'Genome({})'.format(self.i)
    def children(self):
        return self._children

g1, g2 = Genome(), Genome()
g1._children = [Genome(), Genome()]
g2._children = [Genome(), Genome(), Genome()]
a_list_of_genomes = [g1, g2]
all_genomes = [g1.children()[0], g2.children()[2]]

Now, your algorithm should give us genomes #2 and #6. So, let's try your non-threaded code:
valid_children = set()
for focus_genome in a_list_of_genomes: # a list of objects
    for child in focus_genome.children(): # a list of objects (again)
        if child in all_genomes:
            valid_children.update(set([child]))
print(valid_children)

I get {Genome(2), Genome(6)}, which is correct.
Now, your threaded code. Copying and pasting the inner loop body as the function body to make sure it's identical:
def threaded_function(focus_genome):
    for child in focus_genome.children(): # a list of objects (again)
        if child in all_genomes:
            valid_children.update(set([child]))

Running it as you tried:
for focus_genome in a_list_of_genomes: # a list of objects
    t = threading.Thread(target=threaded_function, args=(focus_genome,))
    t.start()
    t.join()

print(valid_children)

I get {Genome(2), Genome(6)}, which is not empty as you claim, and correct, and exactly the same as the non-threaded version.

That being said, you're not actually doing anything useful here—and, if you did, you'd have a problem.
First, join sits around waiting for the background thread to finish. So, there is no benefit in starting a thread and immediately joining it. Instead, you need to start a bunch of threads, then join all the threads. For example:
threads = [threading.Thread(target=threaded_function, args=(focus_genome,))
           for focus_genome in a_list_of_genomes]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()

But if the threads are doing nothing but running CPU-intensive Python code, this won't help anyway, because the Global Interpreter Lock ensures that only one thread can run Python code at a time. Threads are great when you spend all your time doing I/O (reading files or HTTP URLs), waiting on user interaction, or calling slow functions in a handful of libraries like NumPy that are designed for threading. But for running Python code in parallel, they're not going to speed things up at all. For that, you need processes.
Meanwhile, you've got multiple threads trying to mutate a shared object, without any kind of synchronization. That's a race condition, which will lead to corrupted data. You need to use a lock or other sync object to protect shared data if you want to use threads:
valid_children_lock = Lock()

def threaded_function(focus_genome):
    for child in focus_genome.children(): # a list of objects (again)
        if child in all_genomes:
            with valid_children_lock():
                valid_children.update(set([child]))

And this kind of mutable-shared-data threading gets even worse when you're using processes. If you try to directly share a set between two processes, it sometimes works on Unix, and never on Windows.
If you can reorganize your logic to not use mutable shared data, everything gets a lot easier. One really easy way to do this is to write everything in terms of tasks that take parameters and return values—that is, functions without side-effects. Then you can just use a thread pool or executor to run all those tasks and give you back the results. Which has the added advantage that you run as many tasks at a time as you have workers, queuing the rest up automatically, instead of trying to run all of them at once (which is a lot faster).
Can we do that here? Maybe. If each task returns a set of valid_children found for a given focus_genome, then we can just union together all those sets and get the complete result, right? So:
def threaded_task(focus_genome):
    valid_children = set()
    for child in focus_genome.children(): # a list of objects (again)
        if child in all_genomes:
            valid_children.add(child)
    return valid_children

valid_children = set()
with multiprocessing.Pool() as pool:
    for subset in pool.imap_unordered(threaded_task, a_list_of_genomes):
        valid_children.update(subset)

We could simplify this even further by using a couple of comprehensions:
def threaded_task(focus_genome):
    return {child for child in focus_genome.children() if child in all_genomes}
with multiprocessing.Pool() as pool:
    valid_children = set.union(pool.imap_unordered(threaded_task, a_list_of_genomes))
",A,2
16546424,2013-05-14 14:52:41.177000+00:00,"Pickle is not safe when transfered by a untrusted 3rd party. Local files are just fine, and if something can replace files on your filesystem then you have a different problem.
That said, if your dictionary contains nothing but string keys and the values are nothing but Python lists, numbers, strings or other dictionaries, then use JSON, via  the json module.",A,5
15277825,2013-03-07 17:28:54.827000+00:00,"You want to return the result of your functions, not the functions themselves. Or rather, since all your functions return either True or False, you want to return True if the function returned True:
def determina_vencedor(t):
    if vencedor_coluna(t):
        return True

    if vencedor_linha(t)
        return True

    if vencedor_diagonal(t)
        return True

    return False

This can be combined into a loop:
def determina_vencedor(t):
    for func in (vencedor_coluna, vencedor_linha, vencedor_diagonal):
        if func(t):
            return True
    return False

or you can use any() to test all functions:
def determina_vencedor(t):
    return any(f(t) for f in (vencedor_coluna, vencedor_linha, vencedor_diagonal))
",A,3
20085327,2013-11-20 00:51:55.217000+00:00,"The itertools docs come with a nice set of recipes, one of which, unique_everseen does exactly what you want: ""List unique elements, preserving order. Remember all elements ever seen.""
You can copy it paste it from the recipes, or you can pip install more-itertools and just use it as more_itertools.unique_everseen.
Or you can look at the code, and see that it's essentially just a slightly more flexible and slightly optimized version of exactly what Raymond Hettinger's answer does.*
And that's exactly what the recipes are there for—when you want to do something to an iterable, there's often a recipe for it, which not only does what you want, but is so simple that if you take a couple minutes to work out how it works, you'll know how to do it yourself in the future.

* This shouldn't surprise anyone who knows anything about Python, or just looks at the source to itertools…",A,2
11702377,2012-07-28 15:37:50.573000+00:00,"request.params is a dict (or dict-like) object, so you can just use .get() with a default value:
<input type=""text"" name=""foo"" value=""${request.params.get('foo', '')}"" />
",A,0
27066977,2014-11-21 17:25:55.807000+00:00,"You need to add DATA to the Request object; you are sending the text 'POST' as a post body instead now.
The correct method would be:
request = urllib.request.Request(
    ""http://www.altenheim-adressen.de/schnellsuche/index.cfm"",
    DATA)
# adding charset parameter to the Content-Type header.
request.add_header(""Content-Type"", ""application/x-www-form-urlencoded;charset=utf-8"")
request.add_header(""User-Agent"", ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:33.0) Gecko/20100101 Firefox/33.0"")
f = urllib.request.urlopen(request)

Note that I also added User-Agent and Content-Type headers there; whatever you were adding was not a recognisable header.
I do note that the 2 forms on this page use different targets; neither posts to index.cfm:
>>> for form in soup.find_all('form'):
...     print(form.attrs.get('action'))
... 
suche1.cfm
suche1b.cfm

so if you expected to use one of these forms you'll need to use the correct target URL here. You'd also need to verify your POST form fields; I see that the form posting to suche1b.cfm has similar form fields, but they use plz_ffb and plz_ff2b, not plz_ff and plz_ffb.
You'll likely also want to send all form fields, even with empty strings:
params = {
    'name_ffb': '',
    'ort_ffb': '',
    'strasse_ffb': '',
    'plz_ffb': '50000',
    'plz_ff2b': '50030',
    'land_ff': '',
    'rubrik_ff': '',
    'submit22': 'Suchen'
}
DATA = urllib.parse.urlencode(params)
DATA = DATA.encode('utf-8')
",A,1
46538886,2017-10-03 07:05:09.067000+00:00,"I'm pretty sure the 'power of 2' advice is based on an error in editing, and should not be taken as a requirement.
That specific piece of advice was added to the Python 2.5 documentation (and backported to Python 2.4.3 docs), in response to Python issue #756104. The reporter was using an unreasonably large buffer size for socket.recv(), which prompted the update.
It was Tim Peters that introduced the 'power of 2' concept:

I expect you're the only person in history to try passing such 
  a large value to recv() -- even if it worked, you'd almost 
  certainly run out of memory trying to allocate buffer space for 
  1.9GB.  sockets are a low-level facility, and it's common to 
  pass a relatively small power of 2 (for best match with 
  hardware and network realities).

(Bold emphasis mine). I've worked with Tim and he has a huge amount of experience with network programming and hardware, so generally speaking I'd take him on his word when making a remark like that. He was particularly 'fond' of the Windows 95 stack, he called it his canary in a coalmine for its ability to fail under stress. But note that he says it is common, not that it is required to use a power of 2.
It was that wording that then led to the documentation update:

This is a documentation bug; something the user should be
  ""warned"" about. 
This caught me once, and two different persons asked about
  this in #python, so maybe we should put something like the
  following in the recv() docs.
""""""
  For best match with hardware and network realities, the
  value of ""buffer"" should be a relatively small power of 2,
  for example, 4096.
  """"""  
If you think the wording is right, just assign the bug to
  me, I'll take care of it.

No one challenged the 'power of 2' assertion here, but the editor moved from it is common to should be in the space of a few replies.
To me, those proposing the documentation update were more concerned with making sure you use a small buffer, and not whether or not it is a power of 2. That's not to say it is not good advice however; any low-level buffer that interacts with the kernel benefits with alignment with the kernel data structures.
But although there may well be an esoteric stack where buffers with a size that is a power of 2 matters even more, I doubt Tim Peters ever meant for his experience (that it is common practice) to be cast in such iron-clad terms. Just ignore it if a different buffer size makes more sense for your specific use cases.",A,6
447133,2009-01-15 15:19:08.730000+00:00,"Revised
You're updating 10 separate, individual, distinct objects.
The 10 separate, individual, distinct updates can't easily be collapsed into one magical update that somehow touches 10 objects.",A,3
14004629,2012-12-22 16:45:21.127000+00:00,"When timing a statement, you want to time just that statement, not the setup. The setup could be considerably slower than the statement-under-test.
Note that timeit runs your statement thousands of times to get a reasonable average. It does this to eliminate the effects of OS scheduling and other processes (including but not limited to disk buffer flushing, cronjob execution, memory swapping, etc); only an average time would have any meaning when comparing different code alternatives.
For your case, just test lfactor(number) directly, and just use the timeit() function:
timeit.timeit('lfactor(number)', 'from __main__ import lfactor, number')

The setup code retrieves the lfactor() function, as well as number taken from sys.argv from the main script; the function and number won't otherwise be seen.
There is absolutely no point in performance testing the print statement, that's not what you are trying to time. Using timeit is not about seeing the result of the call, just the time it takes to run it. Since the code-under-test is run thousands of times, all you'd get is thousands of prints of (presumably) the same result.
Note that usually timeit is used to compare performance characteristics of short python snippets; to find performance bottlenecks in more complex code, use profiling instead.
If you want to time just one run, use the timeit.default_timer() function to get the most accurate timer for your platform:
timer = timeit.default_timer
start = timer()
print lfactor(number)
time_taken = timer() - start
",A,2
29316467,2015-03-28 11:16:40.883000+00:00,"Here's how you could parse AT+CCLK? response into a datetime object representing time in UTC:
from datetime import datetime, timedelta

cclk_answer = ""+CCLK: 15/03/26,14:23:22+40""
local_time = datetime.strptime(cclk_answer[:-3], ""+CCLK: %y/%m/%d,%H:%M:%S"")
utc_offset = timedelta(minutes=15*int(cclk_answer[-3:]))
utc_time = local_time - utc_offset
# -> datetime.datetime(2015, 3, 26, 4, 23, 22)

Note: use %Y if the year is 4-digit, though the manual says that the year is represented using 2 digits (%y).
Once you have the utc time, you can compare it directly with other datetime objects (in UTC):
if abs(utc_time - alarm_time) < timedelta(minutes=5):
    print(""the modem time is within 5 minutes from the alarm time"")

btw, to get the current time in UTC, current_time = datetime.utcnow().",A,0
17599050,2013-07-11 16:45:41.433000+00:00,"Use str.rstrip() to remove characters of the end of a string:
somestring.rstrip('\t')

You can give it multiple characters; the argument is treated as a set and all characters at the end of the string that match that set are removed:
somestring.rstrip('\t\n ')

would remove all tabs, newlines and spaces from the end of the line.
If you want to remove any whitespace, regardless, call .rstrip() without an argument:
somestring.rstrip()
",A,5
51335118,2018-07-14 04:01:00.823000+00:00,"I'm going to take a wild guess at what you want, but I could be wrong.
You're asking for .*Glory.*—that is, any string that includes the substring Glory. Greediness isn't going to change the fact that ' to GloryB and GloryE' contains that substring. It could change whether you match '', 'Glory', 'B and GloryE' or 'GloryB and ', 'Glory', 'E', but that doesn't do any good; either way, it matches.
I think what you may want to ask for is .*\bGlory\b.*—that is, any string that contains the substring Glory between word boundaries. Like this Regex101 demonstration:
match = re.match(r'.*(submitted|forwarded)(.*) to .*\b%s\b.*'%train,line)

See Regular Expression Syntax for an explanation of what \b means:

Matches the empty string, but only at the beginning or end of a word. A word is defined as a sequence of word characters. Note that formally, \b is defined as the boundary between a \w and a \W character (or vice versa), or between \w and the beginning/end of the string. This means that r'\bfoo\b' matches 'foo', 'foo.', '(foo)', 'bar foo baz' but not 'foobar' or 'foo3'.

If you wanted to match only Glory at the end of the line, or Glory with whitespace (rather than any non-word character) or EoL, or something else, then of course this is wrong.",A,1
26517455,2014-10-22 21:29:12.577000+00:00,"The Tweepy package is not Python 3 compatible. It will only work with Python 2.
The line that fails is:
from urllib import quote

which is only available in that location on Python 2. The package documentation clearly states this too:

Note only Python 2.6 and 2.7 are supported at the moment. The Python 3 family is not yet supported.

You'll have to use a different package to interface with Twitter. The Twitter Developer documentation lists several options:

https://github.com/sixohsix/twitter lists Python 3.3 and 3.4 in the Trove metadata
https://github.com/bear/python-twitter lists Python 3.3 support
https://github.com/ckoepp/TwitterSearch lists support for Python 3.2 and up
https://github.com/ryanmcgrath/twython states it supports Python 3
",A,0
18966176,2013-09-23 18:23:34.177000+00:00,"I expect you are thinking in terms of Java. In Python, you import things by module, not class name. So if a directory foo contains a file bar.py which defines a class Bar, you must do from foo.bar import Bar, not from foo import Bar.",A,2
23305059,2014-04-26 00:20:33.397000+00:00,"The docs for subprocess.check_call() say:

The timeout argument is passed to Popen.wait(). If the timeout
  expires, the child process will be killed and then waited for again.
  The TimeoutExpired exception will be re-raised after the child process
  has terminated.

The subprocess source code confirms it:
def call(*popenargs, timeout=None, **kwargs):
    with Popen(*popenargs, **kwargs) as p:
        try:
            return p.wait(timeout=timeout)
        except:
            p.kill() # kill on any exception including TimeoutExpired
            p.wait()
            raise

i.e., what you see is the expected behaviour: if the timeout happens then the shell (%COMSPEC% or cmd.exe) that runs the bcp process should be terminated promptly that in turn might terminate the bcp process itself.
You see the buffered output being flushed in the console after the subprocess already exited or (I'm not sure) you see the output from the alive grandchild bcp process while its parent cmd.exe already finished (the new prompt is shown). 
Remove shell=True to avoid creating unnecessary intermediate cmd.exe process so that .kill() is called on the bcp process directly instead of the shell process.",A,4
54787498,2019-02-20 13:29:47.417000+00:00,"The asyncio.wait(..., return_when=asyncio.FIRST_COMPLETED) coroutine returns when at least one of the tasks has completed. Other tasks can still be active. It is not the job of asyncio.wait() to cancel those tasks for you. The use-case of asyncio.wait(..., return_when=asyncio.FIRST_COMPLETED) is to let you monitor tasks and act on their results as they complete; you usually would call it repeatedly until all your tasks are finished.
From the asyncio.wait() documentation:

Run awaitable objects in the aws set concurrently and block until the condition specified by return_when.
[...]
return_when indicates when this function should return. It must be one of the following constants:
FIRST_COMPLETED
  The function will return when any future finishes or is cancelled.
[...]
Unlike wait_for(), wait() does not cancel the futures when a timeout occurs.

The documentation explicitly states that it will not cancel futures, even when you set a timeout (if you do set a timeout, then the first done set is simply empty, the tasks are all still active and listed in the second pending set).
If you need the unfinished tasks to be cancelled, do so explicitly:
while tasks:
    finished, unfinished = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)

    for x in finished:
        result = x.result()

        if result:
            # cancel the other tasks, we have a result. We need to wait for the cancellations
            # to propagate.
            for task in unfinished:
                task.cancel()
            await asyncio.wait(unfinished)
            return result

    tasks = unfinished

Demo with some extra printing and randomised tasks:
>>> import asyncio
>>> import random
>>> async def grab_proxy(taskid):
...     await asyncio.sleep(random.uniform(0.1, 1))
...     result = random.choice([None, None, None, 'result'])
...     print(f'Task #{taskid} producing result {result!r}')
...     return result
...
>>> async def task_manager():
...     tasks = [grab_proxy(i) for i in range(10)]
...     while tasks:
...         finished, unfinished = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
...         for x in finished:
...             result = x.result()
...             print(f""Finished task produced {result!r}"")
...             if result:
...                 # cancel the other tasks, we have a result. We need to wait for the cancellations
...                 # to propagate.
...                 print(f""Cancelling {len(unfinished)} remaining tasks"")
...                 for task in unfinished:
...                     task.cancel()
...                 await asyncio.wait(unfinished)
...                 return result
...         tasks = unfinished
...
>>>
>>> def get_proxy_loop():
...     loop = asyncio.new_event_loop()
...     proxy = loop.run_until_complete(task_manager())
...     loop.close()
...     return proxy
...
>>> get_proxy_loop()
Task #7 producing result None
Finished task produced None
Task #0 producing result 'result'
Finished task produced 'result'
Cancelling 8 remaining tasks
'result'
",A,3
18738187,2013-09-11 10:01:50.647000+00:00,"You are requesting a URL without an end slash, and Django is probably redirecting to the version with the slash, losing the POST data in the process. You should request ""https://www.zzz.com/bulk-loc-add/"".",A,4
11571668,2012-07-20 01:32:08.747000+00:00,"For your main question:
If you're doing something more complex than this—or, in particular, if you're doing this repeatedly—you probably want a ""thread group"" class. There are dozens of them pre-made, but it's pretty trivial to write one yourself if you don't like any of them.
Then, instead of this:
threadList = []
for argchunk in splitIntoChunks(values, 10):
  threadList.append(threading.Thread(target=myThreadFunc, args=argchunk))
...
someTotal = 0
for th in threadList:
  th.join()
  someTotal += th.resultsAttribute

You can do this:
threadGroup = ThreadGroup.ThreadGroup()
for argchunk in splitIntoChunks(values, 10):
  threadGroup.newThread(myThreadFunc, argchunk)
threadGroup.join()
someTotal = sum(th.resultsAttribute for th in threadGroup)

Or, maybe even better, a full thread pool library, so you can do this:
pool = ThreadPool(10)
for argchunk in splitIntoChunks(values, 100):
  pool.putRequest(myThreadFunc, argchunk)
pool.wait()

The advantage here is that you can just as easily have 100 jobs scheduled as appropriate on 10 threads, instead of 10 jobs one per thread, without all the work of maintaining a queue, etc. The disadvantage is that you can't just iterate threads to get the return values, you have to iterate jobs—and ideally, you don't want to keep the jobs alive until the end just so you can iterate them.
Which brings us to your second question, how to get values out of the threads (or jobs). There are many, many ways to do this.
What you've done works. You don't even need any locking.
Using callbacks, as you suggested, also works. But keep in mind that the callback will run on the worker thread, not the main thread, so if it's accessing some global object, you will need some kind of synchronization.
If you're going to synchronize anyway, there may not be any benefit to the callbacks. For example, if all you're trying to do is sum a bunch of values, you can just set total=[0], and have each thread just do total[0] += myValue inside a lock. (Of course in this case, it probably makes more sense to just do the summing in the main thread and avoid the lock, but if the work of amalgamating the results is much heftier, that choice may not be as simple.)
You can also use an atomic object of some kind, instead of locking explicitly. For example, the standard Queue.Queue and collections.deque are both atomic, so each thread can just set q = Queue.Queue(), then each thread pushes its result by doing q.push(myValue), then after joining you just iterate and sum up the queue's values.
In fact, if each thread is pushing to the queue exactly once, you can just do 10 blocking gets on the queue itself, after which you know that the group.join() or pool.wait() or whatever will return quickly.
Or you can even push callbacks as jobs onto a queue. Again, you can do 10 blocking gets on the queue, executing the result each time.
If each thread can return multiple objects, they can push a sentinel value or callback onto the queue when they're done, and your main thread keeps popping until it reads 10 sentinels.",A,2
8102548,2011-11-12 04:27:31.657000+00:00,"I would try to:

remove browser = None
or
move the code in the __name__==""__main__"" into main() function and add global browser before browser=mechanize.Browser()
or
move code that initializes browser to an initializer

If your tasks are I/O bound then you don't necessarily need multiprocessing to make concurrent requests. For example, you could use concurrent.futures.ThreadPoolExecutor, gevent, Twisted instead.
Related: Problem with multi threaded Python app and socket connections",A,0
42330149,2017-02-19 17:04:59.317000+00:00,"You're passing the entire UserInput object to the nlkt function, which as the error states is only expecting a string. You could fix this by just passing the relevant field:
text1 = user_input_obj.user_input

but to be honest I'm confused about why you're creating that model in the first place - it would be simpler to just pass the result of request.POST.get(...) to the Question_Init function.
Note also that that function returns a list of sentences, which you then call save on - this won't work, as a list doesn't have a save method. Not sure what you are trying to save there.",A,0
4234936,2010-11-20 20:47:49.273000+00:00,"What's wrong with the built-in Python debugger? Just insert this in your code where you want to set a breakpoint:
import pdb; pdb.set_trace()

and execution will pause there, and the console will show a debugging prompt enabling you to inspect variables and step through the code.",A,4
28496766,2015-02-13 09:53:38.397000+00:00,"In the db-api, parameters are for values only, not for columns or other parts of the query. You'll need to insert that using normal string substitution.
column = 'b'
query = ""SELECT a FROM table WHERE {} like %s limit 0,10"".format(column)
cur.execute(query, (""%""+""ccc""+""%"",))

You could make this a bit nicer by using format in the parameters too:
cur.execute(query, (""%{}%"".format(""ccc"",))
",A,1
24081748,2014-06-06 12:22:06.720000+00:00,"The error is not in the use of info, it's in what's inside the for loop: {{ Hello }} is not an item in the context. Use {{ object.title }}, for example.",A,3
31380529,2015-07-13 10:08:18.507000+00:00,"There rarely is a need to nest classes, and other than wrapping the name of the class in another namespace and breaking pickling there is no difference in functionality.
Just put the classes at the global level of your module:
class AA:
    def __init__(self):
        self.name = ""ZZ-top""
        self.flag = False
    def reset(self):
        self.name = ""AArdvark""
        self.flag = True

class BB:
    def __init__(self):
        self.num = 11
    def checker(self):
        if self.num > 10:
            test.AA.name = ""Toasty""   

class test:                  
    def __init__(self):
        self.AA = AA()
        self.BB = BB()
",A,1
19648207,2013-10-29 01:42:21.843000+00:00,"Use a unicode value; preferably with a unicode escape sequence:
os.path.isfile(u'test snowman character --\u2603--.mp3')

Python on Windows will use the correct Windows API for listing UTF16 files when you give it a unicode path. 
For more information on how Python alters behaviour with unicode vs. bytestring file paths, see the Python Unicode HOWTO.",A,3
3338954,2010-07-26 20:58:00.303000+00:00,"
is there a library that i can use that will read the csv into a matrix or should i write one myself?

The csv module handles just about everything you could want.

does python speak with VBA at all?

Iron Python might.  

after i am done processing the data, is it simple to put it back in the form of a CSV so that i can open it in excel for viewing?

The csv module handles just about everything you could want.
Suggestion: Read this: http://docs.python.org/library/csv.html",A,3
11329260,2012-07-04 12:36:50.663000+00:00,"The list comprehension isn't doing what you think it's doing. It's simply assigning each string to the variable float, and returning it. Instead you actually want to use another name and call float on it:
y = [float(x) for x in line.split()]
",A,2
19153040,2013-10-03 07:15:15.557000+00:00,"Your JSON contains a list, with one dictionary in it; there are two square brackets, [ and ], around the dictionary.
Select the first element:
print(data[0]['students'])

Quick demo:
>>> print(data)
[{'instructor': 'hamarneh', 'course': 'CMPT 102 D1', 'title': 'Scientific Cmpt.Prgm', 'students': ['axc5', 'csf10', 'ctu1', 'nmw15', 'nsm12', 'ppy1', 'qtg13', 'tim1', 'tkd10', 'vhm8', 'vsv1', 'wps1', 'xup12', 'yqt6']}]
>>> print(data[0]['students'])
['axc5', 'csf10', 'ctu1', 'nmw15', 'nsm12', 'ppy1', 'qtg13', 'tim1', 'tkd10', 'vhm8', 'vsv1', 'wps1', 'xup12', 'yqt6']

Note that you could have spotted this yourself with a quick print of just data.
If this was a list of multiple courses and you need to count per-student, set up a dictionary keyed on students, containing integers. Using collections.defaultdict() makes that a little easier:
from collections import defaultdict

courses_per_student = defaultdict(int)

for course in data:
    for student in course['students']:
        courses_per_student[student] += 1
",A,3
50919394,2018-06-19 01:18:11.750000+00:00,"Obviously, you can't just return False as soon as you find a factor if you're supposed to be counting the number of factors. Instead, you need to increment some count of factors and keep going through the loop:
for y in range(2,x):
    if not ( x % y ):
        factors += 1

Of course you also need code that starts off with factors = 0.
Also, you can't just return True for prime and False for composite; you have to return the number of factors as well. And, since you're no longer returning early as soon as you find a factor, you can't just return True at the end; you need to decide whether the number is prime or not based on the number of factors you've found.
More importantly, you need to return the number of factors, either instead of or in addition to returning whether the number is prime. Maybe you can just return factors. Although you need to figure out what to do about 0 and 1 now; the existing else: return False doesn't make any sense with that.
And you probably want to change your function name from is_prime_number to, say, count_factors.
And then, finally, you need to change your main loop to do something like this:
factors = count_factors(i)
if not factors: # and maybe some other condition for 0 and 1?
    print(i, 'is prime')
    prime_numbers += 1
else:
    print(i, 'has', factors, 'factors')

Hopefully this is enough for you to fill in the rest yourself.",A,1
15749635,2013-04-01 18:50:30.240000+00:00,"You're almost certainly asking the wrong question, and Raymond Hettinger's answer is almost certainly what you really want.
Something like this might be useful trying to dig into the internals of the CPython interpreter for learning purposes or auditing it for security holes or something… But even then, you're probably better off embedding the Python interpreter into a program and writing functions that expose whatever you want into the Python interpreter, or at least writing a C extension module that lets you manipulate CPython objects.
But, on the off chance that you really do need to do this…
First, there is no reliable way to even get the address from the repr. Most objects with a useful eval-able representation will give you that instead. For example, the repr of ('1', 1) is ""('1', 1)"", not <tuple at 0x10ed51908>. Also, even for objects that have no useful representation, returning <TYPE at ADDR> is just an unstated convention that many types follow (and a default for user-defined classes), not something you can rely on.
However, since you presumably only care about CPython, you can rely on id:

CPython implementation detail: This is the address of the object in memory.

(Of course if you have the object to call id (or repr) on, you don't need to dereference it via pointer, and if you don't have the object, it's probably been garbage collected so there's nothing to dereference, but maybe you still have it and just can't remember where you put it…)
Next, what do you do with this address? Well, Python doesn't expose any functions to do the opposite of id. But the Python C API is well documented—and, if your Python is built around a shared library, that C API can be accessed via ctypes, just by loading it up. In fact, ctypes provides a special variable that automatically loads the right shared library to call the C API on, ctypes.pythonapi.
In very old versions of ctypes, you may have to find and load it explicitly, like pydll = ctypes.cdll.LoadLibrary('/usr/lib/libpython2.5.so') (This is for linux with Python 2.5 installed into /usr/lib; obviously if any of those details differ, the exact command line will differ.)
Of course it's much easier to crash the Python interpreter doing this than to do anything useful, but it's not impossible to do anything useful, and you may have fun experimenting with it.",A,8
26596805,2014-10-27 21:03:54.537000+00:00,"Are you sure you want this to be objects at all? Looks like it would be better as a nested dict:
{
    'foo': [],
    'bar': 1,
    'a': {
        'a': 'Test'
    }
}

This of course is directly serializable to JSON via json.dumps().",A,0
44777029,2017-06-27 09:41:31.607000+00:00,"It is entirely insecure, and your system is wide open to attack. Your first return line doesn't limit what kind of names can be imported, which means the user can execute any arbitrary callable in any importable Python module.
That includes:
/pickle/loads/<url-encoded pickle data>

A pickle is a stack language that lets you execute arbitrary Python code, and the attacker can take full control of your server.
Even a prefixed __import__ would be insecure if an attacker can also place a file on your file system in the PYTHONPATH; all they need is a books directory earlier in the path. They can then use this route to have the file executed in your Flask process, again letting them take full control.
I would not use __import__ at all here. Just import those modules at the start and use a dictionary mapping author to the already imported module. You can use __import__ still to discover those modules on start-up, but you now remove the option to load arbitrary code from the filesystem.
Allowing untrusted data to direct calling arbitrary objects in modules should also be avoided (including getattr()). Again, an attacker that has limited access to the system could exploit this path to widen the crack considerably. Always limit the input to a whitelist of possible options (like the modules you loaded at the start, and per module, what objects can actually be called within).",A,2
52126803,2018-09-01 09:31:27.363000+00:00,"If you want to make a dict with a special rule for handling keys that aren't actually stored in the dict hash table, you want to create a subclass of dict that implements __missing__:

Called by dict.__getitem__() to implement self[key] for dict subclasses when key is not in the dictionary.

Like this:
class SpecialDict(dict):
    def __missing__(self, key):
        if isinstance(key, tuple) and len(key) == 2 and key[1] == 0:
            return [0, key]
        raise KeyError(key)

I don't really understand how your example is supposed to work, so here's a different example to demonstrate it:
>>> d = SpecialDict({(1, 1): [2, 3], (1, 2): [5, 4]})
>>> d[1, 1]
[2, 3]
>>> d[2, 2]
KeyError: (2, 2)
>>> d[20, 0]
[0, 20]

If you store a value for a (n, 0) key, it won't call __missing__ for that key, allowing you to override a single (n, 0) while leaving the rest with their special rule:
>>> d[42, 0] = [23, 23]
>>> d[42, 0]
[23, 23]
>>> d[23, 0]
[0, 23]
",A,1
27610995,2014-12-22 21:55:03.330000+00:00,"Decorators on methods are applied when the class is being defined, which means that the setup and teardown methods are not bound at that time. This just means you need to pass in the self argument manually.
You'll also need to create an outer decorator factory; something that returns the actual decorator, based on your arguments:
def decorator(setup, teardown):
    def decorate_function(test):
        def wrapper(self):
            setup(self)
            test(self)
            teardown(self)
        return wrapper
    return decorate_function

Demo:
>>> def decorator(setup, teardown):
...     def decorate_function(test):
...         def wrapper(self):
...             setup(self)
...             test(self)
...             teardown(self)
...         return wrapper
...     return decorate_function
... 
>>> class run():
...     def setup(self):
...         print(""in setup"")
...     def teardown(self):
...         print(""in teardown"")
...     @decorator(setup, teardown)
...     def test(self):
...         print(""in test"")
... 
>>> run().test()
in setup
in test
in teardown
",A,2
313105,2008-11-23 23:23:44.277000+00:00,"You want an Abstract base class (""virtual"" doesn't mean anything in Python.)
From the documentation:
class CommonInfo(models.Model):
    name = models.CharField(max_length=100)
    age = models.PositiveIntegerField()

    class Meta:
        abstract = True


Edit
""In OO programming when you call object.method() you're supposed to get the lowest-subclass's implementation.""
True.  But not the whole story.
This is not a OO issue.  Or even a Python or Django issue.  This is an ORM issue.
The question is ""What object is reconstructed at the end of the FK reference?""  And the answer is that there's no standard, obvious answer of how to handle the transformation from FK value to object.  
I've got a row in AnimalHome with an animals value of 42.  It refers to Animal.objects.get(pk=42).  Which subclass of Animal?  Cat?  Dog?  How does the ORM layer know if it should do Dog.objects.get(pk=42) or Cat.objects.get(pk=42)?
""But wait,"" you say.  ""It should fetch the Animal object, not a Dog or Cat object.""  You can hope for that, but that's not how the Django ORM works.  Each class is a distinct table.  Cat and Dog are -- by definition -- separate tables, with separate queries.  You're not using an object store.  You're using ORM to relational tables.

Edit
First, your query only works if Dog and Cat share a common key generator, and don't have an overlapping set of PK's.
If you have a Dog with PK of 42 AND a Cat with PK of 42, you've got a problem.  And since you can't easily control the key generation, your solution can't work.
Run Time Type Identification is bad.  It's not Object-Oriented in a number of ways.  Almost anything you can do to avoid RTTI is better than an ever-expanding sequence of if-statements to distinguish subclasses.
However, the model you're trying to build is -- specifically -- a pathological problem for ORM systems.  Indeed, so specifically pathological that I'm almost willing to bet it's homework.  [There are pathological problems for pure SQL systems, also.  They often show up as homework.]
The issue is that the ORM cannot do what you think it should do.  So you have two choices.

Stop using Django.
Do something Django does directly.
Break OO design guidelines and resort to brittle things like RTTI, which make it remarkably hard to add another subclass of animals.

Consider this way to do RTTI -- it includes the class name as well as the PK
KIND_CHOICES = (
   ( ""DOG"", ""Dog"" ),
   ( ""CAT"", ""Cat"" ),
)

class Animal( models.Model ):
    kind = models.CharField( max_length= 1, choices=KIND_CHOICES )
    fk = models.IntegerField()
    def get_kind( self ):
        if kind == ""DOG"":
            return Dog.objects.get( pk = fk )
        elif kind == ""CAT"":
            return Cat.objects.get( pk = fk )
",A,6
35087477,2016-01-29 14:58:03.183000+00:00,"You say you want to pass it, but you have avoided doing that in your view. The third parameter to render, which you explicitly miss out, is the dictionary of keys and values to pass to the template. You need to accept the variable in your view, and pass it in the context.
def personnel_results(request, pk):
    return render(request, 'personnel-results.html', {'pk': pk})

This is well covered in the tutorial - you should go back and do that.",A,1
32138809,2015-08-21 11:09:51.833000+00:00,"To implement this:
>>> 'Drapsicle'[::-2]
'ecsaD'

as your own iterator:
class Reverse:
    def __init__(self, data, step):
        self.data = data
        self.index = len(data) - 1
        self.step = step
    def __iter__(self):
        return self
    def __next__(self):
        if self.index < 0:
            raise StopIteration
        value = self.data[self.index] 
        self.index -= self.step
        return value

Example:
>>> list(Reverse('Drapsicle', 2))
['e', 'c', 's', 'a', 'D']

Note:

self.index starts with len - 1
__next__() does not accept any argument except self
first you get value then you decrement the index

A more flexible design would separate the reversing (e.g., delegate it to the reversed() builtin) and using step != 1 (accept an arbitrary reversible iterable and/or use/implement extended slicing) e.g., based on @Cyphase's suggestion:
>>> list(islice(reversed('Drapsicle'), None, None, 2))
['e', 'c', 's', 'a', 'D']
",A,0
32325856,2015-09-01 07:25:23.283000+00:00,"You can just unpack your nested tuples into your desired variables in the main loop:
[(i, j) for i, j in L if i > 1 and j > 4]

Note that you then do have to 'reconstruct' the original tuple in the left-hand-side expression.
Alternatively, address the elements with indices:
[elem for elem in L if elem[0] > 1 and elem[1] > 4]

Note that I used a list comprehension here (you were using a generator expression inside the list() function, getting you similar results but in a less efficient manner).",A,1
29006756,2015-03-12 10:05:25.573000+00:00,"Use two separate cursors, or create a UNION select to produce one result.
Two separate cursors can be chained together with itertools.chain():
from itertools import chain

cur1 = con.cursor()
cur1.execute('select something from table1') #1st select
cur2 = con.cursor()
cur2.execute('select something_else from table2') #2nd select

for row in chain(cur1, cur2):
    print row

Note that you don't have to call cursor.fetchall(); iteration can be done over the rows directly.
A UNION select requires that your selects return the same number of columns:
cur = con.cursor()
cur.execut('''\
    select something from table1  -- 1st select
    union
    select something_else from table2  -- 2nd select
''')

for row in cur:
    print row
",A,2
15296770,2013-03-08 14:53:14.007000+00:00,"Use \d+ (no brackets) to match the literal slash + digits:
response = re.search(r'^(http://)?(www\.)?(vimeo\.com/)?(\d+)', embed_url)

Result:
>>> re.search(r'^(http://)?(www\.)?(vimeo\.com/)?(\d+)', embed_url).group(4)
'52422837'

You were using a character group ([...]) where none was needed. The pattern [\/\d+] matches exactly one of /, + or a digit.",A,4
20413406,2013-12-06 00:04:59.010000+00:00,"You can filter 'empty' values:
filter(None, myList)

or you can use a list comprehension. On Python 3, filter() returns a generator; the list comprehension returns a list on either Python 2 or 3:
[t for t in myList if t]

If your list contains more than just tuples, you could test for empty tuples explicitly:
[t for t in myList if t != ()]

Python 2 demo:
>>> myList = [(), (), ('',), ('c', 'e'), ('ca', 'ea'), ('d',), ('do',), ('dog', 'ear', 'eat', 'cat', 'car'), ('dogs', 'cars', 'done', 'eats', 'cats', 'ears'), ('don',)]
>>> filter(None, myList)
[('',), ('c', 'e'), ('ca', 'ea'), ('d',), ('do',), ('dog', 'ear', 'eat', 'cat', 'car'), ('dogs', 'cars', 'done', 'eats', 'cats', 'ears'), ('don',)]
>>> [t for t in myList if t]
[('',), ('c', 'e'), ('ca', 'ea'), ('d',), ('do',), ('dog', 'ear', 'eat', 'cat', 'car'), ('dogs', 'cars', 'done', 'eats', 'cats', 'ears'), ('don',)]
>>> [t for t in myList if t != ()]
[('',), ('c', 'e'), ('ca', 'ea'), ('d',), ('do',), ('dog', 'ear', 'eat', 'cat', 'car'), ('dogs', 'cars', 'done', 'eats', 'cats', 'ears'), ('don',)]

Of these options, the filter() function is fastest:
>>> timeit.timeit('filter(None, myList)', 'from __main__ import myList')
0.637274980545044
>>> timeit.timeit('[t for t in myList if t]', 'from __main__ import myList')
1.243359088897705
>>> timeit.timeit('[t for t in myList if t != ()]', 'from __main__ import myList')
1.4746298789978027

On Python 3, stick to the list comprehension instead:
>>> timeit.timeit('list(filter(None, myList))', 'from __main__ import myList')
1.5365421772003174
>>> timeit.timeit('[t for t in myList if t]', 'from __main__ import myList')
1.29734206199646
",A,7
9234951,2012-02-10 21:19:55.907000+00:00,"From ctype.h:
#define isascii(c)  ((c & ~0x7F) == 0)
",A,8
7148001,2011-08-22 13:12:33.337000+00:00,"The sample page is encoded in UTF-16 without properly providing that factoid in the header. 
>>> page = urllib2.urlopen( ""http://securities.stanford.edu/1046/BWEN00_01"" )
>>> page.info().headers
['Date: Mon, 22 Aug 2011 13:13:56 GMT\r\n', 'Server: Apache/1.3.33 (Darwin) mod_jk/1.2.2 DAV/1.0.3\r\n', 'Cache-Control: max-age=60\r\n', 'Expires: Mon, 22 Aug 2011 13:14:56 GMT\r\n', 'Last-Modified: Thu, 21 Jul 2011 22:06:51 GMT\r\n', 'ETag: ""18b9a6e-9af6-4e28a2fb""\r\n', 'Accept-Ranges: bytes\r\n', 'Content-Length: 39670\r\n', 'Connection: close\r\n', 'Content-Type: text/html\r\n']

Try page.decode('utf-16') to see the page in proper Unicode characters instead of bytes.",A,2
48140169,2018-01-07 18:15:25.510000+00:00,"In all those places that said to use save(commit=False), they will also say that you should exclude the field from the form, rather than using fields = '__all__'. That way, the form won't be invalid.",A,1
30472761,2015-05-27 03:41:12.820000+00:00,"I think your problem is that your Event code is broken.
Imagine this scenario:

main process calls sort for m.
sort calls Q[m].put and E[m].set.
Parse wakes up, does Q[m].get, and starts processing.
main process calls sort again for the same m.
sort calls Q[m].put and E[m].set.
Parse finishes processing the first message, calls E[m].clear.

Now Parse is just waiting around for the Event to be set again. Which may not happen for quite a while. And, even if it happens quickly, it's still not going to catch up; it only does one Q[m].get for each Event.wait.
So, what you end up with is Parse appearing to fall farther and farther behind. And when you try to profile it to figure out why, you see that it's spending all its time waiting on E[m].wait. But this isn't because E[m].wait is slow, it's just because the event trigger got lost.
This isn't the only race condition here, it's just the most obvious one.
The general problem is that you can't use event objects this way. Normally, you solve it by using a Condition instead, or one-shot triggering and self-resetting Events, plus looping over the Q[m].get(block=False) after each Event.
But really, there is no need to do this in the first place. If you just remove the Event entirely, when Parse calls Q[m].get, that blocks until there's something there. So, when sort calls Q[m].put, that wakes up Parse, and there's nothing else needed.
In fact, the whole point of Queue is that it's inherently self-synchronized. If you don't want that, use a Pipe, and then you can use a Condition for signaling. But in the simple case, that's just a less efficient version of a Queue.",A,2
278006,2008-11-10 14:36:53.463000+00:00,"The efficiency question is barely relevant.  The dictionary lookup is done with a simple hashing technique, the if-statements have to be evaluated one at a time.  Dictionaries tend to be quicker.
I suggest that you actually have polymorphic objects that do extractions from the DOM.
It's not clear how type gets set, but it sure looks like it might be a family of related objects, not a simple string.
class ExtractTitle( object ):
    def process( dom ):
        return something

class ExtractMetaTags( object ):
    def process( dom ):
        return something

Instead of setting type=""extractTitle"", you'd do this.
type= ExtractTitle() # or ExtractMetaTags() or ExtractWhatever()
type.process( dom )

Then, you wouldn't be building this particular dictionary or if-statement.",A,1
49558850,2018-03-29 14:36:22.980000+00:00,"Use a dictionary:
var1 = 'beagle'
params = {'{}_positive_examples'.format(var1): beagle}
model = visual_recognition.create_classifier('dogs', **params)
",A,4
13471946,2012-11-20 11:10:10.750000+00:00,"The dc: prefix indicates a XML namespace. Use the elementtree API namespace support to deal with it, not just remove it from your input. As it happens, dc usually refers to Dublin Core metadata. 
You need to determine the full namespace URL, then use that URL in your XPath queries:
DCNS = 'http://purl.org/dc/elements/1.1/'
creator = article_node.xpath('.//{{{0}}}creator'.format(DCNS))

Here I used the recommended http://purl.org/dc/elements/1.1/ namespace URL for the dublin core prefix.
You can normally determine the URL from the .nsmap property; your root element probably has the following .nsmap attribute:
{'dc': 'http://purl.org/dc/elements/1.1/'}

and thus you can change your code to:
creator = article_node.xpath('.//{{{0}}}creator'.format(article_node.nsmap['dc']))

This can be simplified further still by passing the nsmap dictionary to the xpath() method as the namespaces keyword, at which point you can use the prefix in your xpath expression:
creator = article_node.xpath('.//dc:creator', namespaces=article_node.nsmap)
",A,2
17128215,2013-06-15 21:48:07.893000+00:00,"Do what the error message tells you to do; run Python from the command line with the -v command line switch to get a traceback. There is an error in the sitecustomize.py module and you need to fix that.
Alternatively, set the PYTHONVERBOSE environment variable to get the same effect.",A,2
45388827,2017-07-29 11:38:04.230000+00:00,"The Google App Engine does not connect your Python code via CGI. CGI examples are not going to work, no.
Instead, a different, more modern standard, the Web Server Gateway Interface is used; see the [Python Runtime Environment documentation]https://cloud.google.com/appengine/docs/standard/python/runtime):

A Python web app interacts with the App Engine web server using the WSGI protocol, so apps can use any WSGI-compatible web application framework. App Engine includes a simple web application framework, called webapp2, to make it easy to get started. For larger applications, mature third-party frameworks, such as Django, work well with App Engine.

Rather than pick up random Python scripts from around the web, start with the Quickstart examples and move from there. ",A,0
22435614,2014-03-16 10:19:15.497000+00:00,"The destination directory must exist for shutil.copy() to work; os.path.isdir(dest) must be True. If dest does not exist, shutil ends up trying to copy the source filename to the directory name (including the trailing /) and that is what raises the exception.
You can call os.makedirs() to ensure that the destination directory is created properly first:
import shutil
   import os
file = ""true.detective.s01e04.720p.hdtv.x264-killers.mkv""
dest = ""/home/sharefolder/things/Videos/Series/True Detective/""
try:
    os.makedirs(dest)
except OSError:
    # destination directory already exists
    pass
shutil.copy(file, dest)
",A,0
26468466,2014-10-20 14:45:46.267000+00:00,"You have corrupted your BeautifulSoup installation:
  File ""D:\Python\Anaconda2\lib\site-packages\bs4\element.py"", line 160, in <module> 
    workbook.close()

That is not BeautifulSoup code; see the actual bs4.element source code.
You appear to have replaced the regular element.py file in that package with one that writes an Excel worksheet (using the xlsxwriter package). You'll have to remove the whole bs4 directory and reinstall BeautifulSoup.",A,1
29207617,2015-03-23 10:12:51.610000+00:00,"You need to specify the correct codec when opening the file. What the correct codec is we can only guess at; the GitHub sample you uploaded decodes fine when using one of the Chinese GB* codecs, but those codecs are quite eager (text not encoded with one of those can also be decoded, just with the wrong results).
You'll need to ask the source of those CSV files if a codec is known. That could be in the documentation, or in the HTTP Content-Type header (look for a charset= parameter), or by some other metadata means.
To specify the codec when opening the file, use the encoding argument:
with open('20120901_20120915_ACCLOG.csv', 'r', encoding='gbk')  as f:

See the Standard Encodings table for what codecs Python 3 can handle out of the box; there are various Chinese codecs to chose from there.",A,3
15999114,2013-04-14 12:33:22.753000+00:00,"SQLAlchemy has an excellent declarative layer itself. Elixir has been obsoleted for some time now. As a result, Elixir has not seen any development for over 2 years, the project has been abandoned.
The point to abandon Elixir is Right Now.",A,1
51713547,2018-08-06 18:21:43.373000+00:00,"Your code is very close to correct; the only problem is that you got the negation backward. 
This:
if any([simbol1 not in item , simbol2 not in item]):

… is asking ""are any of these are not in the item?"" Just as in English, that's true if one is not in the item, but the other is in the item, which isn't what you want. You only want it to be true if neither is in the item.
In other words, you want either ""are all of these not in the item?""
if all([simbol1 not in item, simbol2 not in item]):

… or ""are none of these in the item?""
if not any([simbol1 in item, simbol2 in item]):


However, when you only have a fixed list of two things like this, it's usually easier to use and or or instead of any or all—again, just like in English:
if symbol1 not in item and simbol2 not in item:

… or:
if not (simbol1 in item or simbol2 in item):

If, on the other hand, you had a whole bunch of symbols to check, or a list of them that you couldn't even know until runtime, you'd want a loop:
if all(simbol not in item for simbol in (simbol1, simbol2)):

if not any(simbol in item for simbol in (simbol1, simbol2)):
",A,1
36801007,2016-04-22 18:31:37.313000+00:00,"Because your ranges are strictly adjacent and in increasing order, you can use bisection:
from bisect import bisect

ranges = [1, 51, 101, 151, 201]
if 0 < x <= 250:
    print('range{}'.format(bisect(ranges, x))
else:
    print('Out of bounds')

Bisection takes O(logN) steps to find the matching range out of N possibilities.",A,1
28816124,2015-03-02 17:40:06.013000+00:00,"Find average for each day of the year
#!/usr/bin/env python
from datetime import datetime
import pandas

def same_day(date_string): # remove year
    return datetime.strptime(date_string, ""%m/%d/%Y"").strftime('%m-%d')

df = pandas.read_csv('input.csv', index_col=0,
                     usecols=[0,2], names=['date', 'value'],
                     converters={'date': same_day})
print(df.groupby(level=0).mean())

Output
            value
date             
01-01  143.991035
01-02  123.232340
12-30    0.000000
12-31  100.981233

It assumes that all hourly values have the same weight in different years.
Find average for each date
pandas allows duplicate values in index.
To group the data by date (1st column) and to find the mean of the 3rd column:
#!/usr/bin/env python
import pandas

df = pandas.read_csv('input.csv', parse_dates=True, index_col=0,
                     usecols=[0,2], names=['date', 'value'])
print(df.groupby(level=0).mean())

Output
                 value
date                  
2005-01-01  143.991035
2005-01-02  123.232340

[2 rows x 1 columns]

The code that uses itertools.groupby() produces the same result:
#!/usr/bin/env python
import csv
from collections import OrderedDict
from datetime import datetime
from itertools import groupby
from operator import itemgetter
from pprint import pprint

def groupby_mean(file):
    mean = OrderedDict()
    for day, same_day_rows in groupby(csv.reader(file), key=itemgetter(0)):
        L = [float(row[2]) for row in same_day_rows]
        mean[datetime.strptime(day, '%m/%d/%Y')] = sum(L) / len(L)
    return mean

with open('input.csv') as file:
    pprint(groupby_mean(file))

Output
{datetime.datetime(2005, 1, 1, 0, 0): 143.99103529411764,
 datetime.datetime(2005, 1, 2, 0, 0): 123.23234}

math.fsum(L) leads to the same result as sum(L) with your input.",A,0
15341269,2013-03-11 14:37:17.890000+00:00,"You can't use assignment in an expression, it is a statement. A lambda can only contain one expression, and statements are not included.
You can assign to the map though, by using the operator.setitem() function instead:
import operator

lambda map: operator.setitem(map, 'x', 'y')
",A,15
13053999,2012-10-24 16:57:19.140000+00:00,"You'll find everything in the listobject.c file.
It's methods are enumerated in the list_methods() structure. You may also want to study the PyList_Type structure for further pointers to implementations of various hooks.
Since __getitem__ and __setitem__ are part of both the C sequence protocol, the PyList_Type structure points to a new structure list_as_sequence that defines those hooks, listed in the tp_as_sequence slot of PyList_Type.
The list_item function implements the __getitem__ hook, and __setitem__ is represented by list_ass_item.
Slices, on the other hand, are handled by the mapping protocol, and you'll end up (via list_as_mapping) at list_subscript and list_ass_subscript.",A,4
51142132,2018-07-02 18:48:32.290000+00:00,"'' is a perfectly valid string. You don't need to ""cast"" it, or anything of the sort.
But it's an empty string. So it doesn't have a first character word[0]. If you read the error message:
IndexError: string index out of range

… that's what it's telling you. The problem isn't that you don't have a string, it's that you have a string that isn't big enough to have a first character.

What you want to do here is either check for the empty string before asking for its first character:
if word and word[0] >= 'h':
    print(word.upper())
    word = ''
else:
    word = ''

… or handle the IndexError:
try:
    if word[0] >= 'h':
        print(word.upper())
        word = ''
    else:
        word = ''
except IndexError:
    word = ''

Normally, in Python, it's more idiomatic to just try something and handle errors (Easier to Ask Forgiveness than Permission, as opposed to Look Before You Leap), but in some cases it's going to be more readable to check first, and I think this is one of those cases.",A,2
41564695,2017-01-10 08:50:25.300000+00:00,"You haven't done anything at all to associate your database connection with the global g object. connect_db is a standalone function, and returns the connection itself. So your code needs to be:
db = connect_db()
db.execute(""INSERT INTO users(name, email, password) VALUES (?,?,?)"", (name, email, password))
db.commit()
db.close()
",A,0
10915991,2012-06-06 14:13:18.980000+00:00,"Notice item 3 of the instructions. A quick way to do this is to replace your render_to_response call with render(request, 'reserve/templates/contact_form.html',{'form': form}) (import it via from django.shortcuts import render).",A,2
21210737,2014-01-18 22:32:24.580000+00:00,"First, you can have two separate lists that reference the same elements:
>>> a, b, c = object(), object(), object()
>>> lst0 = [a, b, c]
>>> lst1 = list(reversed([c, b, a]))
>>> id(lst0), id(lst1)
(4418696712, 4399675656)
>>> id(lst0[0]), id(lst1[0])
(4298830320, 4298830320)

And if you make one list from the other, that's usually the case. Slicing a list with old_list[:], constructing a new list with list(old_list), copying it with copy.copy(old_list), etc.—all of these create a shallow copy. If you want a 2-level copy, you have to do it explicitly—e.g., [copy.copy(x) for x in old_lst]. If you want an all-the-way-down deep copy, use copy.deepcopy(old_lst).
So, when you check the ids of the elements of the two lists, you shouldn't be surprised to see them the same. Ashwini Chaudhary's answer explains the consequences of this.

On top of that, Python is always allowed to ""intern"" immutable objects like integers, strings, even tuples and frozen sets.* Unlike, e.g., Java, you can control this by using new or not, in Python it's entirely up to the implementation, and you should never count on getting a new immutable object when you ask for one.
CPython only does this for a small set of values, but that includes a range of ""tiny integers"" specified at compile time, which default to -6 through 255 in 2.7. So, even if you did a deep copy—or even if you tried to create brand-new integer objects (e.g., int(x*2.0+0.1)//2), you would likely still get the same objects again.

On top of that, for literals of built-in immutable types, the compiler may merge two identical literals, so by the time it gets to the interpreter, what looked like two different equal values in the source code are actually a single constant value in the bytecode. For example, at the interactive interpreter (which compiles each statement separately), try this:
>>> 123456 is 123456
True
>>> a = 123456
>>> a is 123456
False

You aren't guaranteed to get those results, but that's what I get with CPython 2.7 and 3.4 on my laptop. But in a function, since a function definition is a single compound statement that's all compiled together:
>>> def f():
...     a = 123456
...     return a is 123456
>>> f()
True


In general, you also shouldn't count on not getting a new immutable object—except in the special case of a handful of builtin constants, like None.

Finally, be careful using id for tests. The id is only guaranteed to be unique during the lifetime of an object. So, it's perfectly valid for a Python implementation to return the same id for two objects if the first one has already gone away before the second was created. (In CPython, the id is the address of the underlying PyObject*, so if the same memory gets pulled off the object free list and reused, you get the same id.)

* In theory, it could even presumably intern your own custom objects if it knew they were immutable… but since there's no way it can know they're immutable, that's not going to happen.",A,3
50268776,2018-05-10 08:37:38.407000+00:00,"Rather than trying to mangle URLs like that (and I'd challenge the assertion that your way is ""cleaner""), you could preserve the existing parameters by outputting hidden fields within the form which would then be sent along with the visible ones:
<form method=""GET"">
  {% for key, value in request.GET.items %}
  <input type=""hidden"" name=""{{ key }}"" value=""{{ value }}"">
  {% endfor %}
  .. rest of form ..
</form>
",A,3
14108832,2013-01-01 07:50:54.267000+00:00,"
The first thing unclear to me is that what does below sentence mean? So sp1 is a pointer? a function? or a pointer to a function? 

sp is a shared_ptr<implementation>. If you don't know what that means, there's reference documentation and tutorials to look at. But the short version is: it's an object that acts similar to an implementation * pointer, except that it automatically deletes the implementation object whne you're done with it. That's what makes it a ""smart pointer"". A shared_ptr is a specific kind of smart pointer that lets you make as many copies as you want, and only deletes the underlying object when all of those copies go away. 
One way to look as this is that it gives you a simple form of garbage collection that doesn't require a garbage collector.
Another way to look at it is as part of Resource Acquisition Is Initialization (RAII), one of the central idioms of C++.

and new implementation() means what? The argument of sp1()?

new implementation() creates a new implementation object, calls its default constructor, and returns an implementation * pointer. That pointer is the argument to the shared_ptr constructor. This means that sp1 becomes a smart pointer to that new implementation object, so that object will be destroyed and deleted when sp1, and any copies later made of it, all go away.

The second question is that destroying implementation is given as a result of sp1.reset() and sp2.reset().

Actually, it's given as a result of both sp1 and sp2 being pointed at new values or being destroyed. reset does the former, but just doing nothing and letting them go out of scope does the latter. That's a major part of what RAII is all about.

So it is not necessary to call both sp1.reset() and sp2.reset() to release the shared_ptr, am I right?

Exactly. You very rarely want to explicitly call reset. The whole point of RAII is that you don't have to manage these things manually; you initialize an object (like a shared_ptr) to acquire access to a resource, and just let that object go away to release access.
There are a few cases where it's useful. For example, if you have a shared_ptr as a member of an object, and that object is going to last much longer than the resource it owns, you can release it early by calling reset. (And if you've passed a copy off to someone else in the meantime, you don't have to worry about it being deleted early—it just means that you're no longer involved in keeping it alive.)",A,2
49677760,2018-04-05 17:00:19.497000+00:00,"TCP sockets are streams of bytes, not streams of messages.. If you want a stream of messages, you have to define a protocol on top of that, and code to handle sending and receiving data in that protocol.
If your messages are all strings, and never include newline characters, probably the simplest protocol is just separating the messages with newlines. I think you’ve already worked that out, you just need to know how to implement it.
If the way you’re handling the network is a blocking recv (whether in the main loop of your program, or in the loop of a thread dedicated to reading off the socket), there’s built in support for this protocol: call sock.makefile with an appropriate mode (r plus an encoding if you want Unicode strings for your messages, rb if you want raw bytes), and you can use it like a file—e.g., a for msg in file: loop, or a while loop over file.readline() until you get an exception (meaning socket error) or empty string (meaning EOF—a clean socket close).
If your messages can have newlines in them, you may still be able to use this. Just escape the messages (maybe using full backslash-escape so they’re always readable, for easy debugging, or maybe just msg.replace('\\', '\\\\').replace('\n', '\\n')) before sending, and unescape on receiving.
Under the covers, this works the same way a normal file object does with disk files: when you ask for the next line, if it already has a complete line in the buffer, it just splits it off and returns it; if not, it reads buffers and appends them onto what it has until it finally gets a newline, then splits off the first complete line and returns it to you. So, it will never block waiting for two packets if the first packet contained a newline. But it will also never give you a “no complete message yet” to deal with; it’ll just keep blocking until it reads enough packets to get the next newline.
It is worth learning how to build something like this from scratch at some point—but meanwhile, you can just use what’s already there. If you’re interested, the short version (without good error handling, and some useful optimizations) looks something like this:
def messages(sock):
    buf = b''
    while True:
        data = sock.recv(8192)
        if not data: break
        buf += data
        lines = buf.split('\n')
        for line in lines[:-1]:
            yield line.decode('utf8')
        buf = lines[-1]
    # Should leftover bytes after the last newline be a message, an error, or ignored? Picking arbitrarily...
    if buf: yield buf.decode('utf8')

But of course it’s simpler to just call 'makefile' (and that way you do get the error handling and optimization, too).",A,2
20460562,2013-12-08 23:23:16.817000+00:00,"A().d is never changed, so there is no difference in creating a copy or creating a new instance.
So to create a new instance of A() that holds new, separate instances of B(), just create a new instance:
second_a = A()

Any variations between instances can be created directly on this new instance. If creation of such an object is expensive, use caching strategies to avoid repeated calculations.",A,0
9763841,2012-03-19 01:10:16.707000+00:00,"Assuming the first found positions should be the same using list.index() method:
def f(lst1, lst2, value):
    try: return lst1.index(value) == lst2.index(value)
    except ValueError:
        return False

Allowing all positions using set intersection:
def positions(lst, value):
    return (pos for pos, x in enumerate(lst) if x == value)

def f(lst1, lst2, value):
    return bool(set(positions(lst1, value)).intersection(positions(lst2, value)))

Or even better: zip()-based solution suggested by @wim:
from itertools import izip

def f(lst1, lst2, value):
    return any(x1 == x2 == value for x1, x2 in izip(lst1, lst2))

Note: any() returns as soon as it finds the first True item without needlessly enumerating the rest of the items.",A,3
31782547,2015-08-03 08:19:43.720000+00:00,"Convert it to a dict.
refund_dict = {value: key for key, value in REFUND_STATUS}
actual_status = refund_dict[display_status]
",A,0
502924,2009-02-02 11:38:56.287000+00:00,"As a side note, actually a good DSL could look kinda like this:
btn.clicked { |bet_id| placeBet bet_id }
animator.eachFrame { |newPos| buttons[bet_id].move newPos }

To answer your question: For the simple example you provided a plain bind works just fine.",A,0
7361590,2011-09-09 12:25:57.310000+00:00,"You could use c.nonzero() method:
>>> from scipy.sparse import lil_eye
>>> c = lil_eye((4, 10)) # as an example
>>> c
<4x10 sparse matrix of type '<type 'numpy.float64'>'
        with 4 stored elements in LInked List format>
>>> c.nonzero()
(array([0, 1, 2, 3], dtype=int32), array([0, 1, 2, 3], dtype=int32))
>>> import numpy as np
>>> np.ascontiguousarray(c)
array([  (0, 0) 1.0
  (1, 1)        1.0
  (2, 2)        1.0
  (3, 3)        1.0], dtype=object)

You don't need to calculate c matrix to find out indexes of non-zero elements in c = a - b; you could do (a != b).nonzero():
>>> a = np.random.random_integers(2, size=(4,4))
>>> b = np.random.random_integers(2, size=(4,4))
>>> (a != b).nonzero()
(array([0, 0, 1, 1, 1, 2, 3]), array([1, 2, 1, 2, 3, 2, 0]))
>>> a - b
array([[ 0,  1,  1,  0],
       [ 0,  1, -1, -1],
       [ 0,  0,  1,  0],
       [-1,  0,  0,  0]])
",A,2
352546,2008-12-09 12:08:35.257000+00:00,"Just subclass the type
>>> class X(str):
...     def myMethod( self ):
...             return int(self)
...
>>> s=X(""Hi Mom"")
>>> s.lower()
'hi mom'
>>> s.myMethod()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 3, in myMethod
ValueError: invalid literal for int() with base 10: 'Hi Mom'

>>> z=X(""271828"")
>>> z.lower()
'271828'
>>> z.myMethod()
271828
",A,24
44854477,2017-06-30 20:36:23.647000+00:00,"The object.__setattr__ method includes a specific check to make sure that self passed in is really a (subclass) of object (or another object that reuses the same C function directly). That test doesn't allow for anything else being passed in, including old-style instances.
The check is there to prevent object.__setattr__ being used to modify built-in types (known as the Carlo Verre hack, after its discoverer.
Without the explicit check (or if you compiled Python to disable it), you could do this:
>>> object.__setattr__(str, 'lower', str.upper)
>>> ""This is why we can't have nice things"".lower()
""THIS IS WHY WE CAN'T HAVE NICE THINGS""

See the hackcheck() function, and the original Python-dev discussion.
That old-style instance objects don't pass the test is just a side-effect.
Note that the only reason you'd ever want to call object.__setattr__ is if you have a base class that implements a __setattr__ method you need to bypass.  For old-style instances, you could just use self.__dict__[name] = value instead.",A,4
45729626,2017-08-17 08:02:22.673000+00:00,"Create a mapping from first element to index. I'm presuming the first elements are unique to simplify this example:
indices = {t[0]: i for i, t in enumerate(A)}

Now you can trivially map each element to an index that matches it:
for index, (first, second) in enumerate(A):
    if second in indices:
        print(f'Row {index} matches row {indices[second]}')

Demo:
>>> A = [['a','b'],
...      ['b','c'],
...      ['c','a'],
...      ['d','a'],
...      ['e',None]]
>>> indices = {t[0]: i for i, t in enumerate(A)}
>>> for index, (first, second) in enumerate(A):
...     if second in indices:
...         print(f'Row {index} matches row {indices[second]}')
...
Row 0 matches row 1
Row 1 matches row 2
Row 2 matches row 0
Row 3 matches row 0
",A,3
21058110,2014-01-11 03:13:04.950000+00:00,"Examples from the docs suggest the following directory layout:
<root>
├── setup.py
└── tk
    ├── __init__.py
    └── latex
        └── __init__.py

where setup.py:
from distutils.core import setup

setup(
    name='tk',
    version='1.0',
    description='Graphics package that supplements native Tkinter',
    packages=['tk', 'tk.latex'],
)
",A,2
41359750,2016-12-28 10:10:24.697000+00:00,"You put your event loop in another co-routine. Don't do that. The event loop is the outermost 'driver' of async code, and should be run synchronous.
If you need to process the fetched results, write more coroutines that do so. They could take the data from a queue, or could be driving the fetching directly.
You could have a main function that fetches and processes results, for example:
async def main(loop): 
    for n in range(1, 11):
        future = loop.run_in_executor(None, get, n)
        k, v = await future
        # do something with the result

loop = asyncio.get_event_loop()
loop.run_until_complete(main(loop))

I'd make the get() function properly async too using an async library like aiohttp so you don't have to use the executor at all.",A,6
3128550,2010-06-27 19:14:17.737000+00:00,"To activate the virtualenv from within Python, use the activate_this.py script (which is created with the virtualenv) with execfile.
activate_this = os.path.join(""path/to/radioenv"", ""bin/activate_this.py"")
execfile(activate_this, dict(__file__=activate_this))
",A,3
1100463,2009-07-08 20:46:10.020000+00:00,"A test means you have a pass/fail threshold.   For a performance test, this means too slow and you fail, fast enough and you pass.  If you fail, you start doing rework.
If you can't fail, then you're benchmarking, not actually testing.
When you talk about ""system is capable of running"" you have to define ""capable"".  You could use any of a large number of hardware performance benchmarks.  Whetstone, Dhrystone, etc., are popular.  Or, perhaps you have a database-intensive application, then you might want to look at the TPC benchmark.  Or, perhaps you have a network-intensive application and want to use netperf.  Or a GUI-intensive application and want to use some kind of graphics benchmark.
Any of these give you some kind of ""capability"" measurement.  Pick one or more.  They're all good.  Equally debatable.  Equally biased toward your competitor and away from you.
Once you've run the benchmark, you can then run your software and see what the system actually does.
You could -- if you gather enough data -- establish some correlation between some benchmark  numbers and your performance numbers.   You'll see all kinds of variation based on workload, hardware configuration, OS version, virtual machine, DB server, etc.  
With enough data from enough boxes with enough different configurations, you will eventually be able to develop a performance model that says ""given this hardware, software, tuning parameters and configuration, I expect my software to do [X] transactions per second.""  That's a solid definition of ""capable"". 
Once you have that model, you can then compare your software against the capability number.  Until you have a very complete model, you don't really know which systems are even capable of running the piece of code 1000 times per second.",A,5
54563382,2019-02-06 22:13:57.787000+00:00,"You should redirect rather than calling the view directly.
    if self.model.objects.get(user=self.request.user):
        return redirect(""organisations:update"")
     else:
        return super().get(*args, **kwargs)
",A,1
23416985,2014-05-01 21:31:19.993000+00:00,"You don't need to encode at all. BeautifulSoup has already decoded the response bytes to Unicode values, and print() can take care of the rest here.
However, the page is malformed, as there are no closing </option> tags. This can confuse the standard HTML parser. Install lxml or the html5lib package, and the page can be parsed correctly:
parsedHtml = BeautifulSoup(pageContent, 'lxml')

or
parsedHtml = BeautifulSoup(pageContent, 'html5lib')

Next, you can select the <option> tags with one CSS selector:
possibleMatches = parsedHtml.select('select[name=languageid] option')

for match in possibleMatches:
    print(match.text, ""-"", match.get('value'))

Demo:
>>> possibleMatches = soup.select('select[name=languageid] option')
>>> for match in possibleMatches:
...     print(match.text, ""-"", match.get('value'))
... 
Language - 0
Bahasa Indonesia - 32
Català - 34
Deutsch - 4
Eesti - 41
English - 1
Español - 2
Esperanto - 22
Filipino - 21
Français - 3
Italiano - 11
Język polski - 13
LINGUA LATINA - 35
Magyar - 14
Nederlands - 7
Norsk - 18
Português - 8
Română - 27
Suomi - 20
Svenska - 17
čeština - 31
Русский - 10
देवनागरी - 39
ภาษาไทย - 38
中文 - 5
日本語 - 6
",A,1
21654031,2014-02-09 01:23:35.070000+00:00,"It does not run in O(n) time. When you ask for all values in the dictionary a regular sort is used, a O(NlogN) algorithm.
When asking for a top K results, a heapq.nlargest() call is used, a more efficient approach in O(NlogK) time:
def most_common(self, n=None):
    '''List the n most common elements and their counts from the most
    common to the least.  If n is None, then list all element counts.

    >>> Counter('abcdeabcdabcaba').most_common(3)
    [('a', 5), ('b', 4), ('c', 3)]

    '''
    # Emulate Bag.sortedByCount from Smalltalk
    if n is None:
        return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)
    return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))

The answer talks about the counting being done in linear time; constructing the Counter instance, basically a loop over the input iterable:
for elem in iterable:
    self[elem] = self_get(elem, 0) + 1
",A,4
11328962,2012-07-04 12:18:04.053000+00:00,"You are confusing 'strongly typed' with 'dynamically typed'.
I cannot change the type of 1 by adding the string '12', but I can choose what types I store in a variable and change that during the program's run time.
The opposite of dynamic typing is static typing; the declaration of variable types doesn't change during the lifetime of a program. The opposite of strong typing is weak typing; the type of values can change during the lifetime of a program.",A,30
19938457,2013-11-12 19:44:59.087000+00:00,"Since you haven't explained what problem you're trying to solve, all I can do is guess. But…
Your function never returns a number. If it succeeds, it prints out a number, then falls off the end of the function and returns None. If it fails, it returns False. And there's no other return anywhere in the code.
That's easy to fix: just return the value instead of print-ing it:
def myLog(x, b):
    count = 1

    while x > 0 and b >= 2:
        ans = b ** count
        if (ans == x):
            return count
        elif (ans > x):
            return count-1
        count += 1
    else:
        return False


You can improve performance by doing ans *= b each time through the loop instead of ans = b ** count. If the numbers are huge, dividing x by b might be better—division is usually slower than multiplication, but getting out of the huge-number domain early might help more than avoiding division.
It's also got some style problems, like the unnecessary parentheses some (but not all) of your conditions.
And finally, you may want to consider writing a ""test driver"". For example:
repcount = 100
errcount = 0
for _ in range(repcount):
    x = generate_nice_random_x()
    b = generate_random_base()
    log1, log2 = myLog(x, b), int(math.log(x, b))
    if log1 != log2:
        print('log({}, {}): {} != {}'.format(x, b, log1, log2))
        errcount += 1
print('{}/{} errors'.format(errcount, repcount))

Start with a small repcount to make sure you don't spam the screen; when you're happier with it, use a much larger one. Meanwhile, I'll leave it to you to figure out a good domain to choose from for testing log functions.",A,2
28951093,2015-03-09 20:24:12.520000+00:00,"You'll need to load the data as JSON; the json module can do this for you, but you need to decode the data to text first.
import urllib.request
import json

with urllib.request.urlopen(""http://openexchangerates.org/api/latest.json?app_id=XXX"") as url:
    response = url.read()

charset = url.info(). get_content_charset('utf-8')  # UTF-8 is the JSON default
data = json.loads(response.decode(charset))

From there on out data is a Python object.
Judging by the documenation you should be able to access rates as:
print('Euro rate', data['rates']['EUR'])

for example.",A,1
9549302,2012-03-03 19:50:36.917000+00:00,"Here's another implementation based on .split('.') idea. It might be more readable. Split on '.', right-align the left part, left-align the right part:
width = max(map(len, job_IDs)) # width of ""job id"" field 
for jid, mem, unit in zip(job_IDs, memory_used, memory_units):
  print(""Job {jid:{width}}: {part[0]:>3}{part[1]:1}{part[2]:<3} {unit:3}"".format(
    jid=jid, width=width, part=str(mem).partition('.'), unit=unit))

Output
Job 13453 :  30     MB 
Job 123   : 150.54  GB 
Job 563456:  20.6   MB 
",A,10
287492,2008-11-13 16:52:42.947000+00:00,"The ""overhead"" with cursors is merely part of the API.   Cursors are how parts of the RDBMS work under the hood.  Often CREATE TABLE and INSERT have SELECT statements, and the implementation is the obvious internal cursor implementation. 
Using higher-level ""set-based operators"" bundles the cursor results into a single result set, meaning less API back-and-forth.
Cursors predate modern languages that provide first-class collections.  Old C, COBOL, Fortran, etc., had to process rows one at a time because there was no notion of ""collection"" that could be used widely.  Java, C#, Python, etc., have first-class list structures to contain result sets.
The Slow Issue
In some circles, the relational joins are a mystery, and folks will write nested cursors rather than a simple join.  I've seen truly epic nested loop operations written out as lots and lots of cursors.  Defeating an RDBMS optimization.  And running really slowly.
Simple SQL rewrites to replace nested cursor loops with joins and a single, flat cursor loop can make programs run in 100th the time.  [They thought I was the god of optimization.  All I did was replace nested loops with joins.  Still used cursors.]
This confusion often leads to an indictment of cursors.  However, it isn't the cursor, it's the misuse of the cursor that's the problem.  
The Size Issue
For really epic result sets (i.e., dumping a table to a file), cursors are essential.  The set-based operations can't materialize really large result sets as a single collection in memory.
Alternatives
I try to use an ORM layer as much as possible.  But that has two purposes.  First, the cursors are managed by the ORM component.  Second, the SQL is separated from the application into a configuration file.  It's not that the cursors are bad.  It's that coding all those opens, closes and fetches is not value-add programming.",A,73
53565561,2018-11-30 22:08:30.367000+00:00,"The problem is in what you pass in when you instantiate the class. breakfasts.keys() just gives you the keys of the dict, as the name implies: the keys are the things to the left of the colon, eg ""a bowl of Rice Crispies"". The actual ingredients are the values,  but these never get sent to the Meals instance; so when you iterate through the ""ingredients"" you're actually iterating through the letters of the key.
You could fix this by using .values() instead of .keys() there, although a nicer way might be to pass both key and value so that your __str__ method outputs the description, not the ingredients; I'll leave that as an exercise...",A,1
50688342,2018-06-04 20:32:45.127000+00:00,"The gnome-terminal command is not actually the terminal program; it's a launcher that talks to a factory and asks it to reuse an existing terminal program to open a new window or tab, or to create a new terminal program if necessary.
So, killing this launcher doesn't make any sense. And, even if it would tell you the PID of the actual terminal process, you wouldn't want to kill that, because it could kill a bunch of other terminal sessions.
You can use the --disable-factory flag to avoid this behavior:

Do not register with the activation name server, do not re-use an active terminal.

For more information on the details of gnome-terminal, you probably want to search or ask somewhere else (probably Super User, Unix & Linux, or Ask Ubuntu), but this should be all you need for your problem.",A,-1
21428661,2014-01-29 10:45:23.783000+00:00,"A list comprehension will always produce a list object, in this case with the return values of all the t.join() calls. Python thus produces as list with None values of length len(threads) for you. Python will never try to optimize away the list object creation.
Using map() is also not any more efficient as you add additional stack pushes with the lambda. Just stick with the explicit for loop.
Really, for a series of thread joins there is no point in trying to micro optimize here. 
You are hurting readability for a non-critical piece of code.
In other words, I entirely agree with the commenter. Do not use a list comprehension or map() just for the side effects and saving yourself having to hit ENTER and create two lines of code.
Quoting the Zen of Python:


Readability counts.

",A,7
8507769,2011-12-14 16:14:18.947000+00:00,"a = raw_input(""\n\nInsert A: "")

try: f = float(a)
except ValueError:
     print ""%r is not a number"" % (a,)
else:
     print ""%r is a number"" % (a,)
",A,0
17818546,2013-07-23 18:47:11.503000+00:00,"You have given your url pattern a name, so you should use that name in the {% url %} call:
{% url 'location_detail' pk=location.id %}
",A,3
52779986,2018-10-12 12:55:40.550000+00:00,"
Is there any specific reason why singal.SIGALRM is not working on windows?

Yes, Windows OS doesn't implement that signal. The example you found starts with:

Here is a minimal example program. It uses the alarm() function to limit the time spent waiting to open a file; [...]

and the signal.alarm() function is documented as:

Availability: Unix.

Next, the SIG* section elsewhere on the module documentation page states:

Note that not all systems define the same set of signal names; only those names defined by the system are defined by this module.

So SIGALRM is not available on Windows so you get an attribute error instead.
Note that Windows also does not have a /dev virtual filesystem, so the os.open('/dev/ttyS0', os.O_RDWR) call would fail too.
See python: windows equivalent of SIGALRM for an alternative using threads.",A,3
44874652,2017-07-02 19:51:33.160000+00:00,"The first positional argument to a model instance is the pk, which is an int. Usually you use keyword arguments to specify the fields instead: RequestInformation(list_of_requests=request).
However, this still won't do anything useful, as it will just save a string representation of the request object. If you want to get anything useful out of that object, you would need to access its attributes individually and allocate then to separate fields in the model.",A,1
20624315,2013-12-17 01:28:17.480000+00:00,"
But how would you explain that I dont get this error while running the script locally, but I get the error when running it remotely from a Hudson job? 

When you are running it in a terminal (locally); yes is killed by SIGPIPE signal that is generated when it tries to write to the pipe when MyScript.sh has already exited.
Whatever runs the command (remotely) in Hudson traps that signal (set its handler to SIG_IGN, you can test it by running trap command and searching for SIGPIPE in the output) and it doesn't restore the signal for new child processes (yes and whatever runs MyScript.sh e.g., sh in your case). It leads to the write error (EPIPE) instead of the signal. yes detects the write error and reports it.
You can simply ignore the error message:
yes 2>/dev/null | ./MyScript.sh

You could also report the bug against the component that runs the pipeline. The bug is in not restoring SIGPIPE to the default handler after the child is forked. It is what programs expect when they are run in a terminal on POSIX systems. Though I don't know whether there is a standard way to do it for a java-based program. jvm probably raises an exception for every write error so not-dying on SIGPIPE is not a problem for a java program.
It is common for daemons such as hudson process to ignore SIGPIPE signal. You don't want your daemon to die only because the process you are communicating with dies and you would check for write errors anyway.
Ordinary programs that are written to be run in a terminal do not check status of every printf() for errors but you want them to die if programs down the pipeline die e.g., if you run source | sink pipeline; usually you want source process to exit as soon as possible if sink exits. 
EPIPE write error is returned if SIGPIPE signal is disabled (as it looks like in hudson's case) or if a program does not die on receiving it (yes program does not defined any handlers for SIGPIPE so it should die on receiving the signal).

I don't want to ignore the error, I want to do the right command or fix to get rid of the error.

the only way yes process stops if it is killed or encountered a write error. If SIGPIPE signal is set to be ignored (by the parent) and no other signal kills the process then yes receives write error on ./MyScript.sh exit. There are no other options if you use yes program.
SIGPIPE signal and EPIPE error communicate the exact same information -- pipe is broken. If SIGPIPE were enabled for yes process then you wouldn't see the error. And only because you see it; nothing new happens. It just means that ./MyScript.sh exited (successfully or unsuccessfully -- doesn't matter).",A,25
31323534,2015-07-09 16:41:42.430000+00:00,"Use dictionary views here; they let you treat dictionary keys as sets:
def merge_dicts(d1, d2):
    try:
        # Python 2
        intersection = d1.viewkeys() & d2
    except AttributeError:
        intersection = d1.keys() & d2

    if any(d1[shared] != d2[shared] for shared in intersection):
        return {}  # empty result if there are conflicts

    # leave the rest to C code, execute a fast merge using dict()
    return dict(d1, **d2)

The above code only tests for shared keys referencing non-matching values; the merge itself is best just left to the dict() function.
I made the function work both on Python 2 and Python 3; if you only need to support one or the other, remove the try..except and replace intersection with the relevant expression. In Python 3 the dict.keys() method returns a dictionary view by default.
You could conceivably make this a one-liner; Python 3 version:
def merge_dicts(d1, d2):
    return {} if any(d1[k] != d2[k] for k in d1.keys() & d2) else dict(d1, **d2)

Demo:
>>> d1 = {'x': 'a', 'y': 'b', 'z': 'c'}
>>> d2 = {'z': 'c', 'w': 'r'}
>>> d3 = {'z': 'd', 'w': 'r'}
>>> merge_dicts(d1, d2)
{'y': 'b', 'x': 'a', 'z': 'c', 'w': 'r'}
>>> merge_dicts(d1, d3)
{}
>>> merge_dicts(d2, d3)
{}
",A,5
38866500,2016-08-10 07:10:39.927000+00:00,"You seem to be going to just /profile, but you don't have that defined as a URL, you have /myapp/profile.",A,0
50389839,2018-05-17 11:02:10.163000+00:00,"You can convert a bytearray directly to an integer with the int.from_bytes() method, and back to a bytes object with int.to_bytes():
incremented = int.from_bytes(payload, 'big') + 1
try:
    payload = bytearray(incremented.to_bytes(len(payload), 'big'))
except OverflowError:
    # value won't fit into the payload, wrap round to 0
    payload = bytearray(len(payload))

I'm not sure what should happen when you hit the upper limit; you could revert to 0 (bytearray(len(payload))), sticking to the old (2 ** 128) - 1 value, or doing something else.",A,4
26264429,2014-10-08 18:59:33.050000+00:00,"List comprehensions support if but not else because the if section filters elements, you either include an element or you don't include it, a boolean choice. 
If you wanted to use a conditional expression to build the iterable part of the for loop, use parentheses:
return [tower for tower in (state if tower != space else [])]  

but I suspect that you wanted to alter the value of the expression in the element expression instead; that's not filtering, you are simply producing a different value for certain items. Use a conditional expression to produce your values:
return [tower if tower != space else [] for tower in state]  

or if you really wanted to filter, simply omit the else:
return [tower for tower in state if tower != space]  

When constructing a list comprehension, remember that you need to read the expression as nested from left to right, with the final expression producing the result out on the left:
[element_producing_expression for name in iterable if filter_expression]

is the moral equivalent of:
for name in iterable:
    if filter_expression:
        element_producing_expression

where you can use as many nested loops and if filters as your use case requires.
The three options I described above are then the same as:
# conditional expression producing the iterable
for tower in (state if tower != space else []):
    tower 

# conditional expression in the element expression
for tower in state:
    tower if tower != space else [] 

# filtering expression with no else
for tower in state:
    if tower != space:
        tower
",A,5
52899635,2018-10-19 20:41:20.100000+00:00,"Your HTML is not valid, for a couple of reasons.
First, you put the script block outside the closing </html> tag. That means it's outside the document itself, and may not be read by the browser.
More importantly, you haven't got your code inside a proper script element. You have an opening tag, but you use that to reference the external jQuery library via the src attribute. You don't have a closing tag at all
You need to put the jQuery reference in its own element, and use proper opening and closing tags for your own script.
<html>
<body>
<table>
...
</table>

<script src=""http://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js""></script>
<script type=""text/javascript"">
var append_increment = 0;
setInterval(function() {
    $.ajax({
        type: ""GET"",
        url: ""{% url 'get_more_tables' %}"",
        data: {'append_increment': append_increment}
    })
    .done(function(response) {
        $('#_appendHere').append(response);
        append_increment += 10;
    });
}, 1000)
</script>
</body>
</html>
",A,1
54369257,2019-01-25 16:29:09.457000+00:00,"A deprecation warning is just a warning, and loading succeeds. The pickle file is still being loaded and supported, at least in this version of sklearn (which bundles the 3rd party joblib project). A future version of joblib may stop supporting that specific format, but that hasn't happened yet.
You can re-create the pickle file with the current version, simply by dumping the same object back to disk:
km = joblib.load('doc_cluster.pkl')
joblib.dump(km, 'doc_cluster.pkl', compress=True)

Also see the joblib persistence documentation.
Alternatively, you could suppress the warning, by using a warning filter. You can set filters in the PYTHONWARNINGS environment variable, with the -W command-lne switch (I'd use the string ignore::DeprecationWarning:sklearn.externals.joblib), or by using the warnings module directly:
import warnings
warnings.filterwarnings(
    ""ignore"", category=DeprecationWarning,
    module=r'sklearn\.externals\.joblib'
)
",A,2
40991558,2016-12-06 09:09:56.350000+00:00,"You are painting yourself in a corner by dynamically producing numbered keys. You have a sequence, not a series of distinct pieces of data; extracting those keys again later on is only going to be painful.
Instead of numbered dictionary keys, use a list. You can combine the date and query data together into one tuple per entry:
results = []
for n in range(number_of_entries):
    date = (datetime.datetime.now() + datetime.timedelta(days=0)).date()
    rows = db.execute (""SELECT * FROM events WHERE date LIKE :date ORDER BY date"", date = str(date) + '%')
    results.append((date, rows))

then loop over the resulting list in your Jinja template:
{% for date, rows in results %}
  <tbody><tr><th colspan=""3"" class=""event-date-header"">{{ date }}</th></tr>
  {% for row in rows %}
      <tr>
          <td class=""event-foo""><a href=""{{ row[0] }}"">{{ row[1].title }}</a></td>
          <td class=""event-bar""><{{ row[2] }}</td>
          <td class=""event-baz""><{{ row[3] }}</td>
  {% endfor %}
  <tbody>      
{% endfor %}

Instead of a tuple, a dictionary per entry would also be fine, but then you can't so easily use tuple assignment as shown in the outer Jinja loop above.",A,1
43609709,2017-04-25 11:42:05.923000+00:00,"You have three options:

store method names, then use getattr() to turn the name back into a reference to the method:
settings = {
    'MAKE': 'make',
    'MOD': 'mod',
    'REM': 'rem'
}
if setting in settings:
    rettt = getattr(User, settings[setting])()

I note that at this point this becomes a simple lower-casing exercise, perhaps with a set test to ensure you can't call arbitrary methods:
settings = {'MAKE' 'MOD', 'REM'}
if setting in settings:
    rettt = getattr(User, setting.lower())()

Store the bound methods in the dictionary each time:
settings = {
    'MAKE': User.make,
    'MOD': User.mod,
    'REM': User.rem,
}
if setting in settings:
    rettt = settings[setting]()

Use operator.methodcaller() objects; these take a string method name and can be called with an object to call the method on:
from operator import methodcaller

settings = {
    'MAKE': methodcaller('make'),
    'MOD': methodcaller('mod'),
    'REM': methodcaller('rem')
}
if setting in settings:
    rettt = settings[setting](User)

",A,0
12030877,2012-08-19 23:45:41.770000+00:00,"Here's how you could use the punkt module mentioned by @BigHandsome to truncate the paragraph:
from nltk.tokenize.punkt import PunktSentenceTokenizer

def truncate_paragraph(text, maxnchars,
                       tokenize=PunktSentenceTokenizer().span_tokenize):
    """"""Truncate the text to at most maxnchars number of characters.

    The result contains only full sentences unless maxnchars is less
    than the first sentence length.
    """"""
    sentence_boundaries = tokenize(text)
    last = None
    for start_unused, end in sentence_boundaries:
        if end > maxnchars:
            break
        last = end
    return text[:last] if last is not None else text[:maxnchars]

Example
text = (""This is the first sentence. This is the second sentence? ""
        ""This is the third\n sentence!"")
for limit in [100, 80, 50, 5]:
    print(truncate_paragraph(text, limit))

Output

This is the first sentence. This is the second sentence? This is the third
 sentence!
This is the first sentence. This is the second sentence?
This is the first sentence.
This 
",A,2
38524389,2016-07-22 10:40:44.243000+00:00,"You probably installed that specific Python version using the official Python installer for OS X; see the Using Python on a Macintosh documentation. The installer creates the /usr/local/bin symlink for you.
If you also, at some point, had 3.4 installed then that installation is still there too. Check for a /usr/local/bin/python3 command; it'll link to the existing Python 3 binary. Use that instead to run Python 3 code.
If you do have a /Library/Frameworks/Python.framework/Versions/3.4/bin/python3.4 command, you could re-create the /usr/local/bin/python symlink to point there instead, but I'd personally only use the python3 name for Python 3 scripts.
Last, you could also have used the homebrew tool to install Python; it can manage symlinks for you. However, homebrew installs Python binaries into the /usr/local/Cellar tree structure instead.",A,1
2554691,2010-03-31 16:33:05.963000+00:00,Custom field types are definitely the way to go here. This is the only reliable way to ensure that the field is compressed on save and decompressed on load. Make sure you set the metaclass as described in your link.,A,2
12012813,2012-08-17 20:34:03.243000+00:00,"To write all or nothing to a file reliably:
import os
from contextlib import contextmanager
from tempfile   import NamedTemporaryFile

if not hasattr(os, 'replace'):
    os.replace = os.rename #NOTE: it won't work for existing files on Windows

@contextmanager
def FaultTolerantFile(name):
    dirpath, filename = os.path.split(name)
    # use the same dir for os.rename() to work
    with NamedTemporaryFile(dir=dirpath, prefix=filename, suffix='.tmp') as f:
        yield f
        f.flush()   # libc -> OS
        os.fsync(f) # OS -> disc (note: on OSX it is not enough)
        f.delete = False # don't delete tmp file if `replace()` fails
        f.close()
        os.replace(f.name, name)

See also Is rename() without fsync() safe? (mentioned by @Mihai Stan)
Usage
with FaultTolerantFile('very_important_file') as file:
    file.write('either all ')
    file.write('or nothing is written')

To implement missing os.replace() you could call MoveFileExW(src, dst, MOVEFILE_REPLACE_EXISTING) (via win32file or ctypes modules) on Windows.
In case of multiple threads you could call queue.put(data) from
different threads and write to file in a dedicated thread:
 for data in iter(queue.get, None):
     file.write(data)

queue.put(None) breaks the loop.
As an alternative you could use locks (threading, multiprocessing,
filelock) to synchronize access:
def write(self, data):
    with self.lock:
        self.file.write(data)
",A,5
26015544,2014-09-24 11:16:15.527000+00:00,"You only have references to objects imported from the module, you don't have the module object itself.
Use the inspect.getmodule() function to get the module object again:
import inspect

mod = inspect.getmodule(reviseOnX)
print os.path.abspath(mod.__file__)

Note that I am using os.path.abspath(), not os.path(). The latter would try to call the module.",A,3
34105337,2015-12-05 13:17:01.047000+00:00,I'm not sure what gave you the idea to import _pymssql; the leading underscore should have given you the clue that this is a private utility module. Just import pymssql.,A,0
16972867,2013-06-06 21:56:05.050000+00:00,"There is at least one row that doesn't have a 4th element. Perhaps the line is empty.
Test for the length, and print the row to test:
if len(artifact) < 4:
    print 'short row', artifact

If it is an empty line, just skip it:
if not artifact: continue

You are using a lot of verbose and redundant code; there is no need to build a separate list when you can just loop over the csv.reader() object directly, and there is no need to add an empty else: pass block either.
Idiomatic Python code would be:
artifact_template = """"""<td>
    <div>
    <img src= ""%(image)s"" alt = ""artifact"" />
    <p>Dimensions: %(dimension)s </p>
    <p>Accession #: %(accession)s </p>
    <p>Display: %(display)s </p>
    <p>index2: %(index2)s </p>
    </div>
    </td>""""""

html_list = []

fields = 'image dimension accession display'.split()

with open(""v2img_10724_list.csv"", 'rb') as inputfile:
    reader = csv.DictReader(inputfile, fields=fields, restval='_ignored')
    for count, artifact in enumerate(reader, 5794):
         if artifact and artifact['display'] == ""This artwork is not on display"":
              artifactinfo[""index2""] = count    
              html_list.append(artifact_template % artifact)

This use a csv.DictReader() instead to create the dictionaries per row, a with statement to ensure the file is closed when done, and enumerate() with a start value to track count.",A,3
52718431,2018-10-09 10:07:50.833000+00:00,"For your second question, the point is that this code is intended to check that every character in the string is unique. Assuming that the string is ASCII, if it is longer than 128 characters it cannot possibly contain only unique characters since there are only 128 characters in the ASCII charset to begin with.",A,1
421359,2009-01-07 18:08:50.117000+00:00,"Sounds like the All-singing-all-dancing XML fetish.
Since you can do anything with XSLT, might as well do everything.  I've had people ask why a data warehouse isn't just XSLT transforms between input, data mart and reports.
Advantage.  Everything's in XML.
Disadvantages.  

Not very readable.  Your page templates are bound up as XSLT transformations with confusing looping and conditional processing features.  
Any change to page templates requires an XSLT expert, in addition to the graphic designer who created (and debugged) the HTML and CSS.
",A,0
31852395,2015-08-06 09:46:46.130000+00:00,You need to define a result backend. See the tutorial.,A,0
28094011,2015-01-22 16:37:12.383000+00:00,"You just need to sort the items by the length of the value.
items = sorted(d.items(), key=lambda i: len(i[1]), reverse=True)
",A,4
19944236,2013-11-13 02:42:20.930000+00:00,"If you have the prototypes of the functions you're trying to call, then we can help you debug the calls without guessing. Or, more importantly, we won't have to help you debug the calls, because you can let ctypes do it for you.
See Specifying the required argument types in the docs.
For example, let's say the function looks like this (just a random guess!):
void AU3_WinClose(LPCWSTR name, LPCWSTR someotherthing);

You can do this:
autoit.AU3_WinClose.argtypes = (LPCWSTR, LPCWSTR)
autoit.AU3_WinClose.restype = None

If you do this, ctypes will try to convert your arguments to the specified types (LPWSTR, which is a pointer to wide char used for Windows UTF-16 strings) if it can, or raise an exception if it can't, and will not expect any return value.
If you don't do this, ctypes will try to guess the right things to convert your arguments to, possibly guessing wrong, and will try to interpret the non-existent return value as an int. So, it will usually crash until you managed to guess exactly what types to throw at it to make it guess the right types to pass to the function.",A,4
15903090,2013-04-09 13:20:01.017000+00:00,"You do not need to pass in self when calling the inherited __init__() method with super():
def __init__(self):
    super(lala, self).__init__()    

Looking at the example listed in the documentation, the attribute should be called rules, not rule:
class lala(CrawlSpider):
    name=""lala""
    start_url=[""http://www.lala.net/""]       
    rules = [
        Rule(SgmlLinkExtractor(), follow=True, callback='self.parse')
    ] 
",A,2
36913729,2016-04-28 11:47:14.090000+00:00,"You are inserting week numbers into a regular dictionary, not an ordered dictionary:
d_new_names[s_number] = {}

Here d_new_names in an ordered dictionary, so the s_number values are kept in insertion order, but the keys refer to normal dictionaries.
Then later you use those normal dictionaries to store per-week information:
d_new_names[s_number][s_week_number] = {}

Your s_week_number week numbers are thus not kept in insertion order, but follow the arbitrary ordering of a normal dictionary instead.",A,3
26170083,2014-10-02 21:41:08.127000+00:00,"It's just a name issue. In the URL for profile you are capturing a ""user"" variable. But the view itself is expecting an ""id"" argument. Make these consistent.",A,0
23422171,2014-05-02 06:34:12.020000+00:00,"You can specify a retries parameter to the decorator: 
@db.transactional(retries=5)
def my_txn...
",A,4
41208051,2016-12-18 11:49:59.147000+00:00,"You'd need to redirect the output. 
Either to /dev/null:
sh.service('nginx','status', _out='/dev/null')

or to a StringIO object for you to check the output:
from io import StringIO

buffer = StringIO()
sh.service('nginx','status', _out=buffer)
output = buffer.getvalue()

Use _err to redirect stderr output.",A,3
26863926,2014-11-11 11:34:45.590000+00:00,"Wheels used to include pre-generated console script wrappers in the package, but this was sub-optimal and the files were removed. The installer is supposed to generate these wrapper scripts instead, but pip has not yet been updated to follow suit, see issue 1067.
Until pull request 1251 is part of a release, you'll have to use a separate command to install console scripts:
python -m wheel install-scripts thing.whl

See Setuptools scripts handling in the Wheel documentation.
The pull request was merged earlier this month (November 2014), and will be part of the upcoming 6.0 release. You could also use pip to upgrade itself to the development version from GitHub with:
pip install git+https://github.com/pypa/pip.git
",A,6
42502903,2017-02-28 07:44:07.573000+00:00,"You don't have any fields called ""unames"" in the template. You have a bunch of unrelated checkboxes called ""targetX"" where X is a number. Replace that with just name=""unames"" and your code will work.
Note two things: firstly, you should be using Django's forms framework; and secondly, your filter is pointless because {{ name }} would already by the value in unames, since that is what you are iterating over (and, indeed, you are already using that as the label).
{% for name in unames %}
    <input type=""checkbox"" name=""unames"" value=""{{ name }}"" checked>{{ name }}<br>
{% endfor %}
",A,1
21090799,2014-01-13 12:01:16.503000+00:00,"Importing a name in a function is also an assignment; essentially your sample could be further reduced to this:
def function(arg1, arg2):
    if False:
        import signal

    os.kill(pid, signal.SIGKILL)

This makes signal a local variable in the function, and Python won't look for the global name.
The import signal line makes signal a local name, but because the line is never executed, signal is never bound and the exception is thrown.
Remove all import signal lines from the function, or move it out of the conditional to be imported unconditionally (and thus always bind signal).",A,3
24637283,2014-07-08 16:42:22.110000+00:00,"You have rebound str to a list object. Don't do that, you are masking the built-in type:
>>> str(42)
'42'
>>> str = ['foo', 'bar']
>>> str(42)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'list' object is not callable

Note that the correct way to test for the type of an object is to use isinstance():
isinstance(item, str)

although in a debugging session, you can also use type() to introspect the object, or use repr() to get a helpful Python literal represention (if available, otherwise a representation suitable for debugging is given):
>>> str = ['foo', 'bar']
>>> type(str)
<type 'list'>
>>> print repr(str)
['foo', 'bar']
>>> del str
>>> type(str)
<type 'type'>
>>> print repr(str)
<type 'str'>
",A,5
18263115,2013-08-15 22:44:44.083000+00:00,"If you already knew all the b values, this would be trivial. You want a list for each b value, where each one has all of the a values in each dictionary with that b value. You can translate that almost directly from English to a comprehension:
[[d['a'] for d in lst if d['b']==b] for b in bs]

You don't have the b values, but you can get them just by doing another pass:
bs = (d['b'] for d in lst)

Except that you obviously only want unique values, and you want to iterate them in sorted order, so:
bs = sorted(set(d['b'] for d in lst))

And that's all there is to it. Putting it together:
>>> lst=[{'a':54,'b':1},{'a':25,'b':0},{'a':53,'b':1},{'a':532,'b':2},{'a':132,'b':0}]
>>> bs = sorted(set(d['b'] for d in lst))
>>> [[d['a'] for d in lst if d['b']==b] for b in bs]
[[25, 132], [54, 53], [532]]
",A,1
9266050,2012-02-13 18:43:39.523000+00:00,"The 2nd and 4th requirements contradict each other. Either you accept only 5 characters strings or you allow also #:## (4 characters form).
import re

def minutes(timestr):
    """"""Return number of minutes in timestr that must be either ##:## or #:##.""""""
    m = re.match(r""(\d?\d):(\d\d)$"", timestr)
    if m is None:
       raise ValueError(""Invalid timestr: %r"" % (timestr,))
    h, m = map(int, m.groups())
    return 60*h + m

If you allow spaces inside the timestr and ##:#, #:#, etc forms then:
def minutes2(timestr):
    h, m = map(int, timestr.partition(':')[::2])
    return 60*h + m

If you'd like to restrict hours to 0..23 and minutes to 0..59 then:
import time

def minutes3(timestr):
    t = time.strptime(timestr, ""%H:%M"")
    return 60*t.tm_hour + t.tm_min

Example
minutes ('12:11') -> 731
minutes2('12:11') -> 731
minutes3('12:11') -> 731
minutes ('  12:11') -> error: Invalid timestr: '  12:11'
minutes2('  12:11') -> 731
minutes3('  12:11') -> error: time data '  12:11' does not match format '%H:%M'
minutes ('12:11  ') -> error: Invalid timestr: '12:11  '
minutes2('12:11  ') -> 731
minutes3('12:11  ') -> error: unconverted data remains:   
minutes ('3:45') -> 225
minutes2('3:45') -> 225
minutes3('3:45') -> 225
minutes ('03:45') -> 225
minutes2('03:45') -> 225
minutes3('03:45') -> 225
minutes ('13:4') -> error: Invalid timestr: '13:4'
minutes2('13:4') -> 784
minutes3('13:4') -> 784
minutes ('13:04') -> 784
minutes2('13:04') -> 784
minutes3('13:04') -> 784
minutes ('24:00') -> 1440
minutes2('24:00') -> 1440
minutes3('24:00') -> error: time data '24:00' does not match format '%H:%M'
minutes ('11:60') -> 720
minutes2('11:60') -> 720
minutes3('11:60') -> error: unconverted data remains: 0
",A,1
51688270,2018-08-04 17:59:29.607000+00:00,"You can't just assign to an open file object. An open file object, even for JSON data, can't act as a mapping, so task['taskid'] = ... fails.
If the file contains a JSON document you need to:

Parse the JSON data into a Python object
Update the Python object to add the new information.
Encode the Python object to JSON and write it back to the file.

You may have to work out what should happen when you update your list of tasks too. What should happen when a task id already exists? You need to think about the data structure; perhaps a list is not as suitable a structure, and a dictionary of {taskid: taskdata} is a better way of updating existing tasks.
Note that you can't do this and support multiple concurrent requests trying to do the same, and keep your data consistent! For example, if you reloaded your webpage while the server is still trying to encode and write to disk, then the other request would see partial data and throw an exception.
You'll need to add file locking to work around that, or write to a separate temporary location that you then move over to replace the old file; the move operation is atomic, in that it'll happen all at once and any other concurrent attempts to read the file will either get the old or the new version, but always as a complete file.
Worrying about concurrency may not be required for a simple homework task, but for real-world web applications this is absolutely something you need to handle. Any website that serves more than just one visitor must take into account what might happen when two or more visitors come and try to do things on your site at the same time.
So, for a working solution that won't immediately break when multiple people try to add tasks, you could use:
import os.path
import shutil
import tempfile

@app.route('/savetask', methods=['POST'])
def savetask():
    tasks_path = os.path.join(app.root_path, 'tasks/tasks.json')
    # load the JSON list
    with open(tasks_path) as tasksfile:
        tasks = json.load(tasksfile)

    # add a new task
    tasks.append({
        'taskid': request.form['taskid'],
        'profilename': request.form['profile'],
    })

    # write out the tasks to a new file, then move
    # it across to replace the old.
    with tempfile.NamedTemporaryFile(dir=os.path.dirname(tasks_path), delete=False) as temporary_file:
        json.dump(tasks, temporary_file)
    shutil.move(temporary_file.name, tasks_path)

    return redirect('/')

Now all you have to worry about is what will happen when two requests at the same time try to add a task. Do you think both will be added to your JSON file correctly, or could data go missing?",A,0
11931729,2012-08-13 09:44:32.027000+00:00,"You are relying on the data from the serial line to contain newlines, you never write your own. It may be that the serial data contains carriage returns (often interpreted in the same way as newlines) as well, you'd need to clean those out.
Remove the carriage returs from the serial data (optional), and add an explicit newline when you write:
linefromserial = linefromserial.rstrip('\n').replace('\00', '').replace('\r', '')
datenow = datetime.datetime.strftime(datetime.datetime.now(), ""%Y-%m-%d %H:%M:%S"")
openfile.write('%s   %s\n' % (datenow, linefromserial))

Now you always write a newline at the end of each received line (even if the readline() call didn't return one), and you are certain that no \r carriage return caused a line break elsewhere.",A,1
34631663,2016-01-06 11:09:11.943000+00:00,"The error message does not match the code you have pasted. It shows you are using names(0), not names[0].",A,1
35870545,2016-03-08 14:50:43.523000+00:00,"Split out your input list into separate groups instead and produce their product. You could use itertools.groupby() if your input is sorted by the second parameter:
from itertools import groupby, product
from operator import itemgetter

source = [['a', 1], ['b', 1], ['d', 2], ['e', 2], ['f', 3]]
grouped = (list(group) for key, group in groupby(source, key=itemgetter(1)))
for combo in product(*grouped):
    print(list(combo))

If you input is not sorted by the second parameter, you'd group them by using a dictionary:
source = [['a', 1], ['b', 1], ['d', 2], ['e', 2], ['f', 3]]
groups = {}
for item in source:
    groups.setdefault(item[1], []).append(item)
grouped = [group for key, group in sorted(groups.items())]

where I assume you wanted to sort on that same second value to inform the final output order.",A,0
51353401,2018-07-16 01:00:05.413000+00:00,"python-vlc is just Python bindings for libVLC. 
Without that library, it won't do you any good, because all it does is try to load that library (a .dylib, .so, or .dll, depending on your platform) and call functions out of it.
There should be installation instructions at the wiki page linked above, but on a Mac, the easiest way is to just install the VLC player. I know that if you install it with Homebrew, you get the library, in a location that python-vlc can find. But I think even the binary installer from the front page of the main VideoLAN website will work as well.
If you're using Homebrew, you'll want to read the docs for when to search brew vs. brew cask vs. other taps,1 or search somewhere like Mac App Store for the current status. But at present, the appropriate command appears to be:
brew cask install vlc


1. Generally, anything that you'd expect to find as a double-clickable app in /Applications, as opposed to a Unix command-line tool or a support library, is going to be a cask, and therefore in the tap cask, which has special shortcuts to access it. But that's a relatively new thing, and not every recipe has been converted yet.",A,0
45907451,2017-08-27 17:18:50.787000+00:00,"I've never done this, but I can't see why not. You can use the Django REST Framework to create an API that your desktop application can talk to, in exactly the same way as you might with a JavaScript single page app.
But no, you should not use the dev server for production, even in a limited scenario like this. Apache/mod_wsgi or nginx/gunicorn are simple to set up and deploy.
For the database, it makes no difference. The Django core devs prefer postgres, but you should use whatever you are comfortable with.",A,1
45946481,2017-08-29 18:57:07.543000+00:00,"Just reference the days attribute of the timedelta object you have there:
print(date1.days)

There are also timedelta.seconds and timedelta.microseconds attributes, modeling the complete delta state. ",A,7
52106763,2018-08-30 23:48:18.810000+00:00,"There's nothing special about the class you created (it's not even a ModuleType subclass), so there's nothing special about its __call__ method. If you want to call it with arguments, just add parameters to the __call__ definition:
import sys

class foo(object):
  def __call__(self, x):
    return f'callable, and called with {x}'

sys.modules[__name__] = foo()

And now, you can pass it an argument, exactly like any other callable object:
import foo

print(foo('hello'))

And the output is:
callable, and called with hello


From the comments, you tried to do this:
def __call__(a, self):
    return a

But, like all methods in Python, __call__ wants self to come first. It doesn't care about the names (unless you call it with keyword arguments), just the order: the first parameter gets the receiver (the foo in foo('hello')), even if it you called that parameter a, and the second parameter gets the first normal argument (the 'hello'), even if you called that parameter self.
So, you're passing the module foo as the first parameter, a, and you return a, so it returns foo.
Which is why you got this:
<sta.foo object at 0x10faee6a0>

That isn't an error, that's the perfectly valid output that you get when you print out an instance of a class that doesn't define __repr__ or __str__.",A,0
18637081,2013-09-05 13:01:00.333000+00:00,"The exception is raised. The finally block is executed when the generator is closed. Closing a generator is done by raising a GeneratorExit exception in the generator context.
The exception in ignored because the generator isn't closed until it is being deleted (automatically in this case, when Python exits); the generator __del__ handler closes the generator, which triggers the finally: block:
>>> def mycoroutine():
...   try:
...     while True:
...       data = (yield)
...       print data
...   finally:
...     raise ValueError
...     print ""END""
... 
>>> co = mycoroutine()
>>> co.next()
>>> co.close()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 7, in mycoroutine
ValueError
>>> co = mycoroutine()
>>> co.next()
>>> del co
Exception ValueError: ValueError() in <generator object mycoroutine at 0x1046a9fa0> ignored

Exceptions raised during cleanup are always ignored; see the object.__del__() documentation:

Warning: Due to the precarious circumstances under which __del__()
  methods are invoked, exceptions that occur during their execution are
  ignored, and a warning is printed to sys.stderr instead.

The solution is to not have exceptions being raised when a generator is cleaned up, or catch the exception by closing the generator explicitly:
>>> co = mycoroutine()
>>> co.next()
>>> try:
...     co.close()
... except ValueError:
...     pass
... 
>>> del co
>>> # No exception was raised
... 

You could also catch the GeneratorExit exception and perform some cleanup at that point:
def mycoroutine():
  try:
    while True:
      data = (yield)
      print data
  except GeneratorExit:
    print ""Generator exiting!""

but note that any exception other than StopIteration or GeneratorExit will always be propagated; see the generator.close() documentation:

If the generator function then raises StopIteration (by exiting normally, or due to already being closed) or GeneratorExit (by not catching the exception), close returns to its caller. If the generator yields a value, a RuntimeError is raised. If the generator raises any other exception, it is propagated to the caller.
",A,10
35878495,2016-03-08 21:36:00.137000+00:00,"self.x is a property, so you'd just assign directly to it like you would with a regular attribute:
def __init__(self, a, b):
    self.x = (a, b)

However, the setter is given one object, always; in the above case, it is passed a tuple; you could unpack it:
@x.setter
def x(self, value):
    a, b = value
    self._x = ""Get this from {} and make a dataframe like {}"".format(a,b)

Note the value argument; that's the result of the assignment being passed to the setter.
Demo:
>>> class test(object):
...     def __init__(self, a, b):
...         self.x = (a, b)
...     @property
...     def x(self):
...         return self._x
...     @x.setter
...     def x(self, value):
...         a, b = value
...         self._x = ""Get this from {} and make a dataframe like {}"".format(a,b)
...
>>> t = test(42, 'foo')
>>> t.x
'Get this from 42 and make a dataframe like foo'
",A,7
8367112,2011-12-03 10:41:15.740000+00:00,"In Plone 4, the cookie function is handled by plone.session.
It uses a tk-auth compatible cookie, meaning you can generate these cookies if you have the shared secret. The package page contains a lot of information on how to use this for a SSO setup.",A,1
15953377,2013-04-11 15:52:42.740000+00:00,"The reverse('adingcomment', kwargs={'post_id' : 1, 'content':'hjhgh'}) failed to find a URL route.
Perhaps you registered a addingcomment route instead? You need to check the name of the route and the provided arguments to match an existing route you registered. For more details see the Reverse resolution of URLs documentation.",A,0
31377574,2015-07-13 07:25:16.510000+00:00,"You're patching the wrong thing: all you've done is change what authenticate refers to within your test, not in the view. You should patch your_view.auth.authenticate.
See the Mock docs on Where to patch.",A,1
1588620,2009-10-19 13:16:27.470000+00:00,"""something will come up and I'll wish I had implemented it earlier""
That's the definition of a good site.  One that evolves and changes.
""future-ready as possible ?""
What can this possibly mean?  What specific things are you worried about?  Technology is always changing.  A good site is always evolving.  What do you want to prevent?  Do you want to prevent technical change?  Do you want to prevent your site from evolving?
There will always be change.  It will always be devastating to previous technology choices you made.
You cannot prevent, stop or even reduce the impact of change except by refusing to participate in new technology.",A,-2
1108750,2009-07-10 10:09:11.910000+00:00,"Few things help as much as testing.
These two quotes are really important.

""how many unit tests you can afford to write.""
""From time to time embarrassing mistakes still occur,""

If mistakes occur, you haven't written enough tests.  If you're still having mistakes, then you can afford to write more unit tests.  It's that simple.  
Each embarrassing mistake is a direct result of not writing enough unit tests. 
Each management report that describes an embarrassing mistake should also describe what testing is required to prevent that mistake from ever happening again.  
A unit test is a permanent prevention of further problems.",A,0
28089817,2015-01-22 13:20:37.530000+00:00,"Values in request.POST are always strings, so your condition will never be true.",A,3
5394248,2011-03-22 16:08:37.567000+00:00,"Don't instantiate the form in the TabularInline subclass:
    form = CommentForm
",A,1
40192433,2016-10-22 12:56:02.543000+00:00,"Your search starts with index set to 0, then finds the href text at position 22. You then increment the index to 2, search again, and again find the text at position 22.
If you want to search to continue past the last match, you need to set index to a position after the last match instead:
index = a + 1

Now the next text.find() call starts searching at index 23 instead.
You'll also need to test if the text is not found:
if a < 0 or b < 0:
    break

Rather than manually search through text like this, consider using a HTML parser. Your search would be trivial with BeautifulSoup for example.",A,1
20045216,2013-11-18 10:14:55.180000+00:00,"An iterator is an object that provides values one by one, and does this just once. You can loop over an iterator, or ask for the next value with the next() function, but you cannot index into an iterator (ask for elements in specific locations in the sequence), for example.
You can collect all those values into a list simply by calling list() on the iterator:
list(all_neighbors(graph, node))
",A,1
34269245,2015-12-14 14:19:18.110000+00:00,"The *-prefixed argument is always a tuple. It captures 0 or more extra positional arguments. You can call your test_stats() function with 3 or 20 or 100 extra arguments (beyond the explicit team argument), and they all would be part of the args tuple in the function:
>>> def foo(*args): return args
...
>>> foo()
()
>>> foo(42)
(42,)
>>> foo(1, 2, 3)
(1, 2, 3)

If you wanted one optional argument, make it a keyword argument with a sentinel default, like None:
def test_stats(team, optional=None):
    if optional is None:
        # ...
    else:
        team_fixtures = (
            Fixtures.objects.filter(home_team=team_details.id) |
            Fixtures.objects.filter(away_team=team_details.id))
                .filter(fixture_datetime__lt=datetime.now())
                .filter(fixture_datetime__year=optional)
        )
",A,4
23392800,2014-04-30 15:57:54.527000+00:00,"You declared the column to be a timestamp:
conn.execute('''CREATE TABLE TEST
            (
             ID     TEXT PRIMARY KEY     NOT NULL,
             DATE    TIMESTAMP
             );''')

Make it type DATE instead; Python picks a converter based on the column type and TIMESTAMP triggers the wrong converter here.
Demo:
>>> import sqlite3
>>> import datetime
>>> conn = sqlite3.connect("":memory:"", detect_types=sqlite3.PARSE_DECLTYPES)
>>> conn.execute('''CREATE TABLE TEST (ID TEXT PRIMARY KEY NOT NULL, DATE DATE)''')
<sqlite3.Cursor object at 0x104a5adc0>
>>> conn.commit()
>>> conn.execute(""INSERT INTO TEST (ID,DATE) VALUES (?, ?)"",
...              ('foo', datetime.date(2014,4,28)))
<sqlite3.Cursor object at 0x104a5ae30>
>>> conn.commit()
>>> cursor = conn.execute(""SELECT ID,DATE from TEST"")
>>> for row in cursor:
...     print row
... 
(u'foo', datetime.date(2014, 4, 28))

Alternatively, include the type name in the SELECT query in square brackets:
cur.execute('SELECT ID, DATE [date] from TEST')
",A,1
23598087,2014-05-11 21:23:14.397000+00:00,"They are not essentially the same. If count is a true value (non-zero, or not an empty container, etc.) then the value of count is assigned in the first form. The second form always assigns a boolean.
Unless the statement is part of a tight high-iteration-count loop, performance shouldn't be an issue. Even if it is part of a loop, the difference will be minute.
What remains is a stylistic choice, but your second form is usually extremely redundant.
Whenever I see a if test: value = True, else: value = False statement, it is usually a sign that someone hasn't understood that the test itself is already returning a boolean or can be made into one.
If you need a true boolean value, use:
retval = bool(count) or num_element > 0

otherwise just stick with the first option.",A,10
15209622,2013-03-04 19:34:43.970000+00:00,"Not really in Python. As mgilson shows, you can do this with trivial functions, but they can only contain expressions, not statements, so are very limited (you can't assign to a variable, for example).
This is of course supported in other languages: in Javascript, for example, creating substantial anonymous functions and passing them around is a very idiomatic thing to do.",A,3
26346438,2014-10-13 18:24:10.687000+00:00,"This is nothing to do with Unicode. You have a list of items, you need to iterate through them.
{% for name in processor.naam %}{{ name }}{% endfor %}
",A,1
19334670,2013-10-12 13:23:20.493000+00:00,"You can use the **mapping calling syntax to apply a dictionary as keyword arguments:
Country('Africa', **dictionary)

If the dictionary has a country key, it'll be passed to the __init__ method as a keyword argument. If it does not, then country is set to the default specified in the method signature.
Demo:
>>> class Country(object):
...     def __init__(self, continent='Europe', country='Great Britain'):
...         print 'Continent: {}, Country: {}'.format(continent, country)
... 
>>> dictionary = {'continent': 'Africa', 'country': 'Zimbabwe'}
>>> Country(**dictionary)
Continent: Africa, Country: Zimbabwe
<__main__.Country object at 0x100582550>
>>> Country(**{'country': 'France'})
Continent: Europe, Country: France
<__main__.Country object at 0x100582510>

There is a mirror syntax to this for function signatures; **mapping in the argument list captures keyword arguments not explicitly named:
def __init__(self, continent='Europe', country='Great Britain', **kw):

Any additional keyword arguments beyond continent and country end up in the dictionary kw that way. You can use it to support arbitrary arguments, or to ignore additional keyword arguments passed in without exceptions being thrown.",A,5
37037638,2016-05-04 20:42:56.353000+00:00,"serializer.save() returns a Photo object, at which point you can access its id.",A,3
40226183,2016-10-24 19:37:08.977000+00:00,"Sets test for equality, and until there are new Python releases, the order in which they do this can differ based on the form you hand the values to the set being constructed, as I'll show below.
Since 0 == x is true and 0 == y is true, but x == y is false, the behaviour here is really undefined, as the set assumes that x == y must be true if the first two tests were true too.
If you reverse the list passed to set(), then you get the same output as using a literal, because the order of equality tests changes:
>>> set([y, x, 0])
set([0j, Decimal('0')])

and the same for reversing the literal:
>>> {y, x, 0}
set([0])

What's happening is that the set literal loads the values onto the stack and then the stack values are added to the new set object in reverse order.
As long as 0 is loaded first, the other two objects are then tested against 0 already in the set. The moment one of the other two objects is loaded first, the equality test fails and you get two objects added:
>>> {y, 0, x}
set([Decimal('0'), 0j])
>>> {x, 0, y}
set([0j, Decimal('0')])

That set literals add elements in reverse is a bug present in all versions of Python that support the syntax, all the way until Python 2.7.12 and 3.5.2. It was recently fixed, see issue 26020 (part of 2.7.13, 3.5.3 and 3.6, none of which have been released yet). If you look at 2.7.12, you can see that BUILD_SET in ceval.c reads the stack from the top down:
# oparg is the number of elements to take from the stack to add
for (; --oparg >= 0;) {
    w = POP();
    if (err == 0)
        err = PySet_Add(x, w);
    Py_DECREF(w);
}

while the bytecode adds elements to the stack in reverse order (pushing 0 on the stack first):
>>> from dis import dis
>>> dis(compile('{0, x, y}', '', 'eval'))
  2           0 LOAD_CONST               1 (0)
              3 LOAD_GLOBAL              0 (x)
              6 LOAD_GLOBAL              1 (y)
              9 BUILD_SET                3
             12 RETURN_VALUE

The fix is to read the elements from the stack in reverse order; the Python 2.7.13 version uses PEEK() instead of POP() (and a STACKADJ() to remove the elements from the stack afterwards):
for (i = oparg; i > 0; i--) {
    w = PEEK(i);
    if (err == 0)
        err = PySet_Add(x, w);
    Py_DECREF(w);
}
STACKADJ(-oparg);

The equality testing issue has the same root cause as the other question; the Decimal() class is having some equality issues with complex here, which was fixed in Python 3.2 (by making Decimal() support comparisons to complex and a few other numeric types it didn't support before).",A,55
23953254,2014-05-30 11:14:27.703000+00:00,"You are using BeautifulSoup version 3, but tried to use syntax only supported by BeautifulSoup version 4.
You'd have to pass in the class attribute query as a dictionary instead:
soup.find('li', {'class':'last'})

Not that that returns a pager element; you'd want to look for ul.pager first:
last_page = soup.find('ul', {'class': 'pager'}).find('li', {'class': 'last'}).a

Demo:
>>> from BeautifulSoup import BeautifulSoup
>>> soup = BeautifulSoup(r.content)
>>> soup.find('li', {'class':'last'})
<li class=""last"">Medical practice</li>
>>> soup.find('ul', {'class': 'pager'}).find('li', {'class': 'last'})
<li class=""last""><a href=""/healthpros/1563"">1563</a></li>

You really should upgrade to BeautifulSoup 4 however, version 3 hasn't seen a new release in over 2 years.
Even better, BeautifulSoup 4 support CSS selectors:
last_page = soup.select('ul.pager li.last a')[0]

Demo:
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(r.content)
>>> soup.select('ul.pager li.last a')[0]
<a href=""/healthpros/1563"">1563</a>
",A,0
14968701,2013-02-19 22:23:23.097000+00:00,"You cannot with the string alone go back to the same object, because Python does not give you a method to look up objects by memory address.
You can go back to another instance of __main__.Test, provided it's constructor doesn't take any arguments, and look up the method again, but it will not have the same memory address.
You'd have to parse the string for it's components (module, classname, and method name), then use getattr() on the various components, instantiating the class as part of the process. I doubt this is what you wanted though.",A,4
3570427,2010-08-25 21:37:09.193000+00:00,"In an order which can be ""determined"" in advance.
Because of the way hashing works, the elements in the map are ""scrambled"" into arbitrary locations.  The scrambling positions cannot easily be determined in advance  -- they aren't determinable -- you don't know the resulting order.",A,13
17032119,2013-06-10 20:19:51.853000+00:00,"See the note box in the get_urls docs, which states that URLs there are included under the URL for the admin. So, assuming your app is HRMS and your model is EmployeePaySlip, that URL will be /admin/HRMS/employeepayslip/HRMS/mos/employeepayslip/, which probably isn't what you want.",A,2
12568621,2012-09-24 15:55:33.030000+00:00,"There is the chapter 8.1 Git and Other Systems - Git and Subversion in Pro Git book, written by Scott Chacon.",A,1
1519645,2009-10-05 11:57:54.870000+00:00,"If your team is distributed -- and on speaking terms -- then a simple subversion repository of source is better than some other kind of server.
Simply create projects and have everyone checkout trunk.  When things change, tell them to update.
If your team is co-located -- and on speaking terms -- then a shared drive with the ""official"" libraries also works well.  Simply mount it and include it on your PYTHONPATH.  
If you want localized copies, provide the official source in subversion (or a shared drive) with a good setup.py file.  They simply CD to the directory and run python setup.py install and everything else happens for them.  It's a fraction simpler than easy_install because setup.py is already part of the Python distribution.
Eggs are for people who are not on speaking terms.  
Your team members usually are on speaking terms and don't need the added complexity of eggs.  The basic setup.py should be sufficient.",A,3
29333860,2015-03-29 19:37:59.727000+00:00,"gevent, twisted, asyncio should handle 50k urls just fine.
To avoid consuming too much memory and to synchronize the processes that download and process urls, you could set the maximum size on the corresponding queues: if the downloading happens too fast; it will block on queue.put() when it reaches its maximum capacity.
Green threads would be useless for a parallel regex processing. Ordinary Python threads that use real OS threads would be useless here if GIL is not released during the regex processing. re does not release GIL. regex module can release GIL in some cases.
If you use re module; you might want to create a processes pool instead of threads where the number of processes ~number of cpus on the host.
Beware of how you use MySQL, Redis (you might need a green driver for some usage scenarios).",A,1
25650943,2014-09-03 18:11:09.560000+00:00,"You are using a numbered reference in the second field; the 0 indicates you want to use the first parameter passed to str.format() (e.g. MyString), not the MyFloat value which is parameter 1.
Since you cannot use the .2f format on a string object, you get your error.
Remove the 0:
print ""{}  ¦  {:.2f}"".format(MyString, MyFloat)

as fields without a name or index number are auto-numbered, or use the correct number:
print ""{}  ¦  {1:.2f}"".format(MyString, MyFloat)

If you chose the latter, it's better to be explicit consistently and use 0 for the first placeholder:
print ""{0}  ¦  {1:.2f}"".format(MyString, MyFloat)

The other option is to use named references:
print ""{s}  ¦  {f:.2f}"".format(s=MyString, f=MyFloat)

Note the keyword arguments to str.format() there.",A,20
30737388,2015-06-09 16:08:40.247000+00:00,"You closed the parentheses in the wrong location:
df = pd.read_csv(StringIO('MYDATA.csv', nrows=17, skiprows=1,skipinitialspace=True, delimiter=','))
#                        ^            ^ not closed here

You'd move the closing parenthesis to close the StringIO() call and leave the keyword arguments for the pd.read_csv() call:
df = pd.read_csv(StringIO('MYDATA.csv'), nrows=17, skiprows=1,skipinitialspace=True, delimiter=',')

Note that StringIO('MYDATA.csv') creates an in-memory file with the contents MYDATA.csv; it does not open a file with that filename. If you wanted to open a file on your filesystem named MYDATA.csv, you need to leave off the StringIO call:
df = pd.read_csv('MYDATA.csv', nrows=17, skiprows=1, skipinitialspace=True, delimiter=',')
",A,2
17746308,2013-07-19 12:40:37.160000+00:00,"Some of your a tags do not have a href attribute, so .get('href') returns None.
Use:
link_domain = tldextract.extract(href.get('href', ''))

to return an empty string in that case, or test for the attribute first:
href = href.get('href')
if not href:
    continue

link_domain = tldextract.extract(href)
",A,0
24347762,2014-06-22 03:02:32.593000+00:00,"Use str.join():
for group in n_split(dblist, 5):
    print ' '.join(group)

You could add enumerate() to add a counter:
for count, group in enumerate(n_split(dblist, 5), 1):
    print 'env{}: {}'.format(count, ' '.join(group))

Demo:
>>> def n_split(iterable, n, fillvalue=None):
...     num_extra = len(iterable) % n
...     zipped = zip(*[iter(iterable)] * n)
...     return zipped if not num_extra else zipped + [iterable[-num_extra:], ]
... 
>>> dblist=['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve']
>>> for group in n_split(dblist, 5):
...     print ' '.join(group)
... 
one two three four five
six seven eight nine ten
eleven twelve
>>> for count, group in enumerate(n_split(dblist, 5), 1):
...     print 'env{}: {}'.format(count, ' '.join(group))
... 
env1: one two three four five
env2: six seven eight nine ten
env3: eleven twelve
",A,3
9782861,2012-03-20 07:29:54.197000+00:00,"Using C++ in Cython doesn't mention anything special:
#file: pya.pyx
cdef extern from ""a.h"":
     cdef cppclass A:
        int a
     cdef cppclass B(A):
        int b

Wrapper class:
#file: pya.pyx
cdef class PyB:
    cdef B* thisptr
    def __cinit__(self):
        self.thisptr = new B();
    def __dealloc__(self):
        del self.thisptr
    property a:
        def __get__(self): return self.thisptr.a
        def __set__(self, int a): self.thisptr.a = a
    property b:
        def __get__(self): return self.thisptr.b
        def __set__(self, int b): self.thisptr.b = b

Example:
import pyximport; pyximport.install(); # pip install cython

from pya import PyB

o = PyB()
assert o.a == 0 and o.b == 0
o.a = 1; o.b = 2
assert o.a == 1 and o.b == 2

To build it you need to instruct pyximport to use c++:
#file: pya.pyxbld
import os
from distutils.extension import Extension

dirname = os.path.dirname(__file__)

def make_ext(modname, pyxfilename):
    return Extension(name=modname,
                     sources=[pyxfilename, ""a.cpp""],
                     language=""c++"",
                     include_dirs=[dirname])
",A,5
364993,2008-12-13 08:18:35.957000+00:00,"ceil(log2(value))

ilog2() can be calculated in 3 asm instructions e.g., http://www.asterisk.org/doxygen/1.4/log2comp_8h-source.html",A,18
51005252,2018-06-23 21:39:34.203000+00:00,"Django doesn't retrieve any related objects by default. You have to specifically do so from the instance via my_restaurant.pizzas_offered.all(). Now, that is a queryset, so you can just as easily filter it as you like: my_restaurant.pizzas_offered.filter(type='vegetarian').",A,3
23975313,2014-05-31 23:14:30.753000+00:00,"You have a birth_date column but used a text field in the form. Flask-Security uses WTForms for the form definitions; it has a dedicated DateField type which I think should be used instead:
from wtforms.fields import DateField

class ExtendedConfirmRegisterForm(ConfirmRegisterForm):
    name  = TextField('Name', 
                    [validators.Length(max=255), 
                    validators.Required()])

    surname = TextField('Surname', 
                        [validators.Length(max=255)])
    birth_date = DateField('Date of birth')
",A,1
44089019,2017-05-20 17:40:44.720000+00:00,"This problem is not related to Jinja at all.
When you iterate over a dict with for x in y, you only get the keys, not the values. If you want to iterate over the values, you need to do that explicitly:
{% for item in dict.values() %}
",A,0
15998859,2013-04-14 12:05:12.077000+00:00,"You need to import modules where they are used.
If your models module uses the google.appengine.ext.db module, you need to import it there, not in your handler module.
Importing things creates a reference to that 'thing' in your module namespace, so that the code there can find it when using it. db is the local name by which you get to use the object defined in google.appengine.ext.
If your handler uses the same object, it needs to import that still. If by importing models all names used by models suddenly where available in your handler module too, you'd end up with name conflicts and hard-to-debug errors all over the place.
Vice versa, if only importing google.appengine.ext.db in your handler module and not in your models module were to work, you'd need to import all the dependencies of given module together with the module itself. This quickly becomes unworkable, as you'd need to document all the things your models module requires just to be able to use it. ",A,2
51165190,2018-07-04 03:01:25.707000+00:00,"This is what replace does:

Return a copy of the string with all occurrences of substring old replaced by new. If the optional argument count is given, only the first count occurrences are replaced.

You're actually relying on exactly that behavior in your replace(' ', '') call.
When you don't want that, don't use replace. (In fact, almost any use of s.replace(s[i], …) has exactly this problem, so watch out whenever you find yourself writing that.)
For example, you can just use slicing:
s = s[:i] + ' ' + s[i+1:]


Or, maybe even better, just build up a list of characters and join them at the end:
s = input()
lst = []
for i in range(len(s)):
    if i % 3:
        lst.append(s[i])
s = ''.join(lst)

… which you can even condense into a one-liner:
s = ''.join(c for i, c in enumerate(s) if i % 3)


If you're really not allowed to use join, you can do the same logic—just less efficient and idiomatic—with string concatenation:
s = input()
t = ''
for i in range(len(s)):
    if i % 3:
        t += s[i]
s = t

Or, just for fun, put all the complexity in the ranges and slicing so you don't even need the %:
s = input()
t = ''
for i in range(1, len(s), 3):
    t += s[i:i+2]
s = t
",A,5
28384686,2015-02-07 16:43:34.293000+00:00,"datetime object is immutable. The only way to change date is to bind it to a new datetime object.
To assign to outer-scope variable, you could use nonlocal in Python 3 as @falsetru suggested or emulate it using a list or a custom object in Python 2:
def button_callback(self, *args):
    self.date -= DAY

See What limitations have closures in Python compared to language X closures?",A,0
21730967,2014-02-12 14:37:30.633000+00:00,"You put a multiplier around the group:
(?:\s+(\w+))*

but groups do not multiply; they have a fixed group number and every match is assigned to that group number. Hence you see only ever the last match.
You'll have to capture all candidates in one group and split afterwards:
[r[:1] + tuple(r[1].split()) 
 for r in re.findall(r""{% url '(\w+)'((?:\s+\w+)*) %}"", inputtext)]

Note that the capturing group now captures all of the (?:\s+\w+)* pattern.
Demo:
>>> import re
>>> inputtext = ""{% url 'a' b c e %}""
>>> [r[:1] + tuple(r[1].split()) 
...  for r in re.findall(r""{% url '(\w+)'((?:\s+\w+)*) %}"", inputtext)]
[('a', 'b', 'c', 'e')]

Your second form is more complex, and requires that you use another regular expression to split out the matches:
from itertools import chain

[r[:1] + tuple(chain(*re.findall(r'(\w+)\s*=\s*(\w+)', r[1])))
 for r in re.findall(r""{% url '(\w+)'((?:\s+\w+\s*=\s*\w+)*) \s*%}"", inputtext)]

Demo:
>>> inputtext = ""{% url 'funcname'  first =fir    second = sec    third=thi    %}""
>>> [r[:1] + tuple(chain(*re.findall(r'(\w+)\s*=\s*(\w+)', r[1])))
...  for r in re.findall(r""{% url '(\w+)'((?:\s+\w+\s*=\s*\w+)*) \s*%}"", inputtext)]
[('funcname', 'first', 'fir', 'second', 'sec', 'third', 'thi')]
",A,1
52835329,2018-10-16 12:19:23.850000+00:00,"You registered the undecorated func1() function. The @IsIllegal() decorator result is applied after registration by the @app.route() decorator, and is never called when the route is accessed.
Decorators are applied in reverse order, from the inside out, so to speak. Reverse the decorators:
@app.route(...,  methods = ['POST']  )
@IsIllegal
def func1():

Now the result of IsIllegal(func1) is registered by @app.route(...), so accessing the route will end up calling decorated().",A,0
42872980,2017-03-18 10:10:14.053000+00:00,"Why don't you make it a form field then?
class Pay_bills(forms.ModelForm):
    service_provider = forms.ChoiceField(choices=ser_providers)  # or ModelChoiceField(queryset=ser_providers)

    class Meta:
        model = Recharge_request
        fields = ['mobile_no','amount']

    def clean_service_provider(self):
        ... whatever....
",A,1
20928772,2014-01-05 01:01:05.823000+00:00,"The JSON standard uses Unicode exclusively, so all keys in a dictionary from json.loads() are always unicode values.
For Python 2.6 and earlier, these are easy enough to encode:
d = dict((key.encode('ascii'), value) for (key, value) in json.loads(raw_json).iteritems())

because Python 2 identifiers can only use ASCII characters. In Python 2.7 and up unicode keywords are auto-encoded for you.
In Python 3, identifiers are unicode already and this isn't an issue.",A,3
16878002,2013-06-01 22:44:18.930000+00:00,You cannot write to the local filesystem in GAE. Use the blobstore or Google Cloud Storage.,A,3
25281610,2014-08-13 08:34:47.870000+00:00,"If you want to support everything that implements every variation of the new-style or old-style buffer interface, then you have to use the C API.
But if you don't care about old-style buffers, you can almost always use a memoryview:

Cython memoryviews support nearly all objects exporting the interface of Python new style buffers. This is the buffer interface described in PEP 3118. NumPy arrays support this interface, as do Cython arrays. The “nearly all” is because the Python buffer interface allows the elements in the data array to themselves be pointers; Cython memoryviews do not yet support this.

This of course includes str (or, in 3.x, bytes), bytearray, etc—if you followed the link, you may notice that it links to the same page to explain what it supports that you linked to explain what you want to support.
For 1D arrays of characters (like str), it's:
cdef char [:] gotdata
",A,2
10911312,2012-06-06 09:13:57.170000+00:00,"Normally, when setting up a LinguaPlone site you let LinguaPlone create top-level per-language folders that act as navigation roots. The root of the site itself is set up to redirect visitors to one of these roots depending on their language preferences and other factors, using a language selector.
Take a look at http://www.norden.org/ for an example of such a site. Notice that when you visit that URL you are automatically redirected to one of the supported languages, usually to http://www.norden.org/en/ for english. You can switch between the available languages on the top left, and you'll see that the first element of the URL changes between the language codes. Those are the top-level folders.
The advantage here is that each of these folders can have it's own default view. Another is that URLs are always crystal clear for visitors; switch from ""About this website"" (http://www.norden.org/en/about-this-website) to the Norwegian translation and you end up on on http://www.norden.org/no/om-nettstedet; anyone can see that the new URL is in the Norwegian language even if they do not understand that language.
You can set up these language folders after the fact, as documented in the LinguaPlone installation instructions; simply visit the URL:
http://localhost:8080/Plone/@@language-setup-folders

but be sure to backup your site first and to adjust the address for your local situation.",A,2
45647959,2017-08-12 08:01:00.080000+00:00,"Put the query args outside the template tag.
""{% static 'css/default.min.css' %}?t=227""
",A,1
7617992,2011-10-01 03:39:07.397000+00:00,"""some warnings happen, see examples of a code that would cause an error and explain the solution.""  
It's best for you to provide code samples. 
Why?
From http://developer.apple.com/library/ios/#documentation/DeveloperTools/gcc-4.2.1/gcc/Standards.html#Standards
""There is no formal written standard for Objective-C or Objective-C++. The most authoritative manual is “Object-Oriented Programming and the Objective-C Language”, available at a number of web sites...""  
This means that tt's best for you to provide a specific warning and the associated code.",A,0
27085260,2014-11-23 03:23:58.253000+00:00,"You read all lines first (into lines, then tried to read just one line but the file already gave you all lines. In that case f.readline() gives you an empty line. From there on out your script is doomed to fail; you cannot count words in an empty line.
You can loop over the file instead:
file = input(""What file would you like to open? "")

search = input(""Enter the words you want to search for (separate with commas): "")
search = [word.strip() for word in search.lower().split("","")]

# create a dictionary for all search words, setting each count to 0
count = dict.fromkeys(search, 0)

with open(file, 'r') as f:
    for line in f:
        for word in line.lower().split():
            if word in count:
                # found a word you wanted to count, so count it
                count[word] += 1

The with statement uses the opened file object as a context manager; this just means it'll be closed again automatically when done.
The for line in f: loop iterates over each separate line in the input file; this is more efficient than using f.readlines() to read all lines into memory at once.
I also cleaned up your search word stripping a little, and set the count dictionary to one with all the search words pre-defined to 0; this makes the actual counting a little easier.
Because you now have a dictionary with all the search words, testing for matching words is best done against that dictionary. Testing against a dictionary is faster than testing against a list (the latter is a scan that takes longer the more words are in the list, while a dictionary test takes constant time on average, regardless of the number of items in the dictionary).",A,1
38061622,2016-06-27 19:19:45.030000+00:00,"Loop over the sys.stdin object; it is a regular file and supports iteration. You can use an itertools.islice() object to skip every second line:
import sys
from itertools import islice

second_lines = islice(sys.stdin, None, None, 2)
seta = set(map(int, next(second_lines).split()))
setb = set(map(int, next(second_lines).split()))
",A,1
36889800,2016-04-27 12:26:01.987000+00:00,"You can build a spiral by starting near the center of the matrix and always turning right unless the element has been visited already:
#!/usr/bin/env python
NORTH, S, W, E = (0, -1), (0, 1), (-1, 0), (1, 0) # directions
turn_right = {NORTH: E, E: S, S: W, W: NORTH} # old -> new direction

def spiral(width, height):
    if width < 1 or height < 1:
        raise ValueError
    x, y = width // 2, height // 2 # start near the center
    dx, dy = NORTH # initial direction
    matrix = [[None] * width for _ in range(height)]
    count = 0
    while True:
        count += 1
        matrix[y][x] = count # visit
        # try to turn right
        new_dx, new_dy = turn_right[dx,dy]
        new_x, new_y = x + new_dx, y + new_dy
        if (0 <= new_x < width and 0 <= new_y < height and
            matrix[new_y][new_x] is None): # can turn right
            x, y = new_x, new_y
            dx, dy = new_dx, new_dy
        else: # try to move straight
            x, y = x + dx, y + dy
            if not (0 <= x < width and 0 <= y < height):
                return matrix # nowhere to go

def print_matrix(matrix):
    width = len(str(max(el for row in matrix for el in row if el is not None)))
    fmt = ""{:0%dd}"" % width
    for row in matrix:
        print("" "".join(""_""*width if el is None else fmt.format(el) for el in row))

Example:
>>> print_matrix(spiral(5, 5))
21 22 23 24 25
20 07 08 09 10
19 06 01 02 11
18 05 04 03 12
17 16 15 14 13
",A,12
29038348,2015-03-13 17:22:01.490000+00:00,"You are interpolating the encoded tweet together with the username:
print('@%s: %s' % (user, tweet_text))

If the user object is a Unicode string this'll fail:
>>> user = u'Héllo'
>>> tweet_text = u'Héllo'.encode('utf8')
>>> '@%s: %s' % (user, tweet_text)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 1: ordinal not in range(128)

because you are mixing types. Python tries to decode the tweet_text value to make it a unicode object again.
Stick to one type; either encode everything, or leave everything Unicode, and encode at the last instance.
You'll have to encode your user value for the CSV file anyway, leave encoding of the tweet until then:
tweet_text = tweet['text'].strip()
tweet_text = u''.join(tweet_text.splitlines())
print i, time,
if tweet['geo'] and tweet['geo']['coordinates'][0]: 
    lat, long = tweet['geo']['coordinates'][:2]
    print u'@%s: %s' % (user, tweet_text), lat, long
else:
    print u'@%s: %s' % (user, tweet_text)

writer.writerow([user.encode('utf8'), time.encode('utf8'), 
                 tweet_text.encode('utf8'), lat, long])
",A,0
25982196,2014-09-22 19:57:44.200000+00:00,"You are using writelines() but passing in one item at a time; file.writelines() expects an iterable (something producing a sequence of 0 or more values) instead.
Use file.writeline() (singular) instead, or even better, just file.write():
caving.write(item[0])
caving.write('\t')
caving.write(item[1])
caving.write('\n')

If you are writing a Tab-separate file, you might want to use the csv module instead:
import csv

def normalize(dataset):    
    twoCol = [item[:2] for item in dataset]
    labels = [item[2] for item in dataset]
    twoColData = preprocessing.scale(float64(twoCol))

    with open('/home/nima/Desktop/ramin-ML-Project/caving.txt', 'wb') as caving:
        writer = csv.writer(caving, delimiter='\t')

        for data, label in itertools.izip(twoColData, labels):
            if label == 'caving':
                writer.writerow(data)

This produces the same output, but with less hassle.",A,3
16589335,2013-05-16 13:52:54.220000+00:00,"No, because the shell removes those spaces, not optparse. Python is handed a list of already-parsed command-line parameters.
Use quoting to preserve spaces:
./yourscript.py --option="" spaces in here ""

To demonstrate, I created the following script:
#!/usr/bin/env python
import sys
print sys.argv

to show you what optparse sees:
$ ./demo.py     foo bar baz
['./demo.py', 'foo', 'bar', 'baz']

Note how the whitespace is all removed and three values are passed to the script. But with quoting:
$ ./demo.py ""    foo bar"" baz
['./demo.py', '    foo bar', 'baz']

the whitespace is preserved, and I joined two strings together into one as well.",A,3
13593230,2012-11-27 21:17:18.640000+00:00,"Printing stats is a fine use of __str__(). Simply use string formatting to return a single string value:
def __str__(self):
    return ('Name: {name}\n'
        'Poisoned: {poisoned}\n'
        # etc.
    ).format(**self.__dict__)
",A,7
35391004,2016-02-14 11:03:52.897000+00:00,"getSuccessors pops an element of the list and returns it. Elements in the list are not themselves instances of Stack, so they have no isEmpty method.
Note that DFS is usually implemented via recursion; you may find it easier to work out where you're going wrong if you do that.",A,2
18411542,2013-08-23 20:44:47.090000+00:00,"The problem appears to be that you have two different Python 3.3 installations, and a shared site-packages directory.* And the tool you run as pip3 is from pip 1.3.1, but the modules it uses to do its work come from pip 1.4.1.
The easiest way to solve this, unless you actually need both Python 3.3 installations, is to get rid of both of them completely, then reinstall the one you want.
The following should work (possibly with sudo for some of the commands—or using Finder and letting it tell you whether you need to authenticate).
Note that this will also partially or completely remove any other third-party Python versions (e.g., a python.org 2.7.5), which I don't think is a problem for you, but could be for future readers.
You might want to make a list of all installed packages before uninstalling anything. (I usually do this in the hackiest way possible: fire up ipython, and let it tab-complete an import statement…)
Finally, some of these details will be different for any future readers with similar problems, but the basic ideas should be the same.

brew uninstall python3
rm -rf /Library/Frameworks/Python.framework
rm -rf /usr/local/lib/python*
rm -rf /usr/local/share/python*
Find where in ~/.bash_profile (or ~/.profile or elsewhere) you add the Python paths to your PATH. You may have /usr/local/share/python3 and/or something inside the Python.framework or Cellar/python3. Scrap all that you find.
Fire up a new shell in Terminal.app.
brew doctor, and fix anything that it complains about that seems potentially relevant (the non-Homebrew MacFUSE stuff is fine as-is; the brew prune suggestion is probably worth doing, but doesn't matter here), and run it again to make sure.
brew install python3.
pip3 to reinstall any packages you deleted that you need again.


* Details:

Homebrew Python 3.3 is installed in /usr/local/Cellar/python3/3.3.2/, with various things symlinked into /usr/local/bin and its siblings, and possibly into /Library somewhere.
Another Python 3.3, possibly from the python.org installer, is installed in /Library/Frameworks/Python.framework/Versions/3.3/, with various things possibly symlinked into /usr/local/bin or otherwise added to your PATH.
Both probably include /usr/local/lib/python3.3 in their site-packages search.
",A,2
26470508,2014-10-20 16:32:17.947000+00:00,"Various solutions spring to mind, but surely the easiest is just to generate the json in the partner_search method itself, and pass it to the template along with the other parameters. Then you can call drawTwelveMonthPlot directly with that data, rather than making a separate Ajax request for it.
Edit
I don't really see what you don't understand. You have a function, populateGraph, which simply contains an Ajax call, and the result of that Ajax call is passed to drawTwelveMonthPlot. So you can simply take the invocation of drawTwelveMonthPlot out of that Ajax call and invoke it directly, passing it the JSON from the template.
function populateGraph() {
    var data = '{{ json_data_from_view|safe }}'
    drawTwelveMonthPlot(data);
}

That assumes that the JS is in your Django template; if it isn't, you can just put the var data... line in a <script> element in your template, and the rest will still work.",A,0
28225177,2015-01-29 21:42:41.027000+00:00,"You are sorting strings, not numbers, and those are sorted lexicographically; like words in a dictionary. - happens to be sorted before digits, but '1' is still sorted before '3'. Furthermore, '10' sorts before '2' because '1' comes before '2' in a character table.
Sort numerically by converting each element to an integer while sorting:
split_str.sort(key=int)

Demo, with your corrected function:
>>> def sort_int_string(this_string):
...     split_str = this_string.split()
...     split_str.sort(key=int)
...     join_str = ' '.join(split_str)
...     return join_str
... 
>>> print(sort_int_string(""4 1 -1 -3""))
-3 -1 1 4
",A,2
19380430,2013-10-15 11:46:18.143000+00:00,"It's really not clear what you are trying to do here. This line - the one that's causing the error -  makes no sense:
bundle.obj.objects.create_user()

Since you haven't provided the code for bundle, I have to guess that this is a model with an obj ForeignKey, which points to CompanyUser (rather strange naming conventions, though). But as the error says, you can't call objects on an instance, only on the class. But even if you could, the line still doesn't make sense: obj is already a User, so why would you want to create one? And again, create_user takes at least an email parameter, which you're not supplying.",A,1
2684552,2010-04-21 16:03:26.390000+00:00,"There are a number of hooks in the ModelAdmin class to allow you to do things this - look at the code in django.contrib.admin.options.
Two methods that might help you are ModelAdmin.save_form and ModelAdmin.save_model, both of which are passed the request object. So you can override these methods in your Admin subclass and do any extra processing you need.
Edited after comment
You're quite right that this won't let you validate the form dependent on the user's privileges. Unfortunately the form instantiation is buried deep within the add_view and change_view methods of ModelAdmin. 
There aren't many possibilities without duplicating a lot of existing code. You could override the *_view methods; or you could try and override the modelform_factory function to return a new class with the request object baked in already; or you could try fiddling with the form class __new__ method to do the same thing, but that's tricky because of the form metaclass.",A,0
15303599,2013-03-08 21:26:37.700000+00:00,"You must make sure that whatever creates file1.txt has closed the file handle.
File writing is buffered, and if you do not close the file, the buffer is not flushed. The missing data at the end of a file is still sitting in that buffer.
Preferably you ensure that the file is closed by using the file object as a context manager:
with open('file1.txt', 'w') as openfile:
    # write to openfile

# openfile is automatically closed once you step outside the `with` block.
",A,3
21624163,2014-02-07 09:41:52.753000+00:00,This is exactly why you should use virtualenv.,A,4
25315084,2014-08-14 18:26:43.927000+00:00,"If the socket() call on the second line raises an exception, then connSock is never bound.
Sockets can be used as context managers, use that to your advantage:
try:
    with socket(AF_INET, SOCK_STREAM) as connSock:
        connSock.connect((tgtHost, tgtPort))
except socket.error:
    pass

This'll close the socket when the with block is exited, without having to reference it, and only if the socket() call actually succeeded.
Note that you should rarely use a blanket except: pass. Better to catch specific exceptions.",A,7
17219154,2013-06-20 16:41:33.573000+00:00,"If elements are sorted as in your example; you could use itertools.groupby():
from itertools import groupby

print "" "".join([k for k, group in groupby(d['key']) if len(list(group)) == 1])
# -> a b d
",A,3
14967041,2013-02-19 20:45:14.783000+00:00,"To create one byte of a given value, you can use the chr() function:
>>> chr(5)
'\x05'
>>> chr(5) * 5
'\x05\x05\x05\x05\x05'

or you could use a bytearray() with the right number of integers:
>>> str(bytearray(5 * [5]))
'\x05\x05\x05\x05\x05'

or use an array.array() with the same:
>>> import array
>>> array.array('B', 5*[5]).tostring()
'\x05\x05\x05\x05\x05'

or use the struct.pack() function to pack your integers into bytes:
 >>> import struct
 >>> struct.pack('{}B'.format(5), *(5 * [5]))
 '\x05\x05\x05\x05\x05'

There may be more ways.. :-)",A,6
1550635,2009-10-11 12:54:32.113000+00:00,"If they're in the code, you can pass them directly to the template context:
render_to_response('mytemplate.html', {
                      'col_choices': COL_CHOICES,
                      'col2_choices': COL2_CHOICES
                   })

Edit in response to comment: If you need this on every page including generic views, the best thing to do is to use a template tag.",A,0
26433339,2014-10-17 20:58:01.287000+00:00,"You don't need to create a fake directory structure, all you need to do is mock the isdir() and listdir() functions.
Using the unittest.mock library (or the external mock library, which is the exact same thing for Python versions < 3.3):
try:
    # Python >= 3.3 
    from unittest import mock
except ImportError:
    # Python < 3.3
    import mock

with mock.patch('yourmodule.isdir') as mocked_isdir, \
        mock.patch('yourmodule.listdir') as mocked_listdir:
    mocked_isdir.return_value = True
    mocked_listdir.return_value = ['filename1', 'filename2']

    yourmodule.foo('/spam/eggs')

    mocked_isdir.assert_called_with('/spam/eggs/baz/foo')
    mocked_listdir.assert_called_with('/spam/eggs/baz/foo')
",A,6
53150862,2018-11-05 08:37:24.253000+00:00,"You're confusing the URL syntaxes. url() takes a regex; for the new-style format you need to use path().
path(""article/<int:pk>/"" , views.post_detail_view.as_view() , name=""detail""),
",A,0
37110813,2016-05-09 08:19:07.200000+00:00,"Include the x value in a tuple returned from the key; this second element in the key will be then used when there is a tie for the y value. To inverse the comparison (from smallest to largest), just negate that value:
min(x, key=lambda t: (t[1], -t[0]))

After all, -4 is smaller than -2.
Demo:
>>> x = [(2, 3), (4, 3), (6, 9)]
>>> min(x, key=lambda t: (t[1], -t[0]))
(4, 3)
",A,17
35699493,2016-02-29 12:23:27.113000+00:00,"Send the data as JSON. Then you can deserialize directly to a dict, without any processing needed.
$.post( ""/api"", { verb: 'create', my_id: my_id, properties: JSON.stringify(properties) })

...
properties = json.loads(self.request.get('properties', {}))
",A,1
11281590,2012-07-01 11:33:29.090000+00:00,"Alternative:
a = Blog.objects.get(pk=3)
b = a.blog_set.all()
",A,1
35404076,2016-02-15 07:54:00.797000+00:00,"That package won't install on any Python version < 3.5, because the syntax is indeed invalid on anything but Python 3.5 and newer.
You can't put the **kwargs syntax in front of other keyword arguments. The two should be swapped:
setup(install_requires=['redis', 'tabulate', 'tqdm', 'msgpack-python'], **sdict)

Reporting this as a bug was the correct thing to do; the package states it supports Python 3.4 and up.
Python 3.5 added support for an arbitrary number of *args and **kwargs expansions through PEP 448, opening the door for the above to work too.",A,2
892571,2009-05-21 11:57:05.013000+00:00,"No.
Sometimes you can quote the filename.
""C:\Program Files\Something""

Some programs will tolerate the quotes.  Since you didn't provide any specific program, it's impossible to tell if quotes will work for you.",A,1
28124094,2015-01-24 09:33:53+00:00,"You are getting your operator precedence wrong. You are formatting 2.17, then multiplying that by a long integer:
>>> r = ""%f"" % 2.17
>>> r
'2.170000'
>>> r * 10 ** 27
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
OverflowError: cannot fit 'long' into an index-sized integer

Put parentheses around the multiplication:
>>> ""%f"" % (2.17 * 10**27)
'2169999999999999971109634048.000000'

This is one of the drawbacks of overloading the modulus operator for string formatting; the newer Format String syntax used by the str.format() method and the Format Specification Mini-Language it employs (and can be used with the format() function) neatly skirt around that issue. I'd use format() for this case:
>>> format(2.17 * 10**27, 'f')
'2169999999999999971109634048.000000'
",A,4
32975741,2015-10-06 17:13:10.317000+00:00,"You are using the SimpleCache setup:
cache = Cache(config={'CACHE_TYPE': 'simple'})

This uses a single global dictionary to hold the cache, and this in turn will only work if you are using a WSGI server that uses one Python interpreter to serve all your WSGI requests. If you use a WSGI server that uses separate child processes to handle requests, you'll get a new copy of that dictionary each time and nothing is cached, effectively.
The code works fine when run with the built-in app.run() development server, given you don't use the processes argument.
Your update shows that you run the server with 5 separate processes. Each process will get its own dictionary, and the cache is not shared between them. Use a different caching backend instead, like filesystem:
cache = Cache(config={'CACHE_TYPE': 'filesystem', 'CACHE_DIR': '/tmp'})
",A,16
43933242,2017-05-12 08:44:49.297000+00:00,"You have string literals with \xhh hex escapes. You can decode these with the string_escape encoding:
text.decode('string_escape')

See the Python Specific Encodings section of the codecs module documentation:

string_escape
  Produce a string that is suitable as string literal in Python source code

Decoding reverses that encoding:
>>> ""\\x69\\x73\\x41\\x72\\x72\\x61\\x79"".decode('string_escape')
'isArray'
>>> ""\\x63\\x61\\x6C\\x6C"".decode('string_escape')
'call'
>>> ""\\x74\\x6F\\x53\\x74\\x72\\x69\\x6E\\x67"".decode('string_escape')
'toString'

Being a built-in codec, this is a lot faster than using regular expressions:
>>> from timeit import timeit
>>> import re
>>> def unescape(text):
...     return re.sub(r'\\x([0-9a-fA-F]{2})',
...         lambda m: chr(int(m.group(1), 16)), text)
...
>>> value = ""\\x69\\x73\\x41\\x72\\x72\\x61\\x79""
>>> timeit('unescape(value)', 'from __main__ import unescape, value')
6.254786968231201
>>> timeit('value.decode(""string_escape"")', 'from __main__ import value')
0.43862390518188477

That's about 14 times faster.",A,2
25671900,2014-09-04 17:46:44.597000+00:00,"First, using 1000 threads will almost certainly slow things down, not speed it up. Even if your code is completely I/O bound, 1000 is pushing the limits of many platforms' schedulers, and you'll spend more time context switching than doing actual work.
Next, you need to know whether your code is CPU-bound (that is, doing actual processing on information in memory) or I/O-bound (that is, waiting on things like disk reads and writes).

If your code is CPU-bound, and you can keep the CPU busy pretty consistently, you want exactly 1 thread per core. That way, you get the maximum amount of parallelism with the minimum amount of context switching (and cache thrashing, assuming most of the work is done on either immutable or non-shared values).
Also (unless that work is being done in specially-designed C extensions like numpy), you want these threads to be in separate processes, because only 1 thread per process can run the Python interpreter at a time, thanks to the Global Interpreter Lock.
So, what you want is almost certainly a process pool. The easiest way to do that is to use the concurrent.futures.ProcessPoolExecutor, possibly with a max_workers argument (maybe start with 16, then try tweaking it up and down to see if it helps).

If, on the other hand, your code is mostly I/O-bound, then a couple dozen threads is reasonable, especially if the delays are unpredictable, but not 1000. And threads in the same process will work fine, because one thread can run the Python interpreter while the others are all waiting for the OS to finish a disk operation.
So, in this case, you want a concurrent.futures.ThreadPoolExecutor.

If you're not sure, and don't know how to find out, build it with a thread pool first, then use ActivityMonitor or whatever Windows now calls its process manager or your favorite of the 300 options on Linux to watch it run; if you end up with one core at 100% and the others below 25%, then you're too CPU-bound to be using threads. Fortunately, switching to a process pool is a trivial change—replace ThreadPoolExecutor with ProcessPoolExecutor, and remove the max_workers argument so Python will pick the best default, and now you're done.

In either case, the examples in the docs are good enough that there's no reason to ask for other sample code.",A,5
52526258,2018-09-26 21:27:44.400000+00:00,"You can use a generator expression with any:
if any(item in row['product'].lower() for item in (""smoke"", ""grill"", ""choco"")):
",A,3
24057881,2014-06-05 10:34:28.960000+00:00,"Your recursive call still returns to the parent function that invoked it. In that scope n has not changed.
Function locals are still local to the current frame even if you keep calling the function from itself.
Your recursion call looks like this:
counter(3):
  n = 3
  |
  | counter(n - 1)
  |   n = 2
  |   |
  |   | counter(n - 1)
  |   |   n = 1
  |   |   |
  |   |   | counter(n - 1)
  |   |   |   n = 0
  |   |   |   |
  |   |   |   return
  |   |   |
  |   |   n is still 1 here
  |   |   return
  |   |
  |   n is still 2 here
  |   return
  |
  n is still 3 here
  return
",A,8
13210092,2012-11-03 14:31:08.670000+00:00,"Indeed, the test runner does create an instance of your test class (SimpleTest) for each test method that you create. self refers to that instance.
From the unittest documentation:

Each instance of TestCase will run a single test method: the method named methodName.

This means you can use additional 'helper' methods on your test class and call these with self.name_of_helper_method().
You generally don't have to worry about how the unittest framework loads and runs your tests, but the unittest documentation can explain this in further detail.",A,3
41219364,2016-12-19 09:24:22.353000+00:00,"Django doesn't do anything at all. It is entirely up to the server, which has already determined (according to its configuration) whether to run Django in multiple processes and/or threads, and so distributes incoming requests among those.",A,4
14642360,2013-02-01 08:34:24.457000+00:00,"It sounds like the problem here is that the python.org Python is expecting real readline, and not being happy with the libedit substitute that Apple provides.
See the documentation for readline at PyPI for an explanation of the issue.
You can fix it as follows:
sudo /path/to/easy_install readline

Note that readline is one of the handful of things that cannot be installed properly by pip, so you have to use easy_install (or do it manually).
The python.org 2.x installers don't come with easy_install. Install it by following the directions on the setuptools page.
On top of that, keep in mind that, in some cases, you can end up with Apple-python easy_install in /usr/local/bin as well as /usr/bin, which means you can't be sure /usr/local/bin/easy_install will get the python.org version, so explicitly use easy_install-X.Y.
And even that doesn't help if you're using a python.org (or other) installation of an X.Y version that Apple already gave you. /usr/local/bin/easy_install-2.7 may well be Apple's (as it is on the 10.8.2 machine I'm sitting at right now). The only way to be safe is to check the shebang line and see which Python interpreter it uses.
Or, more simply, just don't install a python-X.Y if Apple's already given you one. Seriously, there are hundreds of questions all over SO from people who did this and have problems, and all of them could be avoided by just using the Apple build. Apple used to ship broken, incomplete, and/or woefully out-of-date Python, but since either 10.5 or 10.6, they've been shipping working, complete, reasonably-recent versions, with extras like easy_install and PyObjC included.",A,2
24828980,2014-07-18 15:42:23.137000+00:00,"You'll need to build a fresh copy for your older Python version. cx_oracle comes with the source, all you need is the Oracle client libraries (which come with headers).",A,0
2833559,2010-05-14 10:55:56.300000+00:00,"
import data from a file and possibly reformat it 

Python excels at this.  Be sure to read up on the csv module so you don't waste time inventing it yourself.
For binary data, you may have to use the struct module.   [If you wrote the C++ program that produces the binary data, consider rewriting that program to stop using binary data.  Your life will be simpler in the long run.  Disk storage is cheaper than your time; highly compressed binary formats are more cost than value.]

Import the munged data into a database. 
  Extract data from the database
  Perform calculations on the data and either insert or update tables in the database.

Use the mysqldb module for MySQL.  SQLite is built-in to Python.
Often, you'll want to use Object-Relational mapping rather than write your own SQL.  Look at sqlobject and sqlalchemy for this.
Also, before doing too much of this, buy a good book on data warehousing.  Your two ""task groups"" sound like you're starting down the data warehousing road.  It's easy to get this all fouled up through poor database design.  Learn what a ""Star Schema"" is before you do anything else.",A,3
23369276,2014-04-29 15:42:04.217000+00:00,"You can, by importing everything from <package-name> into the __init__.py file; add a line:
from .<package-name> import *

in the __init__.py module.",A,4
21087303,2014-01-13 09:06:54.050000+00:00,"Simon's answer and Volcano's together explain what you're doing wrong, and Simon explains how you can fix it by redesigning your interface.
But if you really need to read 1 character, and then later read 1 line, you can do that. It's not trivial, and it's different on Windows vs. everything else.
There are actually three cases: a Unix tty, a Windows DOS prompt, or a regular file (redirected file/pipe) on either platform. And you have to handle them differently.
First, to check if stdin is a tty (both Windows and Unix varieties), you just call sys.stdin.isatty(). That part is cross-platform.
For the non-tty case, it's easy. It may actually just work. If it doesn't, you can just read from the unbuffered object underneath sys.stdin. In Python 3, this just means sys.stdin.buffer.raw.read(1) and sys.stdin.buffer.raw.readline(). However, this will get you encoded bytes, rather than strings, so you will need to call .decode(sys.stdin.decoding) on the results; you can wrap that all up in a function.
For the tty case on Windows, however, input will still be line buffered even on the raw buffer. The only way around this is to use the Console I/O functions instead of normal file I/O. So, instead of stdin.read(1), you do msvcrt.getwch().
For the tty case on Unix, you have to set the terminal to raw mode instead of the usual line-discipline mode. Once you do that, you can use the same sys.stdin.buffer.read(1), etc., and it will just work. If you're willing to do that permanently (until the end of your script), it's easy, with the tty.setraw function. If you want to return to line-discipline mode later, you'll need to use the termios module. This looks scary, but if you just stash the results of termios.tcgetattr(sys.stdin.fileno()) before calling setraw, then do termios.tcsetattr(sys.stdin.fileno(), TCSAFLUSH, stash), you don't have to learn what all those fiddly bits mean.
On both platforms, mixing console I/O and raw terminal mode is painful. You definitely can't use the sys.stdin buffer if you've ever done any console/raw reading; you can only use sys.stdin.buffer.raw. You could always replace readline by reading character by character until you get a newline… but if the user tries to edit his entry by using backspace, arrows, emacs-style command keys, etc., you're going to get all those as raw keypresses, which you don't want to deal with.",A,11
354416,2008-12-09 22:06:19.713000+00:00,"See http://java.sun.com/docs/books/jls/second_edition/html/typesValues.doc.html for information on the postfix evaluation rules.
Also see http://java.sun.com/docs/books/jls/second_edition/html/expressions.doc.html#39438 for information.",A,-1
15613559,2013-03-25 11:16:03.690000+00:00,"You'll need to set the property on the class (since it is a descriptor), so using a enable_callback call in the initializer is not going to work.
You could use a class decorator to set the properties from a pattern:
def callback_properties(callbacks_attribute, *names):
    def create_callback_property(name):
        def getter(self):
            return getattr(self, '_' + name)
        def setter(self, value):
            setattr(self, '_' + name, value)
            for f in getattr(self, callbacks_attribute):
                f(value)
        return property(getter, setter)

    def add_callback_properties(cls):
        for name in names:
            setattr(cls, name, create_callback_property(name)

        return cls

    return add_callback_properties

Then use that as:
@add_callback_properties('var_callbacks', 'var1', 'var2')
class A(object):
    # everything else
",A,3
21390791,2014-01-27 20:17:21.460000+00:00,"You are presumably trying to build a list of length lenp here. You'd need to create a list by multiplication here:
Temp = [None] * lenp

but you'd be better off building the list by appending to it:
Temp = []
for i in p:
    Temp.append(T * (i / po) ** rp)

where you don't use p[i] but i directly; Python for loops are for each loops really. 
Your range() produces values in the series [0, 10, 20, ... 1200) and the for loop assigns each of those values to i per iteration. If you use i to index into p again you'd run into problems; p[0] would still be 0, but p[10] would then be 100, p[20] is 200, etc. until p[120] throws an IndexError because there are only 119 different values in that range.
You can collapse the for loop appending to Temp into a list comprehension to build the list in one go:
rp=1.331
po=1000
T=280

Temp = [T * (i / po) ** rp for i in range(0, 1200, 10)]
",A,3
3040148,2010-06-14 19:12:48.267000+00:00,"This is a job for regex:
>>> import re
>>> text = 'this is a bunch of text that is not {[really]} useful in any {[way]}. I need to {[get]} some items {[from]} it.'
>>> re.findall(r'\{\[(\w+)\]\}', text)
['really', 'way', 'get', 'from']
",A,3
23804771,2014-05-22 10:52:32.217000+00:00,"This is completely mistaken. A many-to-many field is already double-ended. You don't need to define it on both ends. When you define a members field on Page, then MyUser will automatically get a page_set accessor which is the other end of the relation, and any page that adds a user to its members will automatically show up in the user's page_set. There is no need for any code.",A,0
27638796,2014-12-24 15:03:37.850000+00:00,"Your operator.itemgetter(0) approach can only get you so far; it'll select the first character of your lines, always.
To properly sort your file numerically, your key function will have to do a few things:

Properly separate out the number at the start of the line from the rest.
Turn the string representing the number into a proper integer value.

The latter is important, because sorting strings is done lexicographically; e.g. '10' is sorted before '9', because 1 comes before 9.
To support an arbitrary run of numeric characters at the start of the line, separated by whitespace, you need to split on whitespace. The str.split() method can do this for you if you don't give it any arguments or use None for the first argument. To keep things efficient, limit the number of splits to just 1, and turn the first element of the result into an integer:
fo = open('3a.txt','r')
x = fo.readlines()
sort_key = lambda line: int(line.split(None, 1)[0])
sorted_x = sorted(x, key=sort_key, reverse=True)
print(sorted_x)

So the key argument is given an anonymous function (a lambda), and that function takes one argument, the line being sorted. That line is split on whitespace just once (line.split(None, 1) and the first element of that split is turned into an integer:
>>> '11 Hello World'.split(None, 1)
['11', 'Hello World']
>>> '11 Hello World'.split(None, 1)[0]
'11'
>>> int('11 Hello World'.split(None, 1)[0])
11

You can improve the rest of your implementation too; there is no need to call file.readlines() as files are iterable. sorted() will take everything in an iterable and sort that, so you can just pass the whole file object directly to the function.
You also want to handle files such that they are closed automatically when done. Make use of the fact that they are context managers; the with statement will signal to them when the context is 'done' (has ended) and file objects will close themselves automatically:
sort_key = lambda line: int(line.split(None, 1)[0])

with open('3a.txt','r') as fo:
    sorted_x = sorted(fo, key=sort_key, reverse=True)

print(sorted_x)

Demo:
>>> from io import StringIO
>>> fo = StringIO('''\
... 5 Helen 
... 4 judy
... 25 Hanna
... 11 Elsa
... 8 Rachel
... '''
... )
>>> sort_key = lambda line: int(line.split(None, 1)[0])
>>> sorted(fo, key=sort_key, reverse=True)
['25 Hanna\n', '11 Elsa\n', '8 Rachel\n', '5 Helen \n', '4 judy\n']

You could accomplish this and still use operator.itemgetter(0) only if you first split your lines up into lists, where element 0 is an integer number:
import operator

sort_key = operator.itemgetter(0)

with open('3a.txt','r') as fo:
    split_lines = (line.split(None, 1) for line in fo)
    numeric_lines = ((int(line[0]), line[1]) for line in split_lines)
    sorted_x = sorted(numeric_lines, key=sort_key, reverse=True)

print(sorted_x)

This makes use of generator expressions to process the lines as they are being read. However now you have a list with each element a tuple, an integer and the rest of your line:
>>> import operator
>>> fo = StringIO('''\
... 5 Helen 
... 4 judy
... 25 Hanna
... 11 Elsa
... 8 Rachel
... ''')
>>> sort_key = operator.itemgetter(0)
>>> split_lines = (line.split(None, 1) for line in fo)
>>> numeric_lines = ((int(line[0]), line[1]) for line in split_lines)
>>> sorted(numeric_lines, key=sort_key, reverse=True)
[(25, 'Hanna\n'), (11, 'Elsa\n'), (8, 'Rachel\n'), (5, 'Helen \n'), (4, 'judy\n')]
",A,11
57367,2008-09-11 19:10:29.190000+00:00,"Product of everything except current in Python
from numpy import array

def product(input, index):
    a = array(input)[index]

    if a[a == 0].size != 1:
        a = a.prod() / a # product except current
    else:
        # exaclty one non-zero-valued element in `a`
        nzi = a.nonzero() # indices of non-zero-valued elements
        a[a == 0] = a[nzi].prod()
        a[nzi] = 0

    return a

Example:
for input in ([2,3,4,5], [2,0,4,5], [0,3,0,5]):
    print product(input, [1,3,2,0]) 

Output:
[40 24 30 60]
[40  0  0  0]
[0 0 0 0]
",A,3
1269125,2009-08-12 22:42:33.920000+00:00,"As others have said, we really need the models, and some explanation of what you're actually trying to achieve.
But it looks like you want to do a related table lookup. Rather than getting all the related objects in a separate nested query, you should use Django's related model syntax to do the join within your query.
Something like:
acceptedFragment.objects.filter(fragment__categories__id = 1)
",A,4
25130880,2014-08-05 03:48:44.417000+00:00,"Why even use groupby here? It's just getting in the way, and you don't want to do anything with the groups as a whole. So why not just select each group manually?
>>> df2[df2.type=='A']['value'].min()
-1.4442888428898644
>>> df2[df2.type=='B']['value'].max()
1.0361392902054989
>>> df2[df2.type=='C']['value'].mean()
0.89822391958453074
",A,1
23128569,2014-04-17 08:48:13.530000+00:00,"This just seems to be a problem with your Javascript. You're getting the value for selectedResultValue when the page is first loaded, presumably when no value is selected. The Ajax request is made when the button is clicked, but you don't fetch the new value inside that click function: so you still use the old, empty value. Just move that line inside the function.",A,0
20477349,2013-12-09 17:56:37.330000+00:00,"Use a list comprehension and any():
[d for d in inputlist if any(d.itervalues())]

Use any(d.values()) in Python 3.
any() only returns True if there are any non-empty values in the input list. By using d.itervalues() we test the minimal number of values in the dictionary to prove there is a non-empty value among them.
Demo:
>>> inputlist = [{'Key1': 'JJ', 'Key2': 'GG', 'Key3':''}, {'Key1': '', 'Key2': '', 'Key3': ''}, {'Key1': '', 'Key2': 'GG', 'Key3': ''}, {'Key1': '', 'Key2': '', 'Key3': ''}]
>>> [d for d in inputlist if any(d.itervalues())]
[{'Key3': '', 'Key2': 'GG', 'Key1': 'JJ'}, {'Key3': '', 'Key2': 'GG', 'Key1': ''}]

If any values other than the empty strings could be tested as false as well (such as None or 0), you can use an explicit test as well:
[d for d in inputlist if any(v != '' for v in d.itervalues())]
",A,5
51584331,2018-07-29 20:34:04.713000+00:00,"The way to optimize this isn't to figure out a faster way to generate the permutations, it's to generate as few permutations as possible.

First, how would you do this if you only wanted the combination that were in sorted order?
You don't need to generate all possible combinations of 0 to 100 and then filter that. The first number, a, can be anywhere from 0 to 100. The second number, b, can be anywhere from 0 to (100-a). The third number, c, can only be 100-a-b. So:
for a in range(0, 101):
    for b in range(0, 101-a):
        c = 100-a-b
        yield a, b, c

Now, instead of generating 100*100*100 combination to filter them down to 100*50*1+1, we're just generating the 100*50*1+1, for a 2000x speedup.
However, keep in mind that there are still around X * (X/2)**N answers. So, computing them in X * (X/2)**N time instead of X**N may be optimal—but it's still exponential time. And there's no way around that; you want an exponential number of results, after all.
You can look for ways to make the first part more concise with itertools.product combined with reduce or accumulate, but I think it's going to end up less readable, and you want to be able to extend to any arbitrary N, and also to get all permutations rather than just the sorted ones. So keep it understandable until you do that, and then look for ways to condense it after you're done.

You obviously need to either go through N steps. I think this is easier to understand with recursion than a loop.
When n is 1, the only combination is (x,).
Otherwise, for each of the values a from 0 to x, you can have that value, together with all of the combinations of n-1 numbers that sum to x-a. So:
def sum_to_x(x, n):
    if n == 1:
        yield (x,)
        return
    for a in range(x+1):
        for result in sum_to_x(x-a, n-1):
            yield (a, *result)


Now you just need to add in the permutations, and you're done:
def perm_sum_to_x(x, n):
    for combi in sum_to_x(x, n):
        yield from itertools.permutations(combi)

But there's one problem: permutations permutes positions, not values. So if you have, say, (100, 0, 0), the six permutations of that are (100, 0, 0), (100, 0, 0), (0, 100, 0), (0, 0, 100), (0, 100, 0), (0, 0, 100).

If N is very small—as it is in your example, with N=3 and X=100—it may be fine to just generate all 6 permutations of each combination and filter them:
def perm_sum_to_x(x, n):
    for combi in sum_to_x(x, n):
        yield from set(itertools.permutations(combi))

… but if N can grow large, we're talking about a lot of wasted work there as well.
There are plenty of good answers here on how to do permutations without repeated values. See this question, for example. Borrowing an implementation from that answer:
def perm_sum_to_x(x, n):
    for combi in sum_to_x(x, n):
        yield from unique_permutations(combi)

Or, if we can drag in SymPy or more-itertools:
def perm_sum_to_x(x, n):
    for combi in sum_to_x(x, n):
        yield from sympy.multiset_permutations(combi)

def perm_sum_to_x(x, n):
    for combi in sum_to_x(x, n):
        yield from more_itertools.distinct_permutations(combi)
",A,5
27035360,2014-11-20 08:55:32.403000+00:00,"You don't show your urls.py, but the problem is probably there: you need to use as_view when mapping a class based view:
url('^register/$', SignupView.as_view(), name='register')
",A,3
5900648,2011-05-05 15:50:13.770000+00:00,"Portlets are registered as utilities with the IPortletType interface with the zope component machinery. These registrations are generated for you when registering portlets with portlets.xml. The portlet management UI then uses these utility registrations to enumerate portlets you can add.
Luckily, plone.portlets.utils provides a handy API to unregister these portlets again:
def unregisterPortletType(site, addview):
    """"""Unregister a portlet type.

    site is the local site where the registration was made. The addview 
    should is used to uniquely identify the portlet.
    """"""

The addview parameter is a string, and is the same as used in a portlet.xml registration. For example, the calendar portlet is registered as:
<portlet
  addview=""portlets.Calendar""
  title=""Calendar portlet""
  description=""A portlet which can render a calendar.""
  i18n:attributes=""title;
                   description""
  >
  <for interface=""plone.app.portlets.interfaces.IColumn"" />
  <for interface=""plone.app.portlets.interfaces.IDashboard"" />
</portlet>

You can thus remove the calendar portlet from your site by running the following code snippet:
from plone.portlets.utils import unregisterPortletType
unregisterPortletType(site, 'portlets.Calendar')

You can also just use the GenericSetup portlets.xml file to remove the portlets during setup time, just list the portlets addview parameter and add a remove attribute to the element:
<?xml version=""1.0""?>
<portlets>
  <portlet addview=""portlets.Calendar"" remove=""true"" />
</portlets>

Thanks to David Glick for finding that one for us.",A,7
3679204,2010-09-09 17:55:17.747000+00:00,".filter( post__gender=1 )
",A,1
18936559,2013-09-21 19:13:07.380000+00:00,"new_location_interfaces is not a file object. It is a string, the result of the raw_input() call:
new_location_interfaces = raw_input()

The next line, the open() call, is not assigned to anything:
open(new_location_interfaces, 'w')

Perhaps you wanted to truncate that object?
For example:
new_location_interfaces = raw_input()
fh = open(new_location_interfaces, 'w')
print ""Truncating/erasing contents . .""
fh.truncate()
print ""Writing contents . .""
fh.write(text_to_copy)
fh.close()

However, opening a file for writing (mode set to w) already truncates the file, your .truncate() calls are entirely redundant.",A,3
12974437,2012-10-19 12:36:44.627000+00:00,"You only have one list (you reset klantgegevens on each loop), and you write that list as if it is a full set of multiple rows.
The CSV module sees that one list as a set of sequences, meaning that each string entry is seen as a sequence of individual characters, and that is what is then written to your csv file:
>>> klantgegevens
['Slever klopt 42', 'Slever ', ['AVB'], 'klopt 42', 'KD2220115', '17', 'Geachte heer Slever', 'De heer T. Slever', 'info@company.com']
>>> list(klantgegevens[0])
['S', 'l', 'e', 'v', 'e', 'r', ' ', 'k', 'l', 'o', 'p', 't', ' ', '4', '2']

You can write each row separately while collecting the klantgegevens lists:
import csv
with open('export.csv', 'w') as f:
    writer = csv.writer(f)
    for k,v in l_final:
       info_rest = v[0][:5]+v[0][5:]
       info_combine = map(operator.itemgetter(5),v)
       uniekid = k
       verz = info_combine
       naam = info_rest[0]
       risicoadr = info_rest[2]
       polisnummer = info_rest[3]
       relatienummer = info_rest[4]
       aanhef = info_rest[6]
       contactpersoon = info_rest[7]
       emailadr = info_rest[8]
       klantgegevens = [uniekid,naam,verz,risicoadr,polisnummer,relatienummer,aanhef,contactpersoon,emailadr,]
       writer.writerow(klantgegevens)

Now the list will be treated as a sequence of columns, writing each line as you complete it.
Alternatively, you'd have to collect each klantgegevens list into a results list:
results = []    
for k,v in l_final:
    # processing
    klantgegevens = [uniekid,naam,verz,risicoadr,polisnummer,relatienummer,aanhef,contactpersoon,emailadr,]
    results.append(klantgegevens)

Then write that list of lists to your CSV file:
import csv
with open('export.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerows(results)
",A,5
32934078,2015-10-04 13:37:14.537000+00:00,"ORMs use special methods on classes for a and b to hook into operators and customise what is produced.
>= for is handled by the object.__ge__() method, while != calls object.__ne__().
Typically, the ORM object used for a returns a new object with the operation applied, allowing you to chain operations, and the fn() function expects such an ORM object and will read the operation status from there.",A,8
29742763,2015-04-20 08:30:21.387000+00:00,"
I don't know why I have so many versions of python, but I am sure when I chose interpreter in Pycharm, it is the one that has numpy installed.

No it isn't.
According to your which, the Python that gets run when you type python python.py at the terminal is going to be /opt/local/bin/python, the MacPorts one.
But according to the error message in PyCharm, the Python that it's running is /Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7, which is not the MacPorts one. (It's probably from a binary installer from Python.org.)
So, that's your problem.
Well, it's just barely possible that you're running the MacPorts Python, but you've somehow set a PYTHONPATH or PYTHONHOME to force it to try to use the Python.org site-packages instead of its own. But in that case, you've got even bigger problems…
Of course your bigger problem is that you have 3 different copies of Python 2.7, and a total of 6 path entires for them, and you don't even know how you got them or which ones' site-packages you're installing into. It would probably be easier to clean things up than to keep trying to muddle through, but it's your choice.",A,2
39437408,2016-09-11 15:01:21.700000+00:00,"Trying to index with [0] into an empty string raises an IndexError.
You need to test if the string is empty first, as eventually S[1:] will pass an empty sting into transcribe():
if not S:
    return ''

You should drop the tran string altogether; that is always an empty string. '' * <any number> is still ''.
Rather than test each possible letter in a separate if statement, use a dictionary to map one character to another:
mapping = {'A': 'U', 'C': 'G', 'G': 'C', 'T': 'A'}
def transcribe(S):
    if not S:
        return ''
    replacement = mapping.get(S[0], '')  # default is empty
    return replacement + transcribe(S[1:])
",A,3
31424568,2015-07-15 07:54:27.020000+00:00,"A response can be anything. If you've posted to a REST endpoint, it will usually respond with JSON. If so, requests will detect that and allow you to decode it via the .json() method.
But it's perfectly possible for you to post to a normal web URL, in effect pretending to be a browser, and unless the server is doing something really clever it will just respond with the standard HTML it would serve to the browser. In that case, doing response.json() will raise a ValueError.",A,5
28613556,2015-02-19 17:59:07.797000+00:00,"For properties with setters and deleters to work properly, your classes need to inherit from object: Why does @foo.setter in Python not work for me?
You can just copy the property object itself over to the new class. It'll hold references to the getter, setter and deleter functions and there is no need to copy those across.
For new-style classes, your code is not working; you cannot assign to a class __dict__ attribute:
>>> item = SetProperties(Item01Object)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 4, in SetProperties
TypeError: 'dictproxy' object does not support item assignment

Use setattr() instead to set attributes on new-style classes:
def SetProperties( ItemObject ):
    for propName, cls in AttrContainer.__dict__.iteritems():
        if propName in ItemObject.properties:
            setattr(ItemObject, propName, cls.__dict__[propName])
    return ItemObject()

Note that the property object is copied across wholesale.
Demo:
>>> class Item01Object(object):
...     properties = [""a"",""c""]
...     ATTR = None
...
>>> def SetProperties( ItemObject ):
...     for propName, cls in AttrContainer.__dict__.iteritems():
...         if propName in ItemObject.properties:
...             setattr(ItemObject, propName, cls.__dict__[propName])
...     return ItemObject()
... 
>>> item = SetProperties(Item01Object)
>>> item
<__main__.Item01Object object at 0x108205850>
>>> item.a
>>> item.a = 20
>>> item.a
23.021

You only have to copy across property objects to the target class once though; that your function returns an instance implies you are planning to use it for all instances created.
I'd make it a decorator instead:
def set_properties(cls):
    for name, propcls in vars(AttrContainer).iteritems():
        if name in cls.properties:
            setattr(cls, name, vars(propcls)[name])
    return cls

then use this on each of your Item*Object classes:
@set_properties
class Item01Object(object):
    properties = [""a"",""c""]
    ATTR = None

@set_properties
class Item02Object(object):
    properties = [""b"",""c""]
    ATTR = None

Demo:
>>> def set_properties(cls):
...     for name, propcls in vars(AttrContainer).iteritems():
...         if name in cls.properties:
...             setattr(cls, name, vars(propcls)[name])
...     return cls
... 
>>> @set_properties
... class Item01Object(object):
...     properties = [""a"",""c""]
...     ATTR = None
... 
>>> @set_properties
... class Item02Object(object):
...     properties = [""b"",""c""]
...     ATTR = None
... 
>>> item01 = Item01Object()
>>> item01.c = 20
>>> item01.c
3
>>> item02 = Item02Object()
>>> item02.b = 42
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: can't set attribute
>>> item02.b
'Something'
",A,2
1580367,2009-10-16 20:56:19.787000+00:00,"The point of Agile is to eliminate all the ""reporting lines"" and pare things down to the essential relationships and nothing more.
Scrum teams are intended to be self-organizing, not have organization imposed on them.",A,3
17601336,2013-07-11 19:02:00.587000+00:00,"This is exactly what you should expect. As the docs say:

On Unix, return the current processor time as a floating point number expressed in seconds. The precision, and in fact the very definition of the meaning of “processor time”, depends on that of the C function of the same name…

POSIX says that the C function:

… shall return the implementation's best approximation to the processor time used by the process since the beginning of an implementation-defined era related only to the process invocation.

In other words, this is processor time, not wall-clock time. If your processor is doing anything else (like, say, running other programs), or waiting around doing nothing while your program blocks on I/O, it isn't counted.
Actually, it's even worse than that—such time may be counted on some Unix and Unix-like systems, and will be counted on Windows… so if you don't understand your platform, it's not necessarily easy to understand what clock actually means.
At any rate, the time reported by clock should be <= the wall-clock time, not ==.",A,2
24140734,2014-06-10 12:15:10.360000+00:00,"Scripts running under py2exe do not have a __file__ global. Detect this and use sys.argv[0] instead:
import os.path

try:
    approot = os.path.dirname(os.path.abspath(__file__))
except NameError:  # We are the main py2exe script, not a module
    import sys
    approot = os.path.dirname(os.path.abspath(sys.argv[0]))
",A,25
35786852,2016-03-04 02:20:40.213000+00:00,"You should prefer 2-lines variant from @Mike Müller's answer:
w, f = line.split(',')
w, f = w.strip(), int(f)

Here's yet another one-liner, not recommended:
[(w, f)] = [(word.strip(), int(n)) for word, n in [line.split(',')]]
",A,0
41480352,2017-01-05 08:30:43.763000+00:00,"The first parameter to dispatch, like all instance methods, should be self. request is passed as a keyword argument. 
Note however that you should not be overriding dispatch. By doing so you are negating all the benefits of using class based views; you might as well use a standalone function. Instead, subclass TemplateView, set template_name as a class attribute, and define get_context_data when you actually need to pass some data to the template.",A,1
25763215,2014-09-10 10:36:00.073000+00:00,"A Profile instance is not a queryset.
You shouldn't be overriding get_queryset, you should be overriding get_object, which returns the specific object you want to display.",A,1
51567115,2018-07-28 00:48:54.043000+00:00,"You don't actually need to read the file as text to replace the image URLs. Those values don't appear anywhere else in the file but in the current column of the current row. So you don't need to call replace on the contents of the whole file, just on the current column of the current row.
This means you can just transform the CSV row by row.
Writing to a temporary file and then moving it over the original is better than overwriting the file in-place. That way, if you, say, trip over the power cord in the middle of a write, you either have the complete original file, or the complete new file, not half the new file and everything else is lost forever. It's also more efficient—we can have both files open at once, and never have to store more than the current row in memory. And it's simpler—no need to worry about any conflicts between two handles to the same file, because you never have two handles to the same file.
While we're at it, you don't even need replace here. Your column 1 is the original images piped together. You split them up into separate strings. You want to replace one of those strings completely with a different string. You can do that without replace—just use the different string instead of the original one.
So, what you want is something like this:
with open(""row_data.csv"") as fin, open(""row_data.csv.tmp"", ""w"") as fout:
    csv_in = csv.reader(fin)
    csv_out = csv.writer(fout)
    for row in csv_in:
        desktop_images_link = row[1]
        desktop_images = desktop_images_link.split('|')
        replacement_images = [hosted_url for desktop_image in desktop_images]
        row[1] = '|'.join(replacement_images)
        csv_out.writerow(row)
os.replace('row_data.csv.tmp', 'row_data.csv')

That's it. 

If you really did need to globally replace the images, because they could appear arbitrarily in other columns of other rows anywhere in the file, you could still improve your code—making it simpler, and also fixing a serious problem with it.
The problem is that, in the middle of a loop over the rows in row_data.csv, you open(""row_data.csv"", ""w"") and overwrite the whole file. You can't do that. What you have to do is one of:

Use a temporary file (as above) that you keep reading and writing, and then move it over the original file after you're done looping.
Read all the rows into memory, and then do the loop.
Read the file contents into memory, then loop over the rows and do all the replaces in the loop, then write it out until the end.
Build up a list of replacements as you loop over the file, and then apply them all at the end.

The last one is probably the simplest, and (except for the first) the most efficient.
While we're at it, you should be using with statements to make sure all of your file objects get closed.
So:
all_desktop_images = []
with open(""row_data.csv"") as f:
    csv_f = csv.reader(f)
    for row in csv_f:
        desktop_images_link = row[1]
        desktop_images = desktop_images_link.split('|')
        all_desktop_images.extend(desktop_images)

with open(""row_data.csv"") as f:
    data = f.read()
for desktop_image in all_desktop_images:
    data = data.replace(desktop_image, hosted_url)
with open(""row_data.csv"", ""w"") as f:
    f.write(data)

We still have the same number of calls to open in the source code, but now each one of them only executes once, instead of opening and reopening (and overwriting) the file over and over again, and we only ever have one of them open at a time, instead of having all three open at once.",A,1
18203677,2013-08-13 08:00:12.377000+00:00,"dict.popitem() is not the same thing as dict.iteritems(); it removes one pair from the dictionary as a tuple, and you are looping over that pair.
The most efficient method is to use a while loop instead; no need to call len(), just test against the dictionary itself, an empty dictionary is considered false:
while d:
    key, value = d.popitem()
    print key, value

The alternative is to use reversed():
for key, item in reversed(d.items()):
    print key, value

but that requires the whole dictionary to be copied into a list first.
However, if you were looking for a FIFO queue, use collections.deque() instead:
from collections import deque

d = deque([""a0.csf"", ""b1.csf"", ""c2.csf""])

while d:
    item = d.pop()

or use deque.reverse().",A,2
51971989,2018-08-22 17:11:36.810000+00:00,"First, you really are looking at building exactly what io.BytesIO already does. It’s a file-like object that’s stored entirely in memory. Each process’s objects are completely independent of every other process’s objects. It just is everything you want. But it isn’t going to do you any good here. The fact that it’s a file-like object doesn’t mean that it’s accessible from other processes. In fact, that’s the whole point of file-like objects that aren’t files: they aren’t files.
But you could just explicitly lock your files.
It’s true that, other than Windows, most operating systems don’t automatically lock files, and some don’t even have “mandatory” locks, only cooperative locks that don’t actually protect files unless all of the programs are written to use the locks. But that’s not a problem.
One option is to write separate code for Windows and Unix: On Windows, rely on opening the files in exclusive mode; on Unix, use flock.
The other option is to create manual lockfiles. You can atomically try to create a file and fail if someone else did it first on every platform by just using os.open with the O_CREAT|O_EXCL flags, and you can build everything else you need on top of that.

If you’re thinking about using shared memory, unless you’re using multiprocessing, it’s pretty painful to do that in a cross-platform way.
But you can get the same effect by using a regular file and using mmap in each process to access the file as if it were normal memory. Just make sure to only use the cross-platform values for length and access (and not to use platform-specific parameters like prot or flags) and it works the same way everywhere.
Of course you can’t put Python objects into either shared memory or an mmap, but you can put raw bytes, or “native values”, or arrays of them, or ctypes Structures of them, or, best of all, multidimensional numpy arrays of them. For all but the last one, you can use the appropriate wrapper objects out of multiprocessing even if you aren’t otherwise using the module. For the last one, just use np.memmap instead of using mmap directly, and it takes care of everything.

However, you may be right about a queue being faster. If that really is a concern (although I’d actually build and test it to see whether that’s a problem before solving it…), then go for it. But you seem to have some misconceptions there.
First, I don’t know why you think a queue has anything to do with appending to pandas DataFrames. I suppose you could use a df as a queue, but there’s no intrinsic connection between the two.
Meanwhile, a list is fine for a small queue, but for a very large one, it’s not. Either you append to the right and pop from the left, or you append to the left and pop from the right. Either way, that operation on the left takes time linear in the size of the queue, because you have to shift the whole rest of be list left or right by one slot. The solution to this is collections.deque, an object that’s almost the same as a list, except that it can insert or delete in constant time on both sides, instead of just the right side.
But again, that doesn’t solve anything because it’s not actually shared in any way. You need some kind of interprocess queue, and neither a DataFrame nor a list (nor a deque) helps there.
You can build an interprocess queue on top of a pipe. Depending on how your processes are run, this could be an anonymous pipe, where the launcher program hands an end of the pipe to the child program, or this could be a named pipe, which is slightly different on Windows vs. Unix, but in both cases it works by both programs having some globally-known name (like a filesystem path) to use to open the same pipe.
You can also build an interprocess queue on top of a TCP socket. If you bind to localhost and connect to localhost, this is almost as efficient as a pipe, but it’s simpler to write cross-platform.
So, how do you build a queue on top of a pipe or socket? The only problem is that you have just a stream of bytes instead of a stream of messages. 

If your messages are all the same size, you just sendall on one side, and recv in a loop until you have MESSAGESIZE bytes.
If they’re in some self-delimiting format like pickle, there’s no problem; just sendall on one side, and recv until you have a complete pickle on the other side. You can even use socket.makefile (only for sockets, not pipes, of course) to get a file-like object you can pass straight to pickle.dump‘ andpickle.load`. 
You can use some kind of delimiter (e.g., if your messages are text that can never include a newline, or can never include a NUL byte, you can just use newline or 0 as a delimiter—and if you use newline, makefile takes care of this for you again).
Or you can send the size of each message before the message itself (e.g., using a trivial protocol like netstring).

If you are (or could be) using the multiprocessing library to control all of your separate processes, it comes with a Queue class built in, which builds an IPC queue on top of sending pickles over a pipe in an efficient way for every major platform, but you don’t have to worry about how it works; you just hand the queue off to your child processes, and you can put on one end and get on the other and it just works.",A,3
42467036,2017-02-26 09:55:57.007000+00:00,"You declare form fields when defining your form. They are what determine the types and names of the fields, what widgets they use, and what validation they run.
You never create bound fields directly; they are produced - via that method - when you instantiate a form and iterate over its fields. They deal with displaying the field and populating it with any values. The BoundField has an attribute .field which contains the form field. The relationship is one of composition, not inheritance.",A,1
27454134,2014-12-13 00:17:28.497000+00:00,"A one-item tuple is defined by the comma, not the parentheses:
>>> 1,
(1,)
>>> (1)
1

Parentheses are just used to delineate a tuple when commas could mean something else. 
See Parenthesized Forms in the expressions documentation:

A parenthesized expression list yields whatever that expression list yields: if the list contains at least one comma, it yields a tuple; otherwise, it yields the single expression that makes up the expression list.

So use:
if not isinstance(i, tuple):
    i = i,

note the use of isinstance() as well to test for the type. ",A,3
15442600,2013-03-15 21:45:09.927000+00:00,"Just loop over the first list to build a dictionary with a dict comprehension:
aDict = {(e[0], e[1]): (e[2], e[3]) for e in aList[1:]}

Next loop over the other, with a helper function to pick one or the other based on the - values:
def pick(vals):
    return vals[1] if vals[0] == '-' else vals[0]

for e in newList:
    key = (e[0], e[1])
    existing = aDict.get(key, ('-', '-'))
    value = (e[2], e[3])
    aDict[key] = tuple(map(pick, zip(existing, value)))

For your first input that results in:
{('Arreguin', 'Jeffrey'): ('81000', '-'),
 ('Bradley', 'Shannon'): ('89000', '-'),
 ('Daniels', 'Christina'): ('85000', '137000'),
 ('Edwards', 'Kathryn'): ('117000', '97000'),
 ('Fix', 'Anna'): ('76000', '126000'),
 ('Franklin', 'Claude'): ('94000', '-'),
 ('Graham', 'Veronica'): ('-', '136000'),
 ('Harper', 'Crystal'): ('80000', '-'),
 ('Hedrick', 'James'): ('-', '105000'),
 ('Hullinger', 'Molly'): ('70000', '-'),
 ('Last Name', 'First Name'): ('2000', '2012'),
 ('Meyer', 'Loretta'): ('123000', '116000'),
 ('Mielke', 'George'): ('137000', '-'),
 ('Myers', 'George'): ('-', '86000'),
 ('Oneal', 'Kevin'): ('96000', '77000'),
 ('Paz', 'Barbara'): ('-', '77000'),
 ('Roberts', 'Gloria'): ('-', '123000'),
 ('Thigpen', 'Michael'): ('79000', '-'),
 ('Thomas', 'Lewis'): ('132000', '-'),
 ('Willis', 'Mabel'): ('112000', '-'),
 ('Young', 'Gary'): ('-', '94000')}

All put together again in your merge function:
def pick(vals):
    return vals[1] if vals[0] == '-' else vals[0]

def mergeData(newList, aList):
    aDict = {(e[0], e[1]): (e[2], e[3]) for e in aList[1:]}

    for e in newList:
        key = (e[0], e[1])
        existing = aDict.get(key, ('-', '-'))
        value = (e[2], e[3])
        aDict[key] = tuple(map(pick, zip(existing, value)))

    return aDict
",A,1
7967574,2011-11-01 13:59:11.680000+00:00,"How could this work? What event date could it be referring to? When you call Event.past_school_year_events.all(), you don't have a model instance. self.model, as the name implies, refers to the model class. So how could it know what date you mean?
I can't actually work out what it is you're trying to do - where you are going to get the date from. Do you want to start from an event, then get all the other events in the same year as that one? In which case, I suspect you just want to make past_school_year_events into a model method, not a manager.
Edit If you want to start from a specific year, a manager certainly is appropriate, but you'll need to pass the year into your manager method as a parameter. For this, I'd use a separate method on the manager, rather than overriding get_query_set - in fact, add another method to SchoolYearManager:
class SchoolYearManager(models.Manager):
    def live_events(self, start_date, end_date):
       return self.filter(status=self.model.LIVE).filter(event_date__range=(start_date, end_date))

    def this_year(self):
        now = datetime.datetime.now()
        current_year = now.year
        start_date = datetime.date(current_year, 7, 1)
        end_date = datetime.date((current_year + 1), 6, 30)
        return self.live_events(start_date, end_date)

    def from_year(self, year):
        start_date = datetime.date(year, 7, 1)
        end_date = datetime.date(year+1, 6, 30)
        return self.live_events(start_date, end_date)

Now you've got just one manager, that doesn't override get_query_set, so you don't even need to preserve the default one - you can keep this one as objects. So you can do:
Event.objects.this_year()  # all this year's events
Event.objects.from_year(2010) # all events from 2010
",A,1
16446101,2013-05-08 16:52:14.340000+00:00,"You are misunderstanding how or works. Use:
if move_upper in ('X', 'R'):

instead.
The expression move_upper == 'X' or 'R' is interpreted as (move_upper == 'X') or 'R' instead, and an non-empty string is always considered True. Thus, you were in essence testing for (move_upper == 'X') or True, and then it doesn't matter anymore what move_upper is, really.
You don't really need to use a done flag variable here; use break to end the loop instead:
while True:
    # ...

    if move_upper in ('X', 'R'):
        break
",A,3
29577507,2015-04-11 11:48:08.530000+00:00,"2((-2**a)/(2**a)+2) is trying to use that first 2 as a function. You are asking Python to call 2() by passing in the result of the (-2**a)/(2**a)+2 expression, and that doesn't work:
>>> 2()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'int' object is not callable

Perhaps you were forgetting to use a * multiplication operator there:
2 * ((-2 ** a) / (2 ** a) + 2)

Your UnboundLocal error stems from you removing the b = 0 line, which was not the cause of your original error.",A,2
13365490,2012-11-13 17:19:10.750000+00:00,"You loop over each entry in the dict, and match the value, and if it doesn't match, then for each key-value pair you print that it doesn't match.
It is the equivalent of the following simplified loop:
>>> for i in range(3):
...     if i == 5:
...         print(i)
...     else:
...         print('Not 5')
... 
Not 5
Not 5
Not 5

Use the else: clause of the for loop instead, it'll only be invoked if you completed going through all the values; use a break if you find a match:
for ol_man, kid in paternal_pairs.items():
    if temp_son == kid:
        print(""\nThe grandfather of"", son, ""is"", ol_man)
        break
else:
    print(""\nNo grandfather listed for"", son)

A small demonstration of how the else: clause works when used with a for loop:
>>> for i in range(3):
...     if i == 1:
...         print(i)
...         break
... else:
...     print('Through')
... 
1
>>> for i in range(3):
...     if i == 5:
...         print(i)
...         break
... else:
...     print('Through')
... 
Through

In the first example, we broke out of the loop with a break, but in the second example we never reached the break statement (i never was equal to 5) so the else: clause is reached and Through is printed.",A,3
16527803,2013-05-13 17:31:02.797000+00:00,"Your two cases are quite different; in the first you have a callable (str.format) in the second you built a complete string.
You'd need to create a callable for the second option too; in this case a lambda would work:
fList=[lambda t: ""<td>{}</td>"".format(time.strftime(""%a %H&#58;%M %d %b %y"", time.localtime(t)))]

This is now a list with one callable, a lambda that accepts one argument t, and returns the result of the full expression where t is passed to time.localtime() then formatted using time.strftime then passed to str.format().
Demo:
>>> import time
>>> vList=[1236745404]
>>> fList=[lambda t: ""<td>{}</td>"".format(time.strftime(""%a %H&#58;%M %d %b %y"", time.localtime(t)))]
>>> [f(x) for f, x in zip(fList, vList)]
['<td>Wed 05&#58;23 11 Mar 09</td>']
",A,4
15742362,2013-04-01 11:13:45.457000+00:00,"The information is URL encoded, so use urllib2.unquote to decode that.
>>> input = '''\
... https://www.mysite.com/myworks/accaply/inquiry.asp 
... http://www.peoplesmonton.com/amb/cgi-bin/bank/bank/ambt%20Bank%20Of%20Frnak%20PLC_asp.htm 
... http://www.peoplesmonton.com/comblk/cgi-bin/bank/bank/ambt%20Bank%20Of%20ambt%20PLC_asp.htm 
... '''
>>> import urllib2
>>> print urllib2.unquote(input)
https://www.mysite.com/myworks/accaply/inquiry.asp 
http://www.peoplesmonton.com/amb/cgi-bin/bank/bank/ambt Bank Of Frnak PLC_asp.htm 
http://www.peoplesmonton.com/comblk/cgi-bin/bank/bank/ambt Bank Of ambt PLC_asp.htm 
",A,2
107836,2008-09-20 10:27:39.860000+00:00,"A Python file is called a ""module"" and it's one way to organize your software so that it makes ""sense"".  Another is a directory, called a ""package"".
A module is a distinct thing that may have one or two dozen closely-related classes.  The trick is that a module is something you'll import, and you need that import to be perfectly sensible to people who will read, maintain and extend your software.
The rule is this: a module is the unit of reuse.  
You can't easily reuse a single class.  You should be able to reuse a module without any difficulties.  Everything in your library (and everything you download and add) is either a module or a package of modules.
For example, you're working on something that reads spreadsheets, does some calculations and loads the results into a database.  What do you want your main program to look like?
from ssReader import Reader
from theCalcs import ACalc, AnotherCalc
from theDB import Loader

def main( sourceFileName ):
    rdr= Reader( sourceFileName )
    c1= ACalc( options )
    c2= AnotherCalc( options )
    ldr= Loader( parameters )
    for myObj in rdr.readAll():
        c1.thisOp( myObj )
        c2.thatOp( myObj )
        ldr.laod( myObj )

Think of the import as the way to organize your code in concepts or chunks.  Exactly how many classes are in each import doesn't matter.  What matters is the overall organization that you're portraying with your import statements.",A,298
18184710,2013-08-12 10:28:16.370000+00:00,"A list comp will do:
['{{{0[score]}-{0[numrep]}}}'.format(d) for d in lst]

This outputs a list of strings, so with quotes:
['{7-0}', '{2-0}', '{9-0}', '{2-0}']

We can format that a little more:
'[{}]'.format(', '.join(['{{{0[score]}-{0[numrep]}}}'.format(d) for d in lst]))

Demo:
>>> print ['{{{0[score]}-{0[numrep]}}}'.format(d) for d in lst]
['{7-0}', '{2-0}', '{9-0}', '{2-0}']
>>> print '[{}]'.format(', '.join(['{{{0[score]}-{0[numrep]}}}'.format(d) for d in lst]))
[{7-0}, {2-0}, {9-0}, {2-0}]

Alternative methods of formatting the string to avoid the excessive {{ and }} curl brace escaping:

using old-style % formatting:
'{%(score)s-%(numrep)s}' % d

using a string.Template() object:
from string import Template

f = Template('{$score-$numrep}')

f.substitute(d)


Further demos:
>>> print '[{}]'.format(', '.join(['{%(score)s-%(numrep)s}' % d for d in lst]))
[{7-0}, {2-0}, {9-0}, {2-0}]
>>> from string import Template
>>> f = Template('{$score-$numrep}')
>>> print '[{}]'.format(', '.join([f.substitute(d) for d in lst]))
[{7-0}, {2-0}, {9-0}, {2-0}]
",A,2
12760806,2012-10-06 14:46:38.360000+00:00,"pexpect raises an exception on timeout (in 30 seconds by default). If you don't want an exception; specify pexpect.TIMEOUT as a pattern for .expect().
In your example git clone produces pexpect.Timeout exception after default timeout on .expect(pexpect.EOF). Try:
i = child.expect([EOF, TIMEOUT], timeout=git_clone_timeout)
if i == 1: 
    # handle timeout
",A,3
51832123,2018-08-14 00:15:43.217000+00:00,"The problem is that you've applied a decorator designed for functions to a class. The result is not a class, but a function that wraps up a call to the class. This causes a number of problems (e.g., as pointed out by Aran-Fey in the comments, you can't isinstance(feat, mystery), because mystery). 
But the particular problem you care about is that you can't pickle instances of inaccessible classes.
In fact, that's basically what the error message is telling you:
PicklingError: Can't pickle <class '__main__.mystery'>: it's not the 
same object as __main__.mystery

Your feat thinks its type is __main__.mystery, but that isn't a type at all, it's the function returned by the decorator that wraps that type.

The easy way to fix this would be to find a class decorator meant that does what you want. It might be called something like flyweight instead of memoize, but I'm sure plenty of examples exist.

But you can build a flyweight class by just memoizing the constructor, instead of memoizing the class:
class mystery:
    @funcy.memoize
    def __new__(cls, num):
        return super().__new__(cls)
    def __init__(self, num):
        self.num = num

… although you probably want to move the initialization into the constructor in that case. Otherwise, calling mystery(1) and then mystery(1) will return the same object as before, but also reinitialize it with self.num = 1, which is at best wasteful, and at worst incorrect. So:
class mystery:
    @funcy.memoize
    def __new__(cls, num):
        self = super().__new__(cls)
        self.num = num
        return self


And now:
>>> feat = mystery(1)
>>> feat
<__main__.mystery at 0x10eeb1278>
>>> mystery(2)
<__main__.mystery at 0x10eeb2c18>
>>> mystery(1)
<__main__.mystery at 0x10eeb1278>

And, because the type of feat is now a class that's accessible under the module-global name mystery, pickle will have no problem with it at all:
>>> pickle.dumps(feat)
b'\x80\x03c__main__\nmystery\nq\x00)\x81q\x01}q\x02X\x03\x00\x00\x00numq\x03K\x01sb.'


You do still want to think about how this class should play with pickling. In particular, do you want unpickling to go through the cache? By default, it doesn't:
>>> pickle.loads(pickle.dumps(feat)) is feat
False

What's happening is that it's using the default __reduce_ex__ for pickling, which defaults to doing the equivalent of (only slightly oversimplified):
result = object.__new__(__main__.mystery)
result.__dict__.update({'num': 1})

If you want it to go through the cache, the simplest solution is this:
class mystery:
    @funcy.memoize
    def __new__(cls, num):
        self = super().__new__(cls)
        self.num = num
        return self
    def __reduce__(self):
        return (type(self), (self.num,))


If you plan to do this a lot, you might think of writing your own class decorator:
def memoclass(cls):
    @funcy.memoize
    def __new__(cls, *args, **kwargs):
        return super(cls, cls).__new__(cls)
    cls.__new__ = __new__
    return cls

But this:

… is kind of ugly,
… only works with classes that don't need to pass constructor arguments to a base class,
… only works with classes that don't have an __init__ (or, at least, that have an idempotent and fast __init__ that's harmless to call repeatedly),
… doesn't provide an easy way to hook pickling, and
… doesn't document or test any of those restrictions.

So, I think you're better off being explicit and just memoizing the __new__ method, or writing (or finding) something a lot fancier that does the introspection needed to make memoizing a class this way fully general. (Or, alternatively, maybe write one that only works with some restricted set of classes—e.g., a @memodataclass that's just like @dataclass but with a memoized constructor would be a lot easier than a fully general @memoclass.)",A,6
16107626,2013-04-19 15:01:30.570000+00:00,This is what virtualenv is for.,A,1
28812150,2015-03-02 14:24:59.697000+00:00,"Your invocation of super is completely wrong, because I think you have a misconception about inheritance.
A subclass is also a member of its superclass. You don't ask the superclass for an attribute; since you've inherited from it, you gain all the same attributes.
In Python you do need to explicitly call the superclass initialization method:
def __init__(self):
    super().__init__(self)

which is usually responsible for setting the values of the shared properties.
However, there is a more fundamental issue with your code. You never pass any filename attribute when you raise the exception, so there is no way either the subclass or the superclass could possibly report it. You probably need to do this:
def __init__(self, filename):
    super().__init__(self, filename)

and
raise ConfigFileNotFoundError(my_filename)
",A,1
40639908,2016-11-16 18:45:44.627000+00:00,"This is not the right approach.
PRs are not a git concept; they are a Github one. You can't use gitpython for this. You will have to use Github's own API, for which there are several third-party libraries.",A,0
11718113,2012-07-30 08:50:57.927000+00:00,"I've built a similar system; it's called collective.transmogrifier. One of these days I'll make it more generic (it is currently tied to the CMF, one of the underpinnings of Plone).
Decoupling
What you need, is a way to decouple the component registration for your pipeline. In Transmogrifier, I use the Zope Component Architucture (embodied in the zope.component package). The ZCA lets me register components that implement a given interface and later look up those components as either a sequence or by name. There are other ways of doing this too, for example, python eggs have the concept of entry points.
The point is more that each component in the pipeline is referable by a text-only name, de-referenced at construction time. 3rd-party components can be slotted in for re-use by registering their own components independently from your pipeline package.
Configuration
Transmogrifier pipelines are configured using a textual format based on the python ConfigParser module, where different components of the pipeline are named, configured, and slotted together. When constructing the pipeline, each section thus is given a configuration object. Sections don't have to look up configuration centrally, the are configured on instantiation.
Central state
I also pass in a central 'transmogrifier' instance, which represents the pipeline. If any component needs to share per-pipeline state (such as caching a database connection for re-use between components), they can do so on that central instance. So in my case, each section does have a reference to the central pipeline.
Individual components and behaviour
Transmogrifier pipeline components are generators, that consume elements from a preceding component in the pipeline, then yield the results of their own processing. Components generally thus have a reference to the previous stage, but have no knowledge of what consumes their output. I say 'generally' because in Transmogrifier some pipeline elements can produce elements from an external source instead of using a previous element.
If you do need to alter the behaviour of a pipeline component based on individual items to be processed, mark those items themselves with extra information for each component to discover. In Transmogrifier, items are dictionaries, and you can add extra keys to a dictionary that use the name of a component so each component can look for this extra info and alter behaviour as needed.
Summary

Decouple your pipeline components by using an indirect lookup of elements based on a configuration.
When you instantiate your components, configure them at the same time and give them what they need to do their job. That could include a central object to keep track of pipeline-specific state.
When running your pipeline, only pass through items to process, and let each component base it's behaviour on that individual item only.
",A,9
17210441,2013-06-20 09:45:17.187000+00:00,"The time module does not support the %f millisecond formatter because the time.struct_time tuple doesn't support milliseconds.
The datetime module does support milliseconds. Use that module instead:
import datetime

datetime.datetime.utcfromtimestamp(60.5).strftime('%H:%M:%S.%f')

Demo:
>>> datetime.datetime.utcfromtimestamp(60.5).strftime('%H:%M:%S.%f')
'00:01:00.500000'
",A,2
18156903,2013-08-09 22:59:46.793000+00:00,"If we can assume that your input doesn't do any quoting or escaping (your example doesn't, but that doesn't necessarily mean it's a good assumption), and that you can never have comma-separated multiple keys, just multiple values (which probably is a good assumption, because otherwise the format is ambiguous…):
First, let's drop the braces, then split on colons:
>>> A = ""{user_id:34dd833,category:secondary,items:camera,vcr,dvd,type:sg_ser}""
>>> A[1:-1].split(':')
['user_id', '34dd833,category', 'secondary,items', 'camera,vcr,dvd,type', 'sg_ser']

So the first entry is the first key, the last entry is the last value(s), and every entry in between is the Nth value(s) followed by a comma followed by the N+1th key. There may be other commas there, but the last one always splits the Nth value(s) from the N+1th key. (And that even works for N=0—there are no commas, so the last comma splits nothing from the 0th key. But it doesn't work for the very last entry, unfortunately. I'll get to that later.)
There are ways we could make this brief, but let's write that out explicitly as code first, so you understand how it works.
>>> d = {}
>>> entries = A[1:-1].split(':')
>>> for i in range(len(entries)-1):
...     key = entries[i].rpartition(',')[-1]
...     value = entries[i+1].rpartition(',')[0]
...     d[key] = value

This is almost right:
>>> d
{'category': 'secondary', 'items': 'camera,vcr,dvd', 'type': '', 'user_id': '34dd833'}

As mentioned above, it doesn't work for the last one. It should be obvious why; if not, see what rpartition(',') returns for the last value. You can patch that up manually, or just cheat by packing an extra , on the end (entries = (A[1:-1] + ',').split(':')). But if you think about it, if you just rsplit instead of rpartition, then [0] does the right thing. So let's do that instead.
So, how can we clean this up a bit?
First let's transform entries into a list of adjacent pairs. Now, each for each pair (n, nplus1), n.rpartition(',')[-1] is the key, and nplus1.rsplit(',', 1)[0] is the corresponding value. So:
>>> A = ""{user_id:34dd833,category:secondary,items:camera,vcr,dvd,type:sg_ser}""
>>> entries = A[1:-1].split(':')
>>> adjpairs = zip(entries, entries[1:])
>>> d = {k.rpartition(',')[-1]: v.rsplit(',', 1)[0] for k, v in adjpairs}
",A,7
16105641,2013-04-19 13:21:22.603000+00:00,"You should really use r.content, not r.text when working with StringIO here. 
r.text gives you unicode, r.content gives you raw bytes. Next, there is no need to use StringIO.StringIO, etree.fromstring(html) will work just fine. Better yet, pass in the raw response to .parse():
tree = etree.parse(r.raw, parser)

and have the parser read the response directly.
If you stil get an IndexError exception, then your XPath expression did not match anything in the page. You'll have to save the HTML instead and analyze it manually to see what is different about that page.",A,3
15865945,2013-04-07 18:11:22.733000+00:00,"Just cast the key to str() and enumerate():
d = {str(i): value for i, value in enumerate(targets)}

or use formatting if you need to prepend the keys with a string:
d = {'column{}'.format(i): value for i, value in enumerate(targets)}

The key and value expressions in a dict comprehension are just that, python expressions.",A,5
23502935,2014-05-06 19:06:34.667000+00:00,"You could try unidecode to convert Unicode into ascii instead of writing manual regular expressions. It is a Python port of Text::Unidecode Perl module:
#!/usr/bin/env python
import fileinput
import locale
from contextlib import closing
from unidecode import unidecode # $ pip install unidecode

def toascii(files=None, encoding=None, bufsize=-1):
    if encoding is None:
        encoding = locale.getpreferredencoding(False)
    with closing(fileinput.FileInput(files=files, bufsize=bufsize)) as file:
        for line in file: 
            print unidecode(line.decode(encoding)),

if __name__ == ""__main__"":
    import sys
    toascii(encoding=sys.argv.pop(1) if len(sys.argv) > 1 else None)

It uses FileInput class to avoid global state.
Example:
$ echo 'äöüß' | python toascii.py utf-8
aouss
",A,3
20337020,2013-12-02 20:17:27.030000+00:00,"When you say ""I am pondering over Request/Response methods of Scrapy"", I think you're a little confused. Request and Response are classes, not methods.
And, while these classes of course do have methods, you don't ask about any of them, you just ask about one of the data attributes, meta.
Or, if you meant methods in the HTTP sense, well, a Request defaults to GET, but you can specify a different one with the method= argument; what else is there to know?

As the documentation for Request says:

A Request object represents an HTTP request, which is usually generated in the Spider and executed by the Downloader, and thus generating a Response.

In other words, you usually don't want to create one yourself. You give the Spider a list of URLs to start with, and it makes a Request for each URL on the list, and for each additional URL that it discovers while scraping.
You may sometimes need to look at the Request that goes with a Response. And you may occasionally need to customize the creation of Requests inside a complex Spider. But you will rarely need to craft them manually.

Meanwhile, you seem to have confused yourself with your naming. You've got a variable named start_urls, but it's not a list of URLs, it's a single URL. Which, if you actually used it as a start_urls in the normal way, would be treated as a list of single characters. But fortunately, you're not doing that; you're passing start_urls as the url argument to a Request object—and, since it happens to be just a single URL, your two confusions cancel out and you create a valid Request object.
You could then feed this Request object to a Downloader manually to get back a Response object. But there's no good reason to do that. If you want to download files manually instead of spidering them, just use the stdlib module urllib2/urllib.request, or a third-party library specifically designed for making manual downloading easy, like requests, not a spidering library.

Meanwhile, depth= request.meta['depth'] will just return a KeyError. As the meta docs explain, it's just a dictionary, which is empty for new Requests (unless you pass a meta= argument to the constructor, which you didn't), and:

… is usually populated by different Scrapy components (extensions, middlewares, etc). So the data contained in this dict depends on the extensions you have enabled.

Since you haven't actually done anything with the Request after creation, it doesn't matter what extensions you have enabled; it's going to be empty.",A,1
23521810,2014-05-07 15:18:54.807000+00:00,"Something like this would show a nested <ol> structure for your nested objects:
<ol>
  {% for project in P %}
  <li> Project: {{ project.name }}

    <ol type=""A"">
      {% for pgoal in project.goals.all() %}
      <li>Goal: {{ pgoal.goal }}

        <ol type=""a"">
          {% for pstrat in pgoal.strategies.all() %}
          <li> Strategy: {{ pstrat.strategy }}

            <ol type=""1"">
              {% for ptask in pstrat.tasks.all() %}
              <li>Task: {{ ptask.task }} complete? {{ptask.complete}}</li>
              {% endfor %}
            </ol>

           </li>
           {% endfor %}
         </ol>

       </li>
       {% endfor %}

    </ol>

  </li>
  {% endfor %}
</ol>
",A,1
37058144,2016-05-05 18:52:00.167000+00:00,"To strip comments, you could use str.partition() which works whether the comment sign is present or not in the line:
for line in file:
    line, _, comment = line.partition('#')
    if line.strip(): # non-blank line
        key, value = line.split()

line.split() may raise an exception in this code too—it happens if there is a non-blank line that does not contain exactly two whitespace-separated words—it is application depended what you want to do in this case (ignore such lines, print warning, etc).",A,0
19499264,2013-10-21 15:56:30.653000+00:00,Unittest methods need to begin with the word test (not end with it). Your method should be called test_from_all_products_html404.,A,1
34353754,2015-12-18 10:43:32.743000+00:00,"Like the error says, you're passing three arguments to form_valid; that's because you explicitly overrode post to send those three, and modified the signature of form_valid in FormsetMixin to accept them. However in EmergencyLightingCreate you've reverted to only accepting two arguments. You need to be consistent about how many arguments your methods accept when you're subclassing.",A,1
20847591,2013-12-30 21:07:02.933000+00:00,"
Using threads for each session; I suspect with the GIL this will make poor use of available cores

Is your code actually CPU-bound?* If it spends all its time waiting on I/O, then the GIL doesn't matter at all.** So there's absolutely no reason to use processes, or a GIL-less Python implementation.

Of course if your code is CPU-bound, then you should definitely use processes or a GIL-less implementation. But in that case, you're really only going to be able to efficiently handle N clients at a time with N CPUs, which is a very different problem than the one you're describing. Having 10000 users all fighting to run CPU-bound code on 8 cores is just going to frustrate all of them. The only way to solve that is to only handle, say, 8 or 32 at a time, which means the whole ""10000 simultaneous connections"" problem doesn't even arise.
So, I'll assume your code I/O-bound and your problem is a sensible and solvable one.

There are other reasons threads can be limiting. In particular, if you want to handle 10000 simultaneous clients, your platform probably can't run 10000 simultaneous threads (or can't switch between them efficiently), so this will not work. But in that case, processes usually won't help either (in fact, on some platforms, they'll just make things a lot worse).
For that, you need to use some kind of asynchronous networking—either a proactor (a small thread pool and I/O completion), or a reactor (a single-threaded event loop around an I/O readiness multiplexer). The Socket Programming HOWTO in the Python docs shows how to do this with select; doing it with more powerful mechanisms is a bit more complicated, and a lot more platform-specific, but not that much harder.
However, there are libraries that make this a lot easier. Python 3.4 comes with asyncio,*** which lets you abstract all the obnoxious details out and just write protocols that talk to transports via coroutines. Under the covers, there's either a reactor or a proactor (and a good one for each platform), without you having to worry about it.
If you can't wait for 3.4 to be finalized, or want to use something that's less-bleeding-edge, there are popular third-party frameworks like Twisted, which have other advantages as well.****
Or, if you prefer to think in a threaded paradigm, you can use a library like gevent, while uses greenlets to fake a bunch of threads on a single socket on top of a reactor.

From your comments, it sounds like you really have two problems:
First, you need to handle 10000 connections that are mostly sitting around doing nothing. The actual scheduling and multiplexing of 10000 connections is itself a major I/O bound if you try to do it with something like select, and as I said about, running 10000 threads or processes is not going to work. So, you need a good proactor or reactor for your platform, which is all described above.
Second, a few of those connections will be alive at a time.
First, for simplicity, let's assume it's all CPU-bound. So you will want processes. In particular, you want a pool of N processes, where N is the number of cores. Which you do by just creating a concurrent.futures.ProcessPoolExecutor() or multiprocessing.Pool().
But you claim they're doing a mix of CPU-bound and I/O-bound work. If all the tasks spend, say, 1/4th of their time burning CPU, use 4N processes instead. There's a bit of wasted overhead in context switching, but you're unlikely to notice it. You can get N as n = multiprocessing.cpu_count(); then use ProcessPoolExecutor(4*n) or Pool(4*n). If they're not that consistent or predictable, you can still almost always pretend they are—measure average CPU time over a bunch of tasks, and use n/avg. You can fudge this up or down depending on whether you're more concerned with maximizing peak performance or typical performance, but it's just one knob to twiddle, and you can just twiddle it empirically.
And that's it.*****

* … and in Python or in C extensions that don't release the GIL. If you're using, e.g., NumPy, it will do much of its slow work without holding the GIL.
** Well, it matters before Python 3.2. But hopefully if you're already using 3.x you can upgrade to 3.2+.
*** There's also asyncore and its friend asynchat, which have been in the stdlib for decades, but you're better off just ignoring them.
**** For example, frameworks like Twisted are chock full of protocol implementations and wrappers and adaptors and so on to tie all kinds of other functionality in without having to write a mess of complicated code yourself.
***** What if it really isn't good enough, and the task switching overhead or the idleness when all of your tasks happen to be I/O-waiting at the same time kills performance? Well, those are both very unlikely except in specific kinds of apps. If it happens, you will need to either break your tasks up to separate out the actual CPU-bound subtasks from the I/O-bound, or write some kind of application-specific adaptive load balancer.",A,2
51605509,2018-07-31 04:34:57.767000+00:00,"I think what you're trying to do with your if test is this:
if not float(num_questions):

In other words, if float(num_questions) is zero—and therefore falsey—you want to skip the code with the / float(num_questions) in it, which would be illegal. It doesn't matter what correct_questions is here. After all, dividing 0 / 3 isn't a division by zero; it's fine.
What you actually wrote is a completely different test:
if float(num_questions) and float(correct_questions) is None:

You're testing that float(num_questions) is truthy—the exact opposite of what you want—and also that float(correct_questions) is None—which can never be true, because only None is None. So, you'll always hit the else. And therefore, you'll get a division by zero if the num_questions value is zero.
And your attempted fix doesn't make much difference:
if float(num_questions) or float(correct_questions) == 0.0:

You're testing that num_questions is truthy, which is still the opposite of what you want, or that correct_questions is 0. The only way the first part can be false is if there are no questions—in which case there are no correct questions, so the second part will be true. So now, you always hit the if and always show 0.0.

If you get past that, you have another problem:
self.score = float(correct_questions) + ""/"" + float(num_questions) + "" "" + (float(correct_questions) / float(num_questions)) * 100.0 + ""%""

You're trying to add floats and strings. That's illegal. What you probably want here is string formatting, something like this:
pct = float(correct_questions) / float(num_questions) * 100.0
self.score = f""{correct_questions}/{num_questions} {pct}%""

Or, if you're on an older version of Python:
self.score = ""{}/{} {}%"".format(correct_questions, num_questions, pct)

And you probably want your if clause to match the same format instead of just showing 0.0?
self.score = ""0.0/0.0 0.0%""


While we're at it, calling float(correct_questions) over and over again makes your code harder to read (and a bit slower, but that probably doesn't matter). Why not just call it once and store the result?
Also, are these really floats rather than ints? Something called num_questions that comes from a database count operation is surely going to be a whole number, right?
So, putting it all together:
num_questions = int(self.env['etq.result.question'].search_count([('result_id', '=', self.id)]))
correct_questions = int(self.env['etq.result.question'].search_count([('result_id', '=', self.id), ('correct', '=', True)]))

pct = 0.0
if num_questions:
    pct = correct_questions * 100.0 / num_questions
self.score = f""{correct_questions}/{num_questions} {pct}%""
",A,2
14532010,2013-01-25 23:33:44.397000+00:00,"There are actually two problems here.
First, as Mike pointed out, n < len(string1) or len(string2) is equivalent to (n < len(string)) or len(string2). In other words, as long as len(string2) is not zero, this will always be true. To fix this, change it to n < len(string1) or n < len(string2).
But when you fix that, nothing will change; you'll still get an IndexError. The problem here is that, not only did you implement your logic incorrectly, the logic is wrong in the first place. len(string1) is 13, and len(string2) is 17. So, what happens when n is 13? Well, n < 13 or n < 17 is obviously true, so you're still going to hit that next line and try to do string1[n].
You could change the or to an and to fix that.
But really, it's better to write code that's harder to get wrong in the first place.
First, you can do this:
while n < min(len(string1), len(string2)):

Second, whenever you write a loop that starts with n = 0, does a while loop over n < <something>, and does an n += 1, you can, and should, rewrite it as a for loop over a range:
for n in range(min(len(string1), len(string2))):

Meanwhile, if you think about it, I don't think you want to stop early if string2 is shorter than string1. So really, what you want is:
for n in range(len(string1)):

And whenever you find yourself looping over range(len(<something>)) and then doing <something[n]> inside the loop, you really just want to loop over <something> directly:
for ch in string1:

Let's put this together:
string1 = ""doesnt matter""
string2 = ""doesnt matter too""
listt = []
for ch in string1:
    if ch in string2:
        listt.append(ch)

This is a lot more readable—and, more importantly for a novice, it's much harder to get anything wrong. Even experienced programmers often use < when they should have used <=, or get the parentheses wrong in a complex if statement, etc. But if you don't have to write that logic in the first place, you can't make those mistakes.
In fact, you can take this even further, because this pattern is exactly what a list comprehension does (or, alternatively, the filter function), so:
string1 = ""doesnt matter""
string2 = ""doesnt matter too""
listt = [ch for ch in string1 if ch in string2]

But at this point, we're not removing opportunities for error.",A,2
11111028,2012-06-19 23:43:31.333000+00:00,"If you want the myDomainInt32RangeT Structure to be used interchangeably with the my_domain_int32_range_t struct, they have to define compatible types. But they don't:
typedef struct {
    int32_t min;
    int32_t max;
} my_domain_int32_range_t;

This defines a pair of int32_t values.
class myDomainInt32RangeT(Structure):
    _fields_ = [ ('min', c_long),
                 ('max', c_long) ]

This defines a pair of whatever long values.
The problem is that int32_t and long aren't the same type. The fix is easy: change one to match the other (e.g., use c_int32 instead of c_long).
If you want to understand why you get 0, that's a bit more involving.
The rules for int32_t say it must be 32 bits. The rules for long say it must be at least 32 bits. On most 32-bit platforms, and on 64-bit Windows, it's exactly 32 bits. On most other 64-bit platforms, however, it's 64 bits. (See the discussion on LLP64 vs. LP64 at http://en.wikipedia.org/wiki/64-bit for details.)
You're probably on a 64-bit Intel Mac or Linux system, and using the default Python. Therefore, your long, and therefore ctypes.c_long, are 64-bit integers. So, look at the layout of a myDomainInt32RangeT:
1st 32 bits: low half of the 64-bit ""min"" value
2nd 32 bits: high half of the 64-bit ""min"" value
3rd 32 bits: low half of the 64-bit ""max"" value
4th 32 bits: high half of the 64-bit ""max"" value

The layout of a my_domain_int32_range_t, by contrast, is this:
1st 32 bits: 32-bit ""min"" value
2nd 32 bits: 32-bit ""max"" value

So, if you construct a myDomainInt32RangeT(3, 5), what you're creating is:
1st 32 bits: 3 (low half of 64-bit 3)
2nd 32 bits: 0 (high half of 64-bit 3)
3rd 32 bits: 5 (low half of 64-bit 5)
4th 32 bits: 0 (high half of 64-bit 5)

When you try to interpret that as a my_domain_int32_range_t, it sees:
1st 32 bits: 3
2nd 32 bits: 0

So your ""min"" value is 3, and your ""max"" values is 0.
You may also end up slicing objects and/or overwriting memory by passing around things that some code thinks is 128 bits while other code thinks it's 64 bits. For example, if you create a my_domain_int32_range_t, pass it by reference into Python, then try to set its ""max"" value, you're setting the 3rd and 4th 32 bits of an object that only has 2 of them, which means you're actually overwriting the next object in memory.
The details above assume you're on a little-endian system (like x86_64), as opposed to a big-endian system or something different (are there any VAX-endian LP64 platforms? with Python?). On a PowerMac G5 with a 64-bit big-endian PowerPC build of Python, you'll get (0, 3) instead of (3, 0). But the basic idea is the same.",A,2
18069841,2013-08-06 00:11:55.903000+00:00,"On a Mac (at lesat with OS X 10.7.0 through the latest 10.8.x), the pre-installed Python at /usr/bin/python and /usr/bin/python2.7 is Apple's 2.7.2 build.
If you have two different 2.7.5 builds, you must have installed them manually. And nobody but you can possibly know how you did that.
The which command may help. For example, you may find that the first python on your PATH is a /usr/local/bin/python which is a symlink to /usr/local/Cellar/python/2.7.5/bin/python, while the first python2.7 on your PATH is /opt/local/bin/python2.7, which is an executable. That would mean that you installed a Homebrew Python 2.7.5, and you also installed a MacPorts Python 2.7.5, and you did the latter in such a way that it didn't create a python symlink, and you've got MacPorts higher on your PATH than Homebrew.
But whatever you've done, it scarcely matters. If you don't understand how to manage this stuff yourself, the best thing to do is to uninstall all of the extra Pythons you installed and just use a single Python 2.7. That means you will need to reinstall any modules, of course, but it's worth doing.
I know some people believe that it's worth getting one additional Python 2.7 installation and using that in place of Apple's, but nobody is going to tell you to get two additional Python 2.7 installations and use both of them.",A,3
54441649,2019-01-30 13:24:22.343000+00:00,"You made faire_toutes_les_requetes_sans_bloquer an awaitable function, a coroutine, by usingasync def.
When you call an awaitable function, you create a new coroutine object. The code inside the function won't run until you then await on the function or run it as a task:
>>> async def foo():
...     print(""Running the foo coroutine"")
...
>>> foo()
<coroutine object foo at 0x10b186348>
>>> import asyncio
>>> asyncio.run(foo())
Running the foo coroutine

You want to keep that function synchronous, because you don't start the loop until inside that function:
def faire_toutes_les_requetes_sans_bloquer():
    loop = asyncio.get_event_loop()
    # ...
    loop.close()
    print(""Fin de la boucle !"")

However, you are also trying to use a aiophttp.ClientSession() object, and that's an asynchronous context manager, you are expected to use it with async with, not just with, and so has to be run in aside an awaitable task. If you use with instead of async with a TypeError(""Use async with instead"") exception will be raised.
That all means you need to move the loop.run_until_complete() call out of your faire_toutes_les_requetes_sans_bloquer() function, so you can keep that as the main task to be run; you can call and await on asycio.gather() directly then:
async def faire_toutes_les_requetes_sans_bloquer():
    async with aiohttp.ClientSession() as session:
        futures = [requete_sans_bloquer(x, session) for x in range(10)]
        await asyncio.gather(*futures)
    print(""Fin de la boucle !"")

print(""Non bloquant : "")
start = datetime.datetime.now()
loop.run(faire_toutes_les_requetes_sans_bloquer())
exec_time = (datetime.datetime.now() - start).seconds
print(f""Pour faire 10 requêtes, ça prend {exec_time}s\n"")

I used the new asyncio.run() function (Python 3.7 and up) to run the single main task. This creates a dedicated loop for that top-level coroutine and runs it until complete.
Next, you need to move the closing ) parenthesis on the await resp.json() expression:
uid = (await response.json())['uuid']

You want to access the 'uuid' key on the result of the await, not the coroutine that response.json() produces.
With those changes your code works, but the asyncio version finishes in sub-second time; you may want to print microseconds:
exec_time = (datetime.datetime.now() - start).total_seconds()
print(f""Pour faire 10 requêtes, ça prend {exec_time:.3f}s\n"")

On my machine, the synchronous requests code in about 4-5 seconds, and the asycio code completes in under .5 seconds.",A,1
29479498,2015-04-06 20:52:38.923000+00:00,This is exactly what Celery is for. You should use that.,A,3
1451134,2009-09-20 14:26:08.037000+00:00,"The best way to do this would be with an inclusion tag. This is a tag that renders a template fragment which renders the 10 related articles. 
You just pass in the current article into the tag, and return the context for the template fragment - ie the related articles.
@register.inclusion_tag('related_articles.html')
def related_articles(article, count):
    category = article.category
    articles = category.article_set.exclude(id=article.id)[:count]
    return {'articles': articles}

You'll need a related_articles.html file in your templates directory which outputs the articles. Then, to call it from your main template, you would just do 
{% related_articles article 10 %}

where article is the name of the article object.",A,8
17664836,2013-07-15 22:14:00.263000+00:00,"You can't have a variable which is global, but not global, which is effectively what you're asking for. You'd have the same problem without multiprocessing.*
So, you need to pass it in some other way. For example, you can make it local in the top-level function used by each task, or find some other object that does last as long as the whole task and make it a member of that, or pass it as an argument, or pass a list with one value as an argument (which allows you to mutably change the value by setting lst[0] = new_value), or…

In your code, you're not actually using word_list at all outside the while loop in worker, so… there is absolutely no reason for it to be global. However, that's presumably not true in your real code. If you show us your real code (or, better, a SSCCE that demonstrates what you're trying to do without all the extraneous stuff), we can explain how to do what you want. Otherwise, all we can do is give vague explanations.

* Actually, multiprocessing does make a difference here. Your code is relying on the fact that globals are truly global thanks to multiprocessing working on top of fork and the interpreter handling that the way you'd like it to. That isn't guaranteed to work—on Windows, each process will get a separate copy (which means all tasks run by the same process in the pool will share a copy with each other, but not with tasks run by other processes in the pool), and on some uncommon POSIX platforms you'll just segfault.",A,1
9365540,2012-02-20 17:26:24.947000+00:00,"GET parameters are supplied to the view as a dictionary-like object in request.GET. So, you just need another dictionary to map the keys to your desired headings:
header_mapping = {
    'site': 'Site Name',
    'startdate': 'Start Date'.
    ....
}
headers = [header_mapping[k] for k, v in request.GET.items() if v]

The list comprehension simply maps through the list of (key, value) pairs in the GET querydict, and if the value is not empty it looks up the header mapping and adds it to a list.",A,4
24846172,2014-07-20 00:15:04.237000+00:00,"You are including å in the characters to split on. It appears you have misunderstood what the split does.
re.split() will split the words on anything that matches the characters class; in your case that means anything \W matches (anything that is not a letter, a number or _), and you added the characters åÅäÄöÖ to that set.
Omit those characters if you don't want them to be split on:
wordsre = re.compile(r""\W+"")

In a Unicode regular expression, they are not part of the \W character set to begin with.
Demo:
>>> import re
>>> mytext = ""hej. och gå! Med en #katt som leker 9 liv!""
>>> wordsre = re.compile(r""\W+"")
>>> wordsre.split(mytext)
['hej', 'och', 'gå', 'Med', 'en', 'katt', 'som', 'leker', '9', 'liv', '']

As \w is the inverse of \W, you can also use re.findall() instead of re.split(), using the \w pattern to extract the words rather than remove the non-word characters:
>>> re.findall(r'\w+', mytext)
['hej', 'och', 'gå', 'Med', 'en', 'katt', 'som', 'leker', '9', 'liv']

with the added advantage that there now is no trailing empty string left.
Again, when using a Unicode pattern, all letters are included, the character set is not limited to just the ASCII letter characters.",A,3
46689760,2017-10-11 13:41:29.913000+00:00,"Radio buttons need to have a value. (Note that the part after the end of the input is the label, and should be enclosed in a label element.)
<input type=""radio"" id=""student_{{ stud.id }}"" name=""student"" value=""{{ stud.id }}"">
<label for=""student_{{ stud.id }}"">{{ student.nameAndSurname }}</label>
",A,2
29658244,2015-04-15 18:40:49.433000+00:00,"Accessing elements by index in both lists and dicts is O(1) - see the Python wiki entry on time complexity for more details.
Given that, the most significant issue here is space, and since you say your collection is sparse a dict would be the preferred choice.",A,2
30932151,2015-06-19 07:14:39.577000+00:00,"The default cache size is 300, according to the code.
Your snippet won't tell you anything useful about the size of the cache. When the locmem cache is full, a subsequent set will cause it to evict items based solely on the modulus of their key's position in the dict, which is unpredictable. There is no attempt to evict based on LRU, FIFO or any other algorithm. 
This is yet another reason not to use this backend n production.",A,3
28298312,2015-02-03 11:56:34.683000+00:00,"To overwrite a single line, you could use \r instead of a newline:
#!/usr/bin/env python
import sys
import time
from itertools import cycle

for i, c in zip(range(101), cycle(""\-/|"")):
    sys.stderr.write(""\r{} {:.0%}"".format(c, i/1e2))
    time.sleep(.3)

To overwrite multiple lines, you need an ability to put cursor at arbitrary position in a terminal. You could use blessings library. Here's code example.",A,0
29965298,2015-04-30 10:27:44.993000+00:00,"You didn't read the rest of the section:

Restart the Django development server and visit http://127.0.0.1:8000/rango.
",A,0
39624465,2016-09-21 18:57:28.093000+00:00,"You need to terminate your activity_list URL:
url(r'^$', views.activity_list, name='activity_list'),

Otherwise this will simply match all paths, including those that are supposed to point to your activities.",A,0
21202577,2014-01-18 09:59:36.507000+00:00,"If you can destructively modify names, just pop the values instead of copying them. Then they're not there anymore, so you won't be able to repeat. 
If you can't destructively modify names, just do the same to a copy:
tmp = names[:]
result = [tmp.pop(random.randrange(len(tmp))) for _ in range(user_input)]

This does have quadratic performance, since each pop from the middle of the list has to shift half the list up one notch. But for 150 names, this is unlikely to be a problem. For example, on my laptop, picking 100 values out of 150 names takes 83 microseconds.
If you really aren't allowed to use even randrange, you can write it yourself:
def randrange(x): return randint(0, x-1)
",A,3
11485040,2012-07-14 15:36:43.013000+00:00,"This PHP script seems to indicate you'll need to set some cookies before Google gives you results:
/*

 Need a cookie file (scholar_cookie.txt) like this:

# Netscape HTTP Cookie File
# http://curlm.haxx.se/rfc/cookie_spec.html
# This file was generated by libcurl! Edit at your own risk.

.scholar.google.com     TRUE    /       FALSE   2147483647      GSP     ID=353e8f974d766dcd:CF=2
.google.com     TRUE    /       FALSE   1317124758      PREF    ID=353e8f974d766dcd:TM=1254052758:LM=1254052758:S=_biVh02e4scrJT1H
.scholar.google.co.uk   TRUE    /       FALSE   2147483647      GSP     ID=f3f18b3b5a7c2647:CF=2
.google.co.uk   TRUE    /       FALSE   1317125123      PREF    ID=f3f18b3b5a7c2647:TM=1254053123:LM=1254053123:S=UqjRcTObh7_sARkN

*/

This is corroborated by Python recipe for Google Scholar comment, which includes a warning that Google detects scripts and will disable you if you use it too prolifically.",A,3
11917093,2012-08-11 19:40:53.247000+00:00,"Python is clever; it'll use double quotes for strings that contain single quotes when generating the representation, to minimize escapes:
>>> 'no quotes'
'no quotes'
>>> 'one quote: \''
""one quote: '""

Add a double quote in there as well and it'll revert back to single quotes and escape any single quotes contained:
>>> 'two quotes: \'\""'
'two quotes: \'""'
",A,8
24000917,2014-06-02 18:23:16.207000+00:00,"You can't. It is the shell that does the parsing here; it passes in the arguments to the process as a parsed list.
To prevent this, quote your argument:
mything.py -l ""test of foo, game of bar, How I foo bar your mother""
",A,1
49308334,2018-03-15 20:08:24.583000+00:00,"Your problem is these tests:
if board[row][col] == board[0][col]: #top

This isn't actually testing that your current row, col are on the top row. It's testing that whatever value is in your current row, col (which will be 0) is the same as whatever value is in the top of that column (which will be X or O.
You just want to do this:
if (row, col) == (0, col):

Or, more simply:
if row == 0:


The next problem is that you've tried to put the bottom and right together into a single test. But that means you're only going to put an X in the bottom right corner, not in every bottom and every right slot. You either need two if statements, or an if statement with an or in it.

Finally, you have a bunch of separate if statements. This means the else attaches to the last one of them, not to all of them. In other words, everything that isn't on the right, you'll set to O, which will overwrite the X you set for the top and left edges. To fix that, you need to use elif to chain the ifs together.

Putting it all together:
if row == 0: # top
    board[row][col] = ""X""
elif col == 0: # left
    board[row][col] = ""X""
elif row == size-1: # bottom
    board[row][col] = ""X""
elif col == size-1: # right
    board[row][col] = ""X""
else:
    board[row][col] = ""O""

Or you could or all those conditions together, because you do the same thing for every condition:
if row == 0 or col == 0 or row == size-1 or col == size-1:
    board[row][col] = ""X""
else:
    board[row][col] = ""O""
",A,3
25213141,2014-08-08 22:27:25.767000+00:00,"It's blank because the object is not actually receiving an id. That's because the id field should be an AutoField, not IntegerField, so that it is declared as auto-incrementing.
Actually though you should not declare it at all: an primary key AutoField named id is the default, so you should leave it out.
Note you'll need to migrate your database table, or drop and recreate it, after you make this change.",A,5
41532275,2017-01-08 11:38:48.763000+00:00,"You are accessing the unbound method, so self is not passed in. There is no context for A.__init__ to know what instance to bind to.
You have several options to provide that context:

Pass in self explicitly, as you did.
Bind manually, with A.__init__.__get__(self)(*args)
In Python 3, use super().__init__(*args) and the context is then taken from the current method. See Why is Python 3.x's super() magic? 
",A,4
26447359,2014-10-19 04:37:48.303000+00:00,"Use next():
d = next((d for d in Mylist if d['Stringa'] == 'ABC'), None)
if d is not None: # found
    print(d)

See Python: find first element in a sequence that matches a predicate.",A,2
16570014,2013-05-15 16:09:31.390000+00:00,"No, there isn't. Values in Python do not track where they originated from, nor are they 'bound' to one another. Containers can refer to other values, but since any value can be referenced from multiple locations there is no point in tracking back-references.
From function()s point of view, there is absolutely no difference between the following two invocations:
function(d[0])
function('a')

In both cases, k in the function is bound to a python string value, 'a'.
If you need more context in a function, you need to pass it in explicitly:
def function(key, mapping):
    k = mapping[key]

function(0, d)

Now you have the dictionary itself too.",A,5
19331705,2013-10-12 07:31:05.113000+00:00,"You cannot do what you want to do; sqlite cannot parse Python for you and 'understand' what your db.py script does with MySQL.
Your db.py script doesn't contain the MySQL tables either; it at most contains SQL statements that instruct MySQL to create those tables.
You'll have to translate those SQL statements to sqlite instead; each database brand uses their own dialect of SQL. It'll be similar but not the same, as MySQL and sqlite have different philosophies about how databases ought to store and treat data.
If you were using a ORM like SQLAlchemy, then you need to configure the ORM to connect to sqlite instead of MySQL; a good ORM can translate the table structures expressed in Python code to different database SQL dialects for you.",A,1
52028502,2018-08-26 17:09:29.020000+00:00,"You don't need to make tkinter try-except its own call; just try-except your call to tkinter:
try:
    self.image = PhotoImage(file=""cat.png"")
except tkinter.TclError:
    # do whatever you wanted to do instead

For example:
try:
    self.image = PhotoImage(file=""cat.png"")
except tkinter.TclError:
    self.cat = Label(text=""Sorry I have no cat pictures"")
else:
    self.cat = Label(image=self.image)
",A,4
12277584,2012-09-05 08:43:48.707000+00:00,"But you haven't just removed required=True, you've removed the brackets () which instantiate the field. It should be
text = db.TextProperty()
",A,1
35958141,2016-03-12 13:19:25.547000+00:00,"Firstly, please give your variables descriptive names. ""c"" and ""gr"" are impossible to understand.
Secondly, you are not comparing the right things at all. c.author is an instance of User; member is an instance of Person and member.name is a string. Comparing a User instance with a string will always fail.
Finally, this whole thing is horribly inefficient - and probably totally unnecessary. Three nested for loops means a huge number of iterations. If you could explain what the output needs to be, then we can almost certainly come up with a better way of doing it.",A,2
401316,2008-12-30 20:00:33.837000+00:00,"This works out better with a queue and a dispatcher.
You split your processing into two sides: source and dispatch.  These can be separate threads (or separate processes if that's easier).
The Source side creates and enqueues requests at whatever rate makes them happy.
The Dispatch side does this.

Get the request start time, s.
Dequeues a request, process the request through the remote service.
Get the current time, t.  Sleep for rate - (t - s) seconds.

If you want to run the Source side connected directly to the remote service, you can do that, and bypass rate limiting.  This is good for internal testing with a mock version of the remote service.
The hard part about this is creating some representation for each request that you can enqueue.  Since the Python Queue will handle almost anything, you don't have to do much.  
If you're using multi-processing, you'll have to pickle your objects to put them into a pipe.",A,7
41829455,2017-01-24 13:27:51.147000+00:00,"Just use a list comprehension, and filter on the boolean truth. An empty dictionary is considered false:
return [d for d in list_of_dictionaries if d]

In Python 2, you could also use the filter() function, using None as the filter:
return filter(None, list_of_dictionaries)

In Python 3 that returns an iterator, not a list, so you'd have to call list() on that (so return list(filter(None, ...))), at which point the list comprehension is simply more readable. Of course, if you don't actually need to have random access to the result (so direct index access to result[whatever]), then an iterator might still be a good idea anyway.
Note that this has to take O(N) time, you have to test each and every dictionary. Even if lists had some kind of automaticly updated map that lets you get the indices of the dictionaries that are empty in O(1) time, removing items from a list requires moving later entries forward.",A,6
44647815,2017-06-20 08:32:02.163000+00:00,"You can't do that with just a RedirectRoute; you need to get the slug value from somewhere.
You'll need to write a standard route, and in the handler you should get the object from the datastore and return a redirect to the full path using the slug.
Something like (untested):
class RedirectToFullPath(webapp2.RequestHandler):
    def get(self, invoice_id):
        invoice = Invoice.get_by_id(invoice_id)
        self.redirect_to('FullPrintHandler', invoice_id, invoice.slug)
",A,4
16844262,2013-05-30 19:22:20.880000+00:00,"You have commas between the string literal and the % sign. Remove those.
print(""The average distance from the starting point is a %.3f"" % pos1)
",A,0
735593,2009-04-09 19:27:13.813000+00:00,"See http://docs.djangoproject.com/en/dev/ref/forms/validation/#ref-forms-validation

Secondly, once we have decided that
  the combined data in the two fields we
  are considering aren't valid, we must
  remember to remove them from the
  cleaned_data.
In fact, Django will currently
  completely wipe out the cleaned_data
  dictionary if there are any errors in
  the form. However, this behaviour may
  change in the future, so it's not a
  bad idea to clean up after yourself in
  the first place.

The original data is always available in request.POST.

A Comment suggests that the point is to do something that sounds like more sophisticated field-level validation.
Each field is given the unvalidated data, and either returns the valid data or raises an exception.
In each field, any kind of validation can be done on the original contents.",A,17
35406830,2016-02-15 10:24:44.400000+00:00,"You can't do that with the json module, no. It was never the goal for the module to allow this much control over the output.
The indent option is only meant to aid debugging. JSON parsers don't care about how much whitespace is used in-between elements.",A,1
14211779,2013-01-08 09:24:22.777000+00:00,"You've referenced ratings in your view, but defined the manager attribute as rating (no 's').",A,2
44045279,2017-05-18 10:29:01.703000+00:00,"There is no space, that's just how Django prints the URLs.
The problem has nothing to do with spaces, but with your URL. ""duplicate_check"" is included under person/, but you are trying to access p_check/....
Edit There are actually bigger problems with your URL pattern. You haven't actually given the capturing groups anything to capture. You need some kind of pattern inside the parentheses. Something like:
r'^duplicate_check/(?P<entity>\w+)/(?P<full_name>\w+)/?$'

which will capture all alphanumeric characters for entity and full_name.",A,2
12827418,2012-10-10 20:02:12.843000+00:00,"The print function uses a special flag when writing to a file object, causing the PyFile_WriteObject function of the Python C API to retrieve the output encoding to do the unicode-to-bytes conversion, and by replacing the stdout stream you lost the encoding. Unfortunately, you cannot explicitly set it again:
encoding = sys.stdout.encoding
sys.stdout = os.fdopen(sys.stdout.fileno(), 'wb', 0)
sys.stdout.encoding = encoding  # Raises a TypeError; readonly attribute

You also cannot use the io.open function instead, since it doesn't allow buffering to be disabled if you want to be able to use the encoding option you'd require.
The proper way to have the print function flush immediately is to use the flush=True keyword:
print(something, flush=True)

If that's too tedious to add everywhere, consider using a custom print function:
def print(*args, **kw):
    flush = kw.pop('flush', True)  # Python 2.7 doesn't support the flush keyword..   
    __builtins__.print(*args, **kw)
    if flush:
        sys.stdout.flush()

Since Python 2.7's print() function doesn't actually support the flush keyword yet (botheration), you can simulate that by adding an explicit flush instead in that custom version.",A,6
5464904,2011-03-28 20:50:35.060000+00:00,"Not without changing the argument ordering or switching to named parameters.
Here's the named parameters alternative.
printstuff( *mytuple, three="" How are you"" )

Here's the switch-the-order alternative.
def printstuff( three, one, two ):
    print one, two, three

printstuff( "" How are you"", *mytuple )

Which may be pretty terrible.",A,4
17797566,2013-07-22 21:15:18.393000+00:00,"Basically, you want to see if you can traverse the tree; use reduce() to loop over the elements and if a KeyError is raised, the path doesn't exist:
def family_lineage(familytree, lineage):
    if not familytree:
        return False
    try:
        reduce(lambda d, k: d[k], lineage, familytree)
        return True
    except KeyError:
        # No match at this level, recurse down the family tree
        return any(family_lineage(val, lineage) for val in familytree.itervalues())

reduce() applies the lambda function recursively to lineage, starting with familytree.
To support finding lineages deeper down the tree, you need to recurse down the tree on KeyErrors.
Demo:
>>> tree = {'Gina': {'Sam': {'Tina': {}}, 'Li': {}}, 'Guy': {}}
>>> family_lineage(tree, ['Gina'])
True
>>> family_lineage(tree, ['Gina', 'Sam', 'Tina'])
True
>>> family_lineage(tree, ['Gina', 'Tina'])
False
>>> family_lineage(tree, ['Sam', 'Tina'])
True
",A,2
25227415,2014-08-10 09:49:22.063000+00:00,"The platform name for all OS X releases is Darwin:
Python 2.7.6 (default, Apr 28 2014, 17:17:35) 
[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import platform
>>> platform.system()
'Darwin'

platform.system() uses os.uname(), and you can look up responses on the uname Wikipedia page.",A,1
44457036,2017-06-09 11:49:05.773000+00:00,"Yes, it does. See the source code (Python 3.6.1 version linked):
sts = self.wait(timeout=self._remaining_time(endtime))

You do not have to call p2.wait() yourself, the process has terminated by that point.",A,0
731652,2009-04-08 20:20:40.223000+00:00,"This question appears to have two things in it.

Presentation on the web.  This is easy to do in Python -- use Django or TurboGears or any Python-based web framework.
Refresh of the web page to show new data.  This can be done two ways.

Some fancy Javascript to refresh.
Some fancy HTML to refresh the page.  The meta refresh tag is what you want.  If you do this, you have an all-Python solution.

",A,5
37227275,2016-05-14 13:30:00.770000+00:00,"Use a custom sort key, to convert the last element to an integer just when sorting:
sorted(l1, key=lambda l: int(l[2].rstrip(',')))

The key is used to produce the value on which to sort, for each element in the list. So the lambda function is called for each element, and the above code extracts the l[2] value, converting it to an integer. The str.rstrip() call removes the trailing comma first.
Demo:
>>> l1 = [['test', 'hello', '60,'], ['why', 'to', '500,'], ['my', 'choice', '20,']]
>>> sorted(l1, key=lambda l: int(l[2].rstrip(',')))
[['my', 'choice', '20,'], ['test', 'hello', '60,'], ['why', 'to', '500,']]
",A,1
21322926,2014-01-24 01:48:25.590000+00:00,"You're trying to access binary data as if it were text. This means that Requests has to guess an encoding for it (and any guess it makes will be wrong, because it's not text) and decode it… just so you can decode it to utf-8. If you're really lucky, maybe Requests will guess UTF-8, and your data will just happen to be data that can be round-tripped as UTF-8, so it might work one time in a thousand, at best.
Just ask requests for binary response content, and save it to a binary file.
While we're at it, you never actually close the file. You're just sitting there in the interactive interpreter with an open file object that hasn't been flushed yet. So, it's entirely possible that the last buffer worth of data, or even all the data, won't be there yet. That's exactly what the with statement is for.
So:
l = requests.get(""http://www.mosta2bal.com/vb/imgcache/2/9086screen.jpg"")
with open('out.jpg', 'wb') as f:
    f.write(l.content)
",A,5
28985859,2015-03-11 11:48:34.790000+00:00,"You need to encode your query parameters correctly; in URL query paramater encoding, spaces are encoded to +, while + itself is encoded to %2B.
Flask cannot be told to treat specific data differently; you cannot reliably detect what data was correctly encoded and what wasn't. You could extract the parameters from query string manually, however, by using request.query_string.
The better approach is to escape your parameters correctly (in JavaScript, use encodeURIComponent(), for example). The + character is not the only problematic character in a Base64-encoded value; the format also uses / and =, both of which carry meaning in a URL.
In fact, it is probably the = character at the end of that data: URL that is missing, being the more direct cause of the Incorrect padding error message. If you added it back you'd next indeed have problems with all the + characters having been decoded to ' '.",A,6
29290238,2015-03-26 22:44:48.767000+00:00,"None of this code is doing what you think it is. You never get anything from the database query - to do so you would have to iterate over queryCurs.fetchall().
The thing you're printing is a list containing some constants you defined previously. And it's looping because you told it to, with for j in i:",A,0
15139639,2013-02-28 15:41:35.770000+00:00,"Just pass in cls or self as the first positional argument, there is no need to pass them is as keyword arguments:
def paginated_class_method(default_page_size=25):
    def wrap(func):
        @functools.wraps(func)
        def inner(self_or_cls, page=1, page_size=default_page_size, *args, **kwargs):
            objects = func(self_or_cls, *args, **kwargs)
            return _paginate(objects, page, page_size)
        return inner
    return wrap
",A,3
7964400,2011-11-01 09:10:54.207000+00:00,"Django's multi-table inheritance is such an antipattern. It has all sorts of horrible consequences, like the one you've noticed.
The answer to your question is, don't use inheritance. Use an explicit OneToOneField between TwitterProfileInfo and OauthProfile. The database structure is exactly the same, and you'll have complete control of how you refer to your fields. The only difference is that you'll need to instantiate and save the OauthProfile object separately, and explicitly set the relationship:
my_oauth = OauthProfile(oauth_token='1234567890abcdefg', oauth_secret='sometauthtokensecret')
my_oauth.save()
my_twitter = TwitterProfileInfo(oauth_profile=my_oauth, user_id=12345)
my_twitter.save()
",A,1
29600576,2015-04-13 08:04:18.550000+00:00,The third-party six library defines a python_2_unicode_compatible class decorator which takes a class with a __str__ method and translates it to __unicode__ on Python 2.,A,3
8907075,2012-01-18 08:19:31.923000+00:00,"def subStringMatchExact(target, key):
    i = target.find(key)
    while i != -1:
        yield i
        i = target.find(key, i + len(key))

print tuple(subStringMatchExact(target1, key11))

btw, don't use such names as target1, key11 use targets, keys lists instead.",A,1
27585826,2014-12-21 00:58:49.397000+00:00,"There is a . in one of the values, but \w doesn't match dots. Compare:
magic: 'AZJEXYx.ZsExcTHvjH9mwQ',
//             ^

with:
magic: 'xfF9i4YBAQP1EgoNhgEBAw',

A better bet is to allow all characters except a quote:
magic_regex = re.compile(ur""magic:\s+'([^']+)',"")

Demo:
>>> import re
>>> samples = [
...     u""magic: 'xfF9i4YBAQP1EgoNhgEBAw',"",
...     u""magic: 'AZJEXYx.ZsExcTHvjH9mwQ',"",
... ]
>>> magic_regex = re.compile(ur""magic:\s+'([^']+)',"")
>>> for sample in samples:
...     print magic_regex.search(sample).group(1)
... 
xfF9i4YBAQP1EgoNhgEBAw
AZJEXYx.ZsExcTHvjH9mwQ
",A,1
42055975,2017-02-05 18:59:54.170000+00:00,Migration 0008 in the auth app was added in Django 1.10. You are presumably running an older version of Django on the other machine. ,A,1
21809829,2014-02-16 10:11:39.990000+00:00,"Your code works just fine, it just appears that at least some of your data doesn't have a groupid key.
Use .get() to work around this:
for cords in all_hovers[userID]:
    x = cords[""mouse_position""][""x""]
    y = cords[""mouse_position""][""y""]
    viewport_x = cords[""viewport_dimensions""][""width""]
    viewport_y = cords[""viewport_dimensions""][""height""]
    data_attrs = cords[""node_data_attrs""].get(""groupid"")

This sets data_attrs to None if the key is missing. You can set it to a different default by passing in a second argument to dict.get():
    data_attrs = cords[""node_data_attrs""].get(""groupid"", 'default value')
",A,0
12316962,2012-09-07 11:11:37.713000+00:00,"It is called a mutt encoding; the underlying bytes have been mangled beyond their original meaning and they are no longer a real encoding.
It was once URL-quoted UTF-8, but now interpreted as latin-1 without unquoting those URL escapes. I was able to un-mangle this by interpreting it as such:
>>> from urllib2 import unquote
>>> bytesquoted = u'å%8f°å%8d%97 è¦ªå­%90é¤%90å»³'.encode('latin1')
>>> unquoted = unquote(bytesquoted)
>>> print unquoted.decode('utf8')
台南 親子餐廳
",A,4
16225377,2013-04-25 21:52:23.027000+00:00,"Just use the repr() of self.a:
return ""Foo(a={self.a!r})"".format(self=self)

The !r tells .format() to call repr() on self.a, instead of calling __format__() on the value, see Format String syntax:
>>> '{!r}'.format(0)
'0'
>>> '{!r}'.format('foo')
""'foo'""

Demo:
>>> Foo(0)
Foo(a=0)
>>> Foo('foo')
Foo(a='foo')
>>> Foo(None)
Foo(a=None)
",A,3
25951927,2014-09-20 18:18:58.343000+00:00,"Yes, you can use shelve to persist instances of a class. shelve gives you a dictionary interface, making the process relatively transparent.
Underneath, shelve uses the pickle library; if the shelve API doesn't fit your needs, you can go straight to that module.
scikit-learn explicitly support pickle, see Model persistence:

After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. The following section gives you an example of how to persist a model with pickle.
",A,2
21608531,2014-02-06 16:29:49.453000+00:00,"Sure you can. It's a bad idea, but nothing stops you from returning a tuple when a flag is set:
def foo(add_b=False):
    if add_b:
        return a, b

    return a

It's better not to do this; you are changing the type of object returned here. It'd be better if you added a new function instead:
def foo_with_b():
    return a, b

def foo_without_b():
    return foo_with_b()[0]

or just account for the extra return item returned and simply ignore it, which is what foo_without_b() is doing, for example.",A,5
22789533,2014-04-01 14:54:45.747000+00:00,"The docs for psutil.process_iter() say:

Return an iterator yielding a Process class instance for all running
  processes on the local machine. Every instance is only created once and then cached into an internal table which is updated every time an element is yielded. emphasize is mine 

where ""running"" is defined by .is_running() method that returns True for zombie processes. .is_running() is safe to use even if the process is gone and its pid is reused.
My understanding is that _process.is_running() is true at the moment psutil.process_iter() yields it. In principle, before _process.as_dict returns with the info, several things may happen:

process may die -- become a zombie: _process.is_running() is true, pid and status are meaningful
it may be reaped  -- dead-dead: the process info is unavailable (psutil.NoSuchProcess exception) or outdated (if cached)
its pid may be reused -- some _process methods may return incorrectly the info for the new process. .is_running() uses several process attributes so it should detect the case when pid is reused.

Normally, OS doesn't reuse pid immediately therefore p.3 shouldn't occur frequently.",A,0
14389195,2013-01-17 22:19:19.560000+00:00,"Keep in mind that Python's substring search x in a is already optimized pretty well for the general case (and coded in C, for CPython), so you're unlikely to beat it in general, especially with pure Python code.
However, if you have a more specialized case, you can do much better. 
For example, if you have an arbitrary list of millions of strings b that all need to be searched for within one giant static string a that never changes, preprocessing a can make a huge difference. (Note that this is the opposite of the usual case, where preprocessing the patterns is the key.)
On the other hand, if you expect matches to be unlikely, and you've got the whole b list in advance, you can probably get some large gains by organizing b in some way. For example, there's no point searching for ""ABCD"" if ""ABC"" already failed; if you need to search both ""ABC"" and ""ABD"" you can search for ""AB"" first and then check whether it's followed by ""C"" or ""D"" so you don't have to repeat yourself; etc. (It might even be possible to merge all of b into a single regular expression that's close enough to optimal… although with millions of elements, that probably isn't the answer.)
But it's hard to guess in advance, with no more information than you've given us, exactly what algorithm you want.
Wikipedia has a pretty good high-level overview of string searching algorithms. There's also a website devoted to pattern matching in general, which seems to be a bit out of date, but then I doubt you're going to turn out to need an algorithm invented in the past 3 years anyway.",A,1
21008481,2014-01-08 23:06:11.960000+00:00,"You'd use the json library to parse the data.
You must first load the data from the web and decode it to a Unicode string:
import json
from urllib.request import urlopen

response = urlopen('https://s3.amazonaws.com/Minecraft.Download/indexes/legacy.json')
# the default encoding for JSON is UTF-8, but the response can give you
# a different codec
encoding = response.info().get_content_charset('utf-8')

data = json.loads(response.read().decode(encoding))

Now you can loop through the data:
for name, info in data['objects'].items():
    print('{}: {}'.format(name, info['hash']))

This produces:
lang/fr_CA.lang: 6df06576e677d952dc15da3926f4ed822d106354
sounds/random/orb.ogg: e9833a1512b57bcf88ac4fdcc8df4e5a7e9d701d
sounds/mob/villager/yes1.ogg: be73a79fb1ab966887a8b88bed9aa6b3d9173d71
sounds/mob/cat/purreow2.ogg: 08573a1f11058b09c5855122dff47ceb209f771e
sound/mob/spider/say2.ogg: 501b40b97ee55cb7a97943ee620aa05131089fc2
lang/el_GR.lang: 4330e3218548e9f3268d103e5ab71fa2b28d9b20
sound/mob/horse/soft1.ogg: a9a83e3f186b3af56a9844b8d4976c6251ba17eb
sounds/mob/bat/death.ogg: 6df3b2c2b951863fc5200edf3ff3396b105ed897
# etc.
",A,4
23706043,2014-05-16 23:36:53.187000+00:00,"
What I want is a Python script that, when run, will do the exact same thing as the following cURL command in Terminal:

$ curl -k -F docfile=@myLocalFile.csv https://myWebsite.com/extension...

curl -F sends the file using multipart/form-data content type. You could reproduce it easily using requests library:
import requests # $ pip install requests

with open('myLocalFile.csv','rb') as input_file:
    r = requests.post('https://myWebsite.com/extension/...', 
                      files={'docfile': input_file}, verify=False)

verify=False is to emulate curl -k.",A,1
43916500,2017-05-11 13:12:06.123000+00:00,"Yes, you can, by using the normal double-underscore syntax - as explicitly described in the documentation:
Model1.objects.select_related('model2__model3')
",A,2
34960733,2016-01-23 07:15:51.207000+00:00,"To parse a time string from an email, you could use email package from stdlib:
>>> from email.utils import parsedate_tz, mktime_tz
>>> timestamp = mktime_tz(parsedate_tz('Tue, 19 Jan 2016 16:12:18 -0800 (PST)'))

where timestamp is ""seconds since the Epoch"". To get UTC time which you can pass to MongoDB:
>>> from datetime import datetime, timedelta
>>> utc_time = datetime(1970, 1, 1) + timedelta(seconds=timestamp)
>>> utc_time
datetime.datetime(2016, 1, 20, 0, 12, 18)

See also, Python: parsing date with timezone from an email",A,0
36956808,2016-04-30 15:34:30.490000+00:00,"Your specific case could be implemented in Python 3 as:
#!/usr/bin/env python3
from subprocess import check_output

queue_name = ""all.q""
job_name = ""test""
cmd = b""date; sleep 2; date""
job_id = check_output('qsub -cwd -j y -V'.split() +
                      ['-q', queue_name, '-N', job_name],
                      input=cmd).split()[2]

You could adapt it for Python 2, using Popen.communicate().
As I understand, whoever controls the input cmd may run arbitrary commands already and therefore there is no much point to avoid shell=True here:
#!/usr/bin/env python
from pipes import quote as shell_quote
from subprocess import check_output

pipeline = 'echo -e {cmd} | qsub -cwd -j y -V -q {queue_name} -N {job_name}'
job_id = check_output(pipeline.format(
    cmd=shell_quote(cmd),
    queue_name=shell_quote(queue_name),
    job_name=shell_quote(job_name)),
                      shell=True).split()[2]

Implementing the pipeline by hand is error-prone. If you don't want to run the shell; you could use plumbum module that supports a similar pipeline syntax embedded in pure Python:
#!/usr/bin/env python
from plumbum.cmd import echo, qsub # $ pip install plumbum

qsub_args = '-cwd -j y -V -q'.split() + [queue_name, '-N', job_name]
job_id = (echo['-e', cmd] | qsub[qsub_args])().split()[2]
# or (qsub[qsub_args] << cmd)()

See How do I use subprocess.Popen to connect multiple processes by pipes?",A,0
23544805,2014-05-08 14:31:32.133000+00:00,"You never reset k in the inner loop.
The first iteration of for kilpailija in numerot runs the while loop until k < len(tulokset). Because neither k nor len(tulokset) change for the next iteration, that test is always going to be false from there on out.
Your outer loop works just fine, it is your while statement that is then never running the inner loops. Either reset k back to 0 inside the outermost loop, or use a different condition:
for kilpailija in numerot:
    k = 0
    while k < len(tulokset):
        for kilpailijanyt in tulokset[k]:

            if kilpailija == kilpailijanyt.hae_numero():
                tulos.append(kilpailijanyt.hae_tulos())

            if kilpailija != kilpailijanyt.hae_numero():
                tulos.append(""-"")                        

        k += 1
",A,0
51987542,2018-08-23 13:55:48.697000+00:00,"As the error says, that's indeed not a valid regex.
?P introduces a named group, which Django uses as a keyword argument. You need to provide the name of that group, surrounded by angle brackets. For example:
url(r'^(?P<id>[0-9]+)/$'
",A,3
29073780,2015-03-16 09:51:46.997000+00:00,"You have Mojibake strings; text encoded to one (correct) codec, then decoded as another.
In this case, your text was decoded with the Windows 1252 codepage; the U+20AC EURO SIGN in the text is typical of CP1252 Mojibakes. The original encoding could be one of the GB* family of Chinese encodings, or a multiple roundtrip UTF-8 - CP1252 Mojibake. Which one I cannot determine, I cannot read Chinese, nor do I have your full data; CP1252 Mojibakes include un-printable characters like 0x81 and 0x8D bytes that might have gotten lost when you posted your question here.
I'd install the ftfy project; it won't fix GB* encodings (I requested the project add support), but it includes a new codec called sloppy-windows-1252 that'll let you reverse an erroneous decode with that codec:
>>> import ftfy  # registers extra codecs on import
>>> text = u'Ã¨Â¢â€¹Ã¨Â¢âdcx€¹Ã¤Â¸Å½Ã¦Å“â€¹Ã¥Ââ€¹Ã¤Â»Â¬Ã§â€ÂµÃ¥Â­ÂÃ¥â€¢â€'
>>> print text.encode('sloppy-windows-1252').decode('gb2312', 'replace')
猫垄�姑�⑩dcx�盲赂沤忙��姑ヂ�姑ぢ宦�р�得ヂ�氓�⑩�
>>> print text.encode('sloppy-windows-1252').decode('gbk', 'replace')
猫垄鈥姑�⑩dcx�盲赂沤忙艙鈥姑ヂ鈥姑ぢ宦�р�得ヂ�氓鈥⑩�
>>> print text.encode('sloppy-windows-1252').decode('gb18030', 'replace')
猫垄鈥姑⑩dcx�盲赂沤忙艙鈥姑ヂ鈥姑ぢ宦р�得ヂ氓鈥⑩�
>>> print text.encode('sloppy-windows-1252').decode('utf8', 'ignore').encode('sloppy-windows-1252').decode('utf8', 'replace')
袋�dcx与朋�们���

The � U+FFFD REPLACEMENT CHARACTER shows the decoding wasn't entirely successful, but that could be due to the fact that your copied string here is missing anything not printable or using the 0x81 or 0x8D bytes.
You can try to fix your data this way; from the file data, try to decode to one of the GB* codecs after encoding to sloppy-windows-1252, or roundtrip from UTF-8 twice and see what fits best.
If that's not good enough (you cannot fix the data) you can use the ftfy.badness.sequence_weirdness() function to try and detect the issue:
>>> from ftfy.badness import sequence_weirdness
>>> sequence_weirdness(text)
9
>>> sequence_weirdness(u'元大寶來證券')
0
>>> sequence_weirdness(u'John Dove')
0

Mojibakes score high on the sequence weirdness scale. You'd could try and find an appropriate threshold for your data by which time you'd call the data most likely to be corrupted.
However, I think we can use a non-zero return value as a starting point for another test. English text should score 0 on that scale, and so should Chinese text. Chinese mixed with English can still score over 0, but you could not then encode that Chinese text to the CP-1252 codec while you can with the broken text:
from ftfy.badness import sequence_weirdness

def is_valid_unicode_str(text):
    if not sequence_weirdness(text):
        # nothing weird, should be okay
        return True
    try:
        text.encode('sloppy-windows-1252')
    except UnicodeEncodeError:
        # Not CP-1252 encodable, probably fine
        return True
    else:
        # Encodable as CP-1252, Mojibake alert level high
        return False
",A,2
27409558,2014-12-10 19:46:30.560000+00:00,"set is a method, which you have to call. And same for get.
So, instead of this:
my_int_var.set = my_int_var.get + 1

Do this:
my_int_var.set(my_int_var.get() + 1)


But you've got another problem on top of that. my_int_var isn't a Tk IntVar, it's just a plain old int, which has no such methods. And it's also not attached to your Entry widget in any way.
So, you need to create a Tk var with my_int_var = IntVar(), and then you need to pass it to the widget either in the constructor or in a config call.
If you look at the docs where you found the get and set methods, you'll see some examples that should make it obvious. In particular, see the third example under Patterns in the Entry docs (but make sure to read the whole section).

There are other problems beyond the ones directly related to your error. For example:
button_1 = Button(frame_2,text = ""1"", command = button_1_method())

You want to pass the function, button_1_method as the command. But that's not what you're doing; you're calling that function, getting its return value, and passing that as the command. (It's exactly the opposite of your first problem, where you want to call a function but left the parens off. You can't expect Python to guess when you want to call a function and when you want to use it as a value; the parentheses are how you tell it to call the function.)
Similarly, as I mentioned in a comment, even if you get this all right, adding 1 to a number is not what the 1 key on a calculator does. You need to get the difference between a number and its string representation straight or you're going to have a lot more problems with this app. 1 != '1', but str(1) == '1' and 1 == int('1')
In your updated version, you've changed it to use a StringVar, whose value is of course a string, and you're trying to add 1 to its value. That's the same problem. Again, the number 1 and the string '1' are not the same thing. 
And you're also trying to call a set method on an Entry object when no such method exists. Read the Entry docs to see how to change or replace the value of an Entry—but if you're using a StringVar (as in your edited code), you don't need to do this in the first place. That's the whole point of binding a variable to a widget; changes to either one affect the other. So, when you call my_str_var.get, you're getting the current contents of the Entry widget, and when you call my_str_var.set, you're replacing the contents of the widget.",A,1
32276246,2015-08-28 17:03:53.213000+00:00,"You don't have a hex string. You have a regular Python string defined with \xhh hex escape codes.
If you wanted to display these bytes as hex, all you have to do is encode them with the hex codec:
>>> hexstr = ""\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x50\x89\xe2\x53\x89\xe1\xb0\x0b\xcd\x80""
>>> hexstr.encode('hex')
'31c050682f2f7368682f62696e89e35089e25389e1b00bcd80'

An alternative method is to use the binascii.hexlify() function for the same result.
If you need each byte on a separate line, just loop over the string and encode individual characters (bytes):
>>> for c in hexstr:
...     print c.encode('hex')
... 
31
c0
50
68
2f
2f
73
68
68
2f
62
69
6e
89
e3
50
89
e2
53
89
e1
b0
0b
cd
80
",A,2
1839384,2009-12-03 11:35:44.017000+00:00,"""this would simply leave an extra 0 on the binary string--11011001 might get mapped to 01000001 rather than 1000001, and I won't actually save space.""
Correct.  Your plan will do nothing.",A,0
50036860,2018-04-26 07:09:08.587000+00:00,"Calling __init__ on either parent class does not change the inheritance structure of your classes, no. You are only changing what initialiser method is run in addition to C.__init__ when an instance is created. C inherits from both A and B, and all methods of B are shadowed by those on A due to the order of inheritance.
If you need to alter class inheritance based on a value in the constructor, create two separate classes, with different structures. Then provide a different callable as the API to create an instance:
class CA(A):
    # just inherit __init__, no need to override

class CB(B):
    # just inherit __init__, no need to override

def C(path):
    # create an instance of a class based on the value of path
    class_map = {'A': CA, 'B': CB}
    return class_map[path](path)

The user of your API still has name C() to call; C('A') produces an instance of a different class from C('B'), but they both implement the same interface so this doesn't matter to the caller.
If you have to have a common 'C' class to use in isinstance() or issubclass() tests, you could mix one in, and use the __new__ method to override what subclass is returned:
class C:
    def __new__(cls, path):
        if cls is not C:
            # for inherited classes, not C itself
            return super().__new__(cls)
        class_map = {'A': CA, 'B': CB}
        cls = class_map[path]
        # this is a subclass of C, so __init__ will be called on it
        return cls.__new__(cls, path)

class CA(C, A):
    # just inherit __init__, no need to override
    pass

class CB(C, B):
    # just inherit __init__, no need to override
    pass

__new__ is called to construct the new instance object; if the __new__ method returns an instance of the class (or a subclass thereof) then __init__ will automatically be called on that new instance object. This is why C.__new__() returns the result of CA.__new__() or CB.__new__(); __init__ is going to be called for you.
Demo of the latter:
>>> C('A').something()
Function A
>>> C('B').something()
B function with something
>>> isinstance(C('A'), C)
True
>>> isinstance(C('B'), C)
True
>>> isinstance(C('A'), A)
True
>>> isinstance(C('A'), B)
False

If neither of these options are workable for your specific usecase, you'd have to add more routing in a new somemethod() implementation on C, which then calls either A.something(self) or B.something(self) based on self.path. This becomes cumbersome really quickly when you have to do this for every single method, but a decorator could help there:
from functools import wraps

def pathrouted(f):
    @wraps
    def wrapped(self, *args, **kwargs):
        # call the wrapped version first, ignore return value, in case this
        # sets self.path or has other side effects
        f(self, *args, **kwargs)
        # then pick the class from the MRO as named by path, and call the
        # original version
        cls = next(c for c in type(self).__mro__ if c.__name__ == self.path)
        return getattr(cls, f.__name__)(self, *args, **kwargs)
    return wrapped

then use that on empty methods on your class:
class C(A, B):
    @pathrouted
    def __init__(self, path):
        self.path = path
        # either A.__init__ or B.__init__ will be called next

    @pathrouted
    def something(self):
        pass  # doesn't matter, A.something or B.something is called too

This is, however, becoming very unpythonic and ugly.",A,3
22428414,2014-03-15 19:07:41.897000+00:00,"You're passing the form class, OptionForm, rather than the instantiated variable form, to the template.",A,2
49403178,2018-03-21 09:58:12.313000+00:00,"Because your input fields don't have a name attribute; without that the browser won't send any data.
Note, you should really be using the Django forms framework for this. And you certainly absolutely shouldn't have a blank except clause.",A,0
15433168,2013-03-15 13:00:33.317000+00:00,You can instantiate a template from text in Django with just template.Template(my_text).,A,2
29127255,2015-03-18 16:18:44.467000+00:00,"ValueError is not a function; it is a class (an Exception subclass). 
You need to use except as part of a try statement, it cannot be used on its own; you need to put any code you want 'protected' inside the try:
if op == '+':
    left1 = random.randint(1,100)
    right1 = random.randint(1,100)
    print (""What is "" + (str(left1) + op + str(right1) + ""?""))
    answer  = eval(str(left1) + op + str(right1))

    try:
        guess = int(input(""""))
    except ValueError:
        print(""Expected integer"")
    else:    
        if guess == answer:
            print(""Correct!"")
            score = score + 1  # don't forget to *store* the sum!
        else:
            print(""Incorrect"")

You probably want to read the canonical Stack Overflow question on user input in Python to learn more about this:  Asking the user for input until they give a valid response, as well as the section on handling exceptions in the Python tutorial.",A,0
20047900,2013-11-18 12:32:19.637000+00:00,"Python 3 switched to using more efficient data structures and methods throughout; you can get the same dictionary view objects in Python 2 with using dict.viewkeys(), dict.viewvalues() and dict.viewitems().
If you have to have a list, simply use list() to turn collect all values in the view into a list:
>>> d={'A': [1,2,3], 'B': ""Hello"", 'C': [""this is good""]}
>>> d.keys()
dict_keys(['C', 'B', 'A'])
>>> list(d.keys())
['C', 'B', 'A']
>>> list(d.values())
[['this is good'], 'Hello', [1, 2, 3]]
>>> list(d.items())
[('C', ['this is good']), ('B', 'Hello'), ('A', [1, 2, 3])]

For just the keys, you can also call list() on the dictionary, directly:
>>> list(d)
['C', 'B', 'A']
",A,5
18570909,2013-09-02 10:24:51.503000+00:00,"The purpose of subprocess.check_call() is to either return 0 or raise an exception if the exit status of the called process was not 0:

Run command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise CalledProcessError. 

Use subprocess.check_output() instead if you need to read the output of the other command:

Run command with arguments and return its output as a byte string.

The function was added in Python 2.7; if you are using an earlier version of Python, here is a backport:
from subprocess import Popen, PIPE
from subprocess import CalledProcessError as BaseCalledProcessError

class CalledProcessError(BaseCalledProcessError):
    def __init__(self, returncode, cmd, output=None):
        super(CalledProcessError, self).__init__(returncode, cmd)
        self.output = output

def check_output(*popenargs, **kwargs):
    r""""""Run command with arguments and return its output as a byte string.

    If the exit code was non-zero it raises a CalledProcessError.  The
    CalledProcessError object will have the return code in the returncode
    attribute and output in the output attribute.

    The arguments are the same as for the Popen constructor.  Example:

    >>> check_output([""ls"", ""-l"", ""/dev/null""])
    'crw-rw-rw- 1 root root 1, 3 Oct 18  2007 /dev/null\n'

    The stdout argument is not allowed as it is used internally.
    To capture standard error in the result, use stderr=STDOUT.

    >>> check_output([""/bin/sh"", ""-c"",
    ...               ""ls -l non_existent_file ; exit 0""],
    ...              stderr=STDOUT)
    'ls: non_existent_file: No such file or directory\n'
    """"""
    if 'stdout' in kwargs:
        raise ValueError('stdout argument not allowed, it will be overridden.')
    process = Popen(stdout=PIPE, *popenargs, **kwargs)
    output, unused_err = process.communicate()
    retcode = process.poll()
    if retcode:
        cmd = kwargs.get(""args"")
        if cmd is None:
            cmd = popenargs[0]
        raise CalledProcessError(retcode, cmd, output=output)
    return output
",A,4
21185544,2014-01-17 12:03:08.203000+00:00,"You have multiple options:

format each element of your list, then use str.join() to join those elements into a longer string:
''.join(['{0:{1}}'.format(c, width + extra) for c, width in zip(line, maxLength)])

Build a dynamic format string (concatenating various {} parts), then apply the list as a sequence of arguments using the *args splat syntax.
template = ''.join(['{%(i)s:{widths[%(i)s]}}' % {'i': i} for i in range(len(maxLength))])
for line in lines:
    print template(*line, widths=maxLength)


Simplifying your code somewhat:
def print_table(self, lines, sum_columns=None, extra_width=5):
    columns = zip(*lines)

    widths = [max(len(str(l)) for l in column) for column in columns]

    for line in lines:
        print ''.join(['{0:{1}}'.format(c, w + extra_width) for c, w in zip(line, widths)])

    if sum_columns is not None:
        totals = [sum(columns[i]) if i + 1 in sum_columns else 0 for i in range(len(columns))]
        print ''.join(['{0:{1}}'.format(c, w + extra_width) for c, w in zip(totals, widths)])
",A,2
18400382,2013-08-23 10:20:13.230000+00:00,"You have declared your static URL to be /js but in your HTML you are requesting /static_files/js. You should use just /js there too.
The way to start debugging issues like this is by trying to go to the location mentioned in the link, and seeing if that works.",A,1
21958701,2014-02-22 18:40:17.217000+00:00,"You are generating:
var getEmail = e;

which is not working since e is undefined.
You want to quote your variables; use the tojson filter which comes with Flask, together with safe (to keep from quoting the "" quotes generated):
var getEmail = {{ entry.1|tojson|safe }};

which will generate valid JSON, which is a subset of Javascript. Now your line will read:
var getEmail = ""e"";

Alternatively, set the emailArray list in one go (no loop over emails required) with:
var emailArray = {{ emails|map(attribute=1)|list|tojson|safe }};

which would take your emails list, select the 2nd element of each tuple, and output the resulting list as a JSON list, resulting in:
var emailArray = [""e""];
",A,2
25705113,2014-09-06 21:53:11.397000+00:00,"Use a continue statement to skip to the next iteration of a loop.
continue skips the rest of the body of the loop:
>>> for i in range(5):
...     print(i)
...     if i < 3:
...         continue
...     print('i is 3 or higher')
... 
0
1
2
3
i is 3 or higher
4
i is 3 or higher
",A,1
27967687,2015-01-15 16:03:02.287000+00:00,"Output for a terminal (or more precisely, a tty) is typically opened in line buffer mode in Python. When you use a pipe, Python'll use a different buffer, of a fixed size. 
This means that when you write text with a newline, the buffer is automatically flushed when printing to a terminal, but to a pipe it'll only be flushed when the buffer is full or a flush is forced (such as when the Python script exits).
In other words, when writing to the terminal the first print line is flushed out to the terminal before the rsync command is run. When redirecting to a pipe, the text is kept in a buffer, the rsync command output is run (writing to the pipe when it flushes, at least once at the end but possibly more often), after which you write some more to the buffer and that buffer is flushed to the pipe when Python exists.
You can force the flush manually:
import sys

# ....
print ""Hello at first""
sys.stdout.flush()
call([""rsync"", ""-aHvz"", ""root@server1:/tmp/a"", '.'])
print ""Hello at the end""
sys.stdout.flush()
",A,4
10923170,2012-06-06 22:23:02.350000+00:00,"You can't do Unicode in ID3v1, and that module only does ID3v1 (and it doesn't even do it correctly).
If you're explicitly trying to tag your files for some ID3v1-only application, then you really need to figure out how to trick that application into reading Russian characters. Most likely, that will be impossible. But if not, then you can trick your ID3v1 library into outputting strings that will trick your application, by manually pre-processing the strings.
However, if you're trying to tag your files for any modern application, just use Mutagen, Stagger, PyTagLib, or something else that's not a decade out of date. They'll all export valid Unicode in the ID3v2 tags, and do the best possible with the ID3v1 tags (which usually isn't great, but usually doesn't matter).",A,4
12289072,2012-09-05 20:14:46.960000+00:00,"The function body is compiled, then it assigned to the local namespace under the variable name func.
When you then run the second exec statement a new function is stored under that same name, overwriting the first. You can preserve the first one by storing a reference to it in a new name:
foo = func

so you can later on refer to it still as foo:
foo()

You could also store it in a dictionary, a list, or as an attribute on another object.",A,5
41514422,2017-01-06 21:12:01.060000+00:00,"Use Q to do an OR query. 
from django.db.models import Q
UserComment.objects.filter(Q(sender=request.user) | Q(receiver=request.user))
",A,2
21313105,2014-01-23 15:52:57.330000+00:00,"Dict comprehension with two for loops to the rescue:
letter_score = {letter: score for score, letters in scrabble_scores
                              for letter in letters.split()}

E.g. for every letter in the string (separated by whitespace), produce a key and value pair in the output dictionary; the key is the letter, the value the score.
Demo:
>>> scrabble_scores = [(1, ""E A O I N R T L S U""), (2, ""D G""), (3, ""B C M P""),
...                (4, ""F H V W Y""), (5, ""K""), (8, ""J X""), (10, ""Q Z"")]
>>> letter_score = {letter: score for score, letters in scrabble_scores
...                               for letter in letters.split()}
>>> letter_score
{'A': 1, 'C': 3, 'B': 3, 'E': 1, 'D': 2, 'G': 2, 'F': 4, 'I': 1, 'H': 4, 'K': 5, 'J': 8, 'M': 3, 'L': 1, 'O': 1, 'N': 1, 'Q': 10, 'P': 3, 'S': 1, 'R': 1, 'U': 1, 'T': 1, 'W': 4, 'V': 4, 'Y': 4, 'X': 8, 'Z': 10}
>>> letter_score['Q']
10

Bonus word score calculator:
>>> word = 'QUICK'
>>> sum(letter_score[c] for c in word)
20

where word is a uppercase string containing only (scrabble) letters, ignoring double- and triple-letter scoring.",A,3
26811172,2014-11-07 22:29:36.063000+00:00,"First, it often helps to refactor things out into functions. In this case, it helps because you can easily return early from a function; you can't return early from a block of code in the middle of other code.
def first_match(haystack):
    r = re.match('cond1', haystack)
    if r:
        # work with r.group()
        return
    r = re.match('cond2', l)
    if r:
        # work with r.group()
        return
    r = re.match('cond3', l)
    if r:
        # work with r.group()

All the else bits and indentation headaches go away.

Also, in general, when you're asking how to chain 3 or more things together, the right answer is to figure out how to chain any arbitrary number of things together, and just do that with N=3. Which usually means a loop (or a loop hidden inside a function like map, or a recursive function definition, etc.). For example:
def first_match(exprs, haystack):
    for expr in exprs:
        r = re.match(expr, haystack)
        if r:
            return r


In this case, however, what you're trying to do is actually doable. Maybe not a good idea, but… A regex match object is always truthy. And of course None is falsey. So:
r = re.match('cond1', l) or re.match('cond2', l) or re.match('cond3', l)
if r:
    # work with r.group(), which is the group for whichever matched

But notice that if you want to use the truthiness, you can do that in a loop too:
next(filter(bool, (re.match(cond, l) for cond in ('cond1', 'cond2', 'cond3'))))


Finally, you're already using regular expressions, so why not use regular expressions?
r = re.match('cond[123]', l)
if r:
    # work with r.group()
",A,1
39963544,2016-10-10 17:20:57.570000+00:00,"That's not a hex string. You are confusing the Python repr() output for a bytestring, which aims to make debugging easier, with the contents.
Each \xhh is a standard Python string literal escape sequence, and displaying the string like this makes it trivial to copy and paste into another Python session to reproduce the exact same value.
You don't need to hex decode this at all.
An actual hex string consists only of the digits 0 through to 9, and the letters a through to f (upper or lowercase). Your value, converted to hex, looks like this:
>>> value='(\xd2M\x00\x18\x00\x18\x80\x00\x80\x00\x00\x00\x00\x00\x00\xe0\xd2\xe0\xd2.\xd2\x00\x00\x00\x00\x00\x00\n\x00\x18\x00&\x00\x00\x00\x00\x00\x00\x00\x0f0\xfe/\x010\xff/\x000\xff/\x000\xff/\xff/\xff/\xff/\xff/\x000\xff/\xff/\xff/\x000\x000\xff/\x000\x000\x000\xff/\xff/\x000\x000\xff/\x000\xad\xff\x0c\x00\xdd\xff\xc2\xff\xd3\xff\xde\xff\xe9\xff\xca\xff\xd8\xff\xe6\xff\xb5\xff\xb2\xff\xe6\xff\x92\xff\xd0\xff\xa0\xff\xbd\xff\xb4\xff\x82\xff\x90\xfff\xff\xe1\xff\x9f\xff\x94\xff\xd4\xff\xa4\xff\xbb\xff\xe8\xff\x00\x00\x02\x00\xff\x7f\xff\x7f\x97\xff\xd0\xff\xb7\xff~\xffG\xff\xa1\xff\xa1\xff\xcd\xab\x00\x00A\n\x00\x00'
>>> import binascii
>>> binascii.hexlify(value)
'28d24d00180018800080000000000000e0d2e0d22ed20000000000000a00180026000000000000000f30fe2f0130ff2f0030ff2f0030ff2fff2fff2fff2fff2f0030ff2fff2fff2f00300030ff2f003000300030ff2fff2f00300030ff2f0030adff0c00ddffc2ffd3ffdeffe9ffcaffd8ffe6ffb5ffb2ffe6ff92ffd0ffa0ffbdffb4ff82ff90ff66ffe1ff9fff94ffd4ffa4ffbbffe8ff00000200ff7fff7f97ffd0ffb7ff7eff47ffa1ffa1ffcdab0000410a0000'
",A,2
27826904,2015-01-07 19:13:45.333000+00:00,"You can find direct children of the parent element:
soup.select('body > div')

to get all div elements under the top-level body tag.
You could also find the first div, then grab all matching siblings with Element.find_next_siblings():
first_div = soup.find('div')
all_divs = [first_div] + first_div.find_next_siblings('div')

Or you could use the element.children generator and filter those:
all_divs = (elem for elem in top_level.children if getattr(elem, 'name', None) == 'div')

where top_level is the element containing these div elements directly.",A,3
1248556,2009-08-08 10:52:48.883000+00:00,"The reason why the ImageFields are not pre-populated is a browser security issue, nothing to do with Django.
If browsers let web pages pre-populate file inputs, it would be very easy for a site to arbitrarily upload content from a user's hard drive without their consent. So in order to stop that happening, file input fields are always rendered as blank.",A,2
16892682,2013-06-03 08:27:32.450000+00:00,"You have two strings with python lists embedded in them. They are not actual lists.
You can go back to making these into lists with unicode values with the ast.literal_eval() function. That feels like we are fixing a problem that was caused by other code, the better fix would be to not build these values like these in the first place.
from ast import literal_eval
import os.path

output = []
for entry in Master:
    base, lst = entry.split(None, 1)
    for name in literal_eval(lst):
        output.append(os.path.join(base, name))

This produces proper unicode paths (a good idea on Windows):
>>> output
[u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_4_new_district', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_3_old_district', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_2_division', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_1_all', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_5_Upazilla', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_4_old_district', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_6_Union_court', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_6_Union', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BD_exposed_coastal_area', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BD_drought', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_1_River', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_1_River_detail', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BD_international_bnd', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_1_River_1', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_7_Mauza', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\test', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_5_UpazillaAnno', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_4_new_districtAnno', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin\\BGD_4_new_districtAnno2', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BCAS_BD_Infrastructure\\BD_Health_Infrastructures_1', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BCAS_BD_Infrastructure\\BD_Railway_Establishments_1', u'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BCAS_BD_Infrastructure\\BGD_roads_1']
>>> print output[0]
E:\GIS_DOCUMENT\BCAS_Map\BCAS_All.gdb\BD_Admin\BGD_4_new_district

If you can, you want to change the original code to produce a dictionary instead, one that uses the base as keys, and a list of names as the value:
{'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BCAS_BD_Infrastructure': [u'BD_Health_Infrastructures_1',
                                                                      u'BD_Railway_Establishments_1',
                                                                      u'BGD_roads_1'],
 'E:\\GIS_DOCUMENT\\BCAS_Map\\BCAS_All.gdb\\BD_Admin': [u'BGD_4_new_district',
                                                        u'BGD_3_old_district',
                                                        u'BGD_2_division',
                                                        u'BGD_1_all',
                                                        u'BGD_5_Upazilla',
                                                        u'BGD_4_old_district',
                                                        u'BGD_6_Union_court',
                                                        u'BGD_6_Union',
                                                        u'BD_exposed_coastal_area',
                                                        u'BD_drought',
                                                        u'BGD_1_River',
                                                        u'BGD_1_River_detail',
                                                        u'BD_international_bnd',
                                                        u'BGD_1_River_1',
                                                        u'BGD_7_Mauza',
                                                        u'test',
                                                        u'BGD_5_UpazillaAnno',
                                                        u'BGD_4_new_districtAnno',
                                                        u'BGD_4_new_districtAnno2']}
",A,2
49213359,2018-03-10 19:50:17.080000+00:00,"Just test for a non-empty object first:
for season in html:
    images = season['image']
    if not images:
        continue
    test = images['medium']

not images is true if images is None, an empty dictionary, 0, or any other object that tests as false.
You could also explicitly test for `None:
if images is None:
    continue

or you could invert the tests:
if images and 'medium' in images:
    #  there is a medium image

or
if images is not None and 'medium' in images:
    #  there is a medium image
",A,1
18590102,2013-09-03 10:36:29.257000+00:00,"That would work, but there's no need to get the user again. request.user is already the user object.
owner = request.user
owner.username = newusername
owner.save()    
",A,3
19068661,2013-09-28 15:42:11.157000+00:00,"You need to mark total as a global if you are to assign to it in your function:
def get_class_average(students):
    global total
    for student in students:
        total += get_class_average(students)
    return float(total) / len(students)

By assigning to a given variable, you are marking it as a local name unless you explicitly mark it as a global.
You may want to move total into the function instead of making it a global:
def get_class_average(students):
    total = 0
    for student in students:
        total += get_class_average(students)
    return float(total) / len(students)

You don't appear to be using it elsewhere in your code, so making it a global is overkill here.
Your next problem is that you are calling get_class_average() recursively; the function is calling itself, passing in the exact same arguments. The function will never return. I suspect you meant to call the average() function instead, passing in a specific list for each student, like the homework key, or quizzes or tests:
def get_class_average(students, key):
    total = 0
    for student in students:
        total += average(student[key])
    return float(total) / len(students)

homework_average = get_class_average(students, 'homework']
",A,4
13989932,2012-12-21 12:09:40.923000+00:00,"Use a list comprehension:
subst = {0: 'x', 1: 'y'}
return [subst.get(v, v) if v else v for v in search.Search(s, s.run()]

By using return you exit the loop early.
The if v else v helps avoid attempting to use the empty lists as keys (which would raise a TypeError; lists are not hashable and thus not allowed as keys). The alternative strategy would be:
from collections import Hashable

subst = {0: 'x', 1: 'y'}
hashable = lambda v: isinstance(v, Hashable)
return [subst.get(v, v) if hashable(v) else v for v in search.Search(s, s.run()]

The subst map is a little easier to extend; but using a function to map the value (as used in Jacob's answer) could be more readable to you.",A,4
24510379,2014-07-01 12:31:34.077000+00:00,"os.path.join() is not suitable for unsafe input, no. It is entirely deliberate that an absolute path ignores arguments before it; this allows for supporting both absolute and relative paths in a configuration file, say, without having to test the entered path. Just use os.path.join(standard_location, config_path) and it'll do the right thing for you.
Take a look at Flask's safe_join() to handle untrusted filenames:
import posixpath
import os.path

_os_alt_seps = list(sep for sep in [os.path.sep, os.path.altsep]
                    if sep not in (None, '/'))

def safe_join(directory, filename):
    # docstring omitted for brevity
    filename = posixpath.normpath(filename)
    for sep in _os_alt_seps:
        if sep in filename:
            raise NotFound()
    if os.path.isabs(filename) or \
       filename == '..' or \
       filename.startswith('../'):
        raise NotFound()
    return os.path.join(directory, filename)

This uses the posixpath (the POSIX implementation for the platform-agnostic os.path module) to normalise the URL path first; this removes any embedded ../ or ./ path segments, making it a fully normalised relative or absolute path.
Then any alternative separators other than / are excluded; you are not allowed to use /\index.html for example. Last but not least, absolute filenames, or relative filenames are specifically prohibited as well.",A,3
1041206,2009-06-24 21:53:48.150000+00:00,"You are missing the plone_kss layer in your skins. Go to the ZMI (via site setup or by adding /manage to your Plone homepage), find the portal_skins tool, and check your skin layers in the properties tab. Make sure the plone_kss layer is listed before the custom layer and you should be set to go.
In future, you can use a command line tool like find to locate missing skin methods; I found this one in a buildout by using the following command:
$ find parts/plone -name kss_generic_macros.*
parts/plone/CMFPlone/skins/plone_kss/kss_generic_macros.pt

You can automate this with the following Python Script:
from Products.CMFCore.utils import getToolByName
layer = 'plone_kss'
skins = getToolByName(context, 'portal_skins')
for name in skins.getSkinSelections():
    path = skins.getSkinPath(name)
    path = [i.strip() for i in path.split(',')]
    if layer not in path:
        try:
            path.insert(path.index('custom') + 1, layer)
        except ValueError:
            path.append(layer)
        skins.addSkinSelection(name, ','.join(path))

This script loops over all skins in the skins tool, and will insert plone_kss into any one of them where it is missing; either just after the custom layer, or at the end if there is no custom layer present.
The reason you want your custom layer to be first is that this layer by custom (no pun intended) holds customized versions of skin assets. If you were to customize an asset from the portal_kss layer through the ZMI then the customized version will be put in the custom layer. Because skin lookup rules will look through the layer stack top-to-bottom, you want to make sure your customized versions in the custom layer are found before the originals in the lower layers.",A,3
15835083,2013-04-05 13:21:19.743000+00:00,"You need to specify install_requires instead, see New and changed setup keywords.
The requires field was too vague and imprecise, so the setuptools folk (so easy_install, from which pip evolved) added more specific fields. In addition, there are setup_requires and test_requires fields for dependencies required for setup.py and for running tests.",A,10
14452182,2013-01-22 05:34:36.617000+00:00,"On my system tshark's output format differs from the one you've shown in the question.
To make parsing more robust, I've changed the command-line:
#!/usr/bin/env python
import shlex
from itertools import dropwhile
from subprocess import check_output as qx

# get tshark output
command = ""tshark -r capture.pcap -qz 'io,stat,0,COUNT(frame)frame'""
lines = qx(shlex.split(command)).splitlines()

# parse output
lines = dropwhile(lambda line: not line.rstrip().endswith(""COUNT""), lines)
next(lines) # skip header
frames_count = int(next(lines).split()[1])
print(frames_count)

You don't need to call tshark to get statistics from a pcap file. You could use a Python library that can parse pcap files e.g., using scapy:
#!/usr/bin/env python
from scapy.all import rdpcap

a = rdpcap('capture.pcap')
frames_count = len(a)
print(frames_count)

To get count for tshark -r capture.pcap -qz 'io,stat,0,ip.src==192.168.230.146' command using scapy:
#!/usr/bin/env python
from scapy.all import IP, sniff

a = sniff(offline='capture.pcap',
          lfilter=lambda p: IP in p and p[IP].src == '192.168.230.146')
count = len(a)
print(count)
",A,1
54553480,2019-02-06 12:17:24.167000+00:00,"Your code is actually working, but you need to take the treenodes[0] entry (the CEO). The remainder of the key-value pairs in treenodes are there just for bookkeeping, to easily find the given manager for a given employee entry.
If you can't count on 0 being the ID of the root node, then you could use the fact that the CEO is marked as managing themselves; the root node is the one where the manager ID points to their own ID. A more common scenario is that root nodes simply do not have a parent ID.
You also added the CEO to their own children list (the manager ID for the CEO is their own ID), so you have a recursive reference in your tree.
The code you found is not the clearest or most efficient. I'd build a dictionary from id to copied object (so your original lst dictionaries are unchanged), then loop over that structure and and add entries to their manager id entry. I'm using the 'root nodes self-reference' rule (so the manager ID equals their own ID):
employees = {}
managers = set()
root_id = None
for emp in lst:
    id, mid = emp['id'], emp['ManagerID']
    # create a copy of emp, and add a ""children"" list
    employees[id] = {**emp, 'children': []}
    managers.add(mid)
    if id == mid:
        # the root of the tree references itself as the manager
        root_id = id

# add empty manager entries for missing manager IDs, reporting to root ID.
for id in managers - employees.keys():
    employees[id] = {
        'id': id, 'ManagerID': root_id, 'children': [],
        'job': None, 'name': None
    }

for id, emp in employees.items():
    manager = employees[emp.pop('ManagerID')]
    if id != root_id:  # don't add the root to anything
        manager['children'].append(emp)

output = employees[root_id]

The above uses a set to track what manager IDs have been seen, so you can trivially add missing manager entries (reporting to the CEO in this case).
For your input, that produces:
{'id': 0, 'job': 'CEO', 'name': 'John Smith', 'children':
    [{'id': 1, 'job': 'Medical Manager', 'name': 'Medic 1', 'children':
        [{'id': 2, 'job': 'Medical Assist', 'name': 'Medic 2', 'children': []}],
     },
     {'id': 3, 'job': 'ICT Manager', 'name': 'ICT 1', 'children':
        [{'id': 4, 'job': 'ICT Assist', 'name': 'ICT 2', 'children':
            [{'id': 5, 'job': 'ICT Junior', 'name': 'ICT 3', 'children': []}]
         }]
     }]
}
",A,1
29333769,2015-03-29 19:28:24.937000+00:00,"This isn't at all a question about lists.
Presumably you have a separate Comment model, and that model has a ForeignKey to the Question model; if you don't have that structure, then you should.
Then it's simply a matter of following the reverse relationship for each question:
{% for question in latest_question_list %}
    ...
    {% for comment in question.comment_set.all %}
        {{ comment.text }}
    {% endfor %}
{% endfor %}

Note that this exact structure is well described in the tutorial, with the example of Poll and Choice.",A,1
11435979,2012-07-11 15:22:42.010000+00:00,"Here's a modified kindall's answer that allows empty result for non-empty format string:
format = raw_input(""What is the format? >>> "")
prompt = ""What is the value for parameter {0}? >>> ""
params  = []

while True:
    try:
        result = format.format(*params)
    except IndexError:
        params.append(raw_input(prompt.format(len(params))))
    else:
        break
print result
",A,2
14897791,2013-02-15 15:20:54.020000+00:00,"In cPython, objects are automatically destroyed when their reference count drops to 0.
If you delete a entry from your list, the reference count for the object in question is decremented. You do not need to do anything special to have it destroyed, nor should you try to. If any other reference to the object still exists, then destroying the object would break the other reference.
If you want to hold references to an object that do not stop the object from being garbage collected, use a weakref reference instead.",A,3
14379804,2013-01-17 13:06:11.727000+00:00,"Django doesn't care what your urlpatterns file is called. The default base urlconf is by convention called urls.py, but in fact that's just a setting and can be overridden. After that, you need to explicitly include urlconfs by module name, so again it makes no difference what they're called.
I'm not familiar with Django-CMS and I don't know what it's doing in its CMSApp class, but I suspect you're going to have to dig into that to see what's going on.",A,4
598227,2009-02-28 16:07:32.927000+00:00,"""Can someone explain this to me?""
Depends. 
Here's one explanation, but it's not helpful.  Your question stems from your assumptions.  Since the real world conflicts with your assumptions, it must mean  your assumptions are wrong.  Not much of an explanation, but that's why you're asking.
""Exception handling means a dynamic call and a static return, whereas an if statement is static call, static return.""
What does ""dynamic call"" mean?  Searching stack frames for a handler?  I'm assuming that's what you're talking about.  And a ""static call"" is somehow locating the block after the if statement.
Perhaps this ""dynamic call"" is not the most costly part of the operation.  Perhaps the if-statement expression evaluation is slightly more expensive than the simpler ""try-it-and-fail"".
Turns out that Python's internal integrity checks are almost the same as your if-statement, and have to be done anyway.  Since Python's always going to check, your if-statement is (mostly) redundant.
You can read about low-level exception handling in http://docs.python.org/c-api/intro.html#exceptions.  

Edit
More to the point:  The if vs. except debate doesn't matter.
Since exceptions are cheap, do not label them as a performance problem.
Use what makes your code clear and meaningful.  Don't waste time on micro-optimizations like this. ",A,9
21563924,2014-02-04 21:32:39.497000+00:00,"That is not json. It is a string representation of a python dict, which is something quite different.
You can use ast.literal_eval to load it.",A,4
4107136,2010-11-05 15:02:34.640000+00:00,"Define a ManyToMany field on either Customer or Store, pointing to the other model, with Order as the through table. For example:
class Store(models.Model):
    name = models.CharField(max_length=255)
    orders = models.ManyToManyField(Customer, through=Order)

Now you can do:
from django.db.models import Count
Store.objects.annotate(Count(""orders__customer""))
",A,0
13521880,2012-11-23 00:47:43.807000+00:00,"It seems your list (with ""|"", ""@"") is short and ""@"" is always present, just in different locations. In this case you can replace the first occurrence of ""@"" in the appl list in one line:
appl[appl.index(""@"")] = "" ""

If the list is large and the item might not be present:
i = next((i for i, x in enumerate(lst) if x == item), None)
if i is not None:
   lst[i] = replacement
",A,0
2507023,2010-03-24 10:57:29.360000+00:00,"The NULL makes no sense.  It's simply bad SQL.  
The exists clause is supposed to use SELECT *.
People make up stories about the cost of SELECT *.  They claim it does an ""extra"" metadata query.  It doesn't.  They claim it's a ""macro expansion"" and requires lots of extra parse time.  It doesn't.",A,5
12058762,2012-08-21 15:57:47.510000+00:00,"high = mid is correct. The right boundary is not included. Notice that initially high = &tab[n] i.e., it points past the last element in tab.",A,2
50500256,2018-05-24 02:58:25.073000+00:00,"The problem is that your URLs are invalid, because they all end with a newline. You can see the same thing like this:
>>> page = requests.get('http://www3.asiainsurancereview.com//Mock-News-Article/id/42945/Type/eDaily/New-Zealand-Govt-starts-public-consultation-phase-of-review-of-insurance-law\n')
>>> page
<Response [400]>
>>> page.text
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""""http://www.w3.org/TR/html4/strict.dtd"">
<HTML><HEAD><TITLE>Bad Request</TITLE>
<META HTTP-EQUIV=""Content-Type"" Content=""text/html; charset=us-ascii""></HEAD>
<BODY><h2>Bad Request - Invalid URL</h2>
<hr><p>HTTP Error 400. The request URL is invalid.</p>
</BODY></HTML>

BeautifulSoup is parsing that HTML just fine. It's just not very useful HTML. And, in particular, it doesn't have anything with class article-wrap or class mag-article-wrap, so both of your find return None. And you don't have any error handling for that case; you just try to use the None value as if it were an HTML element, hence the exception.
You should have noticed this from printing out each a: there's an extra blank line after each line. That either means that there are newline characters in the strings (which is what's actually happening), or that there are blank lines between the actual lines (which would be an even more invalid URL—you'd get a ConnectionError or some subclass of it).

What you want to do is simple: just strip the newlines off each line:
for a in links:
    a = a.rstrip()
    # rest of your code
",A,5
14738094,2013-02-06 20:23:04.637000+00:00,"The first time you execute for line in f2:, it goes through every line in f2, leaving the current file pointer at the end of the file. So, the second time, it starts from the end of the file, and goes through all 0 of the remaining lines…
If you want to repeat that, there are a few ways to do it. You could move the f2 = open(file1, 'r') into the outer loop. Or you could reset the file with f2.seek(0, 0). Or you could use itertools.tee. 
But, unless you really don't have the memory to hold the whole file at once, you probably want to read the lines into a list the first time and just iterate that list:
l2 = list(f2)
# ...
for line in f1:
    # ...
    for line1 in l2:
        # ...
",A,1
14361362,2013-01-16 15:03:12.237000+00:00,"dir() does much more than look up __dict__
First of all, dir() is a API method that knows how to use attributes like __dict__ to look up attributes of an object.
Not all objects have a __dict__ attribute though. For example, if you were to add a __slots__ attribute to your custom class, instances of that class won't have a __dict__ attribute, yet dir() can still list the available attributes on those instances:
>>> class Foo(object):
...     __slots__ = ('bar',)
...     bar = 'spam'
... 
>>> Foo().__dict__
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'Foo' object has no attribute '__dict__'
>>> dir(Foo())
['__class__', '__delattr__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', 'bar']

The same applies to many built-in types; lists do not have a __dict__ attribute, but you can still list all the attributes using dir():
>>> [].__dict__
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'list' object has no attribute '__dict__'
>>> dir([])
['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']

What dir() does with instances
Python instances have their own __dict__, but so does their class:
>>> class Foo(object):
...     bar = 'spam'
... 
>>> Foo().__dict__
{}
>>> Foo.__dict__.items()
[('__dict__', <attribute '__dict__' of 'Foo' objects>), ('__weakref__', <attribute '__weakref__' of 'Foo' objects>), ('__module__', '__main__'), ('bar', 'spam'), ('__doc__', None)]

The dir() method uses both these __dict__ attributes, and the one on object to create a complete list of available attributes on the instance, the class, and on all ancestors of the class.
When you set attributes on a class, instances see these too:
>>> f = Foo()
>>> f.ham
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'Foo' object has no attribute 'ham'
>>> Foo.ham = 'eggs'
>>> f.ham
'eggs'

because the attribute is added to the class __dict__:
>>> Foo.__dict__['ham']
'eggs'
>>> f.__dict__
{}

Note how the instance __dict__ is left empty. Attribute lookup on Python objects follows the hierarchy of objects from instance to type to parent classes to search for attributes.
Only when you set attributes directly on the instance, will you see the attribute reflected in the __dict__ of the instance, while the class __dict__ is left unchanged:
>>> f.stack = 'overflow'
>>> f.__dict__
{'stack': 'overflow'}
>>> 'stack' in Foo.__dict__
False

TLDR; or the summary
dir() doesn't just look up an object's __dict__ (which sometimes doesn't even exist), it will use the object's heritage (its class or type, and any superclasses, or parents, of that class or type) to give you a complete picture of all available attributes.
An instance __dict__ is just the 'local' set of attributes on that instance, and does not contain every attribute available on the instance. Instead, you need to look at the class and the class's inheritance tree too.",A,104
1362647,2009-09-01 13:58:29.753000+00:00,"Yes.
Separate reusable ""sub-libraries"" into separate projects early and often.
Look at open-source practices: most things are many small projects that are pieced together.
Create many small projects.
Avoid checking in binaries.  Again, follow open source practices.  Keep source in subversion.
Build binaries and put them on a ""project share"" directory separate from the source.",A,1
16510328,2013-05-12 17:46:40.330000+00:00,"It is called a class attribute. Python doesn't distinguish between public and private; privacy is only indicated by convention, and is not enforced.
It is technically good practice if you need to share data among instances. Remember, methods are class attributes too!",A,6
18367068,2013-08-21 20:29:09.437000+00:00,"This is exactly what bytearray is for:
newFileByteArray = bytearray(newFileBytes)
newFile.write(newFileByteArray)

If you're using Python 3.x, you can use bytes instead (and probably ought to, as it signals your intention better). But in Python 2.x, that won't work, because bytes is just an alias for str. As usual, showing with the interactive interpreter is easier than explaining with text, so let me just do that.
Python 3.x:
>>> bytearray(newFileBytes)
bytearray(b'{\x03\xff\x00d')
>>> bytes(newFileBytes)
b'{\x03\xff\x00d'

Python 2.x:
>>> bytearray(newFileBytes)
bytearray(b'{\x03\xff\x00d')
>>> bytes(newFileBytes)
'[123, 3, 255, 0, 100]'
",A,92
377939,2008-12-18 14:07:35.687000+00:00,"""added a numbered order field"" - good.
""update all the items with their new order numbers"" - avoidable.
Use numbers with gaps. 

Floating point.  That way, someone can insert ""1.1"" between 1 and 2.  I find that this works nicely, as most people can understand how the sequencing works.  And you don't have to worry too much about how much space to leave -- there's lots and lots of space between each number.
On the initial load, number the articles by the 100 or 1000 or something with space between each one.  In this case, you have to guess how many digits to leave for reordering.
A comma-separated position.  Initially, they're all (1,0), (2,0), (3,0), etc. But when you want to rearrange things, you might have to introduce (2,1) and (2,2) that go after (2,0) but before (3.0).
This looks kind of complicated, but some people like this kind of complexity.  It's essentially the same as floating-point, except the single number is replace by a (whole-number, implicit-fraction) tuple.  And this extends to handle hierarchies.
",A,2
25399220,2014-08-20 07:37:31.940000+00:00,"You'll need to manually encode Unicode values to bytes; for CSV usually UTF-8 is fine:
for rownum in xrange(sh.nrows):
    wr.writerow([unicode(c).encode('utf8') for c in sh.row_values(rownum)])

Here I use unicode() for column data that is not text.
The character you encountered is the U+2018 LEFT SINGLE QUOTATION MARK, which is just a fancy form of the ' single quote. Office software (spreadsheets, word processors, etc.) often auto-replace single and double quotes with the 'fancy' versions. You could also just replace those with ASCII equivalents. You can do that with the Unidecode package:
from unidecode import unidecode

for rownum in xrange(sh.nrows):
    wr.writerow([unidecode(unicode(c)) for c in sh.row_values(rownum)])

Use this when non-ASCII codepoints are only used for quotes and dashes and other punctuation.",A,1
2115732,2010-01-22 07:30:06.263000+00:00,"It is already being fixed, and will be in Django 1.2.",A,1
43809431,2017-05-05 16:08:46.440000+00:00,"Subclass ModelChoiceField and define label_from_instance to return the value you want.
class MyModelChoiceField(forms.ModelChoiceField):
    def label_from_instance(self, obj):
        return ""{} | {}"".format(obj.val_a, obj.val_b)

Now use that with a standard queryset:
select = MyModelChoiceField(queryset=OrderEntry.objects.all(), required=True)
",A,2
17459374,2013-07-03 22:56:31.337000+00:00,"I'm assuming the main things you want to do are:

Add a new log line to the end.
Add a new log line very near to the end (if your program is multithreaded or otherwise asynchronous).
Search for the line nearest a given timestamp.
Search for all lines within a given time range.

A binary search tree is O(log N) for all of these. So is a skip list or a B-tree, or a number of other data structures.
So, how do you choose between them?
Well, unless you really need to build this yourself, you probably care more about the interface than the exact performance characteristics, as long as they're all ""fast enough"", which they probably are. For example, blist.sorteddict is a carefully-designed class that's as close to a drop-in replacement for dict as anything sorted could be. bintrees.RBTree has some cool features like key slicing that you might not even think to look for, but might use all the time once you find them. sqlite3 can be trivially backed to disk, and can be searched by nice tools you already have on your system. Likely one of these will be more important to you than the differences between a B+Tree, a red-black tree, and an array with a B-tree index.
If squeezing out the last bit of performance does matter, you probably want to test with realistic data sets on characteristic systems, rather than try to guess. There are some good rules of thumb: B-trees are good if you may have enough data to swap to memory; skiplists are good for fine-grained locking; skiplists are good for reading large ranges; binary trees are good for quickly approximating by position instead of key; etc. But you'll still want to test your guess anyway, so don't put too much effort into guessing.",A,0
12303240,2012-09-06 15:17:34.817000+00:00,"When passed a command, pexpect.spawn() forks and invokes the command immediately. The echo command in your example would have been run already.
This is implied in the spawn constructor documentation:

This is the constructor. The command parameter may be a string that
  includes a command and any arguments to the command. For example::
child = pexpect.spawn ('/usr/bin/ftp')
child = pexpect.spawn ('/usr/bin/ssh user@example.com')
child = pexpect.spawn ('ls -latr /tmp')

You may also construct it with a list of arguments like so::
child = pexpect.spawn ('/usr/bin/ftp', [])
child = pexpect.spawn ('/usr/bin/ssh', ['user@example.com'])
child = pexpect.spawn ('ls', ['-latr', '/tmp'])

After this the child application will be created and will be ready to
  talk to.

I've confirmed this by looking at the source code; the constructor calls _spawn() which is documented as follows:

This starts the given command in a child process. This does all the
  fork/exec type of stuff for a pty. This is called by __init__. If args
  is empty then command will be parsed (split on spaces) and args will be
  set to parsed arguments.
",A,3
15251430,2013-03-06 15:31:04.343000+00:00,"Split the line on whitespace using .split() and test if the first element starts with a string using .startswith():
import subprocess

output = subprocess.check_output(['chkconfig', '--list'])

for line in output.splitlines():
    if line.startswith('httpd_'):
        print line.split()[0]

For older python versions, use a Popen() call directly:
output = subprocess.Popen(['chkconfig', '--list'], stdout=subprocess.PIPE).stdout

for line in output:
    if line.startswith('httpd_'):
        print line.split()[0]
",A,3
16997244,2013-06-08 07:36:35.667000+00:00,"The first code example in your question passes incorrect command because shell=True changes the meaning of the first parameter, from the subprocess docs:

On Unix with shell=True, the shell defaults to /bin/sh. If args is a
  string, the string specifies the command to execute through the shell.
  ..[snip]..
If args is a
  sequence, the first item specifies the command string, and any
  additional items will be treated as additional arguments to the shell
  itself. That is to say, Popen does the equivalent of:

Popen(['/bin/sh', '-c', args[0], args[1], ...])

The second code example from you question should work if the following command works in a shell:
$ /bin/sh -c './myfile -u username'


To fix your command, you could omit possibly unnecessary shell=True and use check_call():
import subprocess

subprocess.check_call([""./myfile"", ""-u"", ""username""])
",A,1
13280789,2012-11-08 00:46:12.683000+00:00,"Here's a list of things I can think of that make this dangerous, in rough order from worst to least bad:

It's likely to be confusing to someone reading or debugging your code.
You won't have gotten the right __init__ method, so you probably won't have all of the instance variables initialized properly (or even at all).
The differences between 2.x and 3.x are significant enough that it may be painful to port.
There are some edge cases with classmethods, hand-coded descriptors, hooks to the method resolution order, etc., and they're different between classic and new-style classes (and, again, between 2.x and 3.x).
If you use __slots__, all of the classes must have identical slots. (And if you have the compatible but different slots, it may appear to work at first but do horrible things…) 
Special method definitions in new-style classes may not change. (In fact, this will work in practice with all current Python implementations, but it's not documented to work, so…)
If you use __new__, things will not work the way you naively expected.
If the classes have different metaclasses, things will get even more confusing.

Meanwhile, in many cases where you'd think this is necessary, there are better options:

Use a factory to create an instance of the appropriate class dynamically, instead of creating a base instance and then munging it into a derived one.
Use __new__ or other mechanisms to hook the construction.
Redesign things so you have a single class with some data-driven behavior, instead of abusing inheritance.

As a very most common specific case of the last one, just put all of the ""variable methods"" into classes whose instances are kept as a data member of the ""parent"", rather than into subclasses. Instead of changing self.__class__ = OtherSubclass, just do self.member = OtherSubclass(self). If you really need methods to magically change, automatic forwarding (e.g., via __getattr__) is a much more common and pythonic idiom than changing classes on the fly.",A,21
41573642,2017-01-10 16:30:10.503000+00:00,You can add arbitrary views that within a ModelAdmin that do whatever you want. See the documentation for ModelAdmin.get_urls. You can do the same at a higher level by defining AdminSite.get_urls.,A,2
13179558,2012-11-01 14:48:24.230000+00:00,"You are looking for the locals() and globals() functions.
dir(), without an argument, returns the equivalent of locals().keys().
globals() is essentially the same as locals() at module scope, but in a function it returns the module scope variables.
>>> dir()
['__builtins__', '__doc__', '__name__', '__package__']
>>> locals().keys()
['__builtins__', '__name__', '__doc__', '__package__']
>>> def foo():
...     return locals().keys(), globals().keys()
... 
>>> foo()
([], ['__builtins__', '__name__', 'foo', '__doc__', '__package__'])
",A,4
45243586,2017-07-21 17:35:14.097000+00:00,"You're calling your get_func_names() function, which queries the EmployeeProfile model, at class level in your EmployeeFilter class. This means it is called at import time. Since the views are imported on startup, your migrations have not yet had a chance to run.
You should not call any code that accesses the database at import time.",A,0
46636062,2017-10-08 21:05:53.350000+00:00,"You don't have a list comprehension. You passed in a generator expression to print(). The print() function does not iterate over any arguments you pass in; it only sends the str() conversion to stdout.
If you wanted to send all values in the generator to print(), use *(...):
print(*(i for i in listA))

Here that's overkill, you could just do the same for the values in listA:
print(*listA)

Both will print the values with the default separator, a space. If you wanted to have newlines, tell print() to use \n as the separator:
print(*(i for i in listA), sep='\n')
print(*listA, sep='\n')

And no, not every for loop is suitable to be converted to a list comprehension. A list comprehension creates a new list object, and not all loops build lists.
If you have a list of the form:
some_name = []
for <target> in <iterable>:
    some_name.append(<expression>)

then you can convert that to a list comprehension:
some_name = [<expression> for <target> in <iterable>]
",A,1
18308271,2013-08-19 07:14:13.260000+00:00,"Use while True to create an infinite loop, and use break to end it. Now you can end the loop as soon as you read an empty line and not fail when trying to address elements that won't exist:
while True:
    line = input('Line: ')
    if not line:
        break

    phonic = line.split()
    start = phonic[0]
    start_4 = phonic [3]
    a = start[0]
    if start_4.startswith(a):
        print('Good!')
    else:
        print(""That's not right!"")

Note you don't even have to test more than just if line; an empty string is considered false in a boolean test like if.",A,0
41534720,2017-01-08 16:08:08.373000+00:00,"followings is a manager; to show the members of that relationship, you need to call .all() on it (or another manager/queryset method like order_by).
print current_user.followings.all()
",A,2
16024178,2013-04-15 20:29:13.453000+00:00,"
I need a module that can load lots of sounds with a 'for' condition and then create a 'object' for each sound

As @Aya suggested you could use pyglet (+ AVbin for mp3) to play several mp3 files in parallel:
#!/usr/bin/env python3
import os
import random
from glob import glob

import pyglet 

# load sounds    
paths = glob(os.path.expanduser('~/Music/*.mp3'))
sounds = [pyglet.media.load(p, streaming=False) for p in paths]

# play them in parallel
for sound in sounds:
    player = sound.play()
    player.volume = random.random()
    player.push_handlers(on_eos=lambda: print('done playing the sound'))

# exit in `delay` seconds
pyglet.clock.schedule_once(lambda dt: pyglet.app.exit(), delay=10)
# run event loop
pyglet.app.run()

It works on Windows, Mac OS X, Linux. It supports both Python 2 and 3 (pyglet 1.2).",A,0
12565315,2012-09-24 12:46:49.833000+00:00,"Using urlparse is not sufficient to test for a valid IRI.
Use the rfc3987 package instead:
from rfc3987 import parse

parse('http://fdasdf.fdsfîășîs.fss/ăîăî', rule='IRI')
",A,19
142450,2008-09-26 23:11:19.387000+00:00,"Singletons solve one (and only one) problem.
Resource Contention.
If you have some resource that 
(1) can only have a single instance, and 
(2) you need to manage that single instance, 
you need a singleton.
There aren't many examples. A log file is the big one. You don't want to just abandon a single log file. You want to flush, sync and close it properly. This is an example of a single shared resource that has to be managed.
It's rare that you need a singleton. The reason they're bad is that they feel like a global and they're a fully paid up member of the GoF Design Patterns book.
When you think you need a global, you're probably making a terrible design mistake.",A,415
16309019,2013-04-30 21:11:27.003000+00:00,"First, type(seq)( f(x) for x in seq ) is really just type(seq)(imap(f, seq)). Why not just use that?
Second, what you're trying to do doesn't make sense in general. map takes any iterable, not just a sequence. The difference is, basically, that a sequence has a len and is randomly-accessible.
There is no rule that an iterable of type X can be constructed from values of type Y by calling type(X)(y_iter). In fact, while it's generally true for sequences, there are very few other examples for which it is true.
If what you want is to handle a few special types specially, you can do that:
def map2(f, seq):
    it = imap(f, seq)
    if isinstance(seq, (tuple, list)):
        return type(seq)(it)
    else:
        return it

Or, if you want to assume that all sequences can be constructed this way (which is true for most built-in sequences, but consider, e.g. xrange—which wasn't designed as a sequence but does meet the protocol—and of course there are no guarantees beyond what's built in):
def map2(f, seq):
    it = imap(f, seq)
    try:
        len(seq)
    except:
        return it
    else:
        return type(seq)(it)

You could assume that any iterable type that can be constructed from an iterable is a sequence (as you suggested in your question)… but this is likely to lead to more false positives than benefits, so I wouldn't. Again, remember that len is part of the definition of being a sequence, while ""constructible from an iterator"" is not, and there are perfectly reasonable iterable types that will do something completely different when given an iterator.
Whatever you do is going to be a hack, because the very intention is a hack, and goes against the explicit design wishes of the Python developers. The whole point of the iterator/iterable protocol is that you should care about the type of the iterable as rarely as possible. That's why Python 3.x has gone further and replaced the list-based functions like map and filter with iterator-based functions instead.

So, how do we turn one of these transformations into a decorator?
Well, first, let's skip the decorator bit and just write a higher-order function that takes an imap-like function and returns an equivalent function with this transformation applied to it:
def sequify(func):
    def wrapped(f, seq):
        it = func(f, seq)
        try:
            len(seq)
        except:
            return it
        else:
            return type(seq)(it)
    return wrapped

So:
>>> seqmap = sequify(itertools.imap)
>>> seqmap(int, (1.2, 2.3))
(1, 2)
>>> sequify(itertools.ifilter)(lambda x: x>0, (-2, -1, 0, 1, 2))
(1, 2)

Now, how do we turn that into a decorator? Well, a function that returns a function already is a decorator. You probably want to add in functools.wraps (although you may want that even in the non-decorator case), but that's the only change. For example, I can write a generator that acts like imap, or a function that returns an iterator, and automatically transform either into a seqmap-like function:
@sequify
def map_and_discard_none(func, it):
    for elem in imap(func, it):
        if elem is not None:
            yield elem

Now:
>>> map_and_discard_none(lambda x: x*2 if x else x, (1, 2, None))
(2, 4)


This, of course, only works for functions with map-like syntax—that is, they take a function and an iterable. (Well, it will accidentally work for functions that take various kinds of wrong types—e.g., you can call sequify(itertools.count(10, 5)) and it will successfully detect that 5 isn't a sequence and therefore just pass the iterator back untouched.) To make it more general, you could do something like:
def sequify(func, type_arg=1):
    def wrapped(*args, **kwargs):
        it = func(f, seq)
        try:
            len(args[type_arg])
        except:
            return it
        else:
            return type(seq)(it)
    return wrapped

And now, you can go crazy with sequify(itertools.combinations, 0) or whatever you prefer. In this case, to make it a useful decorator, you probably want to go a step further:
def sequify(type_arg=1):
    def wrapper(func):
        def wrapped(*args, **kwargs):
            it = func(f, seq)
            try:
                len(args[type_arg])
            except:
                return it
            else:
                return type(seq)(it)
        return wrapped
    return wrapper

So you can do this:
@sequify(3)
def my_silly_function(pred, defval, extrastuff, main_iterable, other_iterable):
",A,1
13358975,2012-11-13 10:22:13.243000+00:00,"Sets are just like dict and list; on creation they copy the references from the seeding iterable.
Iterators cannot be sets, because you cannot enforce the uniqueness requirement of a set. You cannot know if a future value yielded by an iterator has already been seen before.
Moreover, in order for you to determine what the intersection is between two iterables, you have to load all data from at least one of these iterables to see if there are any matches. For each item in the second iterable, you need to test if that item has been seen in the first iterable. To do so efficiently, you need to have loaded all the items from the first iterable into a set. The alternative would be to loop through the first iterable from start to finish for each item from the second iterable, leading to exponential performance degradation.",A,4
3395488,2010-08-03 10:09:25.767000+00:00,"
""How can I aggregate a large number of save() calls into a single database operation?""

You don't need to.  Django already manages a cache for you.   You can't improve it's DB caching by trying to fuss around with saves. 

""write performance problems are probably related to the fact that I'm creating a large number of rows""

Correct.
SQLite is pretty slow.  That's the way it is.  Queries are faster than most other DB's.  Writes are pretty slow.
Consider more serious architecture change.  Are you loading rows during a web transaction (i.e., bulk uploading files and loading the DB from those files)?
If you're doing bulk loading inside a web transaction, stop.  You need to do something smarter.  Use celery or use some other ""batch"" facility to do your loads in the background.
We try to limit ourself to file validation in a web transaction and do the loads when the user's not waiting for their page of HTML.",A,0
47560581,2017-11-29 19:44:10.330000+00:00,"Property objects are data descriptor objects that live on the class, while your base class uses data as an instance attribute. By making it a property in the subclass, any and all access to self.data will now address the property, even in methods defined on the base class. And as super() addresses attributes (including descriptors) only on the specific parent class, super().data will fail; there is no such class attribute on the base class.
That means that the line
self.data = {}

will trigger your data property setter, which tries to address a class attribute on a parent class that doesn't exist. Note that even if it did exist, you cant assign to a super()-bound object; super() only supports the __get__ method on descriptors, it does not support the __set__ or __delete__ hooks required mutating attributes; so super().data = ... fails doubly here.
Every read access will trigger the getter, so
for x in range(10):
    self.data[x] = x * x

will call the getter each time, returning the result, then assigning to a key in the resulting dictionary. Because your getter creates a new dictionary each time, that assignment is lost entirely; the result of the dict comprehension is not referenced anywhere else, so after the key-value pair is assigned, the dictionary is garbage collected, changes and all.
You'll need to redirect storage of that dictionary to a new instance attribute, and mutate the dictionary in-place (so that a reference is retained at all times):
some_set_that_changes_at_runtime = set(some_list_that_changes_at_runtime)

class B(A):
    @property
    def data(self):
        # filter out specific keys
        for key in self._data.keys() & some_set_that_changes_at_runtime:
            del self._data[key]
        return self._data

    @data.setter
    def data(self, value):
        self._data = value

So now the property uses an instance attribute _data. Assignment to self.data will trigger the setter, which redirects the new dictionary produced from the value to self._data instead. The getter removes unwanted keys, keeping the same dictionary object so that any other references to it are still valid, and assignments of new values to keys will end up in the right place.
The filtering uses a set intersection with the dictionary view on the keys; this produces a new set so the loop only iterates on keys actually present in both.",A,3
29161740,2015-03-20 07:46:32.210000+00:00,"Query.one() requires that there is only one result in the result set; it is an error if the database returns 0 or 2 or more results and an exception will be raised.
Query.first() returns the first of a potentially larger result set, or None if there were no results. No exception will be raised.
From the documentation for Query.one():

Return exactly one result or raise an exception.

and from Query.first():

Return the first result of this Query or None if the result doesn’t contain any row.

(emphasis mine).
In terms of a Python list, one() would be:
def one(lst):
    if not lst:
        raise NoResultFound
    if len(lst) > 1:
        raise MultipleResultsFound
    return lst[0]

while first() would be:
def first(lst):
    return lst[0] if lst else None

There is also a Query.one_or_none() method, which raises an exception only if there are multiple results for the query. Otherwise it'll return the single result, or None if there were no results.
In list terms, that'd be the equivalent of:
def one_or_none(lst):
    if not lst:
        return None
    if len(lst) > 1:
        raise MultipleResultsFound
    return lst[0]
",A,39
23845025,2014-05-24 12:29:02.720000+00:00,"With BeautifulSoup this is trivial:
from bs4 import BeautifulSoup

soup = BeautifulSoup(yoursource)

for cite in soup.find_all('cite'):
    print cite.string

For the given sample file that produces:
>>> import requests
>>> from bs4 import BeautifulSoup
>>> r = requests.get('https://raw.githubusercontent.com/MortezaLSC/Puplic/master/file.xml')
>>> soup = BeautifulSoup(r.content)
>>> for cite in soup.find_all('cite'):
...     print cite.string
... 
taksuncontrol.com
www.royal-jelve.ir
",A,3
15137646,2013-02-28 14:13:01.907000+00:00,"Leave the index at -1, and catch exceptions instead:
def add_release(self, release, index=-1):
    self.releases.insert(index, release)

When you use .insert() with a negative index, the item is inserted relative to the length of the list. Out-of-bounds indices are brought back to bounds; inserting beyond the length is the same as appending, insertion before the 0 index inserts at 0 instead.",A,6
29393300,2015-04-01 14:20:49.993000+00:00,"You never created an instance of the class, so the Shop.__init__() method was never run either.
Your class doesn't have such an attribute; the only attribute you defined for your Shop class is the __init__ method itself.
Create an instance of the class, then look for the attribute on that instance:
# Initial inventory including 2 of each 6 models available
inventory = 12
# Markup of 20% on all sales
margin = .2
# Revenue minus cost after sale
for bike in bikes.values():
    profit = bike.cost * margin

bikeshop = Shop(""Mike's Bikes"", inventory, margin, profit)
print ""Mike's Bikes has {} bikes in stock."".format(bikeshop.inventory)

In creating the instance with Shop(....), Python created the instance and called the __init__ method on that instance. As a result, the inventory attribute was added to the instance, and you can then access it via bikeshop.inventory.",A,6
11308453,2012-07-03 09:54:04.347000+00:00,"There are a few things wrong here.
Anything inside the inner wrapper is called when the method itself is called. Basically, you're replacing the method with that function, which wraps the original. So, your code as it stands would add the method name to the list each time it is called, which probably isn't what you want. Instead, that append should be at the method_wrapper level, ie outside of the inner wrapper. This is called when the method is defined, which happens the first time the module containing the class is imported.
The second thing wrong is that you never actually call the method - you simply return it. Instead of return method you should be returning the value of calling the method with the supplied args - return method(*args, **kwargs).",A,1
18498447,2013-08-28 21:22:41.240000+00:00,"Lists (and sequences in general) are compared in lexicographical order (element by element), exactly as you want. So, your loop is equivalent to:
return not t1 > t2

Of course if the two lists are different lengths, there are various cases that will be handled differently (e.g., your code would return an IndexError in some cases where the other would return False). But really, you have to define what exactly you want to happen in those cases, and then actually coding tests for them will be easy—except that half the time, what you'll want is exactly what the default list comparison already does.",A,2
6066841,2011-05-20 02:21:28.840000+00:00,fileinput,A,6
33170992,2015-10-16 12:52:07.493000+00:00,"To run a command for each file from a given directory:
#!/usr/bin/env python
import os
from subprocess import Popen, PIPE

working_dir = b""/home/MTS/Dropbox/dir/In_Files""
for filename in os.listdir(working_dir):
    print(""Performing calculations on {filename!r}..."".format(**vars()))
    p = Popen([""run_command.command"", ""-f"", ""a""], cwd=working_dir, stdin=PIPE)
    p.communicate(input=b""\n"".join([b""1"", filename, b""4""]))
    if p.returncode != 0: # non-zero exit status may indicate failure
        raise Error

See @skmrx's answer  for the explanation of why your current code fails.",A,1
17615352,2013-07-12 12:51:37.803000+00:00,"You usually use the zip() function to zip lists; your use of map() is a happy side-effect of how that function works.
zip() still produces tuples, but you can combine that with map():
map(list, zip(a, c, d))

or you can use a list comprehension (slower but more flexibility when filtering, nesting loops and transforming the data):
[list(t) for t in zip(a, c, d)]
",A,0
16825640,2013-05-29 23:35:59.497000+00:00,"Use itertools.groupby() to group your items:
from itertools import groupby
from operator import itemgetter

sums = [(key, sum(float(i[1]) for i in group))
        for key, group in groupby(row, key=itemgetter(0))]

This produces a list of ('a', 10.0), etc. values.
The groupby() tool splits up your input sequence into groups, where the next group is determined by the key callable; when the value returned by the key callable changes, a new group is produced. ",A,2
24203295,2014-06-13 10:33:35.203000+00:00,"You can use zip() to pair up your WebElement objects with the strings:
for welement, text in zip(webelements_list, text_list):
    welement.send_keys(text)
",A,3
45909615,2017-08-27 21:43:43.580000+00:00,It doesn't. That print is executed because it's at the class level itself; the body of a class is executed at the time the class is defined. You would get that output even if you never instantiated MyClass.,A,2
18007430,2013-08-02 01:42:37.517000+00:00,"You're making an invalid assumption:

If the self.path is ip:port/index.html every time, how can someone access files that are above the root / directory?

But self.path is never ip:port/index.html. Try logging it and see what you get.
For example, if I request http://example.com:8080/foo/bar/index.html, the self.path is not example.com:8080/foo/bar/index.html, but just /foo/bar/index.html. In fact, your code couldn't possibly work otherwise, because curdir+ sep + self.path would give you a path starting with ./example.com:8080/, which won't exist.
And then ask yourself what happens if it's /../../../../../../../etc/passwd.
This is one of many reasons to use os.path instead of string manipulation for paths. For examples, instead of this:
f = open(curdir + sep + self.path, ""rb"")

Do this:
path = os.path.abspath(os.path.join(curdir, self.path))
if os.path.commonprefix((path, curdir)) != curdir:
    # illegal!

I'm assuming that curdir here is an absolute path, not just from os import curdir or some other thing that's more likely to give you . than anything else. If it's the latter, make sure to abspath it as well.
This can catch other ways of escaping the jail as well as passing in .. strings… but it's not going to catch everything. For example, if there's a symlink pointing out of the jail, there's no way abspath can tell that someone's gone through the symlink.",A,3
842600,2009-05-09 03:33:23.567000+00:00,"First, always use os.path for path manipulation.
More importantly, all paths should be provided in configuration files.
For logging, use the fileConfig function.
For everything else, be sure to have a configuration file or command-line parameter or environment variable.",A,2
29938759,2015-04-29 08:51:24.997000+00:00,"The problem is that you're using is instead of ==.
The former checks for object identity, not equality. There's no reason to believe that evaluating, say, 300+1 twice will give you the same int object, just that they'll both give you int objects whose value is 301.
This happens to ""work"" for numbers up to 256 because your particular Python implementation* happens to intern integers up to 256. At startup, it creates a singleton object for the number 1, a singleton object for 2, and so on. Any time an expression evaluates to the number 1, it gives you that object, instead of a new one.**
Needless to say, you should not rely on that optimization.

* IIRC, every version of CPython from the 1.x days to 3.5 defaults to this behavior for all integers from -5 to 256, but you can change those limits, or turn off the feature, at build time, and a different implementation might do something different.
** If you're wondering how this works in CPython, at the C API level, PyLong_FromLong does this by looking up numbers from -5 to 256 in an array of singleton values. You can see the 3.4 version of the code, for example, here; the macro CHECK_SMALL_INT and the actual function get_small_int that it calls, and the static array that function uses, are all are in the same file, up near the top.",A,6
54852745,2019-02-24 14:13:25.620000+00:00,"It seems like obj.json is already a JSON file. There is no need to call json.dumps on the data after reading it. Just pass obj directly.
Also, you will need to disable auto escaping in the template by outputting it as {{ obj|safe }}.",A,0
26552308,2014-10-24 16:33:07.293000+00:00,"You you need x random numbers between 1 and x that are all unique then you just want a shuffled range:
x = 70128
numbers = range(1, x + 1)
random.shuffle(numbers)

If you are using Python 3, you want to add a list() call to the range() result.
Demo on Python 2.7 with x = 10 for practicality:
>>> import random
>>> x = 10
>>> numbers = range(1, x + 1)
>>> random.shuffle(numbers)
>>> numbers
[5, 2, 6, 4, 1, 9, 3, 7, 10, 8]
",A,3
26247420,2014-10-08 00:30:25.743000+00:00,"Unfortunately, what you're trying to do doesn't make sense. Command-line programs can have an exit code, but that's just a small integer; they can't return text, or arbitrary Python objects. 
There's a quasi-standard for what these integers means; the simple version is 0 for success, 1 for most errors, 2 for invalid command-line arguments. click is trying to make your function into a good command-line citizen, so when you exit your function, it calls sys.exit with the appropriate number (0 if you return, 1 if you raise, and 2 if it failed to parse your arguments).
So, whatever you return has no effect, and whatever you try to do with the return value at the top level doesn't even get run.
What programs usually do when they need to ""return"" text is to print it to standard output, which is exactly what click.echo is for.",A,6
14246777,2013-01-09 21:24:33.270000+00:00,"You can do this the way squiguy suggested, but it might be more readable to put the whole thing into a loop, so you don't have to repeat the same checks twice:
while True:
    Myfile = raw_input( '''Specify filename (o) or use default chain (d)? ''')

   if Myfile == 'd':
        print 'default choosen '
        Myfile = 'M:/test/testchains_a.csv'
        break

    elif Myfile == 'o':
        print 'own file choosen '
        Myfile = raw_input('Enter absolute path to the .csv file:')
        break

This will loop forever, until it hits a break—in other words, until they select 'd' or 'o'.",A,1
20507165,2013-12-10 23:00:40.057000+00:00,"The replace method on strings doesn't modify the strings in-place. In fact, nothing modifies strings in-place; they're immutable. This is explained in the Tutorial section Strings.
What it does is return a new string with the replacements done on it. From the docs:

str.replace(old, new[, count])

Return a copy of the string with all occurrences of substring old replaced by new. If the optional argument count is given, only the first count occurrences are replaced.


So, what you want is:
ip_start = ip_start.replace(str(top),str(i))
",A,4
35815748,2016-03-05 15:11:10.047000+00:00,"The error is self-explanatory: NotADirectoryError: [Errno 20] Not a directory: '/home/*/guiaDocente_IA.pdf' -> '/home/*/IA/'
You can't rename a file (guiaDocente_IA.pdf) to a directory ('IA/').
The cause of the error is that compruebaArchivo(cadena, archivo) returns an empty string if ""cadena"" not in archivo. To fix the error, your code should handle the case when arch_nuevo is empty.
btw, use English for names in your code otherwise it is harder to help you.",A,1
18258274,2013-08-15 17:32:38.787000+00:00,"If you need to update the count, then you have to read twice and count the number of rows you are keeping first. You can keep a separate counter to rewrite the first column once you are writing the matched lines:
import re

numbered = re.compile(r'N\d+').match

for fn in fns:
     # open for counting
     reader = csv.reader(open(fn,""rb""))
     count = sum(1 for row in reader if row and not any(r.strip() == 'DIF' for r in row) and numbered(row[0]))

     # reopen for filtering
     reader = csv.reader(open(fn,""rb""))

     with open (os.path.join('out', fn), 'wb') as f:
        counter = 0
        w = csv.writer(f)
        for row in reader:
            if row and 'Count' in row[0].strip():
                row = ['Count', count]
            if row and not any(r.strip() == 'DIF' for r in row): #remove DIF
                if numbered(row[0]):
                    counter += 1
                    row[0] = 'N%d' % counter
            w.writerow(row)
",A,2
49227177,2018-03-12 02:03:25.683000+00:00,"The problem here turned out to be that the original standard14Names in the source code was using Unicode MINUS SIGN characters (U+2212) instead of ASCII hyphens (U+002D). The fix is to just replace the - characters in the source with -.

You'll notice that these characters visually look almost identical, making it nearly impossible to debug. Even more of a problem when one is in Eclipse and the other in a terminal window, so they may not even be in the same font…
Also, it's a bit baffling that, when copying to the clipboard, Eclipse apparently converted the Unicode text to ASCII using a replace-type handler. Or maybe this is the terminal doing it on paste, or even X11 or Windows or whatever sits between the two?
Anyway, if some future searcher has a similar problem, here's the smart way to debug it: Just dump out the codepoints of the strings in each environment, something like this:
' '.join(hex(ord(ch)) for name in standard14Names for ch in name)

It should be pretty easy to look over the two and see that one of them is pure ASCII (two-byte hex codepoints), while the other one has a bunch of 0x2212 values.
Once you spot that, you can look for the first difference, and you'll quickly see that every 0x2212 lines up with an 0x2d in the other version, at which point (assuming you don't know the Unicode tables by heart) you can google U+2212 and figure out what's going on.
I actually recommended a dumber first step—it worked anyway, but it was more effort for the OP. You can dump the compilation information for the regex objects by executing re.compile(""|"".join(standard14Names), re.DEBUG) in both environments. That doesn't return a string, it actually dumps the output to the terminal, so you have to scroll back and copy it all into a text file to do anything useful with it. Also, it's going to be very long. But anyway, if you put both versions into text files and diff them, you'll see that every diff is LITERAL 8722 vs. LITERAL 45, and those are the decimal values for 0x2212 and 0x2d, so we've gotten the same information as in the previous version.",A,1
11273592,2012-06-30 11:09:41.410000+00:00,Put an time.sleep(5) in your view before the return to sleep for five seconds.,A,2
18924199,2013-09-20 19:12:09.850000+00:00,"Both f1 and f2 contain perfectly normal, unescaped single quotes.
The fact that their repr looks different is meaningless.
There are a variety of different ways to represent the same string. For example, these are all equivalent literals:
""abc'def'ghi""
'abc\'def\'ghi'
'''abc'def'ghi'''
r""abc'def'ghi""

The repr function on a string always just generates some literal that is a valid representation of that string, but you shouldn't depend on exactly which one it generate. (In fact, you should rarely use it for anything but debugging purposes in the first place.)

Since the language doesn't define anywhere what algorithm it uses to generate a repr, it could be different for each version of each implementation.
Most of them will try to be clever, using single or double quotes to avoid as many escaped internal quotes as possible, but even that isn't guaranteed. If you really want to know the algorithm for a particular implementation and version, you pretty much have to look at the source. For example, in CPython 3.3, inside unicode_repr, it counts the number of quotes of each type; then if there are single quotes but no double quotes, it uses "" instead of '.

If you want ""the"" representation of a string, you're out of luck, because there is no such thing. But if you want some particular representation of a string, that's no problem. You just have to know what format you want; most formats, someone's already written the code, and often it's in the standard library. You can make C literal strings, JSON-encoded strings, strings that can fit into ASCII RFC822 headers… But all of those formats have different rules from each other (and from Python literals), so you have to use the right function for the job.",A,7
1223089,2009-08-03 15:47:08.640000+00:00,"The easiest thing to do is break your program into multiple processes.  The OS will allocate them across the cores.
Somewhat harder is to break your program into multiple threads and trust the JVM to allocate them properly.  This is -- generally -- what people do to make use of available hardware.

Edit
How can a multi-processing program be ""easier""?   Here's a step in a pipeline.
public class SomeStep {
    public static void main( String args[] ) {
        BufferedReader stdin= new BufferedReader( System.in );
        BufferedWriter stdout= new BufferedWriter( System.out );
        String line= stdin.readLine();
        while( line != null ) {
             // process line, writing to stdout
             line = stdin.readLine();
        }
    }
}

Each step in the pipeline is similarly structured.  9 lines of overhead for whatever processing is included.
This may not be the absolute most efficient.  But it's very easy.

The overall structure of your concurrent processes is not a JVM problem.  It's an OS problem, so use the shell.
java -cp pipline.jar FirstStep | java -cp pipline.jar SomeStep | java -cp pipline.jar LastStep

The only thing left is to work out some serialization for your data objects in the pipeline.
Standard Serialization works well.  Read http://java.sun.com/developer/technicalArticles/Programming/serialization/ for hints on how to serialize.  You can replace the BufferedReader and BufferedWriter with ObjectInputStream and ObjectOutputStream to accomplish this.",A,1
15119889,2013-02-27 18:48:37.413000+00:00,"From your comment: you have a relatively small number of quite big elements. That means the speed of the outer loop's iteration is irrelevant, while the speed of the inner loops' iterations is crucial. 
Let's put some actual numbers on this. Your outer array has up to 4 dimensions of up to size 10. This means there are up to 10000 element. Meanwhile, the elements is ""quite big""—let's interpret that conservatively as just 50. So, you've got 510000 loop iterations. Anything you do to improve the speed of the 10000 outer iterations will make less than a 2% difference in your code. In fact, it's far less than that—that 2% assumes there is no work to be done other than the iteration itself, which obviously isn't true.
So, you're focusing on the wrong place. Only the 500000 inner iterations matter. If you could replace the array of arrays with a single one-dimension-higher array and do it all in numpy, it might be faster, but making your code much more complex and hard to understand for a gain on the order of a fraction of 1% is silly. Just use the vectorize answer or comprehension answer, which are simple and obvious.

Meanwhile:

Probably I should try to parallelize the evaluation, using a thread for every matrix element.

Parallelism is a good idea here. But not using threads, and not one per element. 
If you have, say, an 8-core machine, using 8 hardware threads means you get things done nearly 8 times as fast. But using 10000 hardware threads does not mean you get things done 10000 times as fast, or even 8 times as fast—you probably spend so much time context-switching, allocating and freeing thread stacks, etc. that it actually takes longer than the sequential version. So, you want to create a worker pool of 8 hardware threads, and stick your 10000 tasks in a queue to be run by that pool.
Also, 10000 is probably way too granular. Each job has a little bit of overhead, and if your jobs are too small, you're wasting too much time on overhead. The obvious ways to batch things up are per axis—have 1000 jobs, each doing one row of 10 elements, or 100 jobs, each doing one 2D array of 100 elements. Testing out batch sizes of 0, 1, 2, and 3 axes and see which gives the best performance. If the differences are huge, you may want to try slightly more complex batching, like splitting into 3D arrays of 3x10x10 and 4x10x10.
Finally, while Python threads are real OS (and therefore hardware) threads, the GIL prevents more than one of them from running Python code at a time. numpy does some work to get around this, but it's not perfect (especially if you have a lot of overhead between the numpy calls). The simplest way around this is to use multiprocessing, which lets you have a pool of 8 separate processes, instead of 8 threads in 1 process. This ramps up the overhead significantly—you either need to copy the sub-arrays (either implicitly, as the parameters and return values of your task functions, or explicitly, over a pipe) or put them in shared memory. But if your task sizes are large enough, it's usually not a problem.

Here's an example of how to parallelize your existing code:
f = np.mean
result = np.zeros(a.shape)
future_map = {}
with futures.ProcessPoolExecutor(max_workers=8) as executor:
    for i in np.ndindex(a.shape):
        future_map[executor.submit(f, a[i])] = i
    for future in futures.as_completed(future_map):
        i = future_map[future]
        result[i] = future.result()

Obviously, you could simplify it (e.g., replace the submission loop with a dict comprehension), but I wanted to make it as obvious as possible what you need to change.
Also, I'm using futures (which requires 3.2+; if you're on 2.7, install the backport from PyPI) because it makes the code a little simpler; if you need more flexibility, you want the more complex multiprocessing library.
And finally, I'm not doing any batching—each task is a single element—so the overhead is probably going to be pretty bad. 
But start with this, simplify it as far as you're comfortable with, then convert it to use batches of 1, 2, and 3 axes, etc. as described earlier.",A,2
24591918,2014-07-06 01:26:00.517000+00:00,"A recent commit added support for arbitrary file objects to be passed in. You can pass in a file object as a file_ keyword argument, or just as 2nd positional argument. The filename is then still needed, but is only used to determine the mimetype.
On GAE, use a StringIO object holding the image data for that:
from cStringIO import StringIO

image = StringIO(imagedata)
API.update_profile_image('profile.jpg', image)

You need a file(-like) object that supports seeking; the response object returned by urllib2 does not. You'll have to read all data, create a StringIO from that and pass that to the Tweepy method. 
This change is part of Tweepy 2.3.0. ",A,2
13239779,2012-11-05 20:23:41.353000+00:00,"There are three good options here.
First, you can use urllib2 (Python 2) or urllib (Python 3), which are built in, and pretty easy to use.
Second, you can use an even easier third-party library like requests. (Often, code that takes a dozen lines to write with curl or urllib is a two-liner with requests.)
Finally, since you already know how to use php's low-level libcurl wrappers, there are a few different third-party alternatives for Python that are nearly identical. See this search, and look through pycurl, pycurl2, and pyclibcurl to see which one feels most familiar.",A,0
38900876,2016-08-11 15:44:22.567000+00:00,"You already have an answer, but note that dropping the {{ }} tags and keeping everything else the same would have worked fine. 
{% for test in objectives|get_item:objective %}
",A,0
8441518,2011-12-09 05:38:21.017000+00:00,">>> import re
>>> mysentence = 'have a dog a cute cat and a big cow '
>>> myanimal = 'dog', 'cat' , 'cow'
>>> m = re.match(r'have a (?:%s)' % '|'.join(map(re.escape, myanimal)), mysentence)
>>> m.group()
'have a dog'
",A,1
9471668,2012-02-27 20:17:49.713000+00:00,"Read about the built-in safe filter.
Django does not consider data from the database to the ""Safe"". 
It always ""escapes"" any tag-like content in database data to prevent HTML Script Injection attacks.",A,3
23063011,2014-04-14 14:38:49.657000+00:00,"There are two mistakes here:

You use the length of decoded['installed'] to generate an index you then use for the decoded['total_hosts'] list. This will generate index errors because decoded['total_hosts'] doesn't have as many entries.
Accessing a key that doesn't exist will throw a KeyError; use the dict.get() method instead to retrieve a value or a default.

It's much simpler to just loop directly over a list, no need to generate an index:
for host in decoded['total_hosts']:
    row = [host.get('TYPE', ''), host.get('COUNT', '')]
    writer.writerow(row)

You can extend this to handle more than one key:
for key in ('total_hosts', 'installed', 'virtual_machine'):
    for entry in decoded[key]:
        row = [entry.get('TYPE', ''), entry.get('COUNT', '')]
        writer.writerow(row)

If you needed to combine the output of two entries, use itertools.izip_longest() to pair up the lists, using a default value for when the shorter list runs out:
from itertools import izip_longest

for t, i, v in izip_longest(decoded['total_hosts'], decoded['installed'], decoded['version'], fillvalue={}):
    row = [t.get('TYPE', ''), t.get('COUNT', ''), 
           i('ID', ''), i('VERSION', ''), i.get('ADDON_NAME', ''),
           v.get('COUNT', ''), v.get('TYPE', '')]
    writer.writerow(row)

This allows for any one of the three lists to be shorter than the others.
For Python versions before 2.6 (which added itertools.izip_longest) you'd have to assume that installed was always longest, and then use:
for i, installed in decoded['installed']:
    t = decoded['types'][i] if i < len(decoded['types']) else {}
    v = decoded['version'][i] if i < len(decoded['version']) else {}
    row = [t.get('TYPE', ''), t.get('COUNT', ''), 
           installed['ID'], installed['VERSION'], installed['ADDON_NAME'],
           v.get('COUNT', ''), v.get('TYPE', '')]
    writer.writerow(row)
",A,2
19578915,2013-10-25 00:08:00.553000+00:00,"Add an empty print after the loop:
for i in items:
    print (i, end= "" "")
print()

This will print the extra newline you need.
Alternatively, use str.join(), map() and str() to create a new space-separated string from the numbers, printing that with the newline:
items = [10, 12, 18, 8, 8, 9]
print(' '.join(map(str, items)))

and
print(' '.join(map(str, range(1,50, 5))))
",A,2
6130684,2011-05-25 20:43:08.853000+00:00,"http://docs.python.org/reference/datamodel.html#basic-customization
__new__ - constructor.
__init__ - initializer.",A,0
15861364,2013-04-07 10:30:20.397000+00:00,"You are parsing XML, and the library already knows how to handle decoding for you. The API returns unicode objects, but you are trying to treat them as byte strings instead.
Where you call ''.format(), you are using a python bytestring instead of a unicode object, so Python has to encode the Unicode values to fit in a bytestring. To do so it can only use a default, which is ASCII.
The simple solution is to use a unicode string there instead, note the u'' string literal:
print u""DocID:    {0}\nTitle:    {1}\nLast Name: {2}\nFirst Name: {3}\nStreet: {4}\ncity: {5}\nstate: {6}\npostcode: {7}\ncountry: {8}\n"".format(docID,title,lastName,firstName,street,city,state,postcode,country)

Python will still have to encode this when printing, but at least now Python can do some auto-detection of your terminal and determine what encoding it needs to use.
You may want to read up on Python and Unicode:

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) by Joel Spolsky
The Python Unicode HOWTO
Pragmatic Unicode by Ned Batchelder
",A,3
30682302,2015-06-06 11:31:11.377000+00:00,"You're overriding too early. dispatch is the first method called, before self.object has been set. You'd need to either override get and post and do the check in each one (after calling super), or call get_object explicitly in your dispatch method.",A,2
18974703,2013-09-24 06:45:20.553000+00:00,"choices needs to be a series of 2-tuples. Like this:
ACCOUNT_TYPE_CHOICES = (('1234', '1234'), ('1234567', '1234567'))

(Also note that it's customary to put the name in all caps, since it's a constant.)",A,0
25400091,2014-08-20 08:29:15.803000+00:00,"datetime.fromtimestamp() is the correct method to get local time from POSIX timestamp. The issue in your question is that you convert an aware datetime object to POSIX timestamp using time.mktime() that is incorrect. Here's one of correct ways to do it:
expiration_utc_ts = (d - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()
local_dt = datetime.fromtimestamp(expiration_utc_ts)
",A,0
20636216,2013-12-17 14:03:02.297000+00:00,"A generator function cannot return out-of-band data like this.
I'd use a class instead, as an instance gives you something to stick such extra state:
class XLSParser(object):
    def __init__(self, limit):
        self.error_list = []
        self.limit = limit

    def __iter__(self):
        for x in range(self.limit):
            if x%2: #fake error condition
                self.error_list.append(x)
            else:
                yield(x*x) #return

and iterate over that object:
parser = XLSParser(limit)
for result in parser:
    # do something

errors = parser.error_list
",A,5
15771918,2013-04-02 18:45:33.313000+00:00,"The if statement guarantees that x.contacted isn't None.
But x.contacted isn't what you're trying to iterate or index, so it isn't guarding anything. 
There's no reason memotable or memotable[node.gen] can't be None even though x.contacted is something else. For that matter, we have no idea of what the code inside self.legacy(x, memotable) does—maybe it tries to iterate x, or other_table[x], or who knows what, any of which could be None.
This is why you need to look at the entire traceback, not just the error string. It will tell you exactly which statement failed, and why.

And now that you've pasted the traceback:
File ""F:\Dropbox\CS\a4\skeleton\trial.py"", line 138, in legacy nset.union(self.legacy(x, memotable))

Yep, it's something that happens inside that self.legacy line, and it has absolutely nothing to do with x.contacted. The problem is almost certainly that your self.legacy method is returning None, so you're doing nset.union(None).
Again, whether x.contacted is or is not None is completely irrelevant here, so your check doesn't guard you here.
If you want us to debug the problem in that function, you will have to give us the code to that function, instead of code that has nothing to do with the error. Maybe it's something silly, like doing a + b instead of return a + b at the end, or maybe it's some deep logic error, but there's really no way we can guess.",A,3
11395108,2012-07-09 12:34:14.583000+00:00,"Use slice assignment:
my_list[bounds[0]:bounds[1] + 1] = ['foo'] * ((bounds[1] + 1) - bounds[0])

or using local variables to add your + 1 only once:
lower, upper = bounds
upper += 1
my_list[lower:upper] = ['foo'] * (upper - lower)

You may want to store the upper bound as non-inclusive, to play better with python and avoid all the + 1 counts.
Demo:
>>> my_list = range(10)
>>> bounds = (2, 5)
>>> my_list[bounds[0]:bounds[1] + 1] = ['foo'] * ((bounds[1] + 1) - bounds[0])
>>> my_list
[0, 1, 'foo', 'foo', 'foo', 'foo', 6, 7, 8, 9]
",A,13
1351670,2009-08-29 15:11:48.910000+00:00,""" I understand that Python is dynamically typed, whereas C# is strongly-typed. ""
This is weirdly wrong.

Python is strongly typed.  A list or integer or dictionary is always of the given type.  The object's type cannot be changed.
Python variables are not strongly typed.  Indeed, Python variables are just labels on objects.  Variables are not declared; hence the description of Python as ""dynamic"".  
C# is statically typed.  The variables are declared to the compiler to be of a specific type.  The code is generated based on certain knowledge about the variables use at run-time.

Python is ""interpreted"" -- things are done at run-time -- little is assumed.  [Technically, the Python source is compiled into byte code and the byte code is interpreted.  Some folks think this is an important distinction.]
C# is compiled -- the compiler generates code based on the declared assumptions.

What conceptual obstacles should I watch out for when attempting to learn Python?
None.  If you insist that Python should be like something else; or you insist that something else is more intuitive then you've polluted your own thinking with inappropriate concepts.
No programming language has obstacles.  We bring our own obstacles when we impose things on the language.
Are there concepts for which no analog exists in Python?
Since Python has object-oriented, procedural and functional elements, you'd be hard-pressed to find something missing from Python.
How important is object-oriented analysis?
OO analysis helps all phases of software development -- even if you aren't doing an OO implementation.  This is unrelated to Python and should be a separate question.
I need to get up to speed in about 2 weeks time (ridiculous maybe?)
Perhaps not.  If you start with a fresh, open mind, then Python can be learned in a week or so of diligent work.
If, on the other hand, you compare and contrast Python with C#, it can take you years to get past your C# bias and learn Python.  Don't translate C# to Python.  Don't translate Python to C#.  
Don't go to the well with a full bucket.",A,9
13616263,2012-11-28 23:26:57.647000+00:00,"Firstly, please stop referring to that page as a tutorial - it's clearly marked as an example. Secondly, you don't appear to have read the actual documentation page, clearly linked from the first line of the page you give and also from the docs index, which explains exactly what to do to get it up and running.",A,1
27342152,2014-12-07 11:25:00.447000+00:00,"You are using a version of Python that doesn't yet support PEP 328 Relative Imports; e.g. Python 2.4 or older. BeautifulSoup 4 requires Python 2.7 or newer.
Presumably you cannot upgrade to a newer Python version. In that case you can try using BeautifulSoup 3; it'll have a few bugs and you'll be missing some features, but at least you can get past the syntax error.
However, I note that HelioHost does list Python 2.7 as supported.",A,0
20863712,2013-12-31 22:13:00.137000+00:00,"No, it doesn't mean that at all. It means what it says, that the parameter has to be an instance of the ContentType model, whereas you're passing the ID of the object itself.
You might be able to use content_type along with the actual instance:
Progress.objects.get_or_create(user=request.user, content_object=f)

Otherwise you'll need to get the right ContentType using the get_for_model() method, and pass that along with the object id.",A,0
52860030,2018-10-17 16:56:55.550000+00:00,"You shouldn't be doing this as part of the name anyway. The post id is a parameter. The only collocated part is working out how to pass it to the Django URL tag 
function toggleLike(post_id){
    $.ajax({
      url: ""{% url 'photo_blog-post_like_api' ""00000"" %}"".replace(""00000"", post_id)
      success: function(data) {
        $(""#likeCount"" + post_id).html(data.like_count + ' likes');
        $('#imageElement' + post.id).html(data.img);
      }
      });
  };
</script>
<a id=imageElement{{post.id}}  onclick=""toggleLike({{post.id}})""><img src=""/media/nav_buttons/liked.svg"" height=""17"" width=""17"">
",A,0
7589615,2011-09-28 21:08:34.720000+00:00,"import csv
with open(""source"",""rb"") as source:
    rdr= csv.reader( source )
    with open(""result"",""wb"") as result:
        wtr= csv.writer( result )
        for r in rdr:
            wtr.writerow( (r[0], r[1], r[3], r[4]) )

BTW, the for loop can be removed, but not really simplified.
        in_iter= ( (r[0], r[1], r[3], r[4]) for r in rdr )
        wtr.writerows( in_iter )

Also, you can stick in a hyper-literal way to the requirements to delete a column.  I find this to be a bad policy in general because it doesn't apply to removing more than one column.  When you try to remove the second, you discover that the positions have all shifted and the resulting row isn't obvious.  But for one column only, this works.
            del r[2]
            wtr.writerow( r )
",A,41
3668948,2010-09-08 14:48:34.410000+00:00,"""in place"" doesn't mean much.  You want this.
p[i:j] = list( sorted( p[i:j] ) ) 
",A,0
18147825,2013-08-09 13:30:28.790000+00:00,"You cannot tell the default class pickler to ignore something, no.
jsonpickle does support the pickle module __getstate__ and __setstate__ methods. If your classes implement those two methods, whatever is returned is then used by jsonpickle to represent the state instead. Both methods do need to be implemented.
If __getstate__ is not implemented, jsonpickle uses the __dict__ attribute instead, so your own version merely needs to use that same dictionary, remove the _sa_instance_state key and you are done:
def __getstate__(self):
    state = self.__dict__.copy()
    del state['_sa_instance_state']
    return state

def __setstate__(self, state):
    self.__dict__.update(state)

Whatever __getstate__ returns will be processed further, recursively, there is no need to worry about handling subobjects there.
If adding __getstate__ and __setstate__ is not an option, you can also register a custom serialization handler for your class; the disadvantage is that while __getstate__ can get away with just returning a dictionary, a custom handler will need to return a fully flattened value.",A,14
51940753,2018-08-21 02:26:09.950000+00:00,"If you're just calling it directly, using the bound method s.lower() is definitely preferred. It says what you mean (object: do this!), it's shorter, it's more obvious to the reader, and it's even faster with builtin types like str (but about the same speed with types you create yourself).

The difference is when you need to store the method, or pass it around as a callback, etc., instead of calling it right away. In that case, it's almost always obvious what you want to do:

If you're going to want to call the method on a string you haven't seen yet, or on a whole bunch of different strings, you want to store/pass/whatever the unbound method, str.lower.
If you want to call the method on a string you have sitting around, especially if you want to call it over and over on that same string, you want to store/pass/whatever the bound method, s.lower.


For the case of str.lower, it's pretty easy to think of uses for the unbound method, like:
lower_strings = map(str.lower, strings)

Or, shamelessly stealing from Dair's answer, because it's a better example than mine:
sorted_strings = sorted(strings, key=str.lower)

But it's pretty hard to imagine why you'd ever want to call the bound method s.lower over and over again.
But consider these other cases with different methods:

You're building a GUI, and you want a certain onclick method to get called on your window object every time a button is clicked. Obviously, you always want the method to get called on this particular window object, the one that owns the button, so you'll use the bound method self.onclick as your button callback, not the unbound method Window.onclick.
You're looking for unique values, do you keep a set, values_seen, of all the values you've seen so far. values_seen.add might be a useful thing to store or pass to some other function, but set.add wouldn't do you any good.


There's also a third case you didn't mention: 
lambda x: x.lower()

That's just like str.lower, in that you can pass it to map or save it for later, and more complicated, and slower. So, why would you ever do it?
Imagine you need a function that calls lower on anything that ""duck types"" as a string, not just actual strings. For example, b'AbC'.lower() is a perfectly valid thing to do. So, if you have a list of either strings or bytes but you're not sure which, and you want to lowercase them all, you can't map(str.lower, xs) or map(bytes.lower, xs), but you can map(lambda x: x.lower(), xs).
Although for that particular case you can—and usually should—just use a comprehension:
(x.lower() for x in xs)
",A,7
27620110,2014-12-23 12:10:13.900000+00:00,"You can't, not directly. By running on 127.0.0.1 (localhost), you are explicitly not binding to a public IP address and are not visible to the outside world.
Your options are to:

Use SSH port forwarding to redirect traffic from your own machine to that localhost port; add -L 5000:localhost:5000 to your ssh command line and access the Flask server at http://localhost:5000. Use this option if only you should be able to access the server.
Use a 3rd party service like ngrok to tunnel from a public host to your Flask server.
Use another web server serving on a public IP address forwarding connections to localhost:5000. See Proxy Setups in the Flask deployment documentation.
Restart the Flask server to bind to a public IP address, not 127.0.0.1. This is not recommended, as the development web server that comes bundled with Flask is not really suited for the rough world that is the public internet. You can do this by giving app.run() a host argument:
app.run(host='162.74.90.100')

or (using the flask command-line tool) using the --host command-line argument:
flask run --host 162.74.90.100

to bind to your public IP address, or use 0.0.0.0 to bind to all available IP addresses on your server. This will only work if your server is connected directly to the internet (not behind a router) and the firewall allows connections to the port; you'll need to configure the router and firewall otherwise.
",A,3
22866535,2014-04-04 15:11:16.203000+00:00,"You need to copy the docstring over to your decorator:
class log(object):
    def __init__(self, f):
        print ""Inside __init__()""
        self.f = f
        self.__doc__ = f.__doc__

    def __call__(self, *args):
        print ""Inside __call__()""
        try:
            self.f(*args)
        except Exception:
            print ""Sorry""

The decorator replaces the decorated function, and only by copying over the docstring would that attribute be visible to anyone.
You could make use of functools.update_wrapper() here to do the copying of the docstring, as well as several other attributes, for you:
from functools import update_wrapper

class log(object):
    def __init__(self, f):
        print ""Inside __init__()""
        self.f = f
        update_wrapper(self, f)

    def __call__(self, *args):
        print ""Inside __call__()""
        try:
            self.f(*args)
        except Exception:
            print ""Sorry""
",A,4
13641824,2012-11-30 08:56:59.147000+00:00,You need to pass a full URL: ie it must begin with http://.,A,3
34022357,2015-12-01 14:04:09.673000+00:00,"You created class attributes; you'll need to reference them on the class:
class FilterClass:
    ALPHABETIC = ""a""
    NUMERIC = ""n""
    URL = ""u""

    def textRestriction(self, text, arguments):
        if FilterClass.ALPHABETIC in arguments:
            #do crap here
        if FilterClass.NUMERIC in arguments:
            #do other crap here

Class attributes are not globals, nor is the class body definition a scope as far as the methods on the class are concerned.
You could also just reference them as if they are instance attributes; Python will automatically look for class attributes if there is no such instance attribute:
def textRestriction(self, text, arguments):
    if self.ALPHABETIC in arguments:
        #do crap here
    if self.NUMERIC in arguments:
        #do other crap here

Another way of accessing these names is by querying for the current class with type(self), which would allow for subclasses to override the attributes, but ignore instance attributes:
def textRestriction(self, text, arguments):
    if type(self).ALPHABETIC in arguments:
        #do crap here
    if type(self).NUMERIC in arguments:
        #do other crap here
",A,0
10815128,2012-05-30 11:12:21.727000+00:00,"Closures are only introduced if there are variables to be referenced outside of the global (module) scope:
>>> def foo():
...     def bar(): pass
...     return bar
...
>>> foo().func_closure is None
True
>>> spam = 'eggs'
>>> def foo():
...     def bar(): return spam
...     return bar
...
>>> foo().func_closure is None
True

Only when the inner function refers to a variable in the surrounding scope are closures generated:
>>> def foo():
...     spam = 'eggs'
...     def bar(): return spam
...     return bar
...
>>> foo().func_closure is None
False
>>> foo().func_closure
(<cell at 0x108472718: str object at 0x108471de0>,)

Note that you actually have to refer to a variable in the surrounding scope. Simply ignoring the scope gives you None again:
>>> def foo():
...     spam = 'eggs'
...     def bar(): pass
...     return bar
...
>>> foo().func_closure is None
True

In your first example, i is a module-scope variable, only in your second example do you introduce a new scope by wrapping the code in a new function actualFact.",A,4
50257829,2018-05-09 16:13:52.067000+00:00,"Because that's the default number of decimal places for the %f formatter.
Add a precision specifier; for example, if you only want to display 2 decimal places, use %.2f:
print ""total price: %.2f"" % total

and the last line of output then becomes
total price: 117.00

The . followed by a digit in the format specification signals the precision specifier. See the String Formatting Operations documentation:

A conversion specifier contains two or more characters and has the following components, which must occur in this order:
[...]

Precision (optional), given as a '.' (dot) followed by the precision. [...]


Further on in the same documentation, the default for f is documented under footnote #3:

The precision determines the number of digits after the decimal point and defaults to 6.
",A,2
26564311,2014-10-25 15:54:31.247000+00:00,"You either failed to show us your except or finally block that belongs with your try statement, but the if statement must still be part of the try suite. However, it is not indented enough to be part of the try suite.
Indent your if statement to be in the try: block, or put it after the finally: or except: suite. If you don't have either, you'll need to add one, you cannot use try: without one.
You appear to be trying to type out the code in this post; you'll notice that the if is inside the while loop there:
try:
    while True:
        #Get the block hit events
        blockHits = mc.events.pollBlockHits()
        # if a block has been hit
        if blockHits:
            # for each block that has been hit
            for blockHit in blockHits:
                #Create and run the exploding block class in its own thread
                # pass the position of the block, fuse time in seconds and blast radius
                # threads are used so multiple exploding blocks can be created
                explodingBlock = ExplodingBlock(blockHit.pos, 3, 3)
                explodingBlock.daemon
                explodingBlock.start()
        time.sleep(0.1)
except KeyboardInterrupt:
    print(""stopped"")

Indentation is very important in Python; it serves the role of grouping statements into blocks (suites) that belong together, and your attempt broke the expectations of how the try suite was supposed to end.",A,2
19776163,2013-11-04 20:08:12.530000+00:00,"Since you claim to be used to JavaScript:
The Python in operator is similar to the JavaScript in operator.
Here's some JavaScript:
var d = {1: 2, 3: 4};
if (1 in d) {
    alert('true!');
}

And the equivalent Python:
d = {1: 2, 3: 4}
if 1 in d:
    print('true!')

With objects/dicts, they're nearly identical, both checking whether 1 is a key of the object/dict. The big difference, of course, is that JavaScript is sloppily-typed, so '1' in d would be just as true.
With arrays/lists, they're very different. A JS array is an object, and its indexes are the keys, so 1 in [3, 4, 5] will be true. A Python list is completely different from a dict, and its in operator checks the values, not the indexes, which tends to be more useful. And Python extends this behavior to all iterables.
With strings, they're even more different. A JS string isn't an object, so you will get a TypeError. But a Python str or unicode will check whether the other operand is a substring. (This means 1 in '123' is illegal, because 1 can't be a substring of anything, but '1' in '123' is true.)
With objects as objects, in JS there is of course no distinction, but in Python, objects are instances of classes, not dicts. So, in JS, 1 in d will be true for an object if it has a member or method named '1', but in Python, it's up to your class what it means—Python will call d.__contains__(1), then, if that fails, it tries to use your object as an utterable (by calling its __iter__, and, if that fails, by trying to index it with integers starting from 0).
Also, note that JS's in, because it's actually checking for object membership, does the usual JS method-resolution-order search, while Python's in, because it's checking for keys of a dict, members of a sequence, etc., does no such thing. So, technically, it's probably a bit closer to the hasOwnProperty method than the in operator.",A,7
52554709,2018-09-28 11:49:00.263000+00:00,"Rather than add the ['items'] list (which only contains a single album, if I understand your limit=1 query correctly), add a new dictionary with the specific values.
To avoid having to call the Spotify API twice for those two items, put your query loop into a generator expression; that makes it easier to then take the resulting album dictionary and take out specific keys:
results = (result for artist in artists 
           for result in sp.artist_albums(artist, album_type='album', limit=1)['items'])
albums.extend([r['name'], r['release_date']] for r in results)

Here, results is a lazily evaluating sequence of {'album_group': ..., 'album_type', ..., ...} dictionaries; these are all the albums in the 'items' list for each artist queried. There is only 1 for each artist here, but on the off-chance there might be zero albums, or you wanted to raise the limit value, I make sure to loop over the items.
The generator expression in albums.extend() then creates a new list object with two of the keys for each of those results.",A,1
13989690,2012-12-21 11:52:54.543000+00:00,"You are missing a space between def and __init__:
def__init__(self, N):

Add that in:
def __init__(self, N):

Note that your drawBoard(N) method is missing the self argument; N will be set to the instance instead when called.",A,10
15890322,2013-04-08 22:41:53.037000+00:00,"Your question as asked is underspecified, but I think if we pick a concrete example, you should be able to figure out how to adapt it to your actual use case.
So, let's say your values are all either a string representation of a float, or an empty string representing null:
A,B
1.0,2.0
2.0,
,3.0
4.0,5.0

And let's say you're reading this using a csv.reader, and you're explicitly handling the rows one by one with some do_stuff_with function:
with open('foo.csv') as f:
    next(reader) # skip header
    for row in csv.reader(f):
        a, b = map(float, row)
        do_stuff_with(a, b)

Now, if you want to treat null values as 0.0, you just need to replace float with a function that returns float(x) for non-empty x, and 0.0 for empty x:
def nullable_float(x):
    return float(x) if x else 0.0

with open('foo.csv') as f:
    next(reader) # skip header
    for row in csv.reader(f):
        a, b = map(nullable_float, row)
        do_stuff_with(a, b)

If you want to skip any rows that contain a null value in column B, you just check column B before doing the conversion:
with open('foo.csv') as f:
    next(reader) # skip header
    for row in csv.reader(f):
        if not row[1]:
            continue
        a, b = map(nullable_float, row)
        do_stuff_with(a, b)
",A,1
49249769,2018-03-13 06:39:47.420000+00:00,"The error here isn't coming from data.get, but from font_size.
When you use a named parameter in a format string, you have to provide a keyword value with a matching name. But you're only providing positional values, so there's nothing that matches.
You could fix this by doing something clumsy like:
subtitles.format(font_size=data.get('font_size'),
                 sentences=data.get('sentences'))

But really, this is exactly what format_map is for:
subtitles.format_map(data)


For future reference, it's a lot easier to debug things like this if you (temporarily) break up the expression, so you can see exactly which part causes the exception:
font_size = data.get('font_size')
sentences = data.get('sentences')
subtitles.format(font_size, sentences)

Then you'll see that it's the third line, not the first, that's raising the KeyError, and it'll be a lot easier to figure out why.

Finally, I'm not sure why you're using data.get('font_size') instead of just data['font_size']. If you're doing it to get a default value instead of a KeyError if you left the font size out of the data dict, you almost certainly don't want that default value to be None, do you? Anyway, if there is a good reason for it, even that can be simplified: turn data into a collections.defaultdict(lambda: whatever-default-value-you-want), and then any missing key will have that default value. (Even if you use it with format_map.)",A,1
9724438,2012-03-15 16:54:46.907000+00:00,"Because all the in-place functions return None, to emphasize the fact that they're mutating their argument. Otherwise, you might assign the return value to something, not realising that the argument had also changed.",A,4
409684,2009-01-03 19:39:45.817000+00:00,"You're simply creating indexes on your structure, right?
>>> from collections import defaultdict
>>> def indexOn( things, pos ):
...     inx= defaultdict(list)
...     for t in things:
...             inx[t[pos]].append(t)
...     return inx
... 
>>> a=[
...  ['4', '21', '1', '14', '2008-10-24 15:42:58'], 
...  ['3', '22', '4', '2somename', '2008-10-24 15:22:03'], 
...  ['5', '21', '3', '19', '2008-10-24 15:45:45'], 
...  ['6', '21', '1', '1somename', '2008-10-24 15:45:49'], 
...  ['7', '22', '3', '2somename', '2008-10-24 15:45:51']
... ]

Here's your first request, grouped by position 1.
>>> import pprint
>>> pprint.pprint( dict(indexOn(a,1)) )
{'21': [['4', '21', '1', '14', '2008-10-24 15:42:58'],
        ['5', '21', '3', '19', '2008-10-24 15:45:45'],
        ['6', '21', '1', '1somename', '2008-10-24 15:45:49']],
 '22': [['3', '22', '4', '2somename', '2008-10-24 15:22:03'],
        ['7', '22', '3', '2somename', '2008-10-24 15:45:51']]}

Here's your second request, grouped by position 3.
>>> dict(indexOn(a,3))
{'19': [['5', '21', '3', '19', '2008-10-24 15:45:45']], '14': [['4', '21', '1', '14', '2008-10-24 15:42:58']], '2somename': [['3', '22', '4', '2somename', '2008-10-24 15:22:03'], ['7', '22', '3', '2somename', '2008-10-24 15:45:51']], '1somename': [['6', '21', '1', '1somename', '2008-10-24 15:45:49']]}
>>> pprint.pprint(_)
{'14': [['4', '21', '1', '14', '2008-10-24 15:42:58']],
 '19': [['5', '21', '3', '19', '2008-10-24 15:45:45']],
 '1somename': [['6', '21', '1', '1somename', '2008-10-24 15:45:49']],
 '2somename': [['3', '22', '4', '2somename', '2008-10-24 15:22:03'],
               ['7', '22', '3', '2somename', '2008-10-24 15:45:51']]} 
",A,1
18164623,2013-08-10 17:38:38.073000+00:00,"Split on , and map to int():
map(int, inputstring.split(','))

This produces a list; if you need a tuple, just wrap it in a tuple() call:
tuple(map(int, inputstring.split(',')))

In Python 3, map() returns a generator, so you would use a list comprehension to produce the list:
[int(el) for el in inputstring.split(',')]

Demo:
>>> inputstring = '1,-2,3,4,-5'
>>> map(int, inputstring.split(','))
[1, -2, 3, 4, -5]
>>> tuple(map(int, inputstring.split(',')))
(1, -2, 3, 4, -5)
",A,4
21842325,2014-02-18 00:57:58.130000+00:00,"Your dates are formatted using the ISO8601 format, making them lexicographically sortable.
Just sort your list on the Datetime key of each dictionary:
from operator import itemgetter

for entry in sorted(list_of_dicts, key=itemgetter('Datetime')):
    # format your output

Demo:
>>> list_of_dicts = [
...     {'Source': 'Log1', 'Type': 'Connection', 'Datetime': '2014-02-13 14:10:00', 'fullpath':'N/A'},
...     {'Source': 'Log2', 'Type': 'Disconnect', 'Datetime': '2014-05-13 11:00:00', 'fullpath':'N/A'},
...     {'Source': 'Log4', 'Type': 'Other', 'Datetime': '2014-05-10 02:50:00', 'fullpath':'N/A'},
... ]
>>> from operator import itemgetter
>>> for entry in sorted(list_of_dicts, key=itemgetter('Datetime')):
...     print entry
... 
{'Source': 'Log1', 'fullpath': 'N/A', 'Type': 'Connection', 'Datetime': '2014-02-13 14:10:00'}
{'Source': 'Log4', 'fullpath': 'N/A', 'Type': 'Other', 'Datetime': '2014-05-10 02:50:00'}
{'Source': 'Log2', 'fullpath': 'N/A', 'Type': 'Disconnect', 'Datetime': '2014-05-13 11:00:00'}
",A,3
1212083,2009-07-31 11:41:58.787000+00:00,"""Foreseeing, lots and lots of usage in terms of aggregate queries could be implemented""
This is the hallmark of a data warehouse.
Here's the trick with DW processing.

Data is FLAT.  Facts and Dimensions.  Minimal structure, since it's mostly loaded and not updated.
To do aggregation, every query must be a simple SELECT SUM() or COUNT() FROM fact JOIN dimension GROUP BY dimension attribute.  If you do this properly so that every query has this form, performance can be very, very good.
Data can be stored in flat files until you want to aggregate.  You then load the data people actually intend to use and create a ""datamart"" from the master set of data.

Nothing is faster than simple flat files.  You don't need any complexity to handle terabytes of flat files that are (as needed) loaded into RDBMS datamarts for aggregation and reporting.
Simple bulk loads of simple dimension and fact tables can be VERY fast using the RDBMS's tools.  
You can trivially pre-assign all PK's and FK's using ultra-high-speed flat file processing.  This makes the bulk loads all the simpler.
Get Ralph Kimball's Data Warehouse Toolkit books.",A,2
7662174,2011-10-05 13:38:27.797000+00:00,"Animal.objects.filter(**request.GET)
",A,-1
13563055,2012-11-26 10:37:14.020000+00:00,"Your combinations approach was correct, you just need to turn the results of each combination into a dict again:
import itertools

def pairwise(input):
    for values in input.itervalues():
        for pair in itertools.combinations(values.iteritems(), 2):
            yield dict(pair)

This version is a generator, yielding pairs efficiently, nothing is held in memory any longer than absolutely necessary. If you need a list, just call list() on the generator:
list(pairwise(pleio))

Output:
>>> from pprint import pprint
>>> pprint(list(pairwise(pleio)))
[{'enf2': ['48', 'free'], 'enf3': ['34', 'set']},
 {'enf1': ['54', 'set'], 'enf3': ['34', 'set']},
 {'enf3': ['34', 'set'], 'enf4': ['12', 'free']},
 {'enf1': ['54', 'set'], 'enf2': ['48', 'free']},
 {'enf2': ['48', 'free'], 'enf4': ['12', 'free']},
 {'enf1': ['54', 'set'], 'enf4': ['12', 'free']}]

You can even combine the whole thing into a one-liner generator:
from itertools import combinations

for paired in (dict(p) for v in pleio.itervalues() for p in combinations(v.iteritems(), 2)):
    print paired

Which outputs:
>>> for paired in (dict(p) for v in pleio.itervalues() for p in combinations(v.iteritems(), 2)):
...     print paired
... 
{'enf3': ['34', 'set'], 'enf2': ['48', 'free']}
{'enf3': ['34', 'set'], 'enf1': ['54', 'set']}
{'enf3': ['34', 'set'], 'enf4': ['12', 'free']}
{'enf2': ['48', 'free'], 'enf1': ['54', 'set']}
{'enf2': ['48', 'free'], 'enf4': ['12', 'free']}
{'enf1': ['54', 'set'], 'enf4': ['12', 'free']}

If you are on Python 3, replace .itervalues() and .iteritems() by .values() and .items() respectively.",A,3
5532729,2011-04-03 21:42:59.143000+00:00,"There are quite a few problems with your code, but the one that's causing the actual error you quote is that you don't instantiate your FormWizard subclass in your urlconf.
As the documentation shows, you need to actually call the search class in urls.py to instantiate it, so you pass an instance rather than a class.
(r'^MusicGrep/$', 'MusicGrep.musicgrep.forms.search()'),

It might help if you followed the PEP8 guidelines on naming - if your class was called Search with a capital S, it would be more obvious that it was actually a class and not a function.",A,2
1158879,2009-07-21 12:25:23.487000+00:00,"This might work:
ProductType.objects.alL().order_by('parent__name', 'name')
",A,0
18988448,2013-09-24 17:42:00.900000+00:00,"You already have a list; the commas are put there by Python to delimit the values only when printing the list.
Just access element 2 directly:
print ting[2]

This prints:
[1379962800000, 125.539504822835]

Each of the entries in item['values'] (so ting) is a list of two float values, so you can address each of those with index 0 and 1:
>>> print ting[2][0]
1379962800000
>>> print ting[2][1]
125.539504822835

To get a list of all the second values, you could use a list comprehension:
second_vals = [t[1] for t in ting]
",A,2
29990171,2015-05-01 15:45:17.697000+00:00,"You are only turning the login_attr values to a tuple, but forgetting about the region attribute. It is those values that still throw the exception.
You are only testing for login_attr here:
if attr == ""last_login"":
    friendAttr = tuple(friendAttr)

Rather than turn your values to tuples there, just store tuples when reading the file.
Replace
elif field == ""last_login"": #<=== reading last_login
    profile[field] = map(int, raw_data.split(""-""))
elif field == ""region"":
    profile[field] = map(int, raw_data.split("";""))

with
elif field == ""last_login"":
    profile[field] = tuple(map(int, raw_data.split(""-"")))
elif field == ""region"":
    profile[field] = tuple(map(int, raw_data.split("";"")))
",A,1
25283632,2014-08-13 10:19:41.703000+00:00,"Functions retain a reference to their module globals. foo has such a reference:
>>> from test import foo
>>> foo.__globals__
{'x': 20, 'foo': <function foo at 0x102f3d410, ...}

What happens is that Python creates a module object when you import something; this object is stored in sys.modules and it serves as the global namespace for that module. The import statement then binds names in your 'local' module to either that same object or to attributes of that object.
Functions reference the same namespace for looking up globals from where they were defined; they essentially reference the same dictionary object:
>>> import sys
>>> foo.__globals__ is sys.modules['test'].__dict__
True
",A,4
27646378,2014-12-25 10:28:49.097000+00:00,"The only way to track order is to use full-on tracing. You could do that with the sys.settrace() hook, but you'll have to move everything into a main() function as you cannot attach a tracer to the current frame, only new frames:
import sys

_order = []
_seen = set()

def trace_names(frame, event, arg):
    if event == 'call' and arg is None:
        # enter first frame, return 'local' trace
        return trace_names
    if event in ('line', 'return'):
        # (last) line in top-level frame executed
        _order.extend(frame.f_locals.viewkeys() - _seen)
        _seen.update(frame.f_locals)

sys.settrace(trace_names)

# run your script
main()

sys.settrace(None)
print(_order)

This traces the order of locals added in the next scope only; if any functions modify globals you could not determine their exact order other than in whole groups, by inspecting globals() from the tracer.",A,1
3690457,2010-09-11 09:29:22.073000+00:00,"Custom tags are what you want - specifically, inclusion tags that let you render another template within the current one.",A,1
6155693,2011-05-27 17:25:10.297000+00:00,"What makes this needlessly complex is trying to optimize the quantitive answers.  
It's multiple choice.  Treat the quantitative answer as if it were qualitative. Carry the ""points"" as a separate attribute of each answer.  
And yes, there will be (""3 liters"", 3) in the database.  And yes, to a thinking person it may seem redundant.
But for software purposes, it works out well to consider all answers qualitative and keep any quantitative mapping entirely separated.  

Edit.  Don't store the answer as a number.  It's simply wrong.

Like for pets & beer question I have answer values ""0"", ""10"", ""100"" stored in database as strings in answers.value column.

Correct. 

to interpolate values to get score for answer 50 I have all the time to cast answers.value to float.

Incorrect.
Look them up just the same way you handle the pets.  It's a simple join.  Do everything the way you do pets.  Treat all data as ""qualitative"".   One simple rule; not two rules.  That's the correct and standard solution.  ",A,3
38618310,2016-07-27 16:16:22.200000+00:00,"Use itertools.permutations() on an increasing number of repeated digits, up to 9, combining these with all digits between 1 and 9 (to prevent generating numbers with a leading 0).
from itertools import permutations

def generate_unique_numbers():
    yield 0
    for i in range(10):
        for leading in '123456789':
            if not i:  # 1-digit numbers
                yield int(leading)
                continue
            remaining_digits = '0123456789'.replace(leading, '')
            for combo in permutations(remaining_digits, i):
                yield int(leading + ''.join(combo))

This generates all such valid numbers without having to skip anything. There are 8877691 such numbers, ranging from 0 to 9876543210:
>>> sum(1 for _ in generate_unique_numbers())
8877691
>>> next(generate_unique_numbers())
0
>>> for i in generate_unique_numbers(): pass
...
>>> i
9876543210

A few samples of the output:
>>> from itertools import islice
>>> gen = generate_unique_numbers()
>>> list(islice(gen, 15))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15]
>>> list(islice(gen, 150, 165))
[204, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 230]
>>> list(islice(gen, 100000, 100015))
[542319, 542360, 542361, 542367, 542368, 542369, 542370, 542371, 542376, 542378, 542379, 542380, 542381, 542386, 542387]
>>> list(islice(gen, 1000000, 1000015))
[31279056, 31279058, 31279064, 31279065, 31279068, 31279084, 31279085, 31279086, 31279405, 31279406, 31279408, 31279450, 31279456, 31279458, 31279460]

This method is easily faster than generating all numbers with range(9876543211) then filtering out those with repeated digits (which is what Moses Koledoye is doing):
>>> from timeit import timeit
>>> from itertools import islice
>>> def consume_n(it, n): next(islice(it, n, n), None)
...
>>> timeit('consume_n(gen(), 10000)', 'from __main__ import consume_n, unique_count as gen', number=100)
1.825788974761963
>>> timeit('consume_n(gen(), 10000)', 'from __main__ import consume_n, generate_unique_numbers as gen', number=100)
0.6307981014251709

The above code generates just the first 10000 numbers for each approach, and repeats those tests 100 times. My approach is easily 3 times faster!
Increase the count (and adjust the number of repetitions down to keep it manageable), and the contrast grows:
>>> timeit('consume_n(gen(), 100000)', 'from __main__ import consume_n, unique_count as gen', number=10)
4.125269889831543
>>> timeit('consume_n(gen(), 100000)', 'from __main__ import consume_n, generate_unique_numbers as gen', number=10)
0.6416079998016357

Now the difference has grown to 6x faster. Generating the first million numbers just once takes 23 seconds with the range version, .67 seconds with the above generator:
>>> timeit('consume_n(gen(), 1000000)', 'from __main__ import consume_n, unique_count as gen', number=1)
23.268329858779907
>>> timeit('consume_n(gen(), 1000000)', 'from __main__ import consume_n, generate_unique_numbers as gen', number=1)
0.6738729476928711

And the further along the series you go, the more natural numbers must be skipped and the speed difference only grows further; all numbers in the range 8800000000-8899999999 must be skipped for example, and the range() approach will test all of those. That's 100 million wasted cycles!
The generator can produce all possible such numbers in 6.7 seconds on my laptop:
>>> from collections import deque
>>> def consume(it): deque(it, maxlen=0)
...
>>> timeit('consume(gen())', 'from __main__ import consume, generate_unique_numbers as gen', number=1)
6.6731719970703125

I didn't dare test how long the range() approach takes; there are just under 9 million such numbers, but the range() approach will test close to 10 billion possibilities, 10000 times more numbers than needed.",A,3
40874904,2016-11-29 20:08:21.043000+00:00,"Yes, all(e not in p for e in exclude_list) is a call containing a generator expression. Generator expressions that are the only argument passed to a call can omit the parentheses. Here, that's the all() function being called.
From the Generator expressions reference documentation:

The parentheses can be omitted on calls with only one argument.

The all() function (as well as the companion function any() is often given a generator expression, as this allows for lazy evaluation of a series of tests. Only enough e not in p tests are executed to determine the outcome; if there is any e not in p test that is false, all() returns early and no further tests are executed.",A,2
4758731,2011-01-21 12:22:13.103000+00:00,"from array import array

a = array('b', [30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47])
print ', '.join(map(str, a))
",A,2
53668454,2018-12-07 11:16:25.510000+00:00,"Your models are hard to read and use; I realise that they probably started off autogenerated from an existing db schema, but you need to tidy them up more before they are usable.
The most important thing from your point of view is to recognise when you have many-to-many relationships, such as in Ordercompanies, and make those explicit. But you also need to do some renaming to follow Django conventions: start by dropping the Data prefix from all the model names, use proper InitialCaps format, and make them singular. Next drop the id suffix from the foreign keys, and give all the fields lower_case_with_underscore format names. So:
class Color(models.Model):
    ...

class Company(models.Model):
    orders = models.ManyToManyField('Order', through='OrderCompany')
    ...


# what's the point of this table?
class ModelColor(models.Model):
    color = models.ForeignKey(Color, models.DO_NOTHING, db_column='ColorID')
    ...

class ModelData(models.Model):
    colors = models.ManyToManyField('Color', through='ModelDataColor')
    sizes = models.ManyToManyField('Size', through='ModelDataSize')
    ...

class ModelDataColor(models.Model):
    model_data = models.ForeignKey(ModelData, models.DO_NOTHING, db_column='ModelDataID', primary_key=True)
    color = models.ForeignKey(Color, models.DO_NOTHING, db_column='ColorID') 
    ...

class ModelDataSize(models.Model):
    model_data = models.ForeignKey(ModelData, models.DO_NOTHING, db_column='ModelDataID', primary_key=True)
    size = models.ForeignKey('Size', models.DO_NOTHING, db_column='SizeID')
    ...


class ModelSize(models.Model):
    color = models.ForeignKey(ModelColor, models.DO_NOTHING, db_column='ColorID')
    size = models.ForeignKey('DataSize', models.DO_NOTHING, db_column='SizeID')
    ...

class Model(models.Model):
    model_color = models.ForeignKey(ModelColor, models.DO_NOTHING, db_column='ModelColorID')
    order = models.ForeignKey('Order', models.DO_NOTHING, db_column='OrderID')
    model_data = models.ForeignKey(ModelData, models.DO_NOTHING, db_column='ModelDataID')
    ...

class OrderCompany(models.Model):
    order = models.ForeignKey('Order', models.DO_NOTHING, db_column='OrderID', primary_key=True)
    company = models.ForeignKey(Company, models.DO_NOTHING, db_column='CompanyID')
    stage_number = models.IntegerField(db_column='StageNumber')

    ...

class Order(models.Model):
    ...

class Size(models.Model):
    ...

(I haven't understood all your relations, so I may not have got all those right.)
So now you have a proper relation between Order and Company, you can query that directly:
company = Company.objects.get(id=10) # remember to use `get` for a single item
order_list = company.orders.all()

and pass that to the template.
Note you still aren't using the actual names of the relations or reverse relations in your template. Since the relationship between model and order does not define an explicit related_name, the default one is model_set; or, if you stuck with your original names, datamodels_set. You need to use that when you follow the relation:
{% for model in order.model_set.all %}

Similarly, the ForeignKey from Model to Color is called model_color in my version, or modelcolorid in yours; again, that's the name you need to use:
{% for color in model.model_color.all %}
I would say that this is a very complex set of models - and a complex task overall - for someone who is new to Django. You should probably start smaller, with just a couple of models, and experiment with the API until you understand it. It is actually easy to use, but you've made it hard by trying to learn too much at once.",A,1
12504476,2012-09-19 23:59:34.557000+00:00,"There is no way to do this directly.
SMJobSubmit is a simple wrapper around a complicated task. It also returns synchronously despite launching a task asynchronously. So, while it can give you an error if it fails to submit the job, if it successfully submits a job that fails to run, there is no way to find that out.
So, you will have to explicitly write some code to communicate from your helper to your app, to report that it's up and running.
If you've already built some communication mechanism (signals, files, Unix or TCP sockets, JSON-RPC over HTTP, whatever), just use that.
If you're designing something from scratch, XPC may be the best answer. You can't use XPC to launch your helper (since it's privileged), but you can manually create a connection by registering a Mach service and calling xpc_connection_create_mach_service.",A,1
15720872,2013-03-30 16:35:45.887000+00:00,"No, they cannot ever access the same in-memory database from different processes  Instead, a new connection to :memory: always creates a new database.
From the SQLite documentation:

Every :memory: database is distinct from every other. So, opening two database connections each with the filename "":memory:"" will create two independent in-memory databases.

This is different from an on-disk database, where creating multiple connections with the same connection string means you are connecting to one database.
Within one process it is possible to share an in-memory database if you use the file::memory:?cache=shared URI:
conn = sqlite3.connect('file::memory:?cache=shared', uri=True)

but this is still not accessible from other another process.",A,20
25026555,2014-07-29 23:29:47.667000+00:00,"There are a few ways to do this.
The simplest is to keep looping over the file even after EOF. You could do this by just wrapping a while True: around the for line in f:, or by just looping forever around f.readline().
But this will waste a lot of CPU power and possibly even disk access checking over and over as fast as possible whether the file is still at EOF. You can fix that by sleeping whenever you get to the end of the file, like this:
while True:
    for line in f:
        print line
    time.sleep(0.5)

But if the file is not written to for a long time, you're still wasting CPU power (which may not seem like a problem, but imagine what happens when the computer wants to go to sleep, and it can't because you're making it work every half a second). And meanwhile, if the file is being written to a lot faster than twice/second, you're going to lag.
So, a better solution is to block until there's something to read.
Unfortunately, there's no easy cross-platform way to do this. Fortunately, there are relatively easy platform-specific ways to do it on most platforms, but I'd need to know your platform to help.
For example, on OS X or other *BSD systems, you can use kqueue to wait until a file has something to read:
from select import *

# the rest of your code until the reading loop

while True:
    for line in f:
        print line
    kq = kqueue()
    kq.control([kevent(f.fileno(), filter=KQ_FILTER_READ, flags=KQ_EV_ADD)], 0, 0)
    kq.control(None, 1)
    kq.close()

But that won't work on Windows, or linux, or any other platform. (Also, that's a pretty bad way to do it on BSD, it's just shorter to show this way than the right way. If you want to do this for OS X, find a good tutorial on using kqueue in Python, don't copy this code.)",A,1
33438879,2015-10-30 14:55:55.060000+00:00,"No, these things aren't the same at all. The Ruby code instantiates a connection once and assigns it to an instance var; the Python code creates a new instance of Client every time query is called, although they share the connection instance. 
But there is no need for the added wrapper. If you want to create it once, do so at module level:
connection = DBClient()

def query():
    connection.select(...)

Really, you should stop writing Ruby in Python; if you want to write Python, you should learn how to write things idiomatically in Python, not simply translate your Ruby code directly.",A,1
21266650,2014-01-21 18:49:59.240000+00:00,"This has nothing to do with integers in your input; you are calling the function incorrectly. You need to pass in your command as one sequence, a list:
subprocess.call(['samtools','view','-bq', '1','DS130.bam','>','DS130umr.bam'])

This will not, however, be processed by a shell, so the output redirect (>) won't work. If you want to direct the output of the tool to another file, either use shell=True and pass in a string:
subprocess.call('samtools view -bq 1 DS130.bam > DS130umr.bam', shell=True)

for the shell to process, or use output redirection:
with open('DS130umr.bam', 'wb') as redirected:
    subprocess.call(['samtools','view','-bq', '1','DS130.bam'], stdout=redirected)
",A,4
4937526,2011-02-08 19:41:39.473000+00:00,"Python 2:
>>> theArray = [['a','b','c'],['d','e','f'],['g','h','i']]
>>> zip(*theArray)
[('a', 'd', 'g'), ('b', 'e', 'h'), ('c', 'f', 'i')]

Python 3:
>>> [*zip(*theArray)]
[('a', 'd', 'g'), ('b', 'e', 'h'), ('c', 'f', 'i')]
",A,279
6445405,2011-06-22 19:29:35.490000+00:00,There's no point using the system packaging tools if you're using a different Python version to the one provided by the system. You'll have to use the Python packaging tools instead - eg easy_install mysqldb.,A,0
1679854,2009-11-05 11:10:37.283000+00:00,"Please read http://docs.python.org/library/tempfile.html
Use that instead.",A,7
10910657,2012-06-06 08:26:02.900000+00:00,"You can just query on the user field:
referenced_users = Foo.all().filter('user', deleted_user_key)
",A,0
19731987,2013-11-01 17:26:50.970000+00:00,"Not sure synchronicity is your real concern here. Django's signals are always executed in-process: that is, they will be executed by the process that did the rest of that request, before returning a response. However, if your server itself is asynchronous, there's a possibility that one request will finish processing after a second one that was received later, and therefore the signals will be processed in the wrong order.
Celery, of course, is definitely asynchronous, but might well be a better bet if reliable ordering is a priority for you.",A,5
24471329,2014-06-28 22:00:27.953000+00:00,"Use f in the new format too:
>>> '{num:8.3f}'.format(num=1.98567)
'   1.986'

Without the format type, the default is g, and the precision is interpreted as the total number of digits (not counting a 0 before the decimal). With a 1 before the declimal point, only 2 digits are shown after it. 
You'd see the same output with the old string formatting if you used g instead of f:
>>> '%8.3g' % (1.98567)
'    1.99'
",A,6
28012742,2015-01-18 17:38:15.497000+00:00,"Methods that modify existing lists, like sort, do not return anything. So your code sets slst to None.
Instead, just call the method without assigning:
def median(lst):
    lst.sort()
    if len(slst) == 1:
        median = lst[0]

Alternatively, you can use lst = sorted(lst), which does return a new list, but that is unnecessary.",A,1
19824571,2013-11-06 22:54:34.343000+00:00,"You have an else: that isn't attached to any if, for, while, or try statement, which is illegal.
If you meant for the else to be attached to the while, the solution is simple: Change the indentation to attach it:
while (a1base >= 1) and (u1base >= 1) or (a2base >= 1) and (u2base >= 1):
    abases = (a1base+ a2base)
    ubases = (u1base + u2base)
    firstset = min(abases, ubases)
    print(""You have"", firstset,""A/U bases."")
else:
    print(""You have zero A/U bases."")

See break and continue Statements, and else Clauses on Loops in the tutorial (and Compound statements in the language reference for full details).",A,3
32327307,2015-09-01 08:49:10.947000+00:00,"fromtimestamp() already returns your local time i.e., you don't need to attach the utc offset if fromtimestamp() determines it correctly automatically:
#!/usr/bin/env python
from datetime import datetime

local_time = datetime.fromtimestamp(1437323953822 * 1e-3)
# -> datetime.datetime(2015, 7, 19, 12, 39, 13, 822000)

fromtimestamp() may fail in some cases e.g., if the local timezone had a different utc offset in the past and fromtimestamp() does not use a historical timezone database on a given platform (notably, Windows). In that case, construct the local time explicitly from utc time and the given utc offset:
#!/usr/bin/env python
from datetime import datetime, timedelta

utc_time = datetime(1970, 1, 1) + timedelta(milliseconds=1437323953822)
utc_offset = timedelta(milliseconds=-14400000)
local_time = utc_time + utc_offset
# -> datetime.datetime(2015, 7, 19, 12, 39, 13, 822000)

Python always expects POSIX Epoch and therefore it is ok to hardcode it. The explicit formula may be more precise (no rounding error) and it may accept a wider range of input timestamps (fromtimestamp() range depends on platform and may be narrower than the corresponding datetime range).",A,0
11843634,2012-08-07 10:08:13.870000+00:00,"A Plone company called Jarn created a SaaS intranet platform built on Plone, that included a full XMPP stack for in-browser realtime chat (or microblogging, really) and presence management.
This platform was open sourced when the company decided not to pursue that platform anymore, and is available on GitHub: https://github.com/intranett/intranett
The core of the XMPP integration is the jarn.xmpp.core package; it provides the presence and messaging components. The intranett buildout above shows how to use this component's potential.
Disclaimer: I used to be a software engineer at Jarn, the company no longer exists today, this is thus Open Source software without a current maintainer.",A,1
252039,2008-10-30 22:49:35.830000+00:00,Have you looked at flat pages in Django?  It probably does everything you're looking for.,A,6
20204296,2013-11-25 22:01:14.527000+00:00,"This line is invalid:
c = Context({'posts': })

There needs to be a value there:
c = Context({'posts': posts})
",A,1
39660610,2016-09-23 12:18:33.023000+00:00,"The Passlib Password Hash interface either lets you set the salt size, or the salt value itself. From the documentation on pbkdf2_sha256:


salt (bytes) Optional salt bytes. If specified, the length must be between 0-1024 bytes. If not specified, a 16 byte salt will be autogenerated (this is recommended).
salt_size (int) – Optional number of bytes to use when autogenerating new salts. Defaults to 16 bytes, but can be any value between 0 and 1024.


so you can set your own pre-generated salt:
>>> from passlib.hash import pbkdf2_sha256
>>> pbkdf2_sha256.encrypt(""password"", rounds=200000, salt=b'spamhameggs')
'$pbkdf2-sha256$200000$c3BhbWhhbWVnZ3M$WL9OLVcb3f7HqHeNT./kCJeunydLCi4JykzEuAdewcI'

However, note that the salt is part of the returned string. The string contains not only the resulting hash, but also the algorithm, the number of rounds used and the salt used, delimited by $. The salt is encoded to modified form of base64. You can verify this by decoding the string c3BhbWhhbWVnZ3M again::
>>> from passlib.utils import ab64_decode
>>> ab64_decode(b'c3BhbWhhbWVnZ3M')
b'spamhameggs'

See the Format & Algorithm section for the pbkdf2_sha256 docs.
So when you store the full string pbkdf2_sha256 in the database, everything to validate the string is right there in the value, including the salt. Leaving generating a random salt is best left to that library as it'll use a secure method to generate one.",A,2
47160115,2017-11-07 14:15:41.937000+00:00,"This is the documentation for the datetime module; strftime is a method that - as that quote states - is available on all three of the classes in that module.
d is just a name for the instance that you are calling the method on; it could be one of any of those types.",A,0
34617089,2016-01-05 16:56:18.280000+00:00,"You did not import the etree module as a global name in your module, only the lxml package itself. You need to import the etree module from the lxml package:
from lxml import etree

See the lxml.etree tutorial.
If import lxml works but from lxml import etree fails, you either have another lxml.py file in your path that masks the package, or you are trying to use a uncompiled source distribution. Use:
import lxml
print(lxml.__file__)

to find and rename the offending file.

If it points to lxml.py remove or rename that file.
If it points to <PATH>/src/lxml/__init__.py you are trying to use an uncompiled source distribution. You'll have to compile the Python extension code, or find a binary distribution for your platform to install.
",A,3
33777945,2015-11-18 10:45:51.603000+00:00,"The question ""Python Time conversion h:m:s to seconds"" (that you've linked) shows a simple efficient one-line solution on how to get an integer number of seconds since midnight that works for both datetime.datetime and datetime.time types:
from datetime import timedelta

secs = timedelta(hours=rec.hour, minutes=rec.minute, seconds=rec.second).seconds
",A,0
21506067,2014-02-02 01:44:54.207000+00:00,"Store the previous words instead:
prev2 = None
for word2 in vocab_list:
    prev = None
    for elem in sentence2:
        if prev == prev2 and elem == word2:
            # do something
        prev = elem
     prev2 = word2

It is much easier to look behind than to look ahead.
As for using indexes, you can always use the enumerate() function to add an index to a loop; you could look ahead in this case with:
for j, word2 in enumerate(vocab_list):
    for i, elem in enumerate(sentence2):
        if i + 1 < len(sentence2) and j + 1 < len(vocab_list):
            nextElem = sentence2[i + 1]
            nextWord2 = vocab_list[j + 1]
but tracking the previous element is just easier.",A,1
37751832,2016-06-10 15:21:34.393000+00:00,"To reproduce, the shell command:
subprocess.run(""dummy.exe < file.txt > foo.txt"", shell=True, check=True)

without the shell in Python:
with open('file.txt', 'rb', 0) as input_file, \
     open('foo.txt', 'wb', 0) as output_file:
    subprocess.run([""dummy.exe""], stdin=input_file, stdout=output_file, check=True)

It works with arbitrary large files.
You could use subprocess.check_call() in this case (available since Python 2), instead of subprocess.run() that is available only in Python 3.5+.

Works very well thanks. But then why was the original failing ? Pipe buffer size as in Kevin Answer ?

It has nothing to do with OS pipe buffers. The warning from the subprocess docs that @Kevin J. Chase cites is unrelated to subprocess.run(). You should care about OS pipe buffers only if you use process = Popen() and manually read()/write() via multiple pipe streams (process.stdin/.stdout/.stderr).
It turns out that the observed behavior is due to Windows bug in the Universal CRT. Here's the same issue that is reproduced without Python: Why would redirection work where piping fails?
As said in the bug description, to workaround it: 

""use a binary pipe and do text mode CRLF => LF translation manually on the reader side"" or use ReadFile()  directly instead of std::cin
or wait for Windows 10 update this summer (where the bug should be fixed)
or use a different C++ compiler e.g., there is no issue if you use g++ on Windows

The bug affects only text pipes i.e., the code that uses <> should be fine (stdin=input_file, stdout=output_file should still work or it is some other bug).",A,5
15913983,2013-04-09 22:53:57.320000+00:00,"First, I'd use the grouper recipe from the itertools docs to get the groups:
new_list = list(grouper(30, cluster))

Then filter the last group to remove the fillvalue entries, and, if the result isn't ""approximately close to 30"", remove it.
new_list[-1] = list(filter(None, new_list[-1]))
if len(new_list) < chunk_size:
    del result[-1]

If None is a valid element, use something else as a sentinel:
sentinel = object()
new_list = list(grouper(30, cluster, fillvalue=sentinel)
new_list[-1] = [element for element in new_list[-1] if element is not sentinel]
if len(new_list[-1]) < chunk_size:
    del result[-1]

Meanwhile, there's some talk about adding a zip_strict to itertools, which would allow a grouper recipe that returns a short final group, instead of padding it with fillvalue. If this happens in, say, 3.4, you could simplify this to something like:
new_list = list(grouper(30, cluster, strict=True))
if len(new_list[-1]) < chunk_size:
    del result[-1]

Or, of course, you could use one of the ""strict grouper"" implementations being bandied about on the python-ideas list, or just write your own that wraps up the grouper and filter calls above.",A,0
52915954,2018-10-21 13:44:48.250000+00:00,"Your break is never executed because Python can't get there. First, your while loop indentation differs:
while True:
    # do things

if results is True:
    # loop with break

The break can't get out of the while True loop because it is not part of that loop. 
You'd have to adjust your indentation first:
while True:
    # do things

    if results:
        # loop with break

Next you test if results is the same object as the True object:
if results is True:
    # ...

This will never be true, because cursor.fetchall() will always produce a list. If you want to test if the list is not empty, just use:
if results:
    # ...

The if statement already checks if something is true or not, and you should almost never need to use is True or == True in Python.
Next, you are using break in a for loop, so the for loop will be ended, and not the while loop. So even if you corrected the structure to:
while True:
    # ...
    if results:
        for i in results:
            # ...
            break

then the for loop would be broken out of. The while True: loop would happily continue.
You could add another break to the structure:
while True:
    # ...
    if results:
        for i in results:
            # ...
            break
        break

but that is all getting a bit.. verbose. Since you know results is not empty, you can drop the for loop and use:
while True:
    # ...
    if results:
        i = results[0]
        print(""Welcome ""+i[1])
        break

If you just wanted a single result, you wouldn't want to fetch all the rows that match, just the first row, so use cursor.fetchone():
def login():
    db = sqlite3.connect(""wtf.db"")
    while True:
        username = input(""Please enter your username: "")
        password = input(""Please enter your password: "")
        cursor = db.cursor()
        find_user = ""SELECT * FROM user WHERE username=? AND password=?""
        cursor.execute(find_user, [username, password])
        result = cursor.fetchone()
        if result:
            name = result[1]
            print(""Welcome"", name)
            break

I cleaned up the code a bit here; I removed the with context manager use, as there is no need to use this when not making any changes to the database and there is just a single SELECT (no transaction management needed), and the above connects to the database just once (re-connecting for each attempt is a bit of a waste, changes to the database from another process are automatically picked up anyway). 
I also used name instead of i[1] to welcome the logged-in user. Good variable names help make the code easier to understand.",A,2
38857474,2016-08-09 17:56:59.533000+00:00,"Your code is printing the list each iteration, as you build it. Simply don't print each iteration; print only after the for loop has completed (if at all).
get_attributeName = soup.find(True,{""class"": [""product-attributes"", ""product-attribute-value""]}).find_all('li')

allDataList = []        
for attData, attValues in get_attributeName:
    data = [attData, attValues.text]            
    allDataList.append(data)
print(allDataList)
",A,2
15396760,2013-03-13 21:24:21.890000+00:00,".strip() takes a set of characters to remove. You are removing all the a, b, c, (, ', / and ) characters form the start and end. Only t is then left.
You could split on ' instead:
string.split(""'"")[1]

or you could explicitly remove the exact characters from the start and end with slicing:
string[5:-2]
",A,1
26392232,2014-10-15 21:14:11.677000+00:00,"
... The unicode characters é and á in the original file are changed to Ã© and Ã¡ respectively when I read() the file. 

Your actual issue is unrelated to regexes. You are reading utf-8 text using latin-1 encoding that is incorrect. 
>>> print(""é"".encode('utf-8').decode('latin-1'))
Ã©
>>> print(""á"".encode('utf-8').decode('latin-1'))
Ã¡

To read utf-8 file:
with open(filename, encoding='utf-8') as file:
    text = file.read()

Old answer about regexes (unrelated to OPs issue):
In general, a single user-perceived character such as ç, é may span multiple Unicode codepoints therefore [çé] may match these Unicode codepoints separately instead of matching the whole character. (?:ç|é) would fix this one issue, there are others e.g., Unicode normalization (NFC, NFKD).

I want to replace line feeds by spaces when the next line begins with a lowercase character.

regex module supports POSIX character class [:lower:]:
import regex # $ pip install regex

text = (""Test to check\n""
        ""whether it's working.\n""
        ""Aquela \xe9\n""
        ""\xe1rvore pequena.\n"")
print(text)
# -> Test to check
# -> whether it's working.
# -> Aquela é
# -> árvore pequena.
print(regex.sub(r'\n(?=[[:lower:]])', ' ', text))
# -> Test to check whether it's working.
# -> Aquela é árvore pequena.

To emulate [:lower:] class using re module:
import re
import sys
import unicodedata

# \p{Ll} chars
lower_chars = [u for u in map(chr, range(sys.maxunicode)) 
               if unicodedata.category(u) == 'Ll']
lower = ""|"".join(map(re.escape, lower_chars))
print(re.sub(r""\n(?={lower})"".format(lower=lower), ' ', text))

The result is the same.",A,1
25748405,2014-09-09 15:24:04.393000+00:00,"You are using tabs for the lines with less indentation, spaces for the others.
Don't do that. To avoid this problem, in Wing IDE adjust your indentation preferences to use only spaces, and use the indentation manager to repair this file.",A,0
29957401,2015-04-30 01:16:21.667000+00:00,"The actual problem is that, for each y and each x, you're printing one character for each object:
for y in range(mapS[1]):
    numy += 1
    for x in range(mapS[0]):
        numx += 1
        for place in objects:
            if objects[place][0] == numx:
                if objects[place][0] == numy:
                    line += objects[place][2]
                else:
                    line += ""*""
            else:
                line += ""*""

So, if you have 2 objects, you'll print 2 characters at each x, y point; if you have 6 objects, you'll print 6 characters at each x, y point.
What you want to do is only print 1 character at each x, y point. For example:
for y in range(mapS[1]):
    numy += 1
    for x in range(mapS[0]):
        numx += 1
        for place in objects:
            if objects[place][0] == numx and objects[place][0] == numy:
                line += objects[place][2]
                break
        else:
            line += ""*""        

This goes through the objects until it finds the first one that matches that position, and adds only that one object's character, and stops searching with break. Then, only if it gets to the end of the loop without breaking (that's what an else clause on a for statement means), it adds a * instead.

You have a number of other problems here:

mapSize doesn't actually change mapS; you need a global mapS statement.
You're comparing both numx and numy to objects[place][0]. You almost certainly wanted to compare numy to objects[place][1]. One nice way to do this all at once is: if objects[place][:2] == [numx, numy]:.
Your numx and numy are 1-based. Did you want that? If so, it would make your intention more explicit—and a whole lot simpler—to just use numx == x+1 instead of keeping track of it separately from x but in such a way that it always ends up the same as x+1. If not, just use x itself.
The globals()[name] = is a bad idea. Since you don't make any use of it, the simplest way to fix that is to just remove that line.
It's a lot simpler to create things like line at the top of the loop, instead of creating them outside the loop and then re-creating them at the bottom of the loop.

So, putting it all together:
mapS = [5, 5]

objects = {}

def mapSize(x, y):
    global mapS
    mapS = [x, y]

def addObject(name, x, y, symbol):
    objects[name] = [x, y, symbol]

addObject(""House"", 2, 3, ""H"") ##############FIRST POINT
addObject(""Cabin"", 3, 4, ""C"") ##############SECOND POINT

for y in range(mapS[1]):
    line = """"
    numy = y+1
    for x in range(mapS[0]):
        numx = x + 1
        for place in objects:
            if objects[place][:2] == [numx, numy]:
                line += objects[place][2]
                break
        else:
            line += ""*""        
    print(line)
",A,0
1794415,2009-11-25 03:17:46.437000+00:00,"Don't call it ""production schema"".  Call it ""Transactional Schema"", or OLTP.
To make updates work, it must be in (at least) Third Normal Form (3NF).
Data warehouses use ""Star Schema"" which is normalized according to different rules.  Since updates don't really occur the same way they do in a transactional database, the design is normalized completely differently.
Read up on 3rd Normal Form.
Read up on Star Schema Design.
There are lots and lots of Google links that are very good.
Better yet, but Kimball's The Data Warehouse Toolkit book.  That's very clear.",A,4
23248569,2014-04-23 15:16:23.763000+00:00,"Floating point numbers are only approximations; 2.85 cannot be represented exactly:
>>> format(2.85, '.53f')
'2.85000000000000008881784197001252323389053344726562500'

It is slightly over 2.85.
0.5 and 0.75 can be represented exactly with binary fractions (1/2 and 1/2 + 1/4, respectively).
The round() function documents this explicitly:

Note: The behavior of round() for floats can be surprising: for example, round(2.675, 2) gives 2.67 instead of the expected 2.68. This is not a bug: it’s a result of the fact that most decimal fractions can’t be represented exactly as a float. See Floating Point Arithmetic: Issues and Limitations for more information.
",A,27
43027975,2017-03-26 11:11:06.570000+00:00,"You can use the enumerate() function to add the indices in the dictionary:
val_dict = dict(enumerate(some_test_function(1, 2, 3)))

This gives you the exact same integer index to value mapping. However, just a list would give you the exact same index-to-value mapping:
val_list = list(some_test_function(1, 2, 3))

(if some_test_function already returns a list object, just remove the list() call).",A,3
49295530,2018-03-15 09:21:23.580000+00:00,"I can't really see the point of this level of complexity.
form_factory works just as well as a concrete generic view, which takes the parameters and is called directly from the URL. The URLconf can then be simplified to iterate through the models and add a pattern for each which passes those paraemters.
Then you can further simplify things by using actual generic views. These can be instantiated directly in the URLconf without any need to define subclasses in views.py. What's more, the CreateView is capable of building a form itself, without any need for defining in forms.py. So you can get rid of both of those files.
One final simplification is to use the actual API for getting model classes: django.apps.apps.get_models(). So all you need is:
from django.views.generic.edit import CreateView
from django.apps import apps

for model in apps.get_models():
    urlpatterns += path(
        '{}/new'.format(model._meta.model_name),
        CreateView.as_view(
             model=model,
             template_name='core/create.html',
             fields='__all__',
             success_url=reverse_lazy('index')
        ),
        name='create-{}'.format(model._meta.model_name)
    )
",A,1
15911302,2013-04-09 19:55:26.430000+00:00,"Assuming you're just trying to eliminate the overhead of using a text-based format instead of a binary format for your temporary data, and you don't want to rewrite everything to use numpy, there are a few different solutions:

First, you can keep the data in binary format in the first place: mmap the file, and use ctypes to treat it as a giant record of some kind. This is usually more trouble than it's worth, but it's worth mentioning.
Assuming your data is nothing but a long list of tuples of 5 bytes:
class Entry(ctypes.Structure):
    _fields_ = [(""x"", ctypes.c_uint8), (""y"", ctypes.c_uint8),
                (""i"", ctypes.c_uint8), (""j"", ctypes.c_uint8), (""k"", ctypes.c_uint8)]
Entries = ctypes.POINTER(Entry)
with open(fname, 'wb') as f:
    f.truncate(ctypes.sizeof(Entry * (yn - y0)))
    m = mmap.mmap(f.fileno(), access=mmap.ACCESS_WRITE)


Second, you can use struct. You'll have to read the docs for complete details, but I'll give one example. Let's take this line:
print >>f, x, y, 8*i, 0, 0

Now, let's assume that all 5 of those are guaranteed to be bytes (0-255). You can just do:
f.write(struct.pack('BBBBB', x, y, 8*i, 0, 0))

To read them back later:
x, y, i8, 0, 0 = struct.unpack('BBBBB', f.read(struct.calcsize('BBBBB')))
i = i8//8

If any of them needs to be longer than a byte, you need to deal with endianness, but that's pretty trivial. For example, if x and y range from -32768 to 32767:
f.write(struct.pack('>hhBBB', x, y, 8*i, 0, 0))

And make sure to open the file in binary mode.
And you can of course combine this with mmap if you want, which means you can just use the struct.pack_into and struct.unpack_from instead of explicitly using pack plus write and unpack plus read.

Next, there's pickle. Either directly create your list and just pickle.dump it, or manually pickle.dumps each entry and add some simple higher-level structure above that (or just use shelve, if that higher-level structure is, or could be, a simple mapping from keys to entries). This may be larger instead of smaller, and it may be slower, so you always want to do some testing before considering this. But sometimes it's a simple solution.

Finally, you can probably come up with a more compact text format than just printing the str representation of each object. This is usually not worth the effort, but again, it's worth thinking about.",A,2
39319855,2016-09-04 17:53:49.430000+00:00,"You seem to not have read the Flask-Cache documentation very closely. Caching does not ignore parameters and the cache key is customisable. The decorators the project supplies already give you the functionality you seek.
From the Caching View Functions section:

This decorator will use request.path by default for the cache_key.

So the default cache key is request.path, but you can specify a different key. Since Flask view functions get their arguments from path elements, the default request.path makes a great key.
From the @cached() decorator documentation:

cached(timeout=None, key_prefix='view/%s', unless=None)
By default the cache key is view/request.path. You are able to use this decorator with any function by changing the key_prefix. If the token %s is located within the key_prefix then it will replace that with request.path. You are able to use this decorator with any function by changing the key_prefix.

and

key_prefix – [...] Can optionally be a callable which takes no arguments but returns a string that will be used as the cache_key.

So you can set key_prefix to a function, and it'll be called (without arguments) to produce the key.
Moreover:

The returned decorated function now has three function attributes assigned to it. These attributes are readable/writable:
[...]
make_cache_key
  A function used in generating the cache_key used.

This function is passed the same arguments the view function is passed. In all, this allows you to produce any cache key you want; either use key_prefix and pull out more information from the request or g or other sources, or assign to view_function.make_cache_key and access the same arguments the view function receives.
Then there is the @memoize() decorator:

memoize(timeout=None, make_name=None, unless=None)
Use this to cache the result of a function, taking its arguments into account in the cache key.

So this decorator caches return values purely based on the arguments passed into the function. It too supports a make_cache_key function. 
I've used both decorators to make a Google App Engine Flask project scale to double-digit millions of views per month for a CMS-backed site, storing results in the Google memcached structure. Doing this with Redis would only require setting a configuration option.",A,0
49163957,2018-03-08 01:29:53.603000+00:00,"If the file is really big, but not too big--that is, big enough that you don't want to read the whole file (especially in text mode or as a list of lines), but not so big that you can't page it into memory (which means under 2GB on a 32-bit OS, but a lot more on 64-bit), you can do this:
import itertools
import mmap
import re
import sys

n = 5

# Notice that we're opening in binary mode. We're going to do a
# bytes-based regex search. This is only valid if (a) the encoding
# is ASCII-compatible, and (b) the spaces are ASCII whitespace, not
# other Unicode whitespace.
with open(sys.argv[1], 'rb') as f:
    # map the whole file into memory--this won't actually read
    # more than a page or so beyond the last space
    m = mmap.mmap(f.fileno(), access=mmap.ACCESS_READ)

    # match and decode all space-separated words, but do it lazily...
    matches = re.finditer(r'(.*?)\s', m)
    bytestrings = (match.group(1) for match in matches)
    strings = (b.decode() for b in bytestrings)

    # ... so we can stop after 5 of them ...
    nstrings = itertools.islice(strings, n)

    # ... and turn that into a list of the first 5
    contents = list(nstrings)

Obviously you can combine steps together, even cramming the whole thing into a giant one-liner if you want. (An idiomatic version would be somewhere between that extreme and this one.)",A,1
16222271,2013-04-25 18:40:00.803000+00:00,"You need to pass in a sequence of parameters, make it a tuple with one element:
cur.execute(""SELECT quantity FROM cards WHERE ref=?"", (idn,))  

Python strings are sequences too, so if idn is a 5-character string, python sees a sequence of length 5.
The database always returns rows of columns, even for results with only one column. A row is essentially a Python tuple, a sequence. It's the same problem as your idn parameter, but in reverse. Take the one column out of the result row by indexing:
print ""Quantity "" + str(cur.fetchone()[0])

A quick demo with your sample database:
>>> import sqlite3
>>> con = sqlite3.connect(':memory:')
>>> con.executescript('''\
... CREATE TABLE cards (id integer primary key autoincrement not null, ref text unique
... check(ref!=''), name text, description text, quantity integer default 0, cat1 text,
... cat2 text);
... 
... INSERT INTO ""cards"" VALUES(1,'lx247','green door',NULL,20,'abstract','');
... INSERT INTO ""cards"" VALUES(2,'lxx247','green door',NULL,20,'abstract','');
... INSERT INTO ""cards"" VALUES(3,'lxx2f47','green door',NULL,20,'abstract','');
... ''')
<sqlite3.Cursor object at 0x10ad66ab0>
>>> idn = 'lx247'
>>> cur=con.cursor()
>>> cur.execute(""SELECT quantity FROM cards WHERE ref=?"", (idn,))
<sqlite3.Cursor object at 0x10ad66b20>
>>> print ""Quantity "" + str(cur.fetchone()[0])
Quantity 20
",A,1
50340681,2018-05-15 00:15:48.370000+00:00,"What you're trying to do would work, except that Raise and raise are not the same thing, just like pRinT and print are not the same thing. Python is case-sensitive. So:
$ python
>>> Raise SystemExit(0)
          ^
SyntaxError: invalid syntax

But:
$ python
>>> raise SystemExit(0)
$

That being said, you almost never want to raise SystemExit directly. As the docs explain, this exception is there to be used by sys.exit under the covers, not for you to use directly.
So, as Arthur Dent explained, a better solution is:
$ python
>>> import sys
>>> sys.exit(0)
$
",A,2
12308378,2012-09-06 21:05:12.603000+00:00,"You are using a regular expression, and matching XML with such expressions get too complicated, too fast.
Use a HTML parser instead, Python has several to choose from:

ElementTree is part of the standard library
BeautifulSoup is a popular 3rd party library
lxml is a fast and feature-rich C-based library.

ElementTree example:
from xml.etree import ElementTree

tree = ElementTree.parse('filename.html')
for elem in tree.findall('tr'):
    print ElementTree.tostring(elem)
",A,6
28464531,2015-02-11 21:09:19.123000+00:00,"You were missing the : at the end of your if statement.
However, you are using invalid syntax for your test too; it'll lead to a runtime error (TypeError: unsupported operand type(s) for |: 'str' and 'str'). You want to create a tuple, or a set of strings to test against, not use |:
if line[5] in (""Tablespace Space Used (%)"", ""Tablespace Free Space (MB)""):

or
if line[5] in {""Tablespace Space Used (%)"", ""Tablespace Free Space (MB)""}:

The latter is technically more efficient, except if you are using Python 2 where the set isn't optimised into a constant the way the tuple would be in either version of the language. Using {...} to create a set requires Python 2.7 or newer.",A,3
477005,2009-01-25 00:59:54.317000+00:00,"Programs absolutely can be proven to be correct.  Lousy programs are hard to prove.  To do it even reasonably well, you have to evolve the program and proof hand-in-hand. 
You can't automate the proof because of the halting problem.  You can, however, manually prove the post-conditions and preconditions of any arbitrary statement, or sequence of statements.
You must read Dijsktra's A Discipline of Programming.
Then, you must read Gries' The Science of Programming.
Then you'll know how to prove programs correct.",A,30
27544953,2014-12-18 10:59:20.057000+00:00,"If you want to do this from the shell instead of Python, the xargs tool can almost do everything you want.
You give it a command with a fixed list of arguments, and feed it input with a bunch of filenames, and it'll run the command multiple times, using the same fixed list plus a different batch of filenames from its input. The --max-args option sets the size of the biggest group. If you want to run things in parallel, the --max-procs option lets you do that.
But that's not quite there, because it doesn't do the output redirection. But… do you really need 10 separate files instead of 1 big one? Because if 1 big one is OK, you can just redirect all of them to it:
ls | xargs --max-args=10 --max-procs=10 java -cp stanford-ner.jar\
    edu.stanford.nlp.process.PTBTokenizer >> output.txt
",A,1
11325504,2012-07-04 08:43:13.797000+00:00,"You should use the logging library, which has this capability built in. You simply add handlers to a logger to determine where to send the output.",A,3
4761301,2011-01-21 16:37:38.497000+00:00,"You use aggregation. When you're getting your list of posts, you tell Django you want to count the related Comments at the same time:
from django.db.models import Count
posts = Post.objects.all().annotate(comment_count=Count('comment'))

Now each post in the posts queryset has a comment_count attribute, which is the number of related comments.",A,2
13256908,2012-11-06 18:17:02.497000+00:00,"You could provide start_new_session analog for the C subprocess:
#!/usr/bin/env python
import os
import sys
import platform
from subprocess import Popen, PIPE

# set system/version dependent ""start_new_session"" analogs
kwargs = {}
if platform.system() == 'Windows':
    # from msdn [1]
    CREATE_NEW_PROCESS_GROUP = 0x00000200  # note: could get it from subprocess
    DETACHED_PROCESS = 0x00000008          # 0x8 | 0x200 == 0x208
    kwargs.update(creationflags=DETACHED_PROCESS | CREATE_NEW_PROCESS_GROUP)  
elif sys.version_info < (3, 2):  # assume posix
    kwargs.update(preexec_fn=os.setsid)
else:  # Python 3.2+ and Unix
    kwargs.update(start_new_session=True)

p = Popen([""C""], stdin=PIPE, stdout=PIPE, stderr=PIPE, **kwargs)
assert not p.poll()

[1]: Process Creation Flags for CreateProcess()",A,17
30311866,2015-05-18 20:02:46.413000+00:00,"A background program can't do raw_input(). That's the whole point of being a background program: you give up user input so the shell (or some other program) can run and handle it.
If you just want to run forever until signaled, just find a different way of doing that. Almost anything will work, except raw_input. For example, you can loop over time.sleep, or select.select on some fd, or anything else you can think of except for trying to read from an fd that you closed.",A,3
43888323,2017-05-10 09:19:45.990000+00:00,"You'll need to store the previous gap too; set it to 0 to start with. You don't want a list, you are dealing with individual numbers here:
goal = 15
count = 20
previous_gap = evaluate = 0

while count > 0:
    count -= 1
    gap = goal - random.choice(number)
    if previous_gap:
        evaluate = previous_gap - gap
    # remember the gap for the next step
    previous_gap = gap
",A,0
26774803,2014-11-06 08:26:12.780000+00:00,"You'd use a lambda to bind an anonymous function:
label.bind('< Button-1 >', lambda ev: self.add_character(ev, text=number))

Here, I'm assuming that label callbacks usually are passed in an event argument. You may need to adjust the number of arguments that the callback is passed. It this is a tkinter-based GUI, just the event is going to be passed in.
The number value is closed over, while when using a functools.partial() object, the number value would be bound as an argument to the callable.",A,1
31206927,2015-07-03 12:30:16.057000+00:00,"In general, you could use shutil.rmtree() to delete all files/directories in a folder:
#!/usr/bin/env python
import shutil
import tempfile

dirpath = tempfile.mkdtemp()
try:
    # use the temporary directory here
    ...
finally:
    shutil.rmtree(dirpath) # clean up

The above can be written simpler if it is all you need (create a temporary directory from scratch):
#!/usr/bin/env python3
import tempfile

with tempfile.TemporaryDirectory() as dir:
    print(dir.name) # use the temporary directory here
",A,2
29870756,2015-04-25 21:07:05.207000+00:00,"You are not returning anything from your function. You are printing, but that's not the same thing.
Use return to return the value:
def function():
     return ""hi""

print() writes to your terminal, the caller of the function is not given that output.
You can always use print() to print the return value:
print(function())
",A,2
12539097,2012-09-21 22:47:10.723000+00:00,"Use the .content attribute of the response:
resp = requests.get('http://site.com/immage.jpg')
requests.post('http://site2.com/, files=dict(file=resp.content))

I've posted the downloaded image as multipart-encoded POST with the field-name file. It depends on the exact application you are posting to what fieldname they expect.",A,3
45276016,2017-07-24 08:46:19.580000+00:00,"I think model inheritance is the wrong approach here. A model class represents a database table, and your objects all live in the same table, so they should be the same model.
You could just include the logic for each translation method inside if statements, but I understand why you'd want to abstract that out. So how about a separate service class to which you delegate all the operations? You could override __getattr__ to set up the service and delegate the methods.
class GoogleTranslation(object):
    def __init__(self, obj):
        self.obj = obj
    def get_step_translation(self, pk, lang, lesson):
        print('google, {}, {}, {}'.format(pk, lang, lesson))
        ... logic goes here...
    def create_step_translation(self, pk, lang, type):
        ...

class TranslationService(models.Model):
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    service_name = models.CharField(max_length=40)
    base_url = models.CharField(max_length=255)
    api_version = models.FloatField()

    services = {
         'google': GoogleTranslation,
         'bing': BingTranslation,
    }

    @property
    def service(self):
        if not hasattr(self, '_service'):
            # create a new Service object and pass it this model instance
            self._service = self.services[self.service_name](self)
        return self._service

    def __getattr__(self, name):
        if name == '_service':
            raise AttributeError   # the service hasn't been instantiated yet
        # delegate all unknown lookups to the service object
        return getattr(self.service, name)

Now you can get the Google instance via gs = TranslationService.objects.get(service_name='google') and do gs.get_step_translation(1, '2', '3') or whatever, and that method call will be delegated to the GoogleTranslation instance.",A,1
24977356,2014-07-27 02:29:36.483000+00:00,"You can combine datetime.date.today() with a datetime.time() object:
fivepm = datetime.datetime.combine(
    datetime.date.today(), 
    datetime.time(17, 0))

The datetime.datetime.combine() class method creates a new datetime instance from the date and time given.
Demo:
>>> import datetime
>>> datetime.datetime.combine(
...     datetime.date.today(), 
...     datetime.time(17, 0))
datetime.datetime(2014, 7, 27, 17, 0)
",A,3
3841626,2010-10-01 17:31:07.887000+00:00,"
Is this how Scrum should be handled? 

No.",A,7
12554102,2012-09-23 16:38:04.837000+00:00,"To convert the last 3 bytes in MAC to an integer and back:
mac = 'zz:zz:zz:d2:a4:00'
i = int(''.join(mac.split(':')[-3:]), 16) # integer
# -> 13804544

h = '%06x' % i
# -> 'd2a400'

mac = 'zz:zz:zz:%s:%s:%s' % (h[0:2], h[2:4], h[4:6])
# -> 'zz:zz:zz:d2:a4:00'

You should try a dictionary with MACs as keys first as @Lattyware mentioned.
To convert an integer to MAC represented as hexstring:
>>> h = '%012x' % 123
'00000000007b'
>>> ':'.join(h[i:i+2] for i in range(0, 12, 2))
'00:00:00:00:00:7b'
",A,1
20409970,2013-12-05 20:25:53.237000+00:00,"You are trying to treat a string as a callable. If you want to return a new instance of the class, then call the class:
class rectangle:
     def __init__(self, x, y, width, height):
         self.x = x
         self.y = y
         self.width = width
         self.height = height

     def point(self):
         self.x = 5
         self.y = 10
         self.width = 50
         self.height = 100
         return rectangle(self.x, self.y, self.width, self.height)

Note that this does require that your class has a __init__ method!
If you wanted to return a string you have to use string formatting instead:
return ""rectangle({}, {}, {}, {})"".format(self.x, self.y, self.width, self.height)
",A,1
16119374,2013-04-20 10:36:47.090000+00:00,"You are using the implied .find_all() method when you call the element directly, which returns a result set (a list-like object). Using limit does not change what is returned, only how many are returned.
If you want to get the first element of that set, use slicing:
original_content(""div"",""class1 class2"", limit=1)[0].text

or be explicit about it and use .find() instead:
original_content.find(""div"",""class1 class2"").text

To get the text of all matches you'll need to loop over the result set. A list comprehension would be easiest:
[el.text for el in original_content(""div"",""class1 class2"")]
",A,2
27930331,2015-01-13 19:49:26.210000+00:00,"The logging package file handlers will encode any Unicode object you send to it to UTF-8, unless you specified a different encoding.
Use io.open() to read the file as UTF-8 data again, you'll get unicode objects again, ideal for Jinja2:
import io

log = io.open('log.log', encoding='utf8')

You could also specify a different encoding for the TimedRotatingFileHandler but UTF-8 is an excellent default. Use the encoding keyword argument if you wanted to pick a different encoding:
fileLogger = logging.handlers.TimedRotatingFileHandler(
    filename='log.log', when='midnight', backupCount=30,
    encoding='Latin1')
",A,0
54732509,2019-02-17 10:55:07.640000+00:00,"response.raw_json is an encoded JSON document, a serialisation in a Unicode string, so text. It is not a Python data structure, you'd have to decode it from JSON into a Python data structure first, which will give you dictionaries and lists: 
import json

# ...

response_data = json.loads(response.raw_json)
print(response_data['person']['names']

The exception is telling you exactly that; response.raw_json is a unicode object, and unicode objects don't have a .person attribute.
Because response_data is a dictionary, you have to use subscriptions, not attribute access.
JSON is largely a subset of JavaScript, a language that has a common root with Python, so the structure may look like Python but very much is not. You can spot JSON more easily if you are aware of the differences:

Python representation output for strings prefers single quotes (double quotes are used when the value contains single quotes), but JSON only supports double quotes. 
JSON strings are really UTF-16 based and codepoints outside of the Basic Multilingual Plane are represented as UTF-16 surrogate pairs with \udhhh\dhhh notation, where Python Unicode strings would use the \U0010hhhh escape notation for these.
Python booleans use title-case (True, False), JSON notation uses lowercase (true, false).
Python has a None singleton object often used to indicate the absence of a value, JSON has a null object instead.
Using print() on a Python dictionary won't pretty-print the object with indentation either, you'd have to use the pprint library to produce similar output. 

There really is no need to manually decode the raw JSON document again here, as the pipl API library has already done this for you and parsed this out into container objects. There is no real advantage to loading the JSON document into Python dictionaries and lists here, when you could just use response.person.names.",A,5
24920440,2014-07-23 20:35:05.137000+00:00,"First, keep in mind that you're really not saving much space—and you could save a lot more space, and make your style more Pythonic, by just temporarily binding the object to a shorter name:
_ = obj
_.attr1 = 'foo'
_.attr2 = 'bar'

Compare that to VB-ish syntax:
vbwith obj:
    .attr1 = 'foo'
    .attr2 = 'bar'

You're really only typing and reading one extra character per line. Is that too much of a burden?
This wouldn't work in languages with C-style variables with value-copying semantics, because you'd just be setting attributes on a copy of obj. But in Python, a = b makes a into another name for the value that b names, it doesn't copy anything. (If you're coming from a C background, it may help to think of this as _ = &obj followed by a bunch of lines starting _->.)

If you really want to do this, the safest way is to write update_attr as a loop around setattr:
def update_attrs(obj, **attrs):
    for attr, value in attrs.items():
        setattr(obj, attr, value)

update_attr(obj,
    attr1='foo',
    attr2='bar'
)


The reason you want to use setattr rather than modifying the object's __dict__ or vars() is that not all attributes are stored in the object's __dict__. They may be stored in __slots__, or @property descriptors on the class, or get-set methods on a C extension object, or some custom thing provided by the metaclass. So, modifying the __dict__ may fail in some cases, or may appear to work but do nothing, or, worst of all, do the wrong thing in a way that's very hard to tell (like hiding a property instead of setting it). setattr is guaranteed to work whenever direct access would work, and to fail whenever direct access would fail.

While much of the Python community thinks actual VB-style with syntax is a bad thing (although I can't find the link to Guido's blog post about it), your more explicit syntax seems readable and not at all misleading to me, so if you really want to use this function, go ahead. But again, I don't think there's much need.",A,4
280721,2008-11-11 12:05:20.413000+00:00,"Since new browsers are invented at a steady pace, there's no definite value for ""any"" in ""any other browsers"".
You have to (1) pick some browsers you think you'd like to support, (2) check those specific browsers.
Look at a page like BrowserShots to see a list of browsers you might be interested in supporting.
Look at Free HTML Validators for XHTML compatibility.",A,0
19737340,2013-11-01 23:58:37.033000+00:00,"I know nothing about JES, but I'm going to guess that getAllPixels returns the pixels in the usual order: the first row, then the next row, then the next, etc.
If so:
pix = getAllPixels(picture)
third = len(pix) // 3
for p in pix[:third]:
    # do top-third stuff
for p in pix[third:third*2]:
    # do middle-third stuff
for p in pix[third*2:]:
    # do bottom-third stuff

This does assume that the picture s divisible perfectly into thirds. If it's not, you will need to know the picture's width so you can round to the nearest complete row (because otherwise the top third might actually be 250 complete rows and the first 47 pixels of the 251st, which won't look very good). I don't know what function JES has to get the width, but I'm sure it's simple.",A,0
38285486,2016-07-09 19:20:14.567000+00:00,"You appear to have Shift-JIS data inside code-page escapes; you can extract the marked-up bytes and decode those:
import re
from binascii import unhexlify

cp_escapes = re.compile(r""\'([0-9a-fA-F]{2})"")

def extract_cp_escapes(data):
    return unhexlify(''.join(marked_bytes.findall(data)))

then decode; Shift-JIS is codepage 932 on Windows:
>>> text = r""\'82\'b1\'82\'ea\'82\'cd\'93\'fa\'96\'7b\'8c\'ea\'81\'42""
>>> extract_cp_escapes(text)
'\x82\xb1\x82\xea\x82\xcd\x93\xfa\x96{\x8c\xea\x81B'
>>> print extract_marked_bytes(text).decode('cp932')
これは日本語。

You can decode that to another codec like UTF-8 if you need to. 
It may be possible that the exact code page used is also encoded in the RTF document, but I am out of time to research that.",A,0
15435194,2013-03-15 14:40:06.443000+00:00,"You want the part directly preceding the last dot; just split on the dots and take the one-but last part:
for line in data:
    if not '.' in line: continue
    elem = line.strip().split('.')[-2]

For your input, that gives:
>>> for line in data:
...     print line.strip().split('.')[-2]
... 
y
x
x
y
co
s
t
",A,2
26420372,2014-10-17 07:52:26.063000+00:00,"Your JSON input is invalid; you need to quote the first list key:
echo '{""group"" : {""list"" : [1,2,3]}, ""list"" : [""a"",""b"",""c""]}' | python -m json.tool
#                 ^^^^^^

The tool can handle any complexity of JSON, provided you give it valid JSON input. With the error corrected, Python outputs:
$ echo '{""group"" : {""list"" : [1,2,3]}, ""list"" : [""a"",""b"",""c""]}' | python -m json.tool
{
    ""group"": {
        ""list"": [
            1,
            2,
            3
        ]
    },
    ""list"": [
        ""a"",
        ""b"",
        ""c""
    ]
}
",A,1
18412319,2013-08-23 21:42:38.427000+00:00,"If you're just looking for something less verbose, you could do this:
num_points = np.atleast_2d(nparray).shape[1]

That will, of course, make a new temporary array just to take its shape, which is a little silly… but it'll be pretty cheap, because it's just a view of the same memory.
However, I think your explicit code is more readable, except that I might do it with a try:
try:
    num_points = nparray.shape[1]
except IndexError:
    num_points = 1

If you're doing this repeatedly, whatever you do, you should wrap it in a function. For example:
def num_points(arr, axis):
    try:
        return arr.shape[axis]
    except IndexError:
        return 1

Then all you have to write is:
for i in range(num_points(nparray, 1)):

And of course it means you can change things everywhere by just editing one place, e.g.,:
def num_points(arr, axis):
    return nparray[:,...,np.newaxis].shape[1]
",A,4
2191712,2010-02-03 12:10:54.530000+00:00,"Read up on List Comprehensions
[ (x,y) for x, y in a if x  == 1 ]

Also read up up generator functions and the yield statement.
def filter_value( someList, value ):
    for x, y in someList:
        if x == value :
            yield x,y

result= list( filter_value( a, 1 ) )
",A,16
22089885,2014-02-28 08:27:29.777000+00:00,"$.post() doesn't use JSON. It sends a regular POST request, with application/x-www-form-urlencoded as the content type. In a Flask route this means that the fields you posted are found in the request.form object. If your view returns HTML you can insert it into your HTML document.
Just pick a route name that reflects what you want to do:
@app.route('/turn_servo', methods=['POST'])
def turn_servo_ajax():
    direction = request.form['direction']

    # do something with the direction value; it is a string
    if direction == 'left':
        # ...
    else:
        # ...

    return '<div>Turned the servo 50 degrees to the {}</div>'.format(direction)

You then use that route name in your $.post:
$('#left_button').click(function(){
            $.post(""/turn_servo"", {direction:""left""}).done(function (reply) {
                $('#camerapos').empty().append(reply);
                alert(""left button clicked"");});

        });
",A,3
20318581,2013-12-01 23:28:03.140000+00:00,"close_fds=True on POSIX systems on Python 3. Use pass_fds to pass input pipe file descriptor:
#!/usr/bin/env python3
import os
import shlex
import sys
from subprocess import Popen


passphrase = 'passsssphrase'
file_to_encrypt = sys.argv[1] if len(sys.argv) > 1 else 'encrypt_me.py'

in_fd, out_fd = os.pipe()
cmd = 'gpg --passphrase-fd {fd} -c --armor -o -'.format(fd=in_fd)
with Popen(shlex.split(cmd) + [file_to_encrypt], pass_fds=[in_fd]):
    os.close(in_fd) # unused in the parent
    with open(out_fd, 'w', encoding='utf-8') as out_file:
        out_file.write(passphrase)

You could also pass the passphrase via stdin:
#!/usr/bin/env python3
import sys
from subprocess import PIPE, Popen


passphrase = 'passsssphrase'
file_to_encrypt = sys.argv[1] if len(sys.argv) > 1 else __file__

cmd = 'gpg --passphrase-fd 0 -c --armor -o -'.split()
with Popen(cmd + [file_to_encrypt], stdin=PIPE) as process:
    process.stdin.write(passphrase.encode('utf-8'))
",A,4
13594471,2012-11-27 22:38:45.413000+00:00,The documentation describes what you have to do to get CSRF working with Ajax.,A,2
19990269,2013-11-14 23:03:39.380000+00:00,"The problem is that you're not allowed to put colons into filenames on Windows. You're not actually using Windows… but you are using an SMB share, which means you're bound by Windows rules.
The fix is to not put colons into your filenames.
If you want to understand why this bizarre stuff is happening, read on.

The details on Windows filenames are described in Naming Files, Paths, and Namespaces at MSDN, but I'll summarize the relevant parts here.
The NT kernel underneath Windows has no problems with colons, but the Win32 layer on top of it can't handle them (and the quasi-POSIX layer in MSVCRT sits on top of Win32).
So, at the C level, if you call NT functions like NtSetInformationFile, it will save them just fine. If you call Win32 functions like MoveFileEx, they will normally give you an error, but if you use the special \\?\ syntax to say ""pass this name straight through to NT"", it will work. And if you call MSVCRT functions like rename, you will get an error. Older versions of Python called rename, which would just give you an error. Newer versions call MoveFileEx, and will try to wrap the name up in \\?\ syntax (because that also allows you to get around some other stupid limitations, like the excessively short MAX_PATH value).
So, what happens if you give a file a name that Win32 can't understand? Remember that on Windows, every file has two different names: the ""long name"" and the ""short name"". The short name is a DOS-style 8.3 filename. So whenever it can't display the long name, it displays the short name instead.
Where does the short name come from? If you don't create one explicitly, Windows will create one for you from the long name by using the first 6 characters, a tilde, and a number of letter. So, for example, the short name for ""Program Files"" is ""PROGRA~1"". But if Windows can't handle the long name, it will just make up a short name out of 6 random characters, a tilde, and a random character. So you get something like 2A443K~H.
The NTFS filesystem, being designed for Windows, expects to be used in Windows-y ways. So, if you're using an NTFS volume, even on a non-Windows system, the driver will emulate some of this functionality, giving you similar but not identical behavior.
And of course if you're talking to a share from a Windows system, or a share backed by an NTFS drive on a non-Windows system, again, some of the same things will apply.
Even if both your computer and the file server are non-Windows and the filesystem is not NTFS, if you're using SMB/CIFS for file sharing, SMB was also designed for Windows, and you will again get similar behavior.
At least you no longer have to worry about VMS, classic Mac, and other naming systems, just POSIX and Windows.",A,2
1241503,2009-08-06 21:13:39.530000+00:00,"The biggest question is accessibility. What about those people using screenreaders, for which Javascript doesn't work? What about those on mobile phones (non-smartphones), again with very limited or no Javascript functionality? What about those people who have simply disabled JS? Event these days, you simply can't assume that everyone can use JS.
I like the original idea, but perhaps this would be better done via a simple server-side wrapper, which calls out to your data source but which can be quickly and easily changed to point at a different one.",A,0
4849570,2011-01-31 10:02:37.227000+00:00,"On Python 2.4-2.7, pypy, jython:
>>> enc = sdata.encode('zlib').encode('base64')
>>> print enc
eJwdktkNgDAMQxfqR+5j/8V4QUJQUttx3Nrzl0+f+uunPPpm+Tf3Z/tKX1DM5bXP+wUFA777bCob
4HMRfUk14QwfDYPrrA5gcuQB49lQQxdZpdr+1oN2bEA3pW5Nf8NGOFsR19NBszyX7G2raQpkVUEB
dbTLuwSRlcDCYiW7GeBaRYJrgImrM3lmI/WsIxFXNd+aszXoRXuZ1PnZRdwKJeqYYYKq6y1++PXO
YdgM0TlZcymCOdKqR7HYmYPiRslDr2Sn6C0Wgw+a6MakM2VnBk6HwU6uWqDRz+p6wtKTCg2WsfdK
JwfJlHNaFT4+Q7PGfR9hyWK3p3464nhFwpOd7kdvjmz1jpWcxmbG/FJUXdMZgrpzs+jxC11twrBo
3TaNgvsf8oqIYwT4r9XkPnNC1XcP7qD5cW7UHSJZ3my5qba+ozncl5kz8gGEEYOQ
>>> print enc.decode('base64').decode('zlib')[:79]
2,3,5,1,13,7,17,11,89,1,233,29,61,47,1597,19,37,41,421,199,28657,23,3001,521,53
>>> sdata == enc.decode('base64').decode('zlib')
True
>>> F = [int(s) for s in sdata.split(',') if s.strip()]
>>> F[0], F[5]
(2, 7)
",A,1
13624313,2012-11-29 11:02:51.440000+00:00,"This problem is not shaped like a regular expression nail, so please put that hammer down.
The correct tool would be to parse the contents into a python structure, filtering out the items you don't want, then writing out the remaining entries again.
pyparsing would make the parsing job easy; the following is based on an existing example:
from pyparsing import *

LBRACE,RBRACE,SEMI,QUOTE = map(Suppress,'{};""')
ipAddress = Combine(Word(nums) + ('.' + Word(nums))*3)
hexint = Word(hexnums,exact=2)
macAddress = Combine(hexint + (':'+hexint)*5)
hdwType = Word(alphanums)

yyyymmdd = Combine((Word(nums,exact=4)|Word(nums,exact=2))+
                    ('/'+Word(nums,exact=2))*2)
hhmmss = Combine(Word(nums,exact=2)+(':'+Word(nums,exact=2))*2)
dateRef = oneOf(list(""0123456""))(""weekday"") + yyyymmdd(""date"") + \
                                                        hhmmss(""time"")

startsStmt = ""starts"" + dateRef + SEMI
endsStmt = ""ends"" + (dateRef | ""never"") + SEMI
tstpStmt = ""tstp"" + dateRef + SEMI
tsfpStmt = ""tsfp"" + dateRef + SEMI
hdwStmt = ""hardware"" + hdwType(""type"") + macAddress(""mac"") + SEMI
uidStmt = ""uid"" + QuotedString('""')(""uid"") + SEMI
bindingStmt = ""binding"" + Word(alphanums) + Word(alphanums) + SEMI

leaseStatement = startsStmt | endsStmt | tstpStmt | tsfpStmt | hdwStmt | \
                                                        uidStmt | bindingStmt
leaseDef = ""lease"" + ipAddress(""ipaddress"") + LBRACE + \
                            Dict(ZeroOrMore(Group(leaseStatement))) + RBRACE

input = open(DHCPLEASEFILE).read()
with open(OUTPUTFILE, 'w') as output:
    for lease, start, stop in leaseDef.scanString(input):
        if lease.hardware.mac != mac:
            output.write(input[start:stop])

The above code tersely defines the grammar of a dhcp.leases file, then uses scanString() to parse out each lease in the file. scanString() returns a sequence of matches, each consisting of a parse result and the start and end positions in the original string.
The parse result has a .hardware.mac attribute (you may want to catch AttributeError exceptions on that, in case no hardware statement was present in the input), making it easy to test for your MAC address to remove. If the MAC address doesn't match, we write the whole lease back to an output file, using the start and stop positions to get the original text for that lease (much easier than formatting the lease from the parsed information).",A,1
185484,2008-10-09 00:29:51.650000+00:00,"You can make the case that you have a virtually unlimited number of commands for your files.  Think of this class hierarchy.
Command
    CopyCommand
    RenameCommand
    MoveCommand
    DiffCommand
    CompressCommand

These aren't really strategies.  They're just ordinary classes with a simple ""execute"" method.  You provide the options and arguments through ordinary setters.  Then you execute the method.
This borrows from Ant's design pattern for Tasks that can be plugged in.",A,2
22530638,2014-03-20 10:51:48.207000+00:00,"With a values call, you don't get a normal queryset consisting of model instances, you get a ValuesQuerySet consisting of dictionaries. The only values in those dictionaries are those you specify in the call to values. So, there will not even be a key in the dict for b.
Don't do this unless you have a very good reason.",A,0
19126493,2013-10-01 21:43:00.887000+00:00,"You're not doing the same thing. Look at the docs:

name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.

You can't specify a list of positional arguments, just a single positional argument, or a list of option (flag) arguments. So, when you try to give it 'bar', '--foo', it tries to parse 'bar' as an option, and it can't, because it doesn't start with a -.
And, as the next section explains, when you give it a list of options, you're giving it a list of alternative names for the same flag, not a bunch of separate flags. So, you don't want to do what you're trying to do in the first place.",A,2
1999153,2010-01-04 12:08:10.917000+00:00,Read this:  http://www.cs.utexas.edu/users/EWD/transcriptions/EWD03xx/EWD316.2.html,A,0
22997102,2014-04-10 19:18:51.260000+00:00,"islower() and isupper() only return True if all letters in the string are lowercase or uppercase, respectively.
You'd have to test for individual characters; any() and a generator expression makes that relatively efficient:
>>> test = '8765iouy9987OIUY'
>>> any(c.islower() for c in test)
True
>>> any(c.isupper() for c in test)
True
",A,12
32458414,2015-09-08 12:54:27.357000+00:00,"You only have to change your super() call to use explicit arguments:
super(Palette, self).__init__(*args)   

and your code will work just fine in both Python 2 and Python 3. See Why is Python 3.x's super() magic? for background information on why the above is equivalent to super().__init__(*args). Also, do not pass in self again, or you'll create a circular reference as you include self in the contents of the list.
Note that it is more pythonic use property objects instead of explicit getters and setters:
class Palette(list):
    def __init__(self, name=None, description=None, colors=None, *args):
        super(Palette, self).__init__(args)   
        self.name = name
        self.description = description
        self.extend(colors)

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, name):
        self._name = name

    @name.deleter
    def name(self):
        self.name = None

    @property
    def description(self):
        return self._description

    @description.setter
    def description(self, description):
        self._description = description

    @description.deleter
    def description(self):
        self.description = None

then use
palette1.description = ""This is palette 1.""

I also took the liberty of reducing the amount of whitespace in the function definitions; putting each and every argument on a new line makes it very hard to get an overview of the class as function bodies are needlessly pushed down.
As these properties don't actually do anything other than wrap an attribute by the same name with an underscore, you may as well just leave them out altogether. Unlike Java, in Python you can freely switch between using attributes directly, and later on swapping attributes out for a property object; you are not tied into one or the other.
Note that in both Python 2 and Python 3, you cannot pass in positional arguments; the following doesn't work:
Palette('#F1E1BD', '#EEBA85', name='palette2')

because the first positional argument will be assigned to the name argument:
>>> Palette('#F1E1BD', '#EEBA85', name='palette2')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: __init__() got multiple values for argument 'name'

To support that use case, you need to not name the keyword arguments in the signature, and only use **kwargs, then retrieve your keyword arguments from that. Pass in any positional arguments as one argument so that list() takes any number of positional arguments as the contents for the new list:
class Palette(list):
    def __init__(self, *args, **kwargs):
        super(Palette, self).__init__(args)
        self.name = kwargs.pop('name', None)
        self.description = kwargs.pop('description', None)
        self.extend(kwargs.pop('colors', []))
        if kwargs:
            raise TypeError('{} does not take {} as argument(s)'.format(
                type(self).__name__, ', '.join(kwargs)))

Demo:
>>> class Palette(list):
...     def __init__(self, *args, **kwargs):
...         super(Palette, self).__init__(args)
...         self.name = kwargs.pop('name', None)
...         self.description = kwargs.pop('description', None)
...         self.extend(kwargs.pop('colors', []))
...         if kwargs:
...             raise TypeError('{} does not take {} as argument(s)'.format(
...                 type(self).__name__, ', '.join(kwargs)))
... 

>>> palette1 = Palette(
...     name   = ""palette 1"",
...     colors = [
...         ""#F1E1BD"",
...         ""#EEBA85"",
...         ""#E18D76"",
...         ""#9C837E"",
...         ""#5B7887""
...     ]
... )
>>> palette2 = Palette(""#F1E1BD"", ""#EEBA85"", ""#E18D76"", ""#9C837E"", ""#5B7887"",
...                    name=""palette 2"")
>>> palette1
['#F1E1BD', '#EEBA85', '#E18D76', '#9C837E', '#5B7887']
>>> palette2
['#F1E1BD', '#EEBA85', '#E18D76', '#9C837E', '#5B7887']
>>> palette1.name
'palette 1'
>>> palette2.name
'palette 2'
>>> palette1.description = 'This is palette 1.'
>>> palette2.description = 'This is palette 2.'
>>> Palette(foo='bar', spam='eggs')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<stdin>"", line 9, in __init__
TypeError: Palette does not take foo, spam as argument(s)
",A,2
400753,2008-12-30 16:31:05.307000+00:00,"See Function Definitions in the Language Reference.

If the form *identifier is
  present, it is initialized to a tuple
  receiving any excess positional
  parameters, defaulting to the empty
  tuple. If the form **identifier is
  present, it is initialized to a new
  dictionary receiving any excess
  keyword arguments, defaulting to a new
  empty dictionary.

Also, see Function Calls.
Assuming that one knows what positional and keyword arguments are, here are some examples:
Example 1:
# Excess keyword argument (python 2) example:
def foo(a, b, c, **args):
    print ""a = %s"" % (a,)
    print ""b = %s"" % (b,)
    print ""c = %s"" % (c,)
    print args

foo(a=""testa"", d=""excess"", c=""testc"", b=""testb"", k=""another_excess"")

As you can see in the above example, we only have parameters a, b, c in the signature of the foo function. Since d and k are not present, they are put into the args dictionary. The output of the program is:
a = testa
b = testb
c = testc
{'k': 'another_excess', 'd': 'excess'}

Example 2:
# Excess positional argument (python 2) example:
def foo(a, b, c, *args):
    print ""a = %s"" % (a,)
    print ""b = %s"" % (b,)
    print ""c = %s"" % (c,)
    print args

foo(""testa"", ""testb"", ""testc"", ""excess"", ""another_excess"")

Here, since we're testing positional arguments, the excess ones have to be on the end, and *args packs them into a tuple, so the output of this program is:
a = testa
b = testb
c = testc
('excess', 'another_excess')

You can also unpack a dictionary or a tuple into arguments of a function:
def foo(a,b,c,**args):
    print ""a=%s"" % (a,)
    print ""b=%s"" % (b,)
    print ""c=%s"" % (c,)
    print ""args=%s"" % (args,)

argdict = dict(a=""testa"", b=""testb"", c=""testc"", excessarg=""string"")
foo(**argdict)

Prints:
a=testa
b=testb
c=testc
args={'excessarg': 'string'}

And
def foo(a,b,c,*args):
    print ""a=%s"" % (a,)
    print ""b=%s"" % (b,)
    print ""c=%s"" % (c,)
    print ""args=%s"" % (args,)

argtuple = (""testa"",""testb"",""testc"",""excess"")
foo(*argtuple)

Prints:
a=testa
b=testb
c=testc
args=('excess',)
",A,228
20275345,2013-11-28 22:00:13.303000+00:00,"You need to split your line differently, then assign the parts without index and slice assignments:
number, salary, position, rest = items.split(None, 3)
oname, first = rest.rsplit(None, 1)
data.append((first, oname, number, position, salary))

This retains the spaces in the oname field. The first call splits only 3 times, giving you the first three fields, and a remainder. The remainder in rest is then split from the right, once, to give you oname and first.
Demo:
>>> items = '15674 24000 Manager Gregory the 1st John\n'
>>> number, salary, position, rest = items.split(None, 3)
>>> oname, first = rest.rsplit(None, 1)
>>> first, oname, number, position, salary
('John', 'Gregory the 1st', '15674', 'Manager', '24000')
",A,5
20714761,2013-12-21 02:23:33.547000+00:00,"To parse Content-Type http header, you could use cgi.parse_header function:
import cgi
import urllib2

r = urllib2.urlopen('http://lenta.ru')
_, params = cgi.parse_header(r.headers.get('Content-Type', ''))
encoding = params.get('charset', 'utf-8')
unicode_text = r.read().decode(encoding)

Another way to get the charset:
>>> import urllib2
>>> r = urllib2.urlopen('http://lenta.ru')
>>> r.headers.getparam('charset')
'utf-8'

Or in Python 3:
>>> import urllib.request
>>> r = urllib.request.urlopen('http://lenta.ru')
>>> r.headers.get_content_charset()
'utf-8'

Character encoding can also be specified inside html document e.g., <meta charset=""utf-8"">.",A,9
10659265,2012-05-18 20:26:47.903000+00:00,"This is entirely normal, if somewhat irritating. Your setup.py file pulls these in with the setup_requires and paster_plugins sections:
setup(...
    setup_requires=[""PasteScript""],
    paster_plugins=[""ZopeSkel""],
    ...
)

The ZopeSkel paster recipes use these eggs to let you add snippets to your generated egg at a later date. Remove the above two entries and the eggs if you do not need this feature.",A,5
27932344,2015-01-13 22:04:00.937000+00:00,"Yes, simply tell enumerate() where to start; it takes a second argument for that:
for nr, month in enumerate(range(1, 13), 1):

From the enumerate() documentation:

enumerate(sequence, start=0)
  [...] The next() method of the iterator returned by enumerate() returns a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over sequence

Bold emphasis mine.
Not that you really need it here, you are producing a range of numbers already, you can simply re-use month in this case as nr and month will always be equal now.",A,1
23892183,2014-05-27 14:31:05.290000+00:00,"In a normal Flask request-response cycle, the request context is active when the Flask-Restful preprocessors and postprocessors are being run.
As such, using:
from flask import request, abort

def check_auth(instance_id=None, **kw):
    current_user = None
    auth = request.headers.get('Authorization', '').lower()
    try:
        type_, apikey = auth.split(None, 1)
        if type_ != 'your_api_scheme':
            # invalid Authorization scheme
            ProcessingException(message='Not Authorized',
                                status_code=401)
        current_user = user_for_apikey[apikey]       
    except (ValueError, KeyError):
        # split failures or API key not valid
        ProcessingException(message='Not Authorized',
                            status_code=401)

should Just Work.",A,7
26968289,2014-11-17 08:19:37.163000+00:00,"mpg is a method, and like all methods in Python, you need to actually call it to get its result:
p.mpg()
",A,0
8959594,2012-01-22 07:04:16.267000+00:00,"In general regular expression can't parse html. You could use an html parser instead:
from BeautifulSoup import BeautifulSoup # pip install BeautifulSoup

html = """"""...<b>Breakfast</b><hr>...""""""

soup = BeautifulSoup(html)
print soup(text=True) # get all text
# -> [u'...', u'Breakfast', u'...']
print [b.text for b in soup('b')] # get all text for <b> tags
# -> [u'Breakfast']
",A,4
3665187,2010-09-08 06:36:34.977000+00:00,"Setting a dynamic path for the upload_to attribute is absolutely not a good reason for wanting to muck around with model field declaration. 
This is something that Django handles already - if you set upload_to to a callable, you can return the correct value dependent on the model instance. See the documentation.",A,1
52839363,2018-10-16 15:46:53.963000+00:00,"values() explicitly returns a list of dictionaries, so you would need to use dictionary syntax: val['device_id'].
But in your case there is no reason to do that. Skip the values call altogether, and you will get an instance of defaultParams on which you can use normal attribute lookups:
for val in getDefaults:
    print(val.device_id)
",A,1
32681093,2015-09-20 15:33:04.403000+00:00,"Yes, the second line is the same because neither snippet properly closes files.
You would have to call the fr.close() method to close the file object, without the () part the method is only referenced, not executed. So in both examples the 'asd.txt' file object is still open when the second open() call is executed.
In CPython (the default Python implementation downloadable from Python.org) file objects are automatically closed when no longer referenced, so as soon as the second open() call returns and is assigned, the first open file is closed as the reference to the object is removed.
However, you should really use the file object as a context manager instead, using the with statement:
with open(""asd.txt"", ""r"") as fr:
    text = fr.read()

with open(""asd2.txt"", ""r"") as fr:
    text = fr.read()

and more likely, use different variables names for text as one now replaces the other. File objects in a with statement are automatically closed when the block ends or an exception occurs in that block.",A,4
50511414,2018-05-24 14:05:50.157000+00:00,"Your mistake is to create Javascript code in a loop. You don't need to do this.
What you want to do is think of the data sent to the browser as independent. Make it work first without Flask and Jinja2. Create a static page that works and does what you want.
The following code would work with static data:


function changeTemplate(){
    var template = document.getElementById('template_select').value;
    document.getElementById('client_name').innerHTML = template;
}
<label for=""template_select"">Select Template</label>
<select class=""form-control"" name=""template_select"" id=""template_select"" onchange=""changeTemplate()"">
<option value=""""></option>

<option value=""Client 1"">Client 1</option>
<option value=""Client 2"">Client 2</option>
<option value=""Client 3"">Client 3</option>
</select>

<div id=""client_name""><i>No client set</i></div>



That's HTML for a select box, a separate <div> element, and Javascript code to copy the selected option value into the <div>. Note that the Javascript code doesn't know anything about what data is involved; no client names are stored in the code. All that the small function does is to copy the value from the currently selected option, to somewhere else.
Once that works on its own you can start thinking about how you are going to insert the values from your application. In the above code, all that needs replacing is the dropdown options, because the Javascript code can then access everything it needs from the <select> element value.
So the Javascript code doesn't need to be generated at all, you only generate the <option> elements, as you already did in your question.
You rarely need to generate dynamic Javascript code, and it would be much better for your app if you don't. Static Javascript code can be served by a CDN and cached in the browser, removing the need for your app to keep serving that again and again for all clients. Try to minimise that whenever you can.
Instead, generate just the data that the code needs to operate on. 

You can put that information in HTML tags. In the above example, your data is put in the repeated <option> tags. Or
you could add data attributes to your HTML, which are accessible both to Javascript code and to CSS. Or
use AJAX to load data asynchronously; e.g. when you pick an option in the <select> box, use Javascript and AJAX to call a separate endpoint on your Flask server that serves more data as JSON or ready-made HTML, then have the Javascript code update your webpage based on that. Or
generate JSON data and put it directly into your HTML page. Just a <script>some_variable_name = {{datastructure|tojson|safe}};<script> section is enough; then access that some_variable_name from your static Javascript code to do interesting things on the page. JSON is a (almost entirely a) subset of Javascript, and the way the tojson filter works is guaranteed to produce a Javascript-compatible data structure for you. The browser will load it just like any other embedded Javascript code.
",A,1
22169632,2014-03-04 10:41:48.767000+00:00,"Yes, that is setting the attribute on the class. But no, that would not necessarily make it available between requests, although it might.
Your question shows a misunderstanding of how Django requests work. Django is not necessarily served using multiple threads: in fact, in most server configuration, it is hosted by multiple independent processes that are managed by the server. Again, depending on the configuration, each of those processes may or may not have multiple threads. But whether or not threads are involved, processes are started and killed by the server all the time.
If you set an attribute on a class or module in Django during one request, any subsequent requests served by that same process will see that attribute. But there's no way to guarantee which process your next request will be served by. And there's certainly no way to know if the same user will be accessing the next request from that same process. 
Setting things at class or module level can be the source of some very nasty thread-safety bugs. My advice is generally not to do it. If you need to keep things across request, store it in the database, the cache, or (especially if it's specific to a particular user) in the session.",A,3
54241919,2019-01-17 18:13:45.623000+00:00,"You are appending your data to an existing BytesIO object. Additional writes to such an object do not replace existing data; writing adds more data to the end of the file:
>>> from io import BytesIO
>>> out = BytesIO()
>>> out.write(b'123')
3
>>> out.getvalue()
b'123'
>>> out.write(b'456')
3
>>> out.getvalue()
b'123456'

You now have binary data for two images in a single file, but compliant decoders will ignore trailing data in an image file.
Use a new, empty BytesIO() object:
resized = BytesIO()
img.save(resized, format=""PNG"")
contents = resized.getvalue()

You could also seek to the start of the file and truncate:
output.seek(0)
output.truncate()

before writing to it again, but just creating a new in-memory file object is clearer and less error-prone.",A,1
20584158,2013-12-14 13:55:41.453000+00:00,"You are using type dispatching. As of Python 3.4, you can use a dedicated decorator to do this. This decorator is available as a backport package as well.
try:
    from functools import singledispatch
except ImportError:
    from singledispatch import singledispatch

@singledispatch
def build_struct(decl):
    # Don't know how to do the generic version
    return

@build_struct.register(TypeDecl)
def build_struct_typedecl(decl):
    return build_struct(decl.type)

@build_struct.register(IdentifierType)
def build_struct_identifier(decl)
    return "" "".join(decl.names)

@build_struct.register(ID)
def build_struct_id(decl)
    return ['ID', decl.name]

# etc.

You can move the dispatch closer to the classes themselves; import the build_struct generic function and register more dispatch methods as needed.",A,0
6859672,2011-07-28 13:24:27.623000+00:00,"
Given the expected volume of data, how normalized should the data model be? 

Perfectly.
Your volumes are small.  You're doing 10,000 to 355,000 transactions each day?  Let's assume your peak usage is a 12-hour window.  That's .23/sec up to 8/sec.  Until you get to rates like 30/sec (over 1 million rows on a 12-hour period), you've got get little to worry about.  

Would it make sense to store this data in a flat table? 

No.

What kind of sharding approach would be appropriate? 

Doesn't matter.  Pick any one that makes you happy.
You'll need to test these empirically.  Build a realistic volume of fake data.  Write some benchmark transactions.  Run under load to benchmarking sharding alternatives.

Would MySQL's NDB clustering solution fit this use case?

It's doubtful.  You can often create a large-enough single server to handle this load.  
This doesn't sound anything like any of the requirements of your problem.

MySQL Cluster is designed not to have any single point of failure. In
  a shared-nothing system, each component is expected to have its own
  memory and disk, and the use of shared storage mechanisms such as
  network shares, network file systems, and SANs is not recommended or
  supported.
",A,1
241370,2008-10-27 21:00:36.613000+00:00,"""I'd like to build a tool that can install services on a pool of new computers from one central admin workstation.""
So would everyone else who wants to build a bot-net to send spam email all day long.  Sure, you may have good reasons for this.  But there are security considerations that make this undesirable and rather difficult.
Start with Microsoft Deployment Services.
Microsoft has extensive Desktop Deployment information.  ",A,2
5210571,2011-03-06 12:49:14.703000+00:00,"I'd term this a resource, which is data that your application relies on, but not the data your application manages. Images, CSS, and templates are similar resources, and you keep them all version controlled.
In this case, you could split out your data into a separate package. In python distribution terms, use a separate egg that your application depends on; package deployment tools such as pip and buildout will pull in the dependency automatically. That way you can version it independently, you can have other tools depend on it, etc.
Last, choose a format that can be managed effectively by a source control system. That means a textual format. You can initiate parse that format on start-up, but by keeping it textual you can manage it properly through changes to it. This could be a SQL dump (CREATE TABLE and INSERT statements) to be loaded into a sqlite database on start-up, or some other python-based structure. You can also choose to load data on demand, caching it in-memory as needed.
Have a look at the pytz timezone database for a great example of how another resource project manages such structures.",A,1
30594814,2015-06-02 11:28:00.413000+00:00,"Just produce random combinations, tracking what you've seen before:
def random_combinations(matrix, size):
    seen = set()
    n = len(matrix)
    while True:
        new_sample = tuple(sorted(random.sample(xrange(n), size)))
        if new_sample not in seen:
            seen.add(new_sample)
            yield tuple(matrix[i] for i in new_sample)

Iterating through all possible combinations to sample is not efficient, you still end up testing all 10^14 combinations.
The above generator picks a random combination each time you iterate; if you need a certain number, use a loop or itertools.islice(); picking 10 random combinations would be:
combinations_sample = list(islice(random_combinations(matrix, 50), 10))

You may have misunderstood what the function you found does; it does much the same as my function above but produces just the one random combination, without tracking what was produced before. You were supposed to use it on matrix, not on all combinations of matrix.",A,3
14905218,2013-02-15 23:45:59.377000+00:00,"First, instead of creating 26 separate variables named a, b, etc., just create a dict:
values = {'a': 0.01,
          'b': 0.11', 
          #...
}

Now, you can just do this:
for letter in DIRECTORY[0]:
    DIRECTORYT += [values[letter]]

Or, alternatively:
for letter in values:
    if letter in DIRECTORY[0]:
        DIRECTORYT += [values[letter]]

The difference between the two lies in how they handle duplicates. And I'm not sure which one you want (or, if you never have any dups, so it doesn't matter). Try executing both with different sample data until you get the idea.
As a side note, it's generally better to do:
DIRECTORYT.append(values[letter])

than:
DIRECTORYT += [values[letter]]

In other words, don't create a list if you don't need one.
But in this case, you may be able to replace the whole loop with a list comprehension or generator expression:
DIRECTORYT += [values[letter] for letter in DIRECTORY[0]]
DIRECTORYT += [values[letter] for letter in values if letter in DIRECTORY[0]]

or:
DIRECTORYT.extend(values[letter] for letter in DIRECTORY[0])
DIRECTORYT.extend(values[letter] for letter in values if letter in DIRECTORY[0])

The advantage of the extend/generator expression versions is, again, that they don't build temporary lists.",A,2
11390849,2012-07-09 07:42:47.090000+00:00,"You'll need to poll the backend from the browser periodically to see if there are any updates. This does assume that the long-running script is running asynchronously. The periodic update is akin to a log file tail, once in a while you check if new information has been added to the log.
There are several existing jQuery plugins that'll help you build this; PeriodicalUpdater for jQuery is a nice one, in that it'll adjust the poll interval if the server response doesn't change in a while.
Basically, with such a plugin, you'll need a Django view that returns the current status, the log file output of your process so to speak, and have PeriodicalUpdater poll that view. In the callback function for PeriodicalUpdater you'll need to add a check that the process is complete, of course; perhaps your server view could end with an easy to detect ""Process complete"" line at the end of the ""log"", or return a response that only consists of the final status.",A,1
3030427,2010-06-12 23:06:47.363000+00:00,"Take the hint.  Don't do this.
In the olden days (30 years ago -- seriously) we ""updated"" files with complex add/change/delete logic.
Nowadays, life is simpler if you write programs that 

Read the entire file into memory.  
Work on the objects in memory.
Write the objects to a file periodically and when the user wants to save.

It's faster and simpler.  Use pickle to dump your objects to a file.  Don't mess with ""records"" or any attempt to change a file ""in place"".
If you really think you need SQL capabilities (Insert, Update, Delete) then use SQLite.  It's more reliable than what you're attempting to do.  ",A,3
21185012,2014-01-17 11:37:37.267000+00:00,"It is probably easiest to just filter out empty lists after building your dictionary:
propertyDict = {k: v for k, v in propertyDict.iteritems() if v}

The alternative would be to build your dictionary in your loop, and only add a new key-value pair when adding to the list; a collections.defaultdict() instance would make that much easier
from collections import defaultdict

propertyDict = defaultdict(list)
properties = ('required', 'as_banner', 'min', 'chart_layout', 'client_name',
              'filter_text', 'sort_order', 'chart_type', 'chart_color',
              'position', 'order')

for i, subaxis in enumerate(table.Axes['Side'].SubAxes):
    nField = shatter_sae(subaxis.Specification)['varName']
    field = mdd.Fields[nField]
    for prop in properties:
        value = field.Properties.Item[prop]
        if value:
            propertyDict[prop].append(value)

# optional, turn `defaultdict` back into a `dict` object. This step is really
# not needed unless you need the `defaultdict.__missing__` behaviour to cease for
# later code.
# propertyDict = dict(propertyDict)

This code is vastly simpler by using a list of property names to test for as well",A,4
10606874,2012-05-15 18:41:51.237000+00:00,"Why would it work? You call the function, and it recurses until it finds the min. Then what does it do? It exits back out of all the recursions in turn, until it gets back to the top level. At that point, what's the value of min? Exactly what it was before you started recursing, because you never captured the return value from the recursive function.
Don't forget that each call of the function has its own local variables, therefore its own local value of min.",A,2
768881,2009-04-20 15:52:19.833000+00:00,"To do this, you must do the following.  If you don't know what ""options"" and ""arguments"" are, read the optparse background.
Each ""Command"" or ""Request"" is actually an instance of a model.  Define your Request model with all  of the parameters someone might provide.

For simple options, you must provide a field with a specific list of CHOICES.  For options that are ""on"" or ""off"" (-x in the command-line) you should provide a CHOICE list with two human-understandable values (""Do X"" and ""Do not do X"".)
For options with a value, you must provide a field that takes the option's value.  You must write a Form with the validation for this field.  We'll return to option value validation in a bit.
For arguments, you have a second Model (with an FK to the first).  This may be as simple as a single FilePath field, or may be more complex.  Again, you may have to provide a Form to validate instances of this Model, also.

Option validation varies by what kind of option it is.  You must narrow the acceptable values to be narrowest possible set of characters and write a parser that is absolutely sure of passing only valid characters.
Your options will fall into the same categories as the option types in optparse -- string, int, long, choice, float and complex.  Note that int, long, float and complex have validation rules already defined by Django's Models and Forms.  Choice is a special kind of string, already supported by Django's Models and Forms.
What's left are ""strings"".  Define the allowed strings.  Write a regex for those strings.  Validate using the regex.  Most of the time, you can never accept quotes ("", ' or `) in any form.  
Final step.  Your Model has a method which emits the command as a sequence of strings all ready for subprocess.Popen.

Edit
This is the backbone of our app.  It's so common, we have a single Model with numerous Forms, each for a special batch command that gets run.  The Model is pretty generic.  The Forms are pretty specific ways to build the Model object.  That's the way Django is designed to work, and it helps to fit with Django's well-thought-out design patterns.
Any field that is ""available as open text fields"" is a mistake.  Each field that's ""open"" must have a regex to specify what is permitted.  If you can't formalize a regex, you have to rethink what you're doing.
A field that cannot be constrained with a regex absolutely cannot be a command-line parameter.  Period.  It must be stored to a file to database column before being used.

Edit
Like this.
class MySubprocessCommandClass( models.Model ):
    myOption_1 = models.CharField( choice = OPTION_1_CHOICES, max_length=2 )
    myOption_2 = models.CharField( max_length=20 )
    etc.
    def theCommand( self ):
        return [ ""theCommand"", ""-p"", self.myOption_1, ""-r"", self.myOption_2, etc. ]

Your form is a ModelForm for this Model.
You don't have to save() the instances of the model.  We save them so that we can create a log of precisely what was run.",A,4
4835117,2011-01-29 04:15:28.973000+00:00,"
When I design my database layout I dont know exactly what all queries will be used until the application is done being built

Correct.  Don't build indexes until you know all the queries.  It's okay to add, change, alter and remove indexes.  Indeed, good designers change the indexes as the use of the software changes.

Would I create a multiple column index on field1,field3,field3 and field4?

Rarely.

If I had the multiple column index from the first query will that same index work for the second query since field1 is on the farthest left of the index?

No.

And another question I had was is there a specific order mysql looks for indexes?

No.

So for multiple column or a covering index do I add indexes in order of the where clause? 

No

Then anything in group clause then anything in order clause? 

No.

Or does mysql automatically do this?

More-or-less.
Here's the rule.

Design the database.
Write the queries.
Find the most common queries.  20% of your queries do 80% of the work.  Focus on the few, slow queries that need indexes.
Explain the query execution plans for only the most common queries.  There's an EXPLAIN statement for this.
Measure the performance of those queries with realistic loads of data.  You have to build fake data for this.  Some queries will be slow.  Indexes may help.  Some queries will not be slow.  
Now comes the hard part.  Try different indexes until (a) the explain plan looks optimal and (b) the measured query performance meets your expectations.

You cannot get all queries to be fast.
You do not build indexes for all queries.
Focus on the 20% of the queries that cost 80% of the time.",A,2
23240096,2014-04-23 09:24:08.210000+00:00,"You have imported a module named help.
There are two work-arounds:

Access the built-in function directly, bypassing the global module name:
__builtins__.help(max)

Remove the help global by deleting it again:
del help

",A,2
2787759,2010-05-07 10:14:21.877000+00:00,What happens if you use modelformset_factory instead of formset_factory? Does that help?,A,1
18001654,2013-08-01 18:21:30.983000+00:00,"Your question is pretty confusing, and confused. But I think whatever your actual problem is, your first step should be to create a dict. When your problem is ""I need to look up a key and get the corresponding value, or get some default value if it's not present"", the answer is usually dict.get.
For example, you could create a dict mapping each member of copy_from to its succeeding member:
copy_from = ['2.02,1.91', '1.9,2.06', '1.86,1.98']
dict_from = dict(zip(copy_from, copy_from[1:]))

Now, to get the next value for any value in copy_from, or the key itself if not present:
value = dict_from.get(value, value)

In particular:
>>> v = '1.9,2.06'
>>> dict_from.get(v, v)
'1.86,1.98'
>>> v = '1.86,1.98'
>>> dict_from.get(v, v)
'1.86,1.98'

I think that's at least on the road to what you want?",A,2
